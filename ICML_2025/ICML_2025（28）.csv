type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,AuPair: Golden Example Pairs for Code Repair,https://ICML.cc//virtual/2025/poster/45813,"Aditi Mavalankar, Hassan Mansoor, Zita Marinho, Mariia Samsikova, Tom Schaul","Scaling up inference-time compute has proven to be a valuable strategy in improving the performance of Large Language Models (LLMs) without fine-tuning. An important task that can benefit from additional inference-time compute is self-repair; given an initial flawed response or guess, the LLM corrects its own mistake and produces an improved response or fix. We leverage the in-context learning ability of LLMs to perform self-repair in the coding domain. The key contribution of our paper is an approach that synthesises and selects an ordered set of golden example pairs, or AuPairs, of these initial guesses and subsequent fixes for the corresponding problems. Each such AuPair is provided as a single in-context example at inference time to generate a repaired solution. For an inference-time compute budget of $N$ LLM calls per problem, $N$ AuPairs are used to generate $N$ repaired solutions, out of which the highest-scoring solution is the final answer. The underlying intuition is that if the LLM is given a different example of fixing an incorrect guess each time, it can subsequently generate a diverse set of repaired solutions. Our algorithm selects these AuPairs in a manner that maximises complementarity and usefulness. We demonstrate the results of our algorithm on 5 LLMs across 7 competitive programming datasets for the code repair task. Our algorithm yields a significant boost in performance compared to best-of-$N$ and self-repair, and also exhibits strong generalisation across datasets and models. Moreover, our approach shows stronger scaling with inference-time compute budget compared to baselines.","Large Language Models (LLMs) have become increasingly better at writing code over the past few years, but they still often make mistakes. Our aim was to help make LLMs better at fixing their own coding errors, not just by passing in the first attempt and asking it to do better, but by providing a diverse range of examples of corrections, just like a human might think of multiple ways to fix an issue.To address this, we developed an algorithm called AuPair, which curates an ordered set of golden example pairs. An example pair contains an initial flawed piece of code along with an improved version. Our algorithm yields golden example pairs that are complementary and diverse, thus boosting performance. During inference, a different golden example pair is given in each LLM call to fix a problem's solution, exposing the LLM to different types of fixes, thereby guiding it towards a better solution.Our work shows that selecting good in-context examples, or more generally, carefully curated in-context data can serve as a powerful inference technique that can outperform inference-time algorithms like best-of-$N$ or self-repair. While we have focused on code repair, AuPair is a general approach that is compatible with a large range of domains and tasks."
Poster,AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses,https://ICML.cc//virtual/2025/poster/45896,"Nicholas Carlini, Edoardo Debenedetti, Javier Rando, Milad Nasr, Florian Tramer","We introduce AutoAdvExBench, a benchmark to evaluate if large language models (LLMs) can autonomously exploit defenses to adversarial examples. Unlike existing security benchmarks that often serve as proxies for real-world tasks, AutoAdvExBench directly measures LLMs' success on tasks regularly performed by machine learning security experts. This approach offers a significant advantage: if a LLM could solve the challenges presented in AutoAdvExBench, it would immediately present practical utility for adversarial machine learning researchers. While our strongest ensemble of agents can break 87% of CTF-like (""homework exercise"") adversarial example defenses, they break just 37% of real-world defenses, indicating a large gap between difficulty in attacking ""real"" code, and CTF-like code. Moreover, LLMs that are good at CTFs are not always good at real-world defenses; for example, Claude Sonnet 3.5 has a nearly identical attack success rate to Opus 4 on the CTF-like defenses (75% vs 79%), but the on the real-world defenses Sonnet 3.5 breaks just 13% of defenses compared to Opus 4's 30%. We make this benchmark available at https://github.com/ethz-spylab/AutoAdvExBench.","We introduce AutoAdvExBench, a benchmark designed to evaluate whether large language models (LLMs) can autonomously identify and exploit vulnerabilities in adversarial defense systems.Adversarial defenses are defenses that protect machine learning systems from specially crafted inputs meant to fool them. For example, someone might subtly modify an image so that an image classifier misidentifies it. Our benchmark tests whether LLMs can find ways around these protective measures, and automate the work that human security researchers do manually.Our findings reveal an interesting pattern. When we tested our strongest combination of LLM agents on CTF-like challenges (""Capture The Flag"" challenges are practice exercises similar to homework problems that security professionals use for training), the LLMs successfully broke 87% of the defenses. However, when we moved to real-world defense systems that researchers actually wrote for research papers, the best LLMs' success rate dropped significantly to just 37%. This gap highlights how much harder it is to attack real systems compared to educational exercises.We also discovered that performance doesn't transfer predictably between these two settings. For example, we tested two advance language models, Claude Sonnet 3.5 and Opus 4 (where Opus 4 is the more advanced model). On the CTF-like challenges, both performed similarly—Sonnet 3.5 broke 75% of defenses while Opus 4 broke 79%. But on real-world defenses, the difference was dramatic: Sonnet 3.5 succeeded only 13% of the time, while Opus 4 managed 30%.The benchmark is publicly available at https://github.com/ethz-spylab/AutoAdvExBench for other researchers to use and build upon."
Poster,AutoAL: Automated Active Learning with Differentiable Query Strategy Search,https://ICML.cc//virtual/2025/poster/45158,"Yifeng Wang, Xueying Zhan, Siyu Huang","As deep learning continues to evolve, the need for data efficiency becomes increasingly important. Considering labeling large datasets is both time-consuming and expensive, active learning (AL) provides a promising solution to this challenge by iteratively selecting the most informative subsets of examples to train deep neural networks, thereby reducing the labeling cost. However, the effectiveness of different AL algorithms can vary significantly across data scenarios, and determining which AL algorithm best fits a given task remains a challenging problem. This work presents the first differentiable AL strategy search method, named AutoAL, which is designed on top of existing AL sampling strategies. AutoAL consists of two neural nets, named SearchNet and FitNet, which are optimized concurrently under a differentiable bi-level optimization framework. For any given task, SearchNet and FitNet are iteratively co-optimized using the labeled data, learning how well a set of candidate AL algorithms perform on that task. With the optimal AL strategies identified, SearchNet selects a small subset from the unlabeled pool for querying their annotations, enabling efficient training of the task model. Experimental results demonstrate that AutoAL consistently achieves superior accuracy compared to all candidate AL algorithms and other selective AL approaches, showcasing its potential for adapting and integrating multiple existing AL methods across diverse tasks and domains.","Training deep learning models often requires labeling large amounts of data by hand, this is a both time-consuming and cost-expensive process. Active learning offers a solution by enabling the model to select the most informative examples to label next. However, the effectiveness of different selection strategies can vary across datasets and tasks, making it difficult to determine the best approach in practice.We created AutoAL, a system that integrates multiple active learning strategies, and automatically tries multiple strategies at once and learns which combination works best for each specific dataset. Our tests on both everyday and medical images show that AutoAL consistently beats using single strategy alone. This makes it much easier and cheaper to train deep learning models, especially in areas like medical imaging where getting labeled data is particularly expensive and difficult."
Poster,"AutoCATE: End-to-End, Automated Treatment Effect Estimation",https://ICML.cc//virtual/2025/poster/45333,"Toon Vanderschueren, Tim Verdonck, Mihaela van der Schaar, Wouter Verbeke","Estimating causal effects is crucial in domains like healthcare, economics, and education. Despite advances in machine learning (ML) for estimating conditional average treatment effects (CATE), the practical adoption of these methods remains limited, due to the complexities of implementing, tuning, and validating them. To address these challenges, we formalize the search for an optimal ML pipeline for CATE estimation as a counterfactual Combined Algorithm Selection and Hyperparameter (CASH) optimization. We introduce AutoCATE, the first end-to-end, automated solution for CATE estimation. Unlike prior approaches that address only parts of this problem, AutoCATE integrates evaluation, estimation, and ensembling in a unified framework. AutoCATE enables comprehensive comparisons of different protocols, yielding novel insights into CATE estimation and a final configuration that outperforms commonly used strategies. To facilitate broad adoption and further research, we release AutoCATE as an open-source software package.","Understanding the impact of treatments—like new medications, educational programs, or economic policies—is essential in fields like healthcare, economics, and social science. While machine learning can help estimate these causal effects using data, setting up and tuning these models is often too complex for non-experts. AutoCATE solves this by fully automating this process: it selects, configures, and combines the best machine learning methods to estimate causal effects—without requiring users to make technical decisions. This way, our tool makes causal analyses faster, more accurate, and accessible to anyone, regardless of their expertise in machine learning."
Poster,AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation,https://ICML.cc//virtual/2025/poster/45474,"Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang","Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL, a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design.","Designing analog circuits — the basic building blocks inside electronic devices — usually takes a lot of time and expert knowledge. It’s hard to automate because there are so many ways to design a circuit, and each one has to follow strict performance rules.In this work, we created a tool called AUTOCIRCUIT-RL that uses AI to help design these circuits automatically. It works in two steps. First, we use a large language model (LLM) — similar to the ones used in chatbots — and teach it to generate circuit designs based on given instructions, like what the circuit should do. Then, we improve the model's designing ability by giving it feedback from AI. This feedback helps the model learn which designs work well — based on how efficient they are, if they produce the right output voltage, and whether the design actually works.Compared to older methods, our tool makes more valid and efficient circuits, even when it doesn't get much training data. It also avoids generating the same design over and over. This could make circuit design faster and easier for engineers"
Poster,AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling,https://ICML.cc//virtual/2025/poster/45818,"Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi","Large language models (LLMs) acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency often hinder their direct application for predictive tasks where privacy and interpretability are paramount. In fields such as healthcare, biology, and finance, specialised and interpretable linear models still hold considerable value. In such domains, labelled data may be scarce or expensive to obtain. Well-specified prior distributions over model parameters can reduce the sample complexity of learning through Bayesian inference; however, eliciting expert priors can be time-consuming. We therefore introduce AutoElicit to extract knowledge from LLMs and construct priors for predictive models. We show these priors are informative and can be refined using natural language. We perform a careful study contrasting AutoElicit with in-context learning and demonstrate how to perform model selection between the two methods. We find that AutoElicit yields priors that can substantially reduce error over uninformative priors, using fewer labels, and consistently outperform in-context learning. We show that AutoElicit saves over 6 months of labelling effort when building a new predictive model for urinary tract infections from sensor recordings of people living with dementia.","In this work, we propose AutoElicit, a method for using LLMs to aid predictive modelling tasks, with a focus on healthcare. Specifically, we present a method for using LLMs to elicit expert prior distributions for linear predictive models and demonstrate how human experts can aid the process. We then compare the posterior predictions with those made through in-context learning, where language models make predictions directly. Using data from our study on dementia, we show that AutoElicit saves over 6 months of labelling effort when building a new predictive model for urinary tract infections from sensor recordings of participants."
Poster,Autoencoder-Based Hybrid Replay for Class-Incremental Learning,https://ICML.cc//virtual/2025/poster/44510,"Milad Khademi Nori, Il-Min Kim, Guanghui Wang","In class-incremental learning (CIL), effective incremental learning strategies are essential to mitigate task confusion and catastrophic forgetting, especially as the number of tasks $t$ increases. Current exemplar replay strategies impose $\mathcal{O}(t)$ memory/compute complexities. We propose an autoencoder-based hybrid replay (AHR) strategy that leverages our new hybrid autoencoder (HAE) to function as a compressor to alleviate the requirement for large memory, achieving $\mathcal{O}(0.1 t)$ at the worst case with the computing complexity of $\mathcal{O}(t)$ while accomplishing state-of-the-art performance. The decoder later recovers the exemplar data stored in the latent space, rather than in raw format. Additionally, HAE is designed for both discriminative and generative modeling, enabling classification and replay capabilities, respectively. HAE adopts the charged particle system energy minimization equations and repulsive force algorithm for the incremental embedding and distribution of new class centroids in its latent space. Our results demonstrate that AHR consistently outperforms recent baselines across multiple benchmarks while operating with the same memory/compute budgets. The source code is included in the supplementary material and will be open-sourced upon publication.","We propose a hybrid replay strategy for class-incremental learning, a combination of model-based and data-based to overcome catastrophic forgetting with 10x less memory."
Poster,AutoEval Done Right: Using Synthetic Data for Model Evaluation,https://ICML.cc//virtual/2025/poster/45243,"Pierre Boyeau, Anastasios Angelopoulos, Tianle Li, Nir Yosef, Jitendra Malik, Michael Jordan",The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased.,"This paper introduces a method to evaluate machine learning models more efficiently by combining a small set of human-annotated data with a larger set of AI-generated synthetic labels. The core idea is to use the human data to correct biases present in the synthetic labels, leveraging a statistical technique called prediction-powered inference. This approach is demonstrated across diverse applications, including ranking computer vision models, evaluating protein fitness predictors, and assessing large language models via pairwise comparisons from the Chatbot Arena. Results show that this method produces more accurate performance estimates and tighter confidence intervals than traditional evaluation techniques, allowing for more reliable model evaluation with reduced human effort."
Poster,Autoformulation of Mathematical Optimization Models Using LLMs,https://ICML.cc//virtual/2025/poster/46554,"Nicolás Astorga, Tennison Liu, Yuanzhang Xiao, Mihaela van der Schaar","Mathematical optimization is fundamental to decision-making across diverse domains, from operations research to healthcare. Yet, translating real-world problems into optimization models remains a difficult task, often demanding specialized expertise. This paper approaches the problem of $\textit{autoformulation}$: the automated creation of solver-ready optimization models from natural language problem descriptions.We identify three core challenges of autoformulation: $\textit{(1)}$ the vast, problem-dependent hypothesis space, $\textit{(2)}$ efficient and diverse exploration of this space under uncertainty, and $\textit{(3)}$ evaluation of formulation correctness against problem description.To address these challenges, we present a novel method leveraging $\textit{Large Language Models}$ (LLMs) with $\textit{Monte-Carlo Tree Search}$, exploiting the hierarchical nature of optimization modeling to generate and systematically explore possible formulations. To enhance search efficiency, we introduce symbolic pruning to eliminate trivially equivalent search paths (branches), and employ LLM-based evaluation of partial formulations to guide search.Empirical analysis on linear and mixed-integer programming benchmarks demonstrates our method's effectiveness, with significant performance gains from both LLM-based value estimation and symbolic pruning techniques.","**(1)** Building mathematical optimization models from real-world problem descriptions is essential in fields like logistics, finance, and healthcare, but it remains a labor-intensive task requiring expert knowledge. **(2)** We introduce an autoformulator that uses large language models (LLMs) to automatically convert natural language problem descriptions into solver-ready optimization models. Our method uses Monte Carlo Tree Search to systematically explore modeling choices, guided by LLM-generated hypotheses and correctness evaluations, while symbolic tools prune redundant formulations to keep the search efficient. **(3)** This approach not only significantly outperforms prior baselines on real-world benchmarks, but also brings us closer to making powerful optimization tools accessible to non-experts, streamlining decision-making across industries and expanding the applications of AI-assisted modeling."
Poster,AutoGFM: Automated Graph Foundation Model with Adaptive Architecture Customization,https://ICML.cc//virtual/2025/poster/44539,"Haibo Chen, Xin Wang, Zeyang Zhang, Haoyang Li, Ling Feng, Wenwu Zhu","Graph foundation models (GFMs) aim to share graph knowledge across diverse domains and tasks to boost graph machine learning. However, existing GFMs rely on hand-designed and fixed graph neural network (GNN) architectures, failing to utilize optimal architectures *w.r.t.* specific domains and tasks, inevitably leading to suboptimal performance in diverse graph domains and tasks. In this paper, we explore graph neural architecture search (GNAS) for GFMs for the first time, which suffers from the problem of *architecture inconsistency*, i.e., the optimal architectures for different tasks and domains vary. We tackle this problem by discovering an invariant graph-architecture relationship across domains and tasks, which imposes three challenges: i) how to capture invariant and variant patterns; ii) how to customize architectures to adapt to diverse domains and tasks; iii) how to mitigate the data domination phenomenon during the architecture search process.To address these challenges, we propose **Auto**mated **G**raph **F**oundation **M**odel with Adaptive Architecture Customization (**AutoGFM**), providing a theoretical analysis to demonstrate the limitations of existing GNAS. Specifically, we first propose a disentangled contrastive graph encoder to learn invariant and variant patterns. Then, we design an invariant-guided architecture customization strategy to customize architectures for data from diverse domains and tasks. Finally, we propose a curriculum architecture customization mechanism to mitigate the phenomenon of particular data dominating the search process. Extensive experiments demonstrate that **AutoGFM** outperforms baselines, achieving state-of-the-art performance.","We want to make it easier for a graph neural network (GNN) to learn from many different kinds of graph data, such as social networks, molecules, or recommendation systems, without having to start from scratch each time. However, current methods often rely on fixed GNN architectures that don’t perform equally well across different tasks and data types.To address this, we propose a method called AutoGFM, which automatically adapts the GNN architecture based on the specific graph and task. Our method learns shared knowledge across different domains and tasks through a parameter-sharing model, while dynamically combining modular components to suit the unique needs of each individual case.This approach enables researchers to more easily use a single GNN across a wide variety of graph data, making graph learning more flexible, efficient, and accessible."
