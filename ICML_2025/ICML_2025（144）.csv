type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Improving Memory Efficiency for Training KANs via Meta Learning,https://ICML.cc//virtual/2025/poster/46218,"Zhangchi Zhao, Jun Shu, Deyu Meng, Zongben Xu","Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel framework for function approximation by replacing traditional neural network weights with learnable univariate functions. This design demonstrates significant potential as an efficient and interpretable alternative to traditional MLPs. However, KANs are characterized by a substantially larger number of trainable parameters, leading to challenges in memory efficiency and higher training costs compared to MLPs. To address this limitation, we propose to generate weights for KANs via a smaller meta-learner, called MetaKANs. By training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs achieve comparable or even superior performance while significantly reducing the number of trainable parameters and maintaining promising interpretability. Extensive experiments on diverse benchmark tasks, including symbolic regression, partial differential equation solving, and image classification, demonstrate the effectiveness of MetaKANs in improving parameter efficiency and memory usage. The proposed method provides an alternative technique for training KANs, that allows for greater scalability and extensibility, and narrows the training cost gap with MLPs stated in the original paper of KANs. Our code is available at \url{https://github.com/Murphyzc/MetaKAN}.","Kolmogorov-Arnold Networks (KANs) are a new type of neural network that replaces fixed activation functions with more flexible, learnable mathematical functions. This allows them to better understand complex data and offers clearer insights into how decisions are made — a valuable property in scientific and engineering applications. However, this flexibility comes at a cost: KANs require many more parameters than traditional networks, which makes them memory-hungry and harder to train on large datasets.To solve this, we introduce MetaKANs — a new framework that uses a smaller network (a “meta-learner”) to generate the parameters needed by KANs. Instead of learning each function separately, MetaKANs learn how to generate them efficiently using shared patterns. This  reduces memory usage while keeping performance strong.Our experiments show that MetaKANs match or even exceed standard KANs in accuracy across many tasks — from solving physics equations to recognizing images — using just a fraction of the memory. This makes advanced, interpretable neural networks more practical for real-world problems."
Poster,Improving Model Alignment Through Collective Intelligence of Open-Source Models,https://ICML.cc//virtual/2025/poster/45654,"Junlin Wang, Roy Xie, Shang Zhu, Jue Wang, Ben Athiwaratkun, Bhuwan Dhingra, Shuaiwen Song, Ce Zhang, James Zou","Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback, which necessitates high-quality human-labeled data. Constructing such datasets is often expensive and hard to scale, and may face potential limitations on diversity and generalization. To address these challenges, we introduce Mixture of Agents Alignment (MoAA), that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning and preference optimization, leading to improved performance compared to using a single model alone to generate alignment data (e.g. using GPT-4o alone). Evaluation results show that our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising direction for model alignment through this new scalable and diverse synthetic data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement pipeline, where models fine-tuned on MoA-generated data surpass their own initial capabilities, providing evidence that our approach can push the frontier of open-source LLMs without reliance on stronger external supervision. Data and code will be released.","Building large language models that reliably follow human instructions and align with human preferences usually demands huge amounts of costly, hand-labeled data. A common shortcut is to let one strong model such as GPT-4 generate this data, but relying on a single source is expensive, opaque, and can bake in its own biases. We introduce Mixture-of-Agents Alignment (MoAA), where several open-source models collaborate to craft richer training examples.Our method is a two-step pipeline: first the panel of open-source models generate high-quality data collectively, then a second stage in which another panel of models ranks several responses and shows the model which it should prefer. The collective’s varied strengths yield synthetic data that is more diverse, accurate, and safe than any individual model’s output. Using MoAA, an 8-billion-parameter Llama-3 model jumped from 20 % to 48 % wins over GPT-4 on the tough Arena-Hard test and more than doubled its score on AlpacaEval.Because MoAA relies only on freely available models, the method is transparent and low-cost. By turning a crowd of good—but imperfect—models into a self-improving team, MoAA offers a scalable route to safer, more helpful AI. We release our dataset and models used in our pipeline."
Poster,Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques,https://ICML.cc//virtual/2025/poster/45363,"Alon Arad, Saharon Rosset","Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration often produces suboptimal results, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multi-class calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (**NA-FIR**) or modeling the problem as a cumulative bivariate isotonic regression (**SCIR**). Empirical evaluations on a variety of text and image classification datasets across different model architectures reveal that our approach consistently improves log loss and expected calibration error (ECE) metrics. These findings underscore the potential of our approach to enhance a-parametric multi-class calibration practices, offering an adaptable solution for real-world applications.","Modern machine learning systems often predict probabilities - for example, how likely an image contains a dog, a cat, or a car. But these probabilities can be unreliable, especially when a model must choose between many possible classes. This can lead to poor decisions in high-stakes applications.A common way to fix this is by calibrating the model - adjusting its probabilities so they better reflect reality. One popular approach is isotonic regression, which fits a simple model to fix the probabilities under just one assumption: that higher scores from the model should correspond to higher actual chances. This works well for binary decisions, but breaks down when applied to multi-class settings.  We introduce two new methods (NA-FIR and SCIR) that directly account for the multi-class nature of the problem during training, while remaining relatively simple. We tested our methods on various text and image classification tasks and found that they consistently improved key performance metrics. These techniques offer a simple and effective upgrade for practitioners seeking better-calibrated AI systems — without needing to change their model or training pipeline."
Poster,Improving Multimodal Learning Balance and Sufficiency through Data Remixing,https://ICML.cc//virtual/2025/poster/44432,"Xiaoyu Ma, Hao Chen, Yongjian Deng","Different modalities hold considerable gaps in optimization trajectories, including speeds and paths, which lead to *modality laziness* and *modality clash* when jointly training multimodal models, resulting in insufficient and imbalanced multimodal learning.Existing methods focus on enforcing the weak modality by adding modality-specific optimization objectives, aligning their optimization speeds, or decomposing multimodal learning to enhance unimodal learning. These methods fail to achieve both unimodal sufficiency and multimodal balance.In this paper, we, for the first time, address both concerns by proposing multimodal Data Remixing, including decoupling multimodal data and filtering hard samples for each modality to mitigate modality imbalance; and then batch-level reassembling to align the gradient directions and avoid cross-modal interference, thus enhancing unimodal learning sufficiency. Experimental results demonstrate that our method can be seamlessly integrated with existing approaches, improving accuracy by approximately **6.50\%$\uparrow$** on CREMAD and **3.41\%$\uparrow$** on Kinetic-Sounds, without training set expansion or additional computational overhead during inference. The source code is available at Data Remixing.","The *modality imbalance* problem refers to the phenomenon where, during multimodal joint training, the strong modality tends to suppress the learning of the weak one. However, in our study, we observe that the weak modality can also interfere with the learning of the strong one. To investigate this, we delve into the phenomenon and propose the concept of *modality clash*.To address the issues, we introduce an adaptive data allocation mechanism called Data Remixing. This method decouples multimodal inputs by evaluating each sample at the sample level and assigning it to the most appropriate modality for training. This ensures more balanced learning across modalities. Additionally, it reassembles unimodal inputs at the batch level to further mitigate cross-modal interference.Through extensive experiments, we demonstrate that our approach performs well on multimodal co-decision tasks, significantly enhancing both unimodal and multimodal representation capabilities."
Poster,Improving Out-of-Distribution Detection via Dynamic Covariance Calibration,https://ICML.cc//virtual/2025/poster/45102,"Kaiyu Guo, Zijian Wang, Tan Pan, Brian Lovell, Mahsa Baktashmotlagh","Out-of-Distribution (OOD) detection is essential for the trustworthiness of AI systems. Methods using prior information (i.e., subspace-based methods) have shown effective performance by extracting information geometry to detect OOD data with a more appropriate distance metric. However, these methods fail to address the geometry distorted by ill-distributed samples, due to the limitation of statically extracting information geometry from the training distribution. In this paper, we argue that the influence of ill-distributed samples can be corrected by dynamically adjusting the prior geometry in response to new data. Based on this insight, we propose a novel approach that dynamically updates the prior covariance matrix using real-time input features, refining its information. Specifically, we reduce the covariance along the direction of real-time input features and constrain adjustments to the residual space, thus preserving essential data characteristics and avoiding effects on unintended directions in the principal space. We evaluate our method on two pre-trained models for the CIFAR dataset and five pre-trained models for ImageNet-1k, including the self-supervised DINO model. Extensive experiments demonstrate that our approach significantly enhances OOD detection across various models. The code is released at https://github.com/workerbcd/ooddcc.","**Problem:** Out-of-distribution (OOD) data, such as novel categories different from the seen classes in the training data, can lead to overconfident or unreliable predictions of AI models. Thus, detecting such data is crucial to make AI more trustworthy and safe.  Existing methods rely on well-formed in-distribution (ID), i.e., the training data, which ignores the effect of the outlier points on the performance of detection. Therefore, we propose our research question: How to reduce the effect of outliers in OOD detection. **Solution:** We address this problem by modifying matrix-induced distances, which is formed as $d(f,a) = \sqrt{(f-a)\top M^{-1}(f-a)}$, where $M$ is a symmetric matrix derived from ID data, $a$ is the anchor point from the ID data and $f$ is from the test data. For every step, we utilize $f$ to adjust $M$ to alleviate the effect of ID outlier points on $M$. Specifically, we project $f$ to the residual space (the SVD space of ID data corresponding to smallest eigenvalues) and shrink the expansion of $M$ aligned with the direction of projected $f$. This process benefits from the property of ID residual space, which tends to capture more information relevant to OOD data. We adopt this strategy to the Mahalanobis distance where $M$ is the covariance matrix of ID data.  We theoretically analyse the situation when our method can work solidly.**Impact:** Our findings provide a strategy to help improve the performance of distance-based score function in finding the performance boundary of AI models, especially on the relatively poor-clustered models."
Poster,Improving Out-of-Distribution Detection with Markov Logic Networks,https://ICML.cc//virtual/2025/poster/46267,"Konstantin Kirchheim, Frank Ortmeier","Out-of-distribution (OOD) detection is essential for ensuring the reliability of deep learning models operating in open-world scenarios. Current OOD detectors mainly rely on statistical models to identify unusual patterns in the latent representations of a deep neural network. This work proposes to augment existing OOD detectors with probabilistic reasoning, utilizing Markov logic networks (MLNs). MLNs connect first-order logic with probabilistic reasoning to assign probabilities to inputs based on weighted logical constraints defined over human-understandable concepts, which offers improved explainability. Through extensive experiments on multiple datasets, we demonstrate that MLNs can significantly enhance the performance of a wide range of existing OOD detectors while maintaining computational efficiency. Furthermore, we introduce a simple algorithm for learning logical constraints for OOD detection from a dataset and showcase its effectiveness.","Modern AI systems can confidently misinterpret inputs they’ve never seen before, like a blue stop sign, posing risks in real-world applications. Typical detectors look for odd patterns in a model’s internal signals, but these methods can miss obvious semantic errors and offer no clear reason why something is flagged. We propose adding a probabilistic “reasoning layer” based on simple human-readable rules (e.g., “stop signs are red octagons”) to existing detectors. We also introduce an efficient algorithm that automatically learns these rules from examples. On benchmarks like traffic signs and face attributes, our hybrid approach consistently improves detection rates and pinpoints which rules were broken. By combining pattern-based detection with rule-driven reasoning, this work makes AI systems more reliable and transparent."
Poster,Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces,https://ICML.cc//virtual/2025/poster/46525,"Anjiang Wei, Allen Nie, Thiago Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken","Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as *mappers*. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away the low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving $3.8\times$ faster performance. Our approach finds mappers that surpass expert-written mappers by up to $1.34\times$ speedup across nine benchmarks while reducing tuning time from days to minutes.","Can large language models (LLMs) make programs run faster on supercomputers?In this work, we show that they can—by designing a high-level interface that connects an LLM-powered agent with low-level system software. This interface allows LLMs to generate and iteratively refine the high-level programs that control key performance aspects of program execution, without modifying the complex underlying system code. The challenge lies in quickly discovering such effective high-level programs. To address this, we introduce a natural language–based guidance mechanism that interprets execution feedback and helps the LLM improve more efficiently. Our results show that this approach is significantly faster and more effective than traditional reinforcement learning methods.Overall, our work suggests that LLMs could play a major role in solving performance optimization challenges in computer systems."
Poster,Improving Rationality in the Reasoning Process of Language Models through Self-playing Game,https://ICML.cc//virtual/2025/poster/45387,"Pinzheng Wang, Juntao Li, Zecheng Tang, Haijia Gui, Min zhang","Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding.However, recent studies indicate that even the best models lack true comprehension of their reasoning processes.In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models.We design a $\textit{\textbf{C}ritic-\textbf{D}iscernment \textbf{G}ame}~(\textbf{CDG})$ in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback.Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.","Large language models (LLMs) can solve complex reasoning problems, but they often struggle to recognize when their own answers are wrong—even when they do know better. This limits their reliability in critical tasks.We propose a method called the Critic Discernment Game (CDG) to improve their self-evaluation skills. In this game, three roles interact: the Prover, who answers a question step-by-step; the Helpful Critic, who points out flaws in wrong answers without giving away the fix; and the Misleading Critic, who tries to trick the Prover into changing a correct answer by suggesting a fake error.The Prover must learn to accept helpful feedback while ignoring misleading suggestions—strengthening its ability to judge reasoning quality independently. Inspired by training strategies like those used in AlphaGo, this game-like setup encourages deeper reflection.Our results show that CDG helps models better detect their own reasoning mistakes, a step toward building AI systems that are more trustworthy, cautious, and robust in real-world decision-making."
Poster,Improving Reward Model Generalization from Adversarial Process Enhanced Preferences,https://ICML.cc//virtual/2025/poster/46595,"Zhilong Zhang, Tian Xu, Xinghao Du, Xingchen Cao, Yihao Sun, Yang Yu","In sequential decision-making, the reward function serves as the primary supervision signal, guiding agents to acquire the desired behaviors. Traditional reward modeling methods rely heavily on human expertise, limiting their scalability. Automated preference generation from suboptimal demonstrations has emerged as a promising alternative to address this limitation. This approach first generates preference data from suboptimal demonstrations and then trains reward models based on these preferences. Despite its potential, existing methods often struggle to generate preference data with sufficient coverage, limiting the accuracy and generalizability of the resulting reward models. To overcome this limitation, we propose APEC (Automated Preference generation with Enhanced Coverage), a novel method that improves the coverage of preference data. APEC achieves this by selecting policy pairs with significantly different iteration indices from the whole adversarial imitation learning process. We provide a theoretical analysis to validate that the selected policy pairs provably hold preference relationships. Experimental results demonstrate that APEC consistently outperforms baseline methods in generating preferences with broader coverage across both vector-based and pixel-based control tasks. Consequently, the reward models trained with APEC align more closely with ground-truth rewards, deriving improved policy performance.","In reinforcement learning (RL), designing effective reward functions is a major challenge. Poorly designed rewards can lead to agents ""hacking"" the reward or requiring extensive human expertise to refine. Traditional methods like manual reward design, imitation learning, or preference-based learning often rely on perfect expert demonstrations or excessive human feedback, limiting their use for complex tasks.Our study addresses a key question: How can we automatically generate diverse, high-quality preferences without human input to train more robust reward models?We introduce APEC (Automated Preference Generation with Enhanced Coverage), inspired by adversarial imitation learning (AIL). In AIL, we observed that policies naturally improve over training iterations. APEC leverages this insight: by selecting policy pairs from different training stages, we can automatically generate preferences. Unlike prior methods that add random noise to a single policy, APEC’s approach ensures preferences cover a wide range of behaviors, making reward models more accurate and robust.Testing APEC on 8 continuous control tasks (including both simple and pixel-based robot movements), we found that it outperforms baselines. By harnessing the natural progression of AIL, APEC generates diverse preference data that aligns reward models closer to real-world objectives. This reduces reliance on human expertise, speeds up learning, and enables agents to master complex skills with minimal handholding.**Why does this matter?** As RL advances in robotics, large language models and autonomous systems, minimizing human involvement in reward design is critical for scalability. APEC offers a path to more independent, reliable AI that learns meaningful behaviors through automated preference generation, bringing us closer to practical, real-world AI applications that require less constant human guidance."
Poster,Improving Soft Unification with Knowledge Graph Embedding Methods,https://ICML.cc//virtual/2025/poster/44679,"Xuanming Cui, Chionh Peng, Adriel Kuek, Ser-Nam Lim","Neural Theorem Provers (NTPs) present a promising framework for neuro-symbolic reasoning, combining end-to-end differentiability with the interpretability of symbolic logic programming. However, optimizing NTPs remains a significant challenge due to their complex objective landscape and gradient sparcity. On the other hand, Knowledge Graph Embedding (KGE) methods offer smooth optimization with well-defined learning objectives but often lack interpretability. In this work, we propose several strategies to integrate the strengths of NTPs and KGEs, and demonstrate substantial improvements in both accuracy and computational efficiency. Specifically,  we show that by leveraging the strength of structural learning in KGEs, we can greatly improve NTPs' poorly structured embedding space, while by substituting NTPs with efficient KGE operations, we can  significantly reduce evaluation time by over 1000$\times$ on large-scale dataset such as WN18RR with a mild accuracy trade-off.","Neural Theorem Provers (NTPs) present a promising framework for neuro-symbolic reasoning, combining end-to-end differentiability with the interpretability of symbolic logic programming. However, optimizing NTPs remains a significant challenge due to their complex objective landscape and gradient sparcity. On the other hand, Knowledge Graph Embedding (KGE) methods offer smooth optimization with well-defined learning objectives but often lack interpretability. In this work, we propose several strategies to integrate the strengths of NTPs and KGEs, and demonstrate substantial improvements in both accuracy and computational efficiency. Specifically,  we show that by leveraging the strength of structural learning in KGEs, we can greatly improve NTPs' poorly structured embedding space, while by substituting NTPs with efficient KGE operations, we can  significantly reduce evaluation time by over 1000 times on large-scale dataset such as WN18RR with a mild accuracy trade-off."
