type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45358,"Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, Gabriel Synnaeve","Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve the desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks and achieve large performance gains with both small (8B parameters) and large (70B) models, outperforming previous work while reducing the number of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.","We observe that Large language models (LLMs) struggle when deployed as agents, that is, when they perform a user-requested task autonomously and have to rely on feedback from automatic systems such as a compiler, unit tests, or website responses. Our hypothesis is that performance is low because the models cannot effectively incorporate this automatic feedback. In other words, they lack the necessary grounding. Achieving this grounding is the goal of our paper.The preferred way to achieve the desired grounding in automatic feedback is train an LLM in the target domain. This means that we require a learning environment that produces this automatic feedback. We select multi-turn code generation for this, which works as follows: the LLM is first presented with a coding challenge. The reply is evaluated against a small set of tests (""public tests""), and if any of these fail, corresponding feedback is provided and the LLM can propose an updated or new solution. This process is repeated several times. We then train an LLM with reinforcement learning, with the learning objective to produce correct final code as judged by a more complete set of tests (""private tests"").We show that, after reinforcement learning, models produce correct code solutions more reliably and can better incorporate feedback from unit tests. In fact, they obtain top performance relative to their size (recent, orthogonal work on reasoning with LLMs has shown significant gains on coding questions as well). Our conclusion is thus that models should be trained -- with reinforcement learning -- on the domains they will face in deployments."
Poster,RLTHF: Targeted Human Feedback for LLM Alignment,https://ICML.cc//virtual/2025/poster/46173,"Yifei Xu, Tusher Chakraborty, Emre Kiciman, Bibek Aryal, Srinagesh Sharma, Songwu Lu, Ranveer Chandra","Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model's reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF's curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF.","Large language models (LLMs) have shown remarkable capabilities, but making them behave exactly how we want—especially in specific real-world tasks—often requires lots of high-quality human feedback. This process is expensive and slow.Our work introduces RLTHF (Reinforcement Learning from Targeted Human Feedback), a smarter, more efficient way to align LLMs with human preferences. Instead of asking people to label every data point, RLTHF first uses AI to label data and then pinpoints the small portion of examples where the AI is likely wrong or uncertain. Human effort is then focused only on those tricky cases.By combining AI-generated feedback with strategic human corrections, RLTHF achieves results as good as full human annotation—but with only 6–7% of the human effort. Surprisingly, models trained using RLTHF’s curated data even outperform those trained on datasets fully labeled by humans.This approach not only makes LLM alignment cheaper and faster, but also helps organizations customize models without exposing all of their private data to external annotators."
Poster,Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism,https://ICML.cc//virtual/2025/poster/45187,"Haoyuan Cai, Zhenghao Peng, Bolei Zhou","Interactive Imitation Learning (IIL) allows agents to acquire desired behaviors through human interventions, but current methods impose high cognitive demands on human supervisors. We propose the Adaptive Intervention Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive criterion for requesting human demonstrations. AIM utilizes a proxy Q-function to mimic the human intervention rule and adjusts intervention requests based on the alignment between agent and human actions. By assigning high Q-values when the agent deviates from the expert and decreasing these values as the agent becomes proficient, the proxy Q-function enables the agent to assess the real-time alignment with the expert and request assistance when needed. Our expert-in-the-loop experiments reveal that AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks. Compared to the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40% improvement in terms of human take-over cost and learning efficiency.Furthermore, AIM effectively identifies safety-critical states for expert assistance, thereby collecting higher-quality expert demonstrations and reducing overall expert data and environment interactions needed. Code and demo video are available at https://github.com/metadriverse/AIM.","Human-in-the-loop imitation learning often demands constant expert supervision, leading to wasted effort and human fatigue in training autonomous agents. We introduce AIM, which trains a lightweight intervention detector that monitors the agent’s own confidence and safety margins. It requests expert demonstrations only when the agent is likely to make mistakes or act unsafely. As training continues, AIM automatically reduces interruptions, since the detector grows more accurate and the agent’s competence increases. In simulation benchmarks for driving and navigation, AIM matches or exceeds the performance of existing methods while cutting demonstration requests. By concentrating expert attention on critical moments, AIM accelerates learning, lowers human workload, and paves the way for scalable, trustworthy human-AI collaboration."
Poster,Robust and Conjugate Spatio-Temporal Gaussian Processes,https://ICML.cc//virtual/2025/poster/44920,"William Laplante, Matias Altamirano, Andrew Duncan, Jeremias Knoblauch, Francois-Xavier Briol","State-space formulations allow for Gaussian process (GP) regression with linear-in-time computational cost in spatio-temporal settings, but performance typically suffers in the presence of outliers. In this paper, we adapt and specialise the *robust and conjugate GP (RCGP)* framework of Altamirano et al. (2024) to the spatio-temporal setting. In doing so, we obtain an outlier-robust spatio-temporal GP with a computational cost comparable to classical spatio-temporal GPs. We also overcome the three main drawbacks of RCGPs: their unreliable performance when the prior mean is chosen poorly, their lack of reliable uncertainty quantification, and the need to carefully select a hyperparameter by hand. We study our method extensively in finance and weather forecasting applications, demonstrating that it provides a reliable approach to spatio-temporal modelling in the presence of outliers.","Spatio-temporal models are used to study changes in phenomena like weather or market activity over time, but they often fail when the data include unusual observations. We develop a more robust approach that stays accurate and efficient, even with messy data. This improves prediction in fields such as meteorology and finance, where outliers are prevalent."
Poster,Robust Automatic Modulation Classification with Fuzzy Regularization,https://ICML.cc//virtual/2025/poster/46022,"Xinyan Liang, Ruijie Sang, Yuhua Qian, Qian Guo, Feijiang Li, Liang Du","Automatic Modulation Classification (AMC) serves as a foundational pillar for cognitive radio systems, enabling critical functionalities including dynamic spectrum allocation, non-cooperative signal surveillance, and adaptive waveform optimization. However, practical deployment of AMC faces a fundamental challenge: prediction ambiguity arising from intrinsic similarity among modulation schemes and exacerbated under low signal-to-noise ratio (SNR) conditions. This phenomenon manifests as near-identical probability distributions across confusable modulation types, significantly degrading classification reliability. To address this, we propose Fuzzy Regularization-enhanced AMC (FR-AMC), a novel framework that integrates uncertainty quantification into the classification pipeline. The proposed FR has three features: (1) Explicitly model prediction ambiguity during backpropagation, (2) dynamic sample reweighting through adaptive loss scaling, (3) encourage margin maximization between confusable modulation clusters. Experimental results on benchmark datasets demonstrate that the FR achieves superior classification accuracy and robustness compared to compared methods, making it a promising solution for real-world spectrum management and communication applications.","In Automatic Modulation Recognition (AMC), the similarity between modulation methods, especially under low signal-to-noise ratio (SNR) conditions, leads to ambiguous model predictions and undermines recognition reliability. To address this challenge, we explore a new framework designed to alleviate the issue. Our experiments show that the similarity between modulation types is reflected in the model's predicted probability values. Specifically, more similar classes tend to produce closer prediction probabilities. We model this phenomenon by incorporating uncertainty. Building on this, we propose a novel fuzzy regularized AMC framework (FR-AMC). This framework effectively mitigates modulation confusion by quantifying the uncertainty in the predicted probability distribution during training and applying a penalty to high-uncertainty predictions. Experiments on benchmark datasets show that FR-AMC improves classification accuracy and robustness. It offers a practical path toward more reliable cognitive radio systems and shows potential for other fine-grained classification tasks."
Poster,Robust Autonomy Emerges from Self-Play,https://ICML.cc//virtual/2025/poster/43537,"Marco Cusumano-Towner, David Hafner, Alexander Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, Philipp Kraehenbuehl, Vladlen Koltun","Self-play has powered breakthroughs in two-player and multi-player games. Here we show that self-play is a surprisingly effective strategy in another domain. We show that robust and naturalistic driving emerges entirely from self-play in simulation at unprecedented scale -- 1.6 billion km of driving. This is enabled by Gigaflow, a batched simulator that can synthesize and train on 42 years of subjective driving experience per hour on a single 8-GPU node. The resulting policy achieves state-of-the-art performance on three independent autonomous driving benchmarks. The policy outperforms the prior state of the art when tested on recorded real-world scenarios, amidst human drivers, without ever seeing human data during training. The policy is realistic when assessed against human references and achieves unprecedented robustness, averaging 17.5 years of continuous driving between incidents in simulation.","We show that robust and naturalistic driving emerges from self-play in simulation at unprecedented scale. We built a simulator and training environment that allows us to simulate 1.6 billion km of driving (the distance from the sun past Saturn). A policy trained on this amount of simulated driving data learns to drive, communicate with other drivers, and almost never collides purely through self-play, trying to drive among different versions of itself. Our driver generalizes to novel scenarios based on recorded real-world scenes and achieves state-of-the-art performance on three separate benchmarks."
Poster,Robust Conformal Outlier Detection under Contaminated Reference Data,https://ICML.cc//virtual/2025/poster/43852,"Meshi Bashari, Matteo Sesia, Yaniv Romano","Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity.","Machine learning systems are often used in decision-making---such as in detecting financial fraud. In such cases, false alarms---mistakenly flagging legitimate transactions as fraudulent---can lead to costly investigations and therefore need to be controlled. Conformal prediction is a flexible framework that controls the false alarm rate while providing statistical guarantees. It does this by learning what ``normal’’ samples look like from a reference set.But what if the reference set---presumed to contain solely legitimate transactions---accidentally includes frauds? This kind of contamination is common in practice, and it raises an important question: how does it affect the system’s ability to reliably control its false alarm rate?We show that conformal prediction remains robust and reliable even when the reference set is contaminated. In fact, it becomes overly conservative and makes fewer detections, which limits the system’s practical value.To address this, we introduce a method that partially ``cleans’’ the reference data by asking a human expert to check just a small number of the most suspicious examples. By removing only those flagged as outliers, we make the system more powerful while still providing error control guarantees."
Poster,Robust Consensus Anchor Learning for Efficient Multi-view Subspace Clustering,https://ICML.cc//virtual/2025/poster/46197,"Yalan Qin, Nan Pu, Guorui Feng, Nicu Sebe","As a leading unsupervised classification algorithm in artificial intelligence, multi-view subspace clustering segments unlabeled data from different subspaces. Recent works based on the anchor have been proposed to decrease the computation complexity for the datasets with large scales in multi-view clustering. The major differences among these methods lie on the objective functions they define. Despite considerable success, these works pay few attention to guaranting the robustness of learned consensus anchors via effective manner for efficient multi-view clustering and investigating the specific local distribution of cluster in the affine subspace. Besides, the robust consensus anchors as well as the common cluster structure shared by different views are not able to be simultaneously learned. In this paper, we propose Robust Consensus anchors learning for efficient multi-view Subspace Clustering (RCSC). We first show that if the data are sufficiently sampled from independent subspaces, and the objective function meets some conditions, the achieved anchor graph has the block-diagonal structure. As a special case, we provide a model based on Frobenius norm, non-negative and affine constraints in consensus anchors learning, which guarantees the robustness of learned consensus anchors for efficient multi-view clustering and investigates the specific local distribution of cluster in the affine subspace. While it is simple, we theoretically give the geometric analysis regarding the formulated RCSC. The union of these three constraints is able to restrict how each data point is described in the affine subspace with specific local distribution of cluster for guaranting the robustness of learned consensus anchors. RCSC takes full advantages of correlation among consensus anchors, which encourages the grouping effect and groups highly correlated consensus anchors together with the guidance of view-specific projection. The anchor graph construction, partition and robust anchor learning are jointly integrated into a unified framework. It ensures the mutual enhancement for these procedures and helps lead to more discriminative consensus anchors as well as the cluster indicator. We then adopt an alternative optimization strategy for solving the formulated problem. Experiments performed on eight multi-view datasets confirm the superiority of RCSC based on the effectiveness and efficiency.","As a leading unsupervised classification algorithm in artificial intelligence, multi-view subspace clustering segments unlabeled data from different subspaces. Recent works based on the anchor have been proposed to decrease the computation complexity for the datasets with large scales in multi-view clustering. Despite considerable success, these works pay few attention to guaranting the robustness of learned consensus representative data samples via effective manner for efficient multi-view clustering. In this paper, we propose Robust Consensus anchors learning for efficient multi-view Subspace Clustering (RCSC). We first show that if the data are sufficiently sampled from independent subspaces, and the objective function meets some conditions, the achieved graph constructed by representative data samples has the block-diagonal structure. Experiments performed on eight multi-view datasets confirm the superiority of RCSC based on the effectiveness and efficiency."
Poster,RobustLight: Improving Robustness via Diffusion Reinforcement Learning for Traffic Signal Control,https://ICML.cc//virtual/2025/poster/44919,"Mingyuan Li, Jiahao Wang, Guangsheng Yu, Xu Wang, Qianrun Chen, Wei Ni, Lixiang Li, Haipeng Peng","Reinforcement Learning (RL) optimizes Traffic Signal Control (TSC) to reduce congestion and emissions, but real-world TSC systems face challenges like adversarial attacks and missing data, leading to incorrect signal decisions and increased congestion. Existing methods, limited to offline data predictions, address only one issue and fail to meet TSC's dynamic, real-time needs. We propose RobustLight, a novel framework with an enhanced, plug-and-play diffusion model to improve TSC robustness against noise, missing data, and complex patterns by restoring attacked data. RobustLight integrates two algorithms to recover original data states without altering existing TSC platforms. Using a dynamic state infilling algorithm, it trains the diffusion model online. Experiments on real-world datasets show RobustLight improves recovery performance by up to 50.43\% compared to baseline scenarios. It effectively counters diverse adversarial attacks and missing data. The relevant datasets and code are available at Github.","Traffic lights controlled by AI reduce congestion, but attacks or missing data can disrupt them. We propose RobustLight, which is a smart fixer that fixes corrupted data in real time. It recovers the corrupted data and improve the performance of traffic signal control, keeping traffic flowing without replacing existing systems."
Poster,Robust ML Auditing using Prior Knowledge,https://ICML.cc//virtual/2025/poster/46156,"Jade Garcia Bourrée, Augustin Godinot, Sayan Biswas, Anne-Marie Kermarrec, Erwan Le Merrer, Gilles Tredan, Martijn de Vos, Milos Vujasinovic","Among the many technical challenges to enforcing AI regulations, one crucial yet underexplored problem is the risk of audit manipulation.This manipulation occurs when a platform deliberately alters its answers to a regulator to pass an audit without modifying its answers to other users.In this paper, we introduce a novel approach to manipulation-proof auditing by taking into account the auditor's prior knowledge of the task solved by the platform. We first demonstrate that regulators must not rely on public priors (e.g. a public dataset), as platforms could easily fool the auditor in such cases. We then formally establish the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth. Finally, our experiments with two standard datasets illustrate the maximum level of unfairness a platform can hide before being detected as malicious.Our formalization and generalization of manipulation-proof auditing with a prior opens up new research directions for more robust fairness audits.","Do you remember Dieselgate? The car computer would detect when it was on a test-bench and reduce the engine power to fake environmental compliance. Well, this can happen in AI too. An AI audit is pretty straightforward.1/ I, the auditor, come up with questions to ask your model.2/ You, the platform, answer my questions.3/ I look at your answers and decide whether your system abides by the law by computing a series of aggregate metrics.Now, you know the metric. You know the questions. Worst of all, I don’t have access to your model.Thus, nothing prevents you from manipulating the answers of your model to pass the audit.Researcher have proven that this is very easy!In this paper, we formalize a method to avoid manipulations as a search for efficient “audit priors”.We instantiate our framework with a simple idea: just look at the accuracy of the platform’s answers.Our experiments show that this can help reduce the amount of unfairness a platform could hide."
