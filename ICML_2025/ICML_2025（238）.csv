type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Procurement Auctions via Approximately Optimal Submodular Optimization,https://ICML.cc//virtual/2025/poster/44284,"Yuan Deng, Amin Karbasi, Vahab Mirrokni, Renato Leme, Grigorios Velegkas, Song Zuo","We study the problem of procurement auctions, in which an auctioneer seeks to acquire services from a group of strategic sellers with private costs. The quality of the services is measured through some submodular function that is known to the auctioneer. Our goal is to design computationally efficient procurement auctions that (approximately) maximize the difference between the quality of the acquired services and the total cost of the sellers, in a way that is incentive compatible (IC) and individual rational (IR) for the sellers, and generates non-negative surplus (NAS) for the auctioneer. {Our contribution is twofold: \textbf{i)} we provide an improved analysis of existing algorithms for non-positive submodular function maximization and \textbf{ii)} we design computationally efficient frameworks that transform submodular function optimization algorithms to mechanisms that are IC and IR for the sellers, NAS for the auctioneer, and approximation-preserving.} Our frameworks are general and work both in the offline setting where the auctioneer can observe the bids and the services of all the sellers simultaneously, and in the online setting where the sellers arrive in an adversarial order and the auctioneer has to make an irrevocable decision whether to purchase their service or not. We further investigate whether it is possible to convert state-of-art submodular optimization algorithms into descending auctions. We focus on the adversarial setting, meaning that the schedule of the descending prices is determined by an adversary. We show that a submodular optimization algorithm satisfying bi-criteria $(1/2,1)$-approximation in welfare can be effectively converted to a descending auction in this setting. We further establish a connection between descending auctions and online submodular optimization. Finally, we demonstrate the practical applications of our frameworks by instantiating them with different state-of-the-art submodular optimization algorithms and comparing their welfare performance through empirical experiments on publicly available datasets that consist of thousands of sellers.","In this paper, we study procurement auctions where an auctioneer aims to purchase services from strategic sellers who each have a private cost. The value derived from procuring a subset of sellers is captured by a known submodular function, which naturally models diminishing returns. Our goal is to design mechanisms that approximately maximize the difference between the total value obtained and the total cost of the sellers, while ensuring that the mechanism is incentive compatible, individually rational, and guarantees non-negative surplus for the auctioneer. We also require that the mechanism be computationally efficient.We begin by revisiting the problem of maximizing a submodular function minus a modular cost function and provide improved guarantees for existing algorithms, most notably the distorted greedy algorithm. Our analysis shows that this algorithm satisfies a continuum of bi-criteria approximation guarantees, which we later leverage in mechanism design.We then introduce a general framework that transforms any suitable submodular maximization algorithm into a mechanism satisfying all our desired properties. This framework applies both in an offline setting, where sellers submit bids simultaneously, and in an online setting, where sellers arrive sequentially and decisions must be made irrevocably. The key idea is to preserve the allocation behavior of the original algorithm while carefully constructing payments to ensure truthfulness.We also explore the design of descending auctions, where prices start high and decrease over time until sellers accept. In adversarial scenarios, we show that using a perfectly optimal demand oracle can lead to poor welfare outcomes, whereas approximate greedy oracles can sometimes perform better. We further establish a formal connection between descending auctions and online submodular optimization, showing that limitations in one setting imply limitations in the other.Finally, we evaluate our framework empirically on large-scale instances derived from real-world graphs. We compare various mechanisms in terms of welfare and runtime, and demonstrate that our approaches strike a practical balance between economic guarantees and computational efficiency."
Poster,ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation,https://ICML.cc//virtual/2025/poster/46203,"Tianci Bu, Le Zhou, Wenchuan Yang, Jianhong Mou, Kang Yang, Suoyi Tan, Feng Yao, Jingyuan Wang, Xin Lu","Trajectory data is crucial for various applications but often suffers from incompleteness due to device limitations and diverse collection scenarios. Existing imputation methods rely on sparse trajectory or travel information, such as velocity, to infer missing points. However, these approaches assume that sparse trajectories retain essential behavioral patterns, which place significant demands on data acquisition and overlook the potential of large-scale human trajectory embeddings.To address this, we propose ProDiff, a trajectory imputation framework that uses only two endpoints as minimal information. It integrates prototype learning to embed human movement patterns and a denoising diffusion probabilistic model for robust spatiotemporal reconstruction. Joint training with a tailored loss function ensures effective imputation.ProDiff outperforms state-of-the-art methods, improving accuracy by 6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927 correlation between generated and real trajectories, demonstrating the effectiveness of our approach.","Think about GPS tracks from phones or cars; they often have missing parts. This makes it hard to see the full picture of how people move around, which is important for things like planning city services. Current ways to fill in these gaps often need a lot of the original path to be there. We have created a smart system called ProDiff that can draw a likely path even if it only knows where a journey started and ended. It works by first learning the common shapes and patterns of how people usually travel. Then, it uses this knowledge to make an educated guess to fill in the blanks.ProDiff is much better at guessing the missing parts of a journey compared to older methods. This means we can get more complete and accurate information about movement, even from spotty data. This can lead to better traffic management, improved public transport planning, and a clearer understanding of how people use a city."
Poster,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://ICML.cc//virtual/2025/poster/44610,"Daniel Franzen, Jan Disselhoff, David Hartmann","The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for large language models (LLMs), exposing limitations in their abstract reasoning abilities. In this work, we leverage task-specific data augmentations throughout the training, generation, and scoring phases, and employ a depth-first search algorithm to generate diverse, high-probability candidate solutions. Furthermore, we utilize the LLM not only as a generator but also as a scorer, using its output probabilities to select the most promising solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the public ARC-AGI evaluation set, demonstrating state-of-the-art performance among publicly available approaches. While concurrent closed-source work has reported higher scores, our method distinguishes itself through its transparency, reproducibility, and remarkably low inference cost, averaging only around 2ct per task on readily available hardware.","Large language models (LLMs) have achieved remarkable success in various domains, yet they often falter when faced with tasks demanding abstract reasoning. The Abstraction and Reasoning Corpus (ARC) serves as a benchmark to evaluate such capabilities, presenting challenges that, while intuitive for humans, remain elusive for AI systems.In this work, we explored whether refining the training and evaluation processes of LLMs could enhance their performance on such complex tasks. We generated multiple variations of each ARC puzzle and used the same LLM both to produce solution candidates and to judge how well each candidate fits the given patterns. By combining these perspectives using a ""product of experts"" approach that reinforces the independent expert assesments, we achieved state-of-the-art performance among publicly available methods.This methodology led to a significant improvement, achieving a 71.6% success rate on the public ARC evaluation set, surpassing average human performance. Notably, our solution is transparent, reproducible, and cost-effective, requiring only about 2 cents per task on standard hardware. We also evaluate our results on Sudoku puzzles, achieving a high solve rate and provide a mathematical explanation for why this approach increases the performance.Our findings demonstrate that with thoughtful design and leveraging existing model capabilities, it's possible to bridge the gap between AI and human abstract reasoning on the ARC benchmark."
Poster,Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale,https://ICML.cc//virtual/2025/poster/44780,"Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu","Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these fixed rules lack the flexibility to address the unique characteristics of individual examples, yet crafting sample-wise rules is impractical for human experts. In this paper, we show that even small language models, with only 0.3B parameters, can exhibit substantial data refining capabilities. We propose Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, and enables the model to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experiments show that models trained on ProX-refined data consistently outperform other baselines across 10 benchmarks, demonstrating effectiveness across model sizes (up to 1.7B) and pre-training corpora (C4, RedPajama-V2, FineWeb, FineWeb-Edu, and DCLM).ProX also shows great potential in continual pre-training: on math domain, ProX boosts 7B models by up to 20% within 10B tokens—results typically achieved with much larger scale training (e.g., 200B tokens).We believe ProX offers a way to curate high-quality pre-training data, and finally contributes to efficient LLM development.","Large language models (LLMs) are trained on trillions of words from the web, but much of that data is noisy, duplicated, or meaningless junk. Existing data-cleaning pipelines rely on hundreds of manually crafted rules designed for entire datasets, not for the quirks of each individual example. Writing tailored rules for billions of samples is infeasible for human curators.Our work, Programming Every Example (ProX), reframes data cleaning as code generation. A lightweight 0.3B-parameter language model writes and executes small, targeted programs—such as string normalization or HTML stripping—for each record. This fine-grained approach allows ProX to preserve valuable content that traditional filters would discard and remove subtle flaws that older heuristics overlook.We applied ProX to five major corpora: C4, RedPajama-V2, FineWeb, FineWeb-Edu, and DCLM. The resulting datasets are leaner and cleaner, leading to consistent improvements on ten diverse benchmarks across models up to 1.7B parameters. In math-heavy continual pretraining, ProX boosted 7B-parameter models by up to 20% using just 10B additional tokens—gains typically requiring 200B tokens. By reducing data waste, ProX offers a path toward faster, cheaper, and more sustainable language model development."
Poster,Progressively Label Enhancement for Large Language Model Alignment,https://ICML.cc//virtual/2025/poster/46261,"Biao Liu, Ning Xu, Xin Geng","Large Language Models (LLM) alignment aims to prevent models from producing content that misaligns with human expectations, which can lead to ethical and legal concerns.    In the last few years, Reinforcement Learning from Human Feedback (RLHF) has been the most prominent method for achieving alignment.   Due to challenges in stability and scalability with RLHF stages, which arise from the complex interactions between multiple models, researchers are exploring alternative methods to achieve effects comparable to those of RLHF.   However, these methods often rely on large high-quality datasets.   Despite some methods considering the generation of additional data to expand datasets, they often treat model training and data generation as separate and static processes, overlooking the fact that these processes are highly interdependent, leading to inefficient utilization of the generated data.   To deal with this problem, we propose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a framework that dynamically adjusts the model’s training process based on the evolving quality of the generated data.   Specifically, we prompt the model to generate responses for both the original query and a set of carefully designed principle guided query, and then utilize a dynamic threshold to determine the appropriate training approach for both responses based on their corresponding reward scores.    Experimental results demonstrate the effectiveness of PLE compared to existing LLM alignment methods.","As AI becomes more powerful, it’s crucial to ensure that large language models (LLMs) behave in ways that align with human values and expectations. Traditionally, researchers have used a technique called Reinforcement Learning from Human Feedback (RLHF) to train these models to give helpful and safe responses. However, RLHF can be difficult to scale and improve efficiently.Our paper introduces a new method, called Progressively Label Enhancement, to make this training process smarter and more effective. Instead of treating training and data generation as two separate steps, our approach links them together. It lets the model improve by learning not only from standard examples but also from specially crafted, principle-guided questions. The model then decides—based on how well it performs—what kind of training it needs to get better.This dynamic, flexible method helps the model learn faster and more safely. Our experiments show that it outperforms existing approaches in aligning LLMs with human goals."
Poster,Progressive Tempering Sampler with Diffusion,https://ICML.cc//virtual/2025/poster/43740,"Severi Rissanen, RuiKang OuYang, Jiajun He, Wenlin Chen, Markus Heinonen, Arno Solin, Jose Miguel Hernandez-Lobato","Recent research has focused on designing neural samplers that amortize the process of sampling from unnormalized densities. However, despite significant advancements, they still fall short of the state-of-the-art MCMC approach, Parallel Tempering (PT), when it comes to the efficiency of target evaluations. On the other hand, unlike a well-trained neural sampler, PT yields only dependent samples and needs to be rerun---at considerable computational cost---whenever new samples are required. To address these weaknesses, we propose the Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion models sequentially across temperatures, leveraging the advantages of PT to improve the training of neural samplers. We also introduce a novel method to combine high-temperature diffusion models to generate approximate lower-temperature samples, which are minimally refined using MCMC and used to train the next diffusion model. PTSD enables efficient reuse of sample information across temperature levels while generating well-mixed, uncorrelated samples. Our method significantly improves target evaluation efficiency,outperforming diffusion-based neural samplers.","Drawing samples from a complex system is often difficult. Raising the temperature of the system can often accelerate the transition between different states and hence can accelerate the sampling procedure. In this paper, we leverage this property to design faster samplers. We use a network to fit to the high temperature data, and cool down the system gradually using a specific designed ""guidance"". This allows us to efficiently sample from complex targets."
Poster,Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF,https://ICML.cc//virtual/2025/poster/43585,"Nuoya Xiong, Aarti Singh","Reinforcement Learning with Human Feedback (RLHF) is a widely used fine-tuning approach that aligns machine learning models, particularly Language Models (LMs) with human preferences. There are typically multiple objectives driving the preference, hence humans find it easier to express per-objective comparisons rather than a global preference between two choices, e.g. compare two papers on their novelty, clarity, correctness, etc. Multi-Objective RLHF aims to use per-objective preference feedback and achieve a Pareto optimal tradeoff among these objectives by aggregating them into a single unified objective for optimization. However, nearly all prior works rely on linear aggregation, which rules out policies that favor specific objectives such as the worst one. The only existing approach using non-linear aggregation  is computationally expensive due to its reward-based nature and the need for retraining whenever the aggregation parameters change. In this work, we address this limitation by transforming the non-linear aggregation maximization problem into a series of sub-problems. Each sub-problem involves only linear aggregation, making it computationally efficient to solve. We further extend our framework to handle multi-group scenarios, where each group has distinct weights for the objectives. Our method enables achieving consensus or maximizing the aggregated objective across all groups. Theoretically, we demonstrate that our algorithmic framework achieves sublinear regret and can be easily adapted to a reward-free algorithm. Empirically, leveraging our theoretical insights, we propose a nearly training-free algorithm once the optimal policies for individual objectives are obtained.","This paper explores how to better align large language models with human preferences. People often find it easier to evaluate individual aspects—such as fluency, safety, and helpfulness—rather than providing an overall preference. Therefore, effectively aggregating human feedback across these objectives is key to building more aligned models. In this work, we consider weighted p-norm aggregation as a flexible way to combine per-objective reward models, and we introduce a projection-based optimization algorithm to maximize this aggregation. We further extend our approach to multi-group settings, where each group may have different preferences and aggregation methods.  Theoretically, we demonstrate that our algorithmic framework achieves sublinear regret. Empirically, leveraging our theoretical insights, we propose a nearly training-free algorithm once the optimal policies for individual objectives are obtained."
Poster,Projection Pursuit Density Ratio Estimation,https://ICML.cc//virtual/2025/poster/45537,"Meilin Wang, Wei Huang, Mingming Gong, Zheng Zhang","*Density ratio estimation* (DRE) is a paramount task in machine learning, for its broad applications across multiple domains, such as covariate shift adaptation, causal inference, independence tests and beyond. Parametric methods for estimating the density ratio possibly lead to biased results if models are misspecified, while conventional non-parametric methods suffer from the curse of dimensionality when the dimension of data is large. To address these challenges, in this paper, we propose a novel approach for DRE based on the projection pursuit (PP) approximation. The proposed method leverages PP to mitigate the impact of high dimensionality while retaining the model flexibility needed for the accuracy of DRE. We establish the consistency and the convergence rate for the proposed estimator.  Experimental results demonstrate that our proposed method outperforms existing alternatives in various applications.","Self-driving cars often fail in rain after being trained only in sunshine, highlighting a critical AI challenge: how can machines adapt when real-world conditions shift unexpectedly? Our research tackles this using density ratio estimation (DRE), a tool that can help models adjust to data changes. Beyond this, DRE can also help verify if two factors truly relate (e.g., smoking and cancer) or uncover causal relationships (e.g., how vaccines reduce transmission). However, traditional DRE methods have limitations: parametric methods may force data into oversimplified molds, while conventional non-parametric tools fail with complex, high-dimensional data.Our approach employs a stepwise feature prioritization mechanism inspired by projection pursuit, where models learn to sequentially identify and amplify the most statistically significant patterns in the data. By iteratively breaking down data into meaningful layers, our method balances simplicity with flexibility. Rigorous mathematical proofs confirm its reliability, and experiments across diverse fields show it outperforms existing methods. This advance brings us closer to AI systems that robustly handle real-world unpredictability."
Poster,Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models,https://ICML.cc//virtual/2025/poster/43532,"Ngoc Quan Pham, Tuan Truong, Quyen Tran, Tan Nguyen, Dinh Phung, Trung Le","We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel Bayesian inference framework that allows modeling the interactions between particles, thereby enhancing ensemble quality through increased particle diversity. IBDR is grounded in a generalized theoretical framework that connects the distributional population loss with the approximate posterior, motivating a practical dual optimization procedure that enforces distributional robustness while fostering particle diversity. We evaluate IBDR's performance against various baseline methods using the VTAB-1K benchmark and the common reasoning language task. The results consistently show that IBDR outperforms these baselines, underscoring its effectiveness in real-world applications.","Bayesian inference is a powerful tool for managing uncertainty in machine learning models. One approach, particle-based Bayesian inference, involves training multiple models - referred to as ""particles - and combining their outputs to form an ensemble prediction. However, if these particles are too similar, they may collectively make the same errors, reducing the ensemble's effectiveness.In our research, we propose a method to directly model interactions between these particles to encourage diversity among them, thereby promoting diversity in their predictions and reducing the likelihood of all model particles making identical mistakes. We introduce a theoretical framework that connects this interactive framework to distributional robustness optimization - a concept that ensures models perform reliably under shifting distributions.Building on this theory, we develop a practical optimization technique that simultaneously fosters diversity among particles and enhances the ensemble's robustness. This method guides each particle to explore different aspects of the data while maintaining overall model robustness.We tested our approach on tasks like image classification and commonsense reasoning. The results consistently showed improved performance over existing methods. Further ablation studies revealed that encouraging diversity within the ensemble helps prevent the collective failure of particles, leading to more accurate and dependable predictions."
Poster,Prompt-based Depth Pruning of Large Language Models,https://ICML.cc//virtual/2025/poster/44416,"Juyun Wee, Minjae Park, Jaeho Lee","Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a transformer block may be highly task-dependent---a block that is crucial for a task can be removed without degrading the accuracy on another task. Based on this observation, we develop a dynamic depth pruning algorithm, coined PuDDing (**P**rompt-ro**u**ted **D**ynamic **D**epth Prun**ing**), which determines which blocks to omit from the model based on the input prompt. PuDDing operates by training a lightweight router to predict the best omission set among a set of options, where this option set has also been constructed in a data-driven manner. Empirical results on commonsense reasoning benchmarks demonstrate that PuDDing effectively accelerates the inference language models, and achieves better on-task performance than static depth pruning baselines.","Large language models (LLMs) deliver impressive reasoning and question-answering abilities, but executing every layer of these massive networks is both slow and expensive. A widely used shortcut—“depth pruning”—simply removes certain transformer blocks to speed up inference, but this static approach often degrades performance: a layer that appears redundant for one input may be critical for another. We propose PuDDing (Prompt-routed Dynamic Depth Pruning), which dynamically skips layers based on the content of each prompt. First, we profile real examples to create several candidate pruned models. Then, at inference time, a lightweight router analyzes the incoming prompt and selects the variant that retains only the most relevant layers.This prompt-aware strategy accelerates LLM inference without requiring specialized hardware, and even improves accuracy on challenging reasoning benchmarks. By avoiding unnecessary computation, PuDDing lowers energy consumption and brings advanced language understanding closer to real-time applications."
