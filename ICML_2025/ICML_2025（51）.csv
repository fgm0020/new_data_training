type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,COKE: Core Kernel for More Efficient Approximation of Kernel Weights in Multiple Kernel Clustering,https://ICML.cc//virtual/2025/poster/46695,"Weixuan Liang, Xinwang Liu, KE LIANG, Jiyuan Liu, En Zhu","Inspired by the well-known coreset in clustering algorithms, we introduce the definition of the core kernel for multiple kernel clustering (MKC) algorithms. The core kernel refers to running MKC algorithms on smaller-scale base kernel matrices to obtain kernel weights similar to those obtained from the original full-scale kernel matrices. Specifically, the core kernel refers to a set of kernel matrices of size $\widetilde{\mathcal{O}}(1/\varepsilon^2)$ that perform MKC algorithms on them can achieve a $(1+\varepsilon)$-approximation for the kernel weights. Subsequently, we can leverage approximated kernel weights to obtain a theoretically guaranteed large-scale extension of MKC algorithms. In this paper, we propose a core kernel construction method based on singular value decomposition and prove that it satisfies the definition of the core kernel for three mainstream MKC algorithms. Finally, we conduct experiments on several benchmark datasets to verify the correctness of theoretical results and the efficiency of the proposed method.","We found a way to make a type of machine learning, used to group similar pieces of data, much faster and easier to use on large datasets. Normally, these methods need to process a lot of information, which takes time and computer power. Our idea is to use only a small, smartly chosen sample of the data that still gives nearly the same results as using everything.We created a new method to pick out these smaller samples using a well-known math tool, and we proved it works well with several popular techniques. We also tested our method on real-world examples and showed that it’s both accurate and efficient."
Poster,CollabLLM: From Passive Responders to Active Collaborators,https://ICML.cc//virtual/2025/poster/45988,"Shirley Wu, Michel Galley, Baolin Peng, Hao Cheng, Gavin Li, Yao Dou, Weixin Cai, James Zou, Jure Leskovec, Jianfeng Gao","Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responsesusing Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions—a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.","Many people use AI chatbot (language models) to write, code, or solve problems, but current language models often fall short in real conversations. They tend to respond passively to vague questions instead of helping users clarify their goals and drag users into frustrating and inefficient interactions because they don’t plan ahead.We introduce CollabLLM, a new training method that teaches the language model to look several turns into the future. During training we simulate whole conversations and give each reply a “multiturn-aware reward” based on how much it helps the conversation in the future. This reward encourages the language model to ask clarifying questions, surface missing details, and offer constructive next steps.CollabLM was tested on writing assistance, coding, and math tutoring. It beat strong baselines on task success and on how interative and efficient the conversations are. In a study with 201 real users, it raised satisfaction scores and reduced the time needed to finish tasks by 10%. CollabLLM makes everyday language model assistants more proactive, efficient, and genuinely user-centered."
Poster,"Collaborative Mean Estimation Among Heterogeneous Strategic Agents: Individual Rationality, Fairness, and Truthful Contribution",https://ICML.cc//virtual/2025/poster/45442,"Alex Clinton, Yiding Chen, Jerry Zhu, Kirthevasan Kandasamy","We study a collaborative learning problem where $m$ agents aim to estimate a vector $\mu =(\mu_1,\ldots,\mu_d)\in \mathbb{R}^d$ by sampling from associated univariate normal distributions $(\mathcal{N}(\mu_k, \sigma^2))\_{k\in[d]}$. Agent $i$ incurs a cost $c_{i,k}$ to sample from $\mathcal{N}(\mu_k, \sigma^2)$. Instead of working independently, agents can exchange data, collecting cheaper samples and sharing them in return for costly data, thereby reducing both costs and estimation error. We design a mechanism to facilitate such collaboration, while addressing two key challenges: ensuring *individually rational (IR) and fair outcomes* so all agents benefit, and *preventing strategic behavior* (e.g. non-collection, data fabrication) to avoid socially undesirable outcomes.We design a mechanism and an associated Nash equilibrium (NE) which minimizes the social penalty-sum of agents' estimation errors and collection costs-while being IR for all agents. We achieve a $\mathcal{O}(\sqrt{m})$-approximation to the minimum social penalty in the worst case and an $\mathcal{O}(1)$-approximation under favorable conditions. Additionally, we establish three hardness results: no nontrivial mechanism guarantees *(i)* a dominant strategy equilibrium where agents report truthfully, *(ii)* is IR for every strategy profile of other agents, *(iii)* or avoids a worst-case $\Omega(\sqrt{m})$ price of stability in any NE. Finally, by integrating concepts from axiomatic bargaining, we demonstrate that our mechanism supports fairer outcomes than one which minimizes social penalty.","Machine learning has increased the value of data, which is often costly to collect but easyto share. While most existing data sharing platforms assume honesty, participantsmay lie about their contributions to gain access to others’ data.When participants have different data collection costs there are two key challenges indesigning truthful data sharing algorithms. The first is creating a method to validate thesubmission of each contributor using the other participants’ data. The second, is ensuringthere is enough data from all participants so that each agent’s submission can be sufficientlyvalidated against the others, without compromising on efficiency.We address these problem in two parts. First, we determine how to fairly divide thework of data collection. Second, we reward participants based on the quality of the datathey submitted. For each participant we compare the mean of their data to the mean ofthe others’ data. Instead of returning the others’ data to them, we first corrupt it based onthe difference of the means. If a participant wants to receive the others’ data with minimalcorruption, it is in their best interest to collect a sufficient amount of data and share ittruthfully."
Poster,Collapse or Thrive: Perils and Promises of Synthetic Data in a Self-Generating World,https://ICML.cc//virtual/2025/poster/44713,"Joshua Kazdan, Rylan Schaeffer, Apratim Dey, Matthias Gerstgrasser, Rafael Rafailov, David Donoho, Sanmi Koyejo","What happens when generative machine learning models are pretrained on web-scale datasets containing data generated by earlier models? Some prior work warns of “model collapse” as the web is overwhelmed by synthetic data; other work suggests the problem can be contained (i.e. collapse can be avoided) by managing how available data are used in pretraining. In this paper, we report experiments on three ways of using data (training-workflows), across three generative model task-settings (multivariate Gaussian estimation, kernel density estimation, and language-model fine-tuning) to further confirm the possibility of containment: (a) we confirm that the training-workflow of {\it replacing} all real data by successive generations of purely synthetic data indeed suffers model collapse in all task-settings studied; (b) we consider the training-workflow of {\it accumulating} synthetic data alongside real data and training on all data combined and confirming that, although the proportion of real data eventually becomes zero, models remain stable and their test losses do not diverge under this training-workflow; (c) we consider a training-workflow where real and synthetic data accumulate together but successive generations of pretraining are constrained to use fixed-size data subsets each generation. In this workflow, we observe slow and gradual rather than explosive degradation of test loss performance across generations. Our insights are particularly important when forecasting whether future frontier generative models will collapse or thrive, and our results open avenues for empirically and mathematically studying the context-dependent value of synthetic data.","Some previous work has claimed that ai-generated content on the internet will cause future models trained on this data to degrade in quality, or exhibit ""model collapse"".  We refute prior work that promotes model collapse as a major threat to future models by showing that as long as models are trained on a mixture of real and synthetic data, model collapse is contained.  Since no one will delete human data en masse from the internet, model collapse is unlikely to constrain future generations of AI models."
Poster,Collapse-Proof Non-Contrastive Self-Supervised Learning,https://ICML.cc//virtual/2025/poster/43625,"EMANUELE SANSONE, Tim Lebailly, Tinne Tuytelaars","We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that encourages representations to be simultaneously decorrelated and clustered, without explicitly enforcing these properties. This bias provably enhances generalization and suffices to avoid known training failure modes, such as representation, dimensional, cluster, and intracluster collapses. We validate our theoretical findings on image datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet-100. Our approach effectively combines the strengths of feature decorrelation and cluster-based self-supervised learning methods, overcoming training failure modes while achieving strong generalization in clustering and linear classification tasks.","How can we teach computers to understand the world without constant human guidance? One promising approach, known as self-supervised learning, allows computers to learn on their own by discovering patterns in data. But building these systems is often laborious and error-prone, even experts frequently rely on trial and error to make them work.In this work, we identify and categorize the common ways these systems can fail, and we provide a simple set of guidelines to train them more reliably. We also introduce a method to measure how robust they are.Our findings make this powerful type of machine learning easier to use and understand. Ultimately, this could lead to more accessible and trustworthy AI systems that learn meaningful, structured representations of the real world, all without constant human supervision."
Poster,CombiMOTS: Combinatorial Multi-Objective Tree Search for Dual-Target Molecule Generation,https://ICML.cc//virtual/2025/poster/45885,"Thibaud Southiratn, Bonil Koo, Yijingxiu Lu, Sun Kim","Dual-target molecule generation, which focuses on discovering compounds capable of interacting with two target proteins, has garnered significant attention due to its potential for improving therapeutic efficiency, safety and resistance mitigation.Existing approaches face two critical challenges.First, by simplifying the complex dual-target optimization problem to scalarized combinations of individual objectives, they fail to capture important trade-offs between target engagement and molecular properties. Second, they typically do not integrate synthetic planning into the generative process.This highlights a need for more appropriate objective function design and synthesis-aware methodologies tailored to the dual-target molecule generation task.In this work, we propose CombiMOTS, a Pareto Monte Carlo Tree Search (PMCTS) framework that generates dual-target molecules.CombiMOTS is designed to explore a synthesizable fragment space while employing vectorized optimization constraints to encapsulate target affinity and physicochemical properties.Extensive experiments on real-world databases demonstrate that CombiMOTS produces novel dual-target molecules with high docking scores, enhanced diversity, and balanced pharmacological characteristics, showcasing its potential as a powerful tool for dual-target drug discovery.The code and data is accessible through \url{https://github.com/Tibogoss/CombiMOTS}.","Most traditional drugs follow the ""one drug, one target"" paradigm, but complex diseases are often related to several targets. As a response, emerging trends aim to design ""dual-target drugs"". However, existing multi-objective methods oversimplify the task by (i) aggregating objectives into a scalar value, (ii) neglecting real post-hoc feasibility through known synthetic procedures.To address this, we propose CombiMOTS, a combinatorial framework that (i) reduces a chemical search space to interesting building blocks specific to target protein pairs, and (ii) uses a Pareto Monte Carlo Tree Search algorithm to assemble them into potential dual-inhibitors. Our method effectively finds optimal candidates across flexible objectives, while simultaneously accounting for structural and physicochemical constraints.Empirical results show strong and favorable trade-offs across multiple target pairs, showcasing both the quantitative and qualitative performance of CombiMOTS. Additional theoretical analysis and broader preliminarily applications are provided, making CombiMOTS a valuable tool for practitioners interested in multi-objective drug design."
Poster,Combinatorial Reinforcement Learning with Preference Feedback,https://ICML.cc//virtual/2025/poster/43927,"Joongkyu Lee, Min-hwan Oh","In this paper, we consider combinatorial reinforcement learning with preference feedback, where a learning agent sequentially offers an action—an assortment of multiple items—to a user, whose preference feedback follows a multinomial logistic (MNL) model. This framework allows us to model real-world scenarios, particularly those involving long-term user engagement, such as in recommender systems and online advertising. However, this framework faces two main challenges: (1) the unknown value of each item, unlike traditional MNL bandits that only address single-step preference feedback, and (2) the difficulty of ensuring optimism while maintaining tractable assortment selection in the combinatorial action space with unknown values. In this paper, we assume a contextual MNL preference model, where the mean utilities are linear, and the value of each item is approximated by a general function. We propose an algorithm, MNL-VQL, that addresses these challenges, making it both computationally and statistically efficient. As a special case, for linear MDPs (with the MNL preference feedback), we establish the first regret lower bound in this framework and show that MNL-VQL achieves near-optimal regret. To the best of our knowledge, this is the first work to provide statistical guarantees in combinatorial RL with preference feedback.","Imagine a movie app that shows you a set of films—you pick one, and that’s all the app knows. To keep you engaged in the long run, it needs to learn your preferences from such limited feedback and choose better sets of movies over time.We study this problem using a realistic model of how people make choices and introduce an algorithm called MNL-VQL. It learns efficiently from repeated user interactions, selecting sets of items that not only match current preferences but also help the system learn faster for future decisions.Our method is the first to offer both practical efficiency and strong theoretical guarantees in reinforcement learning with this kind of feedback, helping AI systems make smarter long-term decisions."
Poster,CoMemo: LVLMs Need Image Context with Image Memory,https://ICML.cc//virtual/2025/poster/45756,"Shi Liu, Weijie Su, Xizhou Zhu, Wenhai Wang, Jifeng Dai","Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose **CoMemo** - a dual-path architecture that combines a **Co**ntext image path with an image **Memo**ry path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures.Project page is available at [https://lalbj.github.io/projects/CoMemo/](https://lalbj.github.io/projects/CoMemo/).","LVLMs  inherited LLMs architectural designs, which introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of central visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these issues, we propose CoMemo, a novel model architecture. CoMemo employs a dual-path approach for visual processing: one path maps image tokens to the text token representation space for causal self-attention, while the other introduces cross-attention, enabling context-agnostic computation between the input sequence and image information. Additionally, we developed RoPE-DHR, a new positional encoding method tailored for LVLMs with dynamic high-resolution inputs. RoPE-DHR mitigates the remote decay problem caused by dynamic high-resolution inputs while preserving the 2D structural information of images.Evaluated on seven diverse tasks, including long-context understanding, multi-image reasoning, and visual question answering, CoMemo achieves relative improvements of 17.2%, 7.0%, and 5.6% on Caption, Long-Generation, and Long-Context tasks, respectively, with consistent performance gains across various benchmarks."
Poster,"Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation",https://ICML.cc//virtual/2025/poster/44836,"Zhan Zhuang, Xiequn Wang, Wei Li, Yulong Zhang, Qiushi Huang, Shuhao Chen, Xuehao Wang, Yanbin Wei, Yuhe Nie, Kede Ma, Yu Zhang, Ying Wei","Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient fine-tuning technique for adapting large foundation models, yet it often locks adapters into suboptimal minima near their initialization. This hampers model generalization and limits downstream operators such as adapter merging and pruning. Here, we propose CoTo, a progressive training strategy that gradually increases adapters' activation probability over the course of fine-tuning. By stochastically deactivating adapters, CoTo encourages more balanced optimization and broader exploration of the loss landscape. We provide a theoretical analysis showing that CoTo promotes layer-wise dropout stability and linear mode connectivity, and we adopt a cooperative-game approach to quantify each adapter's marginal contribution. Extensive experiments demonstrate that CoTo consistently boosts single-task performance, enhances multi-task merging accuracy, improves pruning robustness, and reduces training overhead, all while remaining compatible with diverse LoRA variants. Code is available at https://github.com/zwebzone/coto.","Big AI models are powerful but expensive to teach new skills. Low-Rank Adaptation (LoRA) helps by training only tiny “adapter” modules, yet the standard practice loads all adapters at once—like dumping a jumbo DLC bundle onto a AAA game.  When everything changes in one splash, it’s tough to trace bugs, measure each pack’s value, or see which tweaks really help.We introduce CoTo, a progressive adapter-dropping strategy that raises each adapter’s activation probability step by step. CoTo rolls adapters out one at a time, flicking them on and off so the system can test how every new piece meshes with what’s already installed. This stochastic rollout reveals each adapter’s marginal contribution and lets them cooperate smoothly once they all run together full-time.Across diverse language and vision benchmarks, CoTo slips into existing LoRA pipelines with zero extra cost, consistently helps large models absorb new knowledge faster, and greatly improves the ability to merge multiple adapters into a single, stronger model."
Poster,Communicating Activations Between Language Model Agents,https://ICML.cc//virtual/2025/poster/45039,"Vignav Ramesh, Kenneth Li","Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via *activations*; concretely, we pause an LM $B$'s computation at an intermediate layer, combine its current activation with another LM $A$'s intermediate activation via some function $f$, then pass $f$'s output into the next layer of $B$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with *zero* additional parameters and data, and saves a *substantial amount of compute* over natural language communication. We test our method with various functional forms $f$ on two experimental setups—multi-player coordination games and reasoning benchmarks—and find that it achieves up to $27$% improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative ""language"" for communication between LMs.","Large language models can better reason through hard problems when multiple instances of the model (called “agents”) think through diverse approaches and communicate with each other, i.e. send each other messages in plain text. We wondered if LLM agents could communicate more efficiently and effectively by tapping into each other’s internal “thoughts” – a.k.a. the “activation vectors” produced as a model computationally processes a prompt. Our method pauses one model mid-computation, merges its activation with another model’s, and then continues processing. Crucially, this requires no extra training data or new model parameters. We evaluated our technique on multi-player coordination tasks and reasoning benchmarks, seeing a 27% boost in performance compared to text-based communication. Even better, this requires less than a fourth of the compute. These findings suggest that by sharing rich internal signals instead of words, LLM agents can collaborate far faster and more efficiently."
