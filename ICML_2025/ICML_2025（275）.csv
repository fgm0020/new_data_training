type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Shielded Diffusion: Generating Novel and Diverse Images using Sparse Repellency,https://ICML.cc//virtual/2025/poster/44976,"Michael Kirchhof, James Thornton, Louis Béthune, Pierre Ablin, Eugene Ndiaye, Marco Cuturi","The adoption of text-to-image diffusion models raises concerns over reliability, drawing scrutiny under the lens of various metrics like calibration, fairness, or compute efficiency. We focus in this work on two issues that arise when deploying these models: a lack of diversity when prompting images, and a tendency to recreate images from the training set. To solve both problems, we propose a method that coaxes the sampled trajectories of pretrained diffusion models to land on images that fall outside of a reference set. We achieve this by adding repellency terms to the diffusion SDE throughout the generation trajectory, which are triggered whenever the path is expected to land too closely to an image in the shielded reference set. Our method is sparse in the sense that these repellency terms are zero and inactive most of the time, and even more so towards the end of the generation trajectory. Our method, named SPELL for sparse repellency, can be used either with a static reference set that contains protected images, or dynamically, by updating the set at each timestep with the expected images concurrently generated within a batch, and with the images of previously generated batches. We show that adding SPELL to popular diffusion models improves their diversity while impacting their FID only marginally, and performs comparatively better than other recent training-free diversity methods. We also demonstrate how SPELL can ensure a shielded generation away from a very large set of protected images by considering all 1.2M images from ImageNet as the protected set.","When artificial intelligence (AI) generates images, there is little control over how it generates images. Often, when giving it a description, an AI model will generate one specific image, and not provide a variety of images to choose from. We introduce a mechanism that makes generative AI models, so called diffusion models, output many different images for a given description. This increases the creativity, without impacting the image quality or its closeness to the description. Our mechanism, called SPELL, has another advantage: It can also avoid generating specific images. For example, we protect 1.2 million images, to make sure that whatever image the AI model generates, it is different enough from all of these existing images."
Poster,SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy,https://ICML.cc//virtual/2025/poster/46391,"Yong Liang Goh, Zhiguang Cao, Yining Ma, Jianan Zhou, Mohammed Haroon Dupty, Wee Sun Lee","Recent advances toward foundation models for routing problems have shown great potential of a unified deep model for various VRP variants. However, they overlook the complex real-world customer distributions. In this work, we advance the Multi-Task VRP (MTVRP) setting to the more realistic yet challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce SHIELD, a novel model that leverages both *sparsity* and *hierarchy* principles. Building on a deeper decoder architecture, we first incorporate the Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both efficiency and generalization by allowing the model to dynamically select nodes to use or skip each decoder layer, providing the needed capacity to adaptively allocate computation for learning the task/distribution specific and shared representations. We also develop a context-based clustering layer that exploits the presence of hierarchical structures in the problems to produce better local representations. These two designs inductively bias the network to identify key features that are common across tasks and distributions, leading to significantly improved generalization on unseen ones. Our empirical results demonstrate the superiority of our approach over existing methods on 9 real-world maps with 16 VRP variants each.","Classical solvers for Vehicle Routing Problems (VRP) can handle multiple variants easily and are distribution agnostic. Recently, neural VRP solvers have started to handle multiple variants in the form of multi-task learning. Real-world scenarios, however, display both variations in task and underlying data distributions. Thus, we attempt to improve the generalization ability of neural solvers given such complexities.Motivated by regularization principles in the VC-dimension, this work presents SHIELD, a powerful model that leverages sparsity and hierarchy to handle the dynamics and challenges of the Multi-task Multi-distribution VRP scenario. We increase the depth of the decoder but introduce Mixture-of-Depths layers capable of controlling and adapting the network's compute power based on the necessary task and distribution variations. We also introduce a context-based latent clustering module that adapts coarse-grained representations according to tasks and distributions. Based on extensive experiments across 9 real-world maps and 16 VRP variants, we empirically show that the sparsity and hierarchy principles are paramount to improving neural solvers' generalization ability, paving the path to foundation VRP models."
Poster,Shifting Time: Time-series Forecasting with Khatri-Rao Neural Operators,https://ICML.cc//virtual/2025/poster/44565,"Srinath Dama, Kevin L Course, Prasanth B Nair","We present an operator-theoretic framework for temporal and spatio-temporal forecasting based on learning a *continuous time-shift operator*. Our operator learning paradigm offers a continuous relaxation of the discrete lag factor used in traditional autoregressive models, enabling the history of a system up to a given time to be mapped to its future values. We parametrize the time-shift operator using Khatri-Rao neural operators (KRNOs), a novel architecture based on non-stationary integral transforms with nearly linear computational scaling. Our framework naturally handles irregularly sampled observations and enables forecasting at super-resolution in both space and time. Extensive numerical studies across diverse temporal and spatio-temporal benchmarks demonstrate that our approach achieves state-of-the-art or competitive performance with leading methods.","Predicting the future, from weather patterns to financial markets, is a difficult task. It's even harder when the information we have is incomplete or collected at irregular intervals. Instead of just looking at isolated moments, our approach learns from the entire history of events to understand the continuous evolution of a system over time.To accomplish this, we built a new, highly efficient machine learning (ML) model, specifically designed to learn from complex spatio-temporal data, even if that data is irregularly sampled. This special ML model can create very detailed predictions that fill in the gaps, offering a much clearer picture of the future. When tested on different challenges in areas like physics, climate, and finance, our model achieved highly competitive results, providing a more powerful and flexible prediction tool for scientists and engineers."
Poster,Shortcut-connected Expert Parallelism for Accelerating Mixture of Experts,https://ICML.cc//virtual/2025/poster/45834,"Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang","Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100\% overlap with computation.Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of $1.49\times$ in training and $1.82\times$ in inference.Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.","**The Problem:** Imagine a team of super-specialized workers (experts) collaborating on a massive project (a large AI model). To get the job done quickly, the work needs to be split efficiently among many different workstations (devices). However, coordinating who does what and passing information back and forth between these workstations creates a huge traffic jam (communication bottleneck). This slowdown is the main thing preventing these powerful models from working even faster.**The Solution (ScMoE):** We created a smarter way to design the team (model architecture) and organize their workflow (parallelization strategy):1.  **New Design (Shortcut Connection):** We added some direct connections within the team structure, allowing information to flow more flexibly.2.  **Smarter Workflow (Overlapping):** Crucially, this new design lets the team members do their own calculations while the information they need is still being passed around. Previously, everyone had to wait for all the information to arrive before anyone could start their real work. Now, communication and calculation happen at the same time.**The Results:** This combined approach eliminates the traffic jam almost completely:*   **Much Faster:** It makes training the AI model **1.5 times faster** and running the finished model (inference) **1.8 times faster** compared to the standard method used today.*   **Just as Smart (or Smarter):** Importantly, our method doesn't sacrifice quality. The AI models built this way perform just as well as, and sometimes even better than, models built using older, slower methods."
Poster,Should Decision-Makers Reveal Classifiers in Online Strategic Classification?,https://ICML.cc//virtual/2025/poster/45611,"Han Shao, Shuo Xie, Kunhe Yang","Strategic classification addresses a learning problem where a decision-maker implements a classifier over agents who may manipulate their features in order to receive favorable predictions. In the standard model of online strategic classification, in each round, the decision-maker implements and publicly reveals a classifier, after which agents perfectly best respond based on this knowledge. However, in practice, whether to disclose the classifier is often debated---some decision-makers believe that hiding the classifier can prevent misclassification errors caused by manipulation. In this paper, we formally examine how limiting the agents' access to the current classifier affects the decision-maker's performance. Specifically, we consider an extended online strategic classification setting where agents lack direct knowledge about the current classifier and instead manipulate based on a weighted average of historically implemented classifiers. Our main result shows that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or $k_{\text{in}}$ times more mistakes compared to the full-knowledge setting, where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph (representing how many distinct feature vectors can be manipulated to appear as a single one), and $\gamma$ is the discount factor indicating agents' memory of past classifiers. Our results demonstrate how withholding access to the classifier can backfire and degrade the decision-maker's performance in online strategic classification.","Algorithmic decision-making is increasingly used in domains such as job hiring, loan approvals, and college admissions, where individuals may strategically manipulate their inputs to receive favorable outcomes. This setting is captured by the problem of strategic classification, where the challenge is to make accurate predictions based on potentially manipulated data. A common belief is that hiding the classifier may reduce manipulation and improve performance, but whether transparency helps or hurts remains unclear.This work formally compares the decision-maker’s performance under transparent and non-transparent settings. In the non-transparent case, individuals still have incentives to manipulate, but their responses are based on past classifiers rather than the current one. We show that, contrary to conventional wisdom, limited transparency can substantially increase misclassification errors.Our findings demonstrate that withholding classifiers may backfire and degrade the decision-maker's performance in online strategic classification."
Poster,Sidechain conditioning and modeling for full-atom protein sequence design with FAMPNN,https://ICML.cc//virtual/2025/poster/45963,"Talal Widatalla, Richard Shuai, Brian Hie, Po-Ssu Huang","Leading deep learning-based methods for fixed-backbone protein sequence design do not model protein sidechain conformation during sequence generation despite the large role the three-dimensional arrangement of sidechain atoms play in protein conformation, stability, and overall protein function. Instead, these models implicitly reason about crucial sidechain interactions based solely on backbone geometry and amino-acid sequence. To address this, we present FAMPNN (Full-Atom MPNN), a sequence design method that explicitly models both sequence identity and sidechain conformation for each residue, where the per-token distribution of a residue’s discrete amino acid identity and its continuous sidechain conformation are learned with a combined categorical cross-entropy and diffusion loss objective. We demonstrate learning these distributions jointly is a highly synergistic task that both improves sequence recovery while achieving state-of-the-art sidechain packing. Furthermore, benefits from explicit full-atom modeling generalize from sequence recovery to practical protein design applications, such as zero-shot prediction of experimental binding and stability measurements.","Proteins are complex molecules that must fold into precise three-dimensional shapes to function properly, but most current computer methods for designing new proteins only consider the main structural backbone while ignoring the chemical groups that branch off from each building block and actually determine how proteins interact with other molecules. We have developed FAMPNN, a new computational approach that designs both the protein sequence and predicts where every atom will be positioned in space by combining traditional sequence design with an advanced modeling technique that arranges these branching chemical groups. While FAMPNN performs well on standard protein design tests, its real strength becomes apparent when predicting how proteins will actually behave in experiments, such as their stability and ability to bind to other molecules, where it significantly outperforms methods that only use backbone information. This improved performance comes from directly modeling the atomic interactions that control protein function, rather than trying to guess these important details from incomplete structural information. The research demonstrates that accounting for complete molecular architecture, rather than simplified representations, could lead to more successful protein designs for drug discovery, industrial enzyme development, and other biotechnology applications."
Poster,Signed Laplacians for Constrained Graph Clustering,https://ICML.cc//virtual/2025/poster/45552,"John Stewart Fabila-Carrasco, He Sun","Given two weighted graphs $G = (V, E, w_G)$ and $H = (V, F, w_H)$ defined on the same vertex set, the constrained clustering problem seeks to find a subset $S \subset V$ that minimises the cut ratio between $w_G(S, V \setminus S)$ and $w_H(S, V \setminus S)$. In this work, we establish a Cheeger-type inequality that relates the solution of the constrained clustering problem to the spectral properties of $ G$ and $H$. To reduce computational complexity, we utilise the signed Laplacian of $H$, streamlining calculations while maintaining accuracy. By solving a generalised eigenvalue problem, our proposed algorithm achieves notable performance improvements, particularly in challenging scenarios where traditional spectral clustering methods struggle. We demonstrate its practical effectiveness through experiments on both synthetic and real-world datasets.","People naturally group things: we put clothes in closets, friends into social circles, and photos into albums. Computers also need to group things, for example, to organise users on social networks, sort weather stations by climate, or recommend products based on customer behaviour. This process is called clustering. However, sometimes we have extra information that should guide how things are grouped. For instance, we might know that two weather stations are in the same region and must be grouped together (this is a must-link). Or we might know that two stations are in very different climates and should not be grouped together (this is a cannot-link). Traditional algorithms do not use this kind of guidance. Our research developed a new mathematical method that allows the computer to follow these human-like rules. We use a tool called a signed Laplacian, which helps balance the natural structure of the data with the extra must-link and cannot-link rules. Our algorithm is not only more accurate, but also faster than existing approaches. This helps computers mimic how humans group things, by seeing patterns, but also by respecting rules. It can improve applications in climate science, public health, education, and areas where both data and expert knowledge matter."
Poster,Simple and Critical Iterative Denoising: A Recasting of Discrete Diffusion in Graph Generation,https://ICML.cc//virtual/2025/poster/44256,Yoann Boget,"Discrete Diffusion and Flow Matching models have significantly advanced generative modeling for discrete structures, including graphs. However, the dependencies of the noisy distributions across time of these models lead to error accumulation and propagation during the reverse denoising process—a phenomenon known as \emph{compounding denoising errors}. To address this problem, we propose a novel framework called \emph{Simple Iterative Denoising}, which simplifies discrete diffusion and circumvents the issue by removing dependencies on previous intermediate states in the noising process. Additionally, we enhance our model by incorporating a \emph{Critic}, which during generation selectively retains or corrupts elements in an instance based on their likelihood under the data distribution. Our empirical evaluations demonstrate that the proposed method significantly outperforms existing discrete diffusion baselines in graph generation tasks.","This paper looks at denoising models for structured data like text or molecules—where the information is made up of distinct elements, such as words or atoms. These models work by intentionally corrupting the data—for example, hiding words in a sentence or altering parts of a molecule—and then training a model to reconstruct the original version. Once trained, the model can generate text from a fully masked sentence or suggest molecular structures starting from random atoms. However, at the beginning of this reconstruction process, the data is highly corrupted, so the model’s predictions are often weak. Previous models relied heavily on these early guesses, which led to error propagation. This paper introduces a simple approach to remove that dependency, resulting in significantly better performance."
Poster,SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning,https://ICML.cc//virtual/2025/poster/43722,"Tianjian Li, Daniel Khashabi","Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off-policy data for preference learning, others indicate that the advantages of on-policy data are task-dependent, highlighting the need for a systematic exploration of their interplay.In this work, we show that on-policy and off-policy data offer complementary strengths: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on subjective tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SimpleMix, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SimpleMix substantially improves language model alignment. Specifically, SimpleMix improves upon on-policy DPO and off-policy DPO by an average of 6.03 on Alpaca Eval 2.0. Moreover, it surpasses prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05. These findings validate the effectiveness and efficiency of SimpleMix for enhancing preference-based alignment.","Language models, like ChatGPT, get better at giving helpful and accurate responses when trained using human preferences—meaning they learn what responses people prefer. However, there's a debate about which data works best for training these models: on-policy data (generated by the same model you're improving) or off-policy data (from different, often publicly available, models).In this study, we discovered that both types of data have unique strengths. On-policy data is especially good for tasks with clear, right-or-wrong answers, like solving math problems or writing computer code. In contrast, off-policy data excels in tasks that are more subjective or creative, such as storytelling or giving personal recommendations.Inspired by these findings, we developed SIMPLEMIX, a straightforward method that combines both types of data to leverage their complementary advantages. SIMPLEMIX significantly improved the overall quality of responses, outperforming existing methods. Specifically, SIMPLEMIX boosted performance by about 6% compared to using just one data type and about 3% compared to previous, more complicated methods.Our approach highlights that the best outcomes don't always come from complicated solutions—sometimes, mixing existing resources intelligently is enough to yield significant improvements."
Poster,Simple Path Structural Encoding for Graph Transformers,https://ICML.cc//virtual/2025/poster/43794,"Louis Airale, Antonio Longa, Mattia Rigon, Andrea Passerini, Roberto Passerone","Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs.This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly in capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting.SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers.","The transformer architecture has achieved prominent performance on regularly-structured data such as images or text, owing both to *global attention* and *positional encoding*.While the former is a measure of the *alignment* between two tokens in a text, or two pixels in an image, the latter informs on the position of each element (text token/pixel) in the input data.Due to the absence of obvious ordering, devising general-purpose positional encodings for graph data, such as social networks, molecules, citation graphs, or electrical circuits, remains an open problem.In this work we highlight failure cases of one of the most successful positional encodings for graph transformers, based on *random walks*, and propose to consider instead *simple paths*, that is, the count and the length of all direct paths between any two points in a graph.This novel *relative positional encoding* overcomes several limitations of random walks, and allows the discovery of certain graph structures such as cycles, which makes it a particularly valuable tool for deep learning on molecular data."
