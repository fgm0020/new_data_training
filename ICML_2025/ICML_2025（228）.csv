type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Position: Editing Large Language Models Poses Serious Safety Risks,https://ICML.cc//virtual/2025/poster/40144,"Paul Youssef, Zhixue Zhao, Daniel Braun, Jörg Schlötterer, Christin Seifert","Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.","Large language models (LLMs), such as the one that powers ChatGPT, store vast amounts of information about the world. This information sometimes needs to be updated with new knowledge. So-called 'knowledge editing methods' can adapt LLMs to 'know' new facts.In this paper, we consider the potential misuse of knowledge editing methods. First, we point out that these methods are easy to use, inexpensive to implement and difficult to detect, making them appealing for malicious actors. We then highlight several risks: these methods could be used to spread misinformation, manipulate opinions or cause LLMs to provide harmful answers. These altered LLMs can easily be uploaded and shared online without any checks to ensure they have not been tampered with. We urge LLM developers to build more secure systems that are harder to manipulate and strengthen safeguards. They should also develop tools that can reveal and undo changes. We also emphasize the need for greater awareness of the risks among researchers, developers, policymakers, and the public."
Poster,Position: Enough of Scaling LLMs! Lets Focus on Downscaling,https://ICML.cc//virtual/2025/poster/40165,"Yash Goel, Ayan Sengupta, Tanmoy Chakraborty","We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.","Artificial intelligence (AI) tools like ChatGPT, Gemini and Perplexity AI work by training large language models (LLMs) on massive amounts of data using powerful computers. While making these models bigger has led to major improvements, it also comes with high costs, huge energy consumption, large carbon footprints, and limited access for smaller companies and researchers.In this paper, we argue that instead of always building bigger models, we should focus on making smaller, smarter, and more efficient ones. We introduce “downscaling laws,” which are principles to guide the design of compact AI models that still perform well. Our approach includes selecting better-quality training data, trimming unnecessary parts of the models, and combining smaller models to achieve strong performance.This shift can make AI more affordable, reduce its environmental impact, and help ensure that more people can benefit from it."
Poster,Position: Evaluating Generative AI Systems Is a Social Science Measurement Challenge,https://ICML.cc//virtual/2025/poster/40182,"Hanna Wallach, Meera Desai, A. Feder Cooper, Angelina Wang, Chad Atalla, Solon Barocas, Su Lin Blodgett, Alexandra Chouldechova, Emily Corvi, P. Alex Dow, Jean Garcia-Gathright, Alexandra Olteanu, Nicholas Pangakis, Stefanie Reed, Emily Sheng, Dan Vann, Jennifer Wortman Vaughan, Matthew Vogel, Hannah Washington, Abigail Z. Jacobs","The measurement tasks involved in evaluating generative AI (GenAI) systems lack sufficient scientific rigor, leading to what has been described as ""a tangle of sloppy tests [and] apples-to-oranges comparisons"" (Roose, 2024). In this position paper, we argue that the ML community would benefit from learning from and drawing on the social sciences when developing and using measurement instruments for evaluating GenAI systems. Specifically, our position is that evaluating GenAI systems is a social science measurement challenge. We present a four-level framework, grounded in measurement theory from the social sciences, for measuring concepts related to the capabilities, behaviors, and impacts of GenAI systems. This framework has two important implications: First, it can broaden the expertise involved in evaluating GenAI systems by enabling stakeholders with different perspectives to participate in conceptual debates. Second, it brings rigor to both conceptual and operational debates by offering a set of lenses for interrogating validity.","We've all seen news headlines claiming that GenAI systems can diagnose illnesses, solve difficult math problems, and write code. We've also seen coverage of risks like memorizing copyrighted data and generating harmful content. But what's the evidence behind these claims? And should we trust it? Much of the evidence comes from ``GenAI evaluations.’’ However, current evaluation practices lack sufficient scientific rigor. A key challenge is that the concepts of interest—like ""diagnostic ability,"" ""memorization,"" and ""harmful content""—are much more abstract than the concepts—like prediction accuracy—that underpinned ML evaluations before the GenAI era. Indeed, these new concepts are much more reminiscent of the abstract, contested concepts studied in the social sciences, such as democracy in political science and personality traits in psychometrics. We describe how adopting a variant of the framework that social scientists use for measuring abstract, contested concepts can improve the scientific rigor of GenAI evaluations. A key part of this framework is clearly defining *what* will be measured and *why* separately from implementation decisions about *how* it will be measured. Separating the what and the why from the how allows us to meaningfully interrogate the validity of evaluations. This allows us to spot, for example, when a concept like memorization is defined in a way that is misaligned with the definitions most relevant to assessing copyright infringement, or when two benchmarks for measuring the stereotyping behaviors of LLMs implement very different understandings of those behaviors. The framework we propose makes it easier to identify and avoid poor evaluations, thereby forming an important step toward maturing current evaluation practices into a rigorous science of GenAI evaluations."
Poster,Position: Explainable AI Cannot Advance Without Better User Studies,https://ICML.cc//virtual/2025/poster/40169,"Matej Pičulin, Bernarda Petek, Irena Ograjenšek, Erik Štrumbelj","In this position paper, we argue that user studies are key to understanding the value of explainable AI methods, because the end goal of explainable AI is to satisfy societal desiderata. We also argue that the current state of user studies is detrimental to the advancement of the field. We support this argument with a review of general and explainable AI-specific challenges, as well as an analysis of 607 explainable AI papers featuring user studies. We demonstrate how most user studies lack reproducibility, discussion of limitations, comparison with a baseline, or placebo explanations and are of low fidelity to real-world users and application context. This, combined with an overreliance on functional evaluation, results in a lack of understanding of the value explainable AI methods, which hinders the progress of the field. To address this issue, we call for higher methodological standards for user studies, greater appreciation of high-quality user studies in the AI community, and reduced reliance on functional evaluation.","In this paper we argue that user studies are essential for assessing the value of explainable AI, as the ultimate goal of explainable AI is to meet societal needs. We show how current user studies fall short, as they are poorly designed, hard to reproduce, and rarely reflect real-world users or applications. This, combined with an overreliance on evaluation without users, weakens our understanding of explainable AI’s actual impact. We call for better user studies, more recognition of quality human subject-based research, and less dependence on purely functional evaluations."
Poster,Position: Formal Mathematical Reasoning—A New Frontier in AI,https://ICML.cc//virtual/2025/poster/40156,"Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, Dawn Song","AI for Mathematics (AI4Math) is intellectually intriguing and is crucial for AI-driven system design and verification. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. This position paper advocates formal mathematical reasoning as an indispensable component in future AI for math, formal verification, and verifiable generation. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success.","Many breakthroughs in artificial intelligence (AI) have come from teaching computers to solve math problems written in everyday language. But as math gets more advanced, this informal approach runs into roadblocks —there’s not enough good data to train on, and it’s hard to check if the answers are truly correct.Our paper makes the case for a more rigorous path: using formal mathematical systems—the kind that mathematicians and software engineers use to write precise, verifiable proofs. These systems can act like automatic proof checkers and give helpful feedback when something is wrong. We explore how combining these systems with modern AI models opens up new possibilities, from solving high-level math problems to ensuring the correctness of critical software and hardware systems. We outline the recent progress, challenges ahead, and what success could look like in the coming years.Formal reasoning might sound technical, but it could be key to building trustworthy AI that can reason reliably—in math, in code, and in the world."
Poster,Position: Future Research and Challenges Remain Towards AI for Software Engineering,https://ICML.cc//virtual/2025/poster/40141,"Alex Gu, Naman Jain, Wen-Ding Li, Manish Shetty Molahalli, Kevin Ellis, Koushik Sen, Armando Solar-Lezama","AI for software engineering has made remarkable progress, becoming a notable success within generative AI. Despite this, achieving fully automated software engineering is still a significant challenge, requiring research efforts across both academia and industry. In this position paper, our goal is threefold. First, we provide a taxonomy of measures and tasks to categorize work towards AI software engineering. Second, we outline the key bottlenecks permeating today's approaches. Finally, we highlight promising paths towards making progress on these bottlenecks to guide future research in this rapidly maturing field.","AI for software engineering has made remarkable progress, becoming a notable success within generative AI. Despite this, achieving fully automated software engineering is still a significant challenge, requiring research efforts across both academia and industry. In this position paper, our goal is threefold. First, we provide a taxonomy of measures and tasks to categorize work towards AI software engineering. Second, we outline the key bottlenecks permeating today's approaches. Finally, we highlight promising paths towards making progress on these bottlenecks to guide future research in this rapidly maturing field."
Poster,Position: General Intelligence Requires Reward-based Pretraining,https://ICML.cc//virtual/2025/poster/40143,"Seungwook Han, Jyothish Pari, Samuel Gershman, Pulkit Agrawal","Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs. To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a \textit{reasoning prior} for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.","Today’s AI models are good at answering questions and solving familiar problems, but they often fail when faced with new, unfamiliar challenges. We believe this is because current models mix up two separate skills: knowledge and reasoning. Our research shows that although these models seem smart, their reasoning often just copies patterns from training data instead of truly understanding how to think.To fix this, we propose a fundamental change to the common pretraining paradigm. Instead of just predicting the next word like most current models do, we propose to train models to *reason* from the ground up using trial-and-error learning—reinforcement learning. We start with a set of simple, synthetic tasks that teach the model how to think step by step, and then move on to real-world language tasks. We also propose to keep the model’s focus small to avoid it relying on spurious features and to have an architectural separation between the memory and the reasoning model.In the long run, by decoupling reasoning from knowledge, we hope to achieve models that can actually generalize to new problems it hasn’t seen before."
Poster,Position: Generative AI Regulation Can Learn from Social Media Regulation,https://ICML.cc//virtual/2025/poster/40119,Ruth Elisabeth Appel,"There is strong agreement that generative AI should be regulated, but strong disagreement on how to approach regulation. While some argue that AI regulation should mostly rely on extensions of existing laws, others argue that entirely new laws and regulations are needed to ensure that generative AI benefits society. In this position paper, we argue that the debates on generative AI regulation can be informed by evidence on social media regulation. For example, AI companies have faced allegations of political bias which resemble the allegations social media companies have faced. First, we compare and contrast the affordances of generative AI and social media to highlight their similarities and differences. Then, we discuss four specific policy recommendations based on the evolution of social media and their regulation: (1) counter bias and perceptions thereof (e.g., via transparency, oversight boards, researcher access, democratic input), (2) address specific regulatory concerns (e.g., youth wellbeing, election integrity) and invest in trust and safety, (3) promote computational social science research, and (4) take on a more global perspective. Applying lessons learnt from social media regulation to generative AI regulation can save effort and time, and prevent avoidable mistakes.","There is strong disagreement about how generative AI should be regulated.This paper argues that generative AI regulation can be informed by social media regulation. Concrete lessons include the need to (1) counter bias or perceptions of bias, for example via transparency measures, (2) invest in trust and safety, (3) promote computational social science research, and (4) take on a more global perspective.Learning from social media regulation can help save time and effort, and prevent avoidable mistakes when it comes to AI regulation."
Poster,Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks,https://ICML.cc//virtual/2025/poster/40105,"Maya Bechler-Speicher, Ben Finkelshtein, Fabrizio Frasca, Luis Müller, Jan M Tönshoff, Antoine Siraudin, Viktor Zaverkin, Michael Bronstein, Mathias Niepert, Bryan Perozzi, Mikhail Galkin, Christopher Morris","While machine learning on graphs has demonstrated promise in drug design and molecular property prediction, significant benchmarking challenges hinder its further progress and relevance. Current benchmarking practices often lack focus on transformative, real-world applications, favoring narrow domains like two-dimensional molecular graphs over broader, impactful areas such as combinatorial optimization, databases, or chip design. Additionally, many benchmark datasets poorly represent the underlying data, leading to inadequate abstractions and misaligned use cases. Fragmented evaluations and an excessive focus on accuracy further exacerbate these issues, incentivizing overfitting rather than fostering generalizable insights. These limitations have prevented the development of truly useful graph foundation models. This position paper calls for a paradigm shift toward more meaningful benchmarks, rigorous evaluation protocols, and stronger collaboration with domain experts to drive impactful and reliable advances in graph learning research, unlocking the potential of graph learning.","Graph learning is a field of machine learning concerned with processing objects describing relationships between entities, mathematically known as graphs. For example, a ""molecular graph"" represents a molecule by describing the relationships between its atoms, i.e., their chemical bonds. An example of a graph learning task is to predict chemically relevant properties for molecular graphs.The effectiveness of machine learning methods is typically measured on benchmarks. These are sets of controlled experiments on predefined data which allow to track relevant research progress. An ideal benchmark originates from, and is closely connected to, a real-world problem: strong benchmark results should indicate research methods can be successfully applied to real-world use cases.In this position paper we claim, however, that the progress of graph learning research is currently hindered by poor benchmarks, along with an unhealthy benchmarking culture. We argue that the most popular benchmarks are overly focussed on prediction tasks over molecular graphs, with benchmarks for other promising applications lacking. Additionally, we identify benchmarks which poorly model the relationships in the underlying data or miss to connect to relevant real-world use-cases. We also underscore how newly proposed methods are often compared against baseline approaches whose performances are not measured at the best of their possibility.We support many of the above claims with experimental evidence and provide multiple examples of real-world applications for which new graph learning benchmarks would be valuable. These include the design of computer chips, weather forecasting, applications to relational databases and combinatorial optimization problems with impacts in, e.g., logistics."
Poster,Position: Graph Matching Systems Deserve Better Benchmarks,https://ICML.cc//virtual/2025/poster/40161,"Indradyumna Roy, Saswat Meher, Eeshaan Jain, Soumen Chakrabarti, Abir De","Data sets used in recent work on graph similarity scoring and matching tasks suffer from significant limitations.  Using Graph Edit Distance (GED) as a showcase, we highlight pervasive issues such as train-test leakage and poor generalization, which have misguided the community's understanding and assessment of the capabilities of a method or model.These limitations arise, in part, because preparing labeled data is computationally expensive for combinatorial graph problems.We establish some key properties of GED that enable scalable data augmentation for training, and adversarial test set generation.Together, our analysis, experiments and insights establish new, sound guidelines for designing and evaluating future neural networks, and suggest open challenges for future research.","We look at how current machine learning systems compare graphs, which is important in tasks like finding similar molecules or detecting fraud. We find that many widely-used datasets contain hidden overlaps between training and test data, sometimes nearing 100%, making it unclear whether models are truly learning meaningful patterns or just memorizing. We propose new ways to fix this and ensure fairer, more meaningful evaluation."
