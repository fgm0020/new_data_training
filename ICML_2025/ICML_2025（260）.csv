type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Robust Multi-Agent Reinforcement Learning with Stochastic Adversary,https://ICML.cc//virtual/2025/poster/44719,"Ziyuan Zhou, Guanjun Liu, Mengchu Zhou, Guo","The performance of models trained by Multi-Agent Reinforcement Learning (MARL) is sensitive to perturbations in observations, lowering their trustworthiness in complex environments. Adversarial training is a valuable approach to enhance their performance robustness. However, existing methods often overfit to adversarial perturbations of observations and fail to incorporate prior information about the policy adopted by their protagonist agent, i.e., the primary one being trained. To address this important issue, this paper introduces Adversarial Training with Stochastic Adversary (ATSA), where the proposed adversary is trained online alongside the protagonist agent. The former consists of Stochastic Director (SDor) and SDor-guided generaTor (STor). SDor performs policy perturbations by minimizing the expected team reward of protagonists and maximizing the entropy of its policy, while STor generates adversarial perturbations of observations by following SDor's guidance. We prove that SDor's soft policy converges to a global optimum according to factorized maximum-entropy MARL and leads to the optimal adversary. This paper also introduces an SDor-STor loss function to quantify the difference between a) perturbations in the agent's policy and b) those advised by SDor. We evaluate our ATSA on StarCraft II tasks and autonomous driving scenarios, demonstrating that a) it is robust against diverse perturbations of observations while maintaining outstanding performance in perturbation-free environments, and b) it outperforms the state-of-the-art methods.","Multi-agent reinforcement learning enables multiple agents to learn and coordinate through interactions with their environment. However, these systems are highly sensitive to small changes or perturbations in their observations, such as sensor noise or adversarial manipulation. This vulnerability undermines their trustworthiness in real-world applications like autonomous driving or multi-robot systems. To address this, this work proposes Adversarial Training with Stochastic Adversary (ATSA) as a new framework that improves a multi-agent system’s robustness against such perturbations. ATSA jointly trains the main learning agent (the protagonist) and a stochastic adversary composed of two modules: 1) Stochastic Director (SDor) that perturbs the agent's policy, and 2) SDor-guided generaTor (STor) that crafts adversarial observations. The former is trained to minimize the agent’s team reward while maximizing entropy, leading to diverse adversarial behaviors. We show that our approach outperforms existing methods across a range of environments, including StarCraft II and driving simulations. Our findings suggest that training agents with stochastic adversaries improves a multi-agent system’s robustness."
Poster,Robust Multi-bit Text Watermark with LLM-based Paraphrasers,https://ICML.cc//virtual/2025/poster/46008,"Xiaojun Xu, jinghan jia, Yuanshun Yao, Yang Liu, Hang Li","We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation.","We propose to inject watermarks into a piece of text by rewriting them. The rewritten version of the text will have same semantic meaning with the original text, while the watermark code can be decoded from the text using a decoder model. The core technical problem of our method is how to have (1) a good rewriter that can inject the watermark while keeping text meaning and (2) a good decoder that can decode the injected watermark code. In our work, both the rewriter and the decoder are implemented using large language model-based models. The two models are trained together, with the training goal defined to improve both detectabililty (i.e. the watermark can be decoded) and fidelity (i.e. the rewritten text has same semantic meaning). Through experiments, we show that the watermark can be detected in over 99.99% cases and it is hard to tell the difference between rewritten sentence and original sentence from human eyes."
Poster,Robust Multimodal Large Language Models Against Modality Conflict,https://ICML.cc//virtual/2025/poster/45224,"Zongmeng Zhang, Wengang Zhou, Jie Zhao, Houqiang Li","Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.","Multimodal Large Language Models (MLLMs) are a type of AI technology that can understand images and answer questions related to them. These models have shown near-human intelligence in many tasks. However, they still often make mistakes, such as generating irrelevant or nonsensical answers, a phenomenon sometimes referred to as ""hallucination."" Our research focuses on reducing these errors to make MLLMs more reliable and accurate.We discovered that one key issue occurs when the question asked by a person conflicts with the content of the image. For instance, if an image shows a small dog but the question mistakenly asks, ""What color is the cat in this picture?"" the model does not recognize this conflict and provides an incorrect answer. To address this, we created specialized datasets to simulate such conflicting scenarios and trained MLLMs to better handle these situations. This additional training enables the model to recognize conflicts and give correct answers more effectively.By identifying this key source of errors and proposing a targeted solution, our work improves the accuracy and reliability of MLLMs. This advancement may accelerate the practical applications of these models in areas such as education, healthcare, and more, where accurate and dependable AI systems are crucial."
Poster,Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs,https://ICML.cc//virtual/2025/poster/46284,Greyson Brothers,"We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based *adaptive pooling* method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks.","How do self-driving cars identify what’s important, like picking out a pedestrian from a sea of vehicles? Most modern AI systems use “transformer” models, which condense many pieces of data into one meaningful summary. This step, called pooling, is critical for making decisions. But in real-world tasks, where much of the data is distracting or noisy, common pooling methods like averaging or picking the strongest signal may not work as intended – causing our self-driving car to ignore that stray pedestrian. Our paper shows that these pooling methods do indeed fail when there’s a large number of distractions. We then predict that a lesser-known technique, called adaptive pooling, can prevent such a performance collapse, even in the presence of many distractors. It does this by using attention to learn which parts of the input matter and what to ignore – like tuning out a noisy crowd to listen to a single voice.Our findings demonstrate that adaptive pooling can closely match the best possible summary of the inputs. More importantly, we show that simply swapping out the old methods for adaptive pooling can significantly improve the reliability and trustworthiness of transformer-based models in many applications, even when their inputs are quite messy."
Poster,Robust Offline Reinforcement Learning with Linearly Structured $f$-Divergence Regularization,https://ICML.cc//virtual/2025/poster/45863,"Cheng Tang, Zhishuai Liu, Pan Xu","The Robust Regularized Markov Decision Process (RRMDP) is proposed to learn policies robust to dynamics shifts by adding regularization to the transition dynamics in the value function. Existing methods mostly use unstructured regularization, potentially leading to conservative policies under unrealistic transitions. To address this limitation, we propose a novel framework, the $d$-rectangular linear RRMDP ($d$-RRMDP), which introduces latent structures into both transition kernels and regularization. We focus on offline reinforcement learning, where an agent learns policies from a precollected dataset in the nominal environment. We develop the Robust Regularized Pessimistic Value Iteration (R2PVI) algorithm that employs linear function approximation for robust policy learning in $d$-RRMDPs with $f$-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, demonstrating that these bounds are influenced by how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. We establish information-theoretic lower bounds to verify that our algorithm is near-optimal. Finally, numerical experiments validate that R2PVI learns robust policies and exhibits superior computational efficiency compared to baseline methods.","When we teach computers to make decisions in uncertain situations—like guiding a robot or recommending medical treatments—we want them to make good choices even when things don’t go exactly as expected. However, there’s a tricky balance: if we prepare for every possible scenario, the system may become too cautious and perform poorly in real situations.In our work, we address this challenge by introducing a new method that helps computers strike a better balance between safety and effectiveness. Instead of assuming that every possible change is equally likely, we add structure to how the computer models uncertainty. This helps it focus on realistic changes while ignoring highly unlikely ones. We call this new framework $d$-RRMDP. We also design a new algorithm called R2PVI, which can learn robust decision-making strategies from pre-collected data—without needing new interactions with the environment—and it does so efficiently.Through both theoretical analysis and experiments, we show that our method learns decision strategies that are both robust to change and more practical than existing approaches. Our work not only introduces new tools but also provides insights that can guide future research in robust and reliable decision-making systems."
Poster,Robust Reward Alignment via Hypothesis Space Batch Cutting,https://ICML.cc//virtual/2025/poster/44015,"Zhixian Xie, Haode Zhang, Yizhe Feng, Wanxin Jin","Reward design in reinforcement learning and optimal control is challenging. Preference-based alignment addresses this by enabling agents to learn rewards from ranked trajectory pairs provided by humans. However, existing methods often struggle from poor robustness to unknown false human preferences. In this work, we propose a robust and efficient reward alignment method based on a novel and geometrically interpretable perspective: hypothesis space batched cutting. Our method iteratively refines the reward hypothesis space through “cuts” based on batches of human preferences. Within each batch, human preferences, queried based on disagreement, are grouped using a voting function to determine the appropriate cut, ensuring a bounded human query complexity. To handle unknown erroneous preferences, we introduce a conservative cutting method within each batch, preventing erroneous human preferences from making overly aggressive cuts to the hypothesis space. This guarantees provable robustness against false preferences, while eliminating the need to explicitly identify them. We evaluate our method in a model predictive control setting across diverse tasks. The results demonstrate that our framework achieves comparable or superior performance to state-of-the-art methods in error-free settings while significantly outperforming existing methods when handling a high percentage of erroneous human preferences.","Teaching AI agents to make good decisions—like balancing a robot or driving a car—often requires giving them a ""reward function"" that tells them how well they are doing. But designing this reward function by hand is difficult and laborious. A promising alternative is to learn the reward from human preference feedback. However, this approach struggles when people make mistakes in their rankings, which is common.We developed a new method that helps machines learn from human preferences, even when some of them are wrong. Our idea is to think of all the possible reward functions as a space, and gradually narrow it down using batches of human preferences. To make this process robust, we designed a way to be conservative when human preferences are inconsistent, so that mistakes don’t lead the machine in the wrong direction.Our approach helps machines learn more safely and effectively from humans, without requiring perfect feedback. This is especially important as AI systems are used more often in real-world settings, where human guidance is valuable but not always reliable. Our experiments show that our method not only works well when feedback is accurate, but also performs much better than existing techniques when there are lots of mistakes in the human input."
Poster,Robust Secure Swap: Responsible Face Swap With Persons of Interest Redaction and Provenance Traceability,https://ICML.cc//virtual/2025/poster/44468,"Yunshu Dai, Jianwei Fei, Fangjun Huang, Chip Hong Chang","As AI generative models evolve, face swap technology has become increasingly accessible, raising concerns over potential misuse. Celebrities may be manipulated without consent, and ordinary individuals may fall victim to identity fraud. To address these threats, we propose Secure Swap, a method that protects persons of interest (POI) from face-swapping abuse and embeds a unique, invisible watermark into nonPOI swapped images for traceability. By introducing an ID Passport layer, Secure Swap redacts POI faces and generates watermarked outputs for nonPOI. A detachable watermark encoder and decoder are trained with the model to ensure provenance tracing. Experimental results demonstrate that Secure Swap not only preserves face swap functionality but also effectively prevents unauthorized swaps of POI and detects different embedded model's watermarks with high accuracy. Specifically, our method achieves a 100% success rate in protecting POI and over 99% watermark extraction accuracy for nonPOI. Besides fidelity and effectiveness, the robustness of protected models against image-level and model-level attacks in both online and offline application scenarios is also experimentally demonstrated.","This paper introduces a new method to make face-swapping technology safer and more responsible. We designed a system that prevents important individuals from having their faces swapped without permission. For other cases, our method adds invisible marks into the generated images, so it’s possible to trace where and how the image was created. This makes it easier to track fake or edited content online. Our system is strong against various attacks and can keep working even when someone tries to trick or break it. In the future, we plan to make this method work with the latest image generation tools to ensure that face-swapping is used in fair and ethical ways."
Poster,Robust Sparsification via Sensitivity,https://ICML.cc//virtual/2025/poster/43947,"Chansophea Wathanak In, Yi Li, David Woodruff, Xuan Wu","Robustness to outliers is important in machine learning. Many classical problems, including subspace embedding, clustering, and low-rank approximation, lack scalable, outlier-resilient algorithms. This paper considers machine learning problems of the form $\min_{x\in \mathbb{R}^d} F(x)$, where $F(x)=\sum_{i=1}^n F_i(x)$, and their robust counterparts $\min_{x\in\mathbb{R}^d} F^{(m)}(x)$, where $F^{(m)}(x)$ denotes the sum of all but the $m$ largest $F_i(x)$ values. We develop a general framework for constructing $\epsilon$-coresets for such robust problems, where an $\epsilon$-coreset is a weighted subset of functions $\{F_1(x),\dots,F_n(x)\}$ that provides a $(1+\epsilon)$-approximation to $F(x)$. Specifically, if the original problem $F$ has total sensitivity $T$ and admits a vanilla $\epsilon$-coreset of size $S$, our algorithm constructs an $\epsilon$-coreset of size $\tilde{O}(\frac{mT}{\epsilon})+S$ for the robust objective $F^{(m)}$. This coreset size can be shown to be near-tight for $\ell_2$ subspace embedding. Our coreset algorithm has scalable running time and leads to new or improved algorithms for the robust optimization problems. Empirical evaluations demonstrate that our coresets outperform uniform sampling on real-world data sets.","The existence of outliers presents formidable challenges for many machine learning problems, including clustering, subspace embedding, and low-rank approximation. This paper considers robust coresets for all the above machine learning problems, which reduce the input dataset into a small subset, thus providing a scalable and robust solution. We design a general framework to construct a robust coreset for any machine learning problem that admits a bounded total sensitivity and vanilla coreset. Our coreset is near-optimal for subspace embedding. Our experimental results show that our coresets outperform the uniform sampling benchmark on real-world data sets."
Poster,Robust Spatio-Temporal Centralized Interaction for OOD Learning,https://ICML.cc//virtual/2025/poster/43964,"Jiaming Ma, Binwu Wang, Pengkun Wang, Zhengyang Zhou, Xu Wang, Yang Wang","Recently, spatiotemporal graph convolutional networks have achieved dominant performance in spatiotemporal prediction tasks. However, most models relying on node-to-node messaging interaction exhibit sensitivity to spatiotemporal shifts, encountering out-of-distribution (OOD) challenges. To address these issues, we introduce \textbf{\underline{S}}patio-\textbf{\underline{T}}emporal \textbf{\underline{O}}OD \textbf{\underline{P}}rocessor (STOP), which employs a centralized messaging mechanism along with a message perturbation mechanism to facilitate robust spatiotemporal interactions. Specifically, the centralized messaging mechanism integrates Context-Aware Units for coarse-grained spatiotemporal feature interactions with nodes, effectively blocking traditional node-to-node messages. We also implement a message perturbation mechanism to disrupt this messaging process, compelling the model to extract generalizable contextual features from generated variant environments. Finally, we customize a spatiotemporal distributionally robust optimization approach that exposes the model to challenging environments, thereby further enhancing its generalization capabilities. Compared with 14 baselines across six datasets, STOP achieves up to \textbf{17.01\%} improvement in generalization performance and \textbf{18.44\%} improvement in inductive learning performance. The code is available at https://github.com/PoorOtterBob/STOP.","Spatiotemporal data, such as traffic patterns or air quality measurements, often needs to be predicted for decision-making in real-world applications. However, existing models that rely heavily on direct interactions between nodes in a graph often struggle when the data environment changes unexpectedly—a challenge known as out-of-distribution (OOD) issues. To address these challenges, we developed a new model called STOP (Spatio-Temporal OOD Processor).STOP uses a novel centralized messaging system, which avoids the limitations of traditional node-to-node communication by instead focusing on higher-level, context-aware interactions. Additionally, to make the model more robust, STOP deliberately disrupts its own messaging process during training, forcing it to learn more generalizable patterns that can adapt to new environments. Finally, we use a special optimization approach that further strengthens the model by exposing it to challenging and diverse data environments during training.When tested on six different datasets, STOP outperformed 14 existing baseline models, improving generalization performance by up to 17.01% and inductive learning performance by up to 18.44%. This means STOP is better equipped to handle unexpected changes in data environments, making it a powerful tool for spatiotemporal predictions. The code for STOP is publicly available on GitHub for further research and application."
Poster,RobustZero: Enhancing MuZero Reinforcement Learning Robustness to State Perturbations,https://ICML.cc//virtual/2025/poster/46000,"Yushuai Li, Hengyu Liu, Torben Pedersen, Yuqiang He, Kim Larsen, Lu Chen, Christian Jensen, Jiachen Xu, TIANYI LI","The MuZero reinforcement learning method has achieved superhuman performance at games, and advances that enable MuZero to contend with complex actions now enable use of MuZero-class methods in real-world decision-making applications. However, some real-world applications are susceptible to state perturbations caused by malicious attacks and noisy sensors. To enhance the robustness of MuZero-class methods to state perturbations, we propose RobustZero, the first MuZero-class method that is $\underline{robust}$ to worst-case and random-case state perturbations, with $\underline{zero}$  prior knowledge of the environment’s dynamics. We present a training framework for RobustZero that features a self-supervised representation network, targeting the generation of a consistent initial hidden state, which is key to obtain consistent policies before and after state perturbations, and it features a unique loss function that facilitates robustness. We present an adaptive adjustment mechanism to enable model update, enhancing robustness to both worst-case and random-case state perturbations. Experiments on two classical control environments, three energy system environments, three transportation environments, and four Mujoco environments demonstrate that RobustZero can outperform state-of-the-art methods at defending against state perturbations.","MuZero, a recent reinforcement learning (RL) method, has achieved remarkable success in games, surpassing human performance. Its strengths have enabled its adoption in real-world decision-making tasks, e.g., autonomous driving and voltage control. However, in these settings, systems often encounter state perturbations—errors in input states caused by sensor noise or malicious attacks. These perturbations can mislead the MuZero agent, leading to suboptimal or unsafe decisions.To address this challenge, we propose RobustZero, a novel robust RL method that extends MuZero to defend against state perturbations. RobustZero incorporates the contrastive learning and an adaptive adjustment mechanism to produce consistent and robust policies before and after perturbations. Notably, RobustZero consistently outperforms existing methods, particularly in environments with noisy or adversarial inputs.Our results highlight the importance of robustness in RL and provide insights into designing agents that remain reliable even under imperfect observations."
