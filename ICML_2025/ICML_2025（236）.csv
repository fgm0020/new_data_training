type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Primitive Vision: Improving Diagram Understanding in MLLMs,https://ICML.cc//virtual/2025/poster/44142,"Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng, Piotr Koniusz, Kai Zou, Anton Hengel, Yuan Xue","Mathematical diagrams have a distinctive structure. Standard feature transforms designed for natural images (e.g., CLIP) fail to process them effectively, limiting their utility in multimodal large language models (MLLMs). Current efforts to improve MLLMs have primarily focused on scaling mathematical visual instruction datasets and strengthening LLM backbones, yet fine‐grained visual recognition errors remain unaddressed. Our systematic evaluation on the visual grounding capabilities of state‐of‐the‐art MLLMs highlights that fine‐grained visual understanding remains a crucial bottleneck in visual mathematical reasoning (GPT-4o exhibits a 70% grounding error rate, and correcting these errors improves reasoning accuracy by 12%). We thus propose a novel approach featuring a geometrically‐grounded vision encoder and a feature router that dynamically selects between hierarchical visual feature maps. Our model accurately recognizes visual primitives and generates precise visual prompts aligned with the language model’s reasoning needs. In experiments, PRIMITIVE-Qwen2.5-7B outperforms other 7B models by 12% on MathVerse and is on par with GPT-4V on MathVista. Our findings highlight the need for better fine‐grained visual integration in MLLMs. Code is available at github.com/AI4Math-ShanZhang/SVE-Math.","Mathematical diagrams—like those showing geometric shapes or formula illustrations—are common in textbooks and exams, but most AI models struggle to “see” them correctly, leading to mistakes when solving the problems they illustrate. To address this, we built a specialized computer “eye” (a vision module) that is trained to recognize the individual lines, angles, and shapes in a math diagram rather than treating it like an ordinary photograph. This module works together with a language‐understanding system, sending it clear, fine‐grained visual cues so that the AI can reason about the math content properly. In tests, our approach helped the AI reduce errors by more than 10 percent when answering diagram‐based questions. By teaching AI to interpret diagrams more accurately, we make it easier to build digital tutors, automated grading systems, and other educational tools that rely on understanding complex visual math content."
Poster,Primphormer: Efficient Graph Transformers with Primal Representations,https://ICML.cc//virtual/2025/poster/44528,"Mingzhen He, Ruikai Yang, Hanling Tian, Youmei Qiu, Xiaolin Huang","Graph Transformers (GTs) have emerged as a promising approach for graph representation learning. Despite their successes, the quadratic complexity of GTs limits scalability on large graphs due to their pair-wise computations. To fundamentally reduce the computational burden of GTs, we propose a primal-dual framework that interprets the self-attention mechanism on graphs as a dual representation. Based on this framework, we develop Primphormer, an efficient GT that leverages a primal representation with linear complexity. Theoretical analysis reveals that Primphormer serves as a universal approximator for functions on both sequences and graphs, while also retaining its expressive power for distinguishing non-isomorphic graphs.Extensive experiments on various graph benchmarks demonstrate that Primphormer achieves competitive empirical results while maintaining a more user-friendly memory and computational costs.","In this paper, we propose a method to reduce the heavy computation in graph Transformers. We investigate a new optimization problem whose dual representation coincides with the standard attention mechanism. This new optimization indicates a new representation in the primal space, where the heavy quadratic computation could be avoided, enhancing efficiency."
Poster,Principal-Agent Bandit Games with Self-Interested and Exploratory Learning Agents,https://ICML.cc//virtual/2025/poster/43545,"Junyan Liu, Lillian Ratliff","We study the repeated principal-agent bandit game, where the principal indirectly explores an unknown environment by incentivizing an agent to play arms. Unlike prior work that assumes a greedy agent with full knowledge of reward means, we consider a self-interested learning agent who iteratively updates reward estimates and may explore arbitrarily with some probability.As a warm-up, we first consider a self-interested learning agent without exploration.We propose algorithms for both i.i.d. and linear reward settings with bandit feedback in a finite horizon $T$, achieving regret bounds of $\widetilde{O}(\sqrt{T})$ and $\widetilde{O}(T^{\frac{2}{3}})$, respectively. Specifically, these algorithms rely on a novel elimination framework coupled with new search algorithms which accommodate the uncertainty from the agent's learning behavior. We then extend the framework to handle an exploratory learning agent and develop an algorithm to achieve a $\widetilde{O}(T^{\frac{2}{3}})$ regret bound in i.i.d. reward setup by enhancing the robustness of our elimination framework to the potential agent exploration. Finally, when our agent model reduces to that in (Dogan et al., 2023a), we propose an algorithm based on our robust framework, which achieves a $\widetilde{O}(\sqrt{T})$ regret bound, significantly improving upon their $\widetilde{O}(T^{\frac{11}{12}})$ bound.","This paper studies the principal-agent bandit games where the principal (e.g., platform) incentivizes an agent (e.g., buyer) to play arms even when the agent is learning over time and may not always act predictably. We develop new strategies that help the principal to provide better incentives, even when the agent explores randomly or is uncertain. These strategies are more reliable and work better than previous approaches, especially in more complex situations."
Poster,Principled Algorithms for Optimizing Generalized Metrics in Binary Classification,https://ICML.cc//virtual/2025/poster/44895,"Anqi Mao, Mehryar Mohri, Yutao Zhong","In applications with significant class imbalance or asymmetric costs, metrics such as the $F_\beta$-measure, AM measure, Jaccard similarity coefficient, and weighted accuracy offer more suitable evaluation criteria than standard binary classification loss. However, optimizing these metrics present significant computational and statistical challenges. Existing approaches often rely on the characterization of the Bayes-optimal classifier, and use threshold-based methods that first estimate class probabilities and then seek an optimal threshold. This leads to algorithms that are not tailored to restricted hypothesis sets and lack finite-sample performance guarantees. In this work, we introduce principled algorithms for optimizing generalized metrics, supported by $H$-consistency and finite-sample generalization bounds. Our approach reformulates metric optimization as a generalized cost-sensitive learning problem, enabling the design of novel surrogate loss functions with provable $H$-consistency guarantees. Leveraging this framework, we develop new algorithms, METRO (*Metric Optimization*), with strong theoretical performance guarantees. We report the results of experiments demonstrating the effectiveness of our methods compared to prior baselines.","In many real-world situations, it's much more important to get certain predictions right than others. For example, in medical diagnosis, failing to detect a disease (a ""false negative"") is often far worse than mistakenly flagging a healthy person for more tests (a ""false positive""). Similarly, when searching for a rare but important piece of information, we want to be sure we find it, even if it means we get some irrelevant results along the way. Standard methods for training machine learning models often aren't designed for these scenarios where mistakes have unequal consequences or when one type of data is much rarer than another. They treat all errors as equally bad, which can lead to poor performance on the tasks we actually care about.Our research introduces a new, more flexible way to train machine learning models that can be directly tailored to these specific, real-world needs. Instead of using a one-size-fits-all approach, we've developed a method that allows us to define what a ""good"" outcome looks like for a particular problem and then directly teach the learning algorithm to optimize for that goal. This avoids a common two-step process where a standard model is first trained and then tweaked, which can be inefficient and unreliable.We call our new method METRO (Metric Optimization). We've proven mathematically that this approach is reliable and effective. In experiments, models trained with METRO outperformed existing methods on these specialized tasks, showing that our technique is a powerful tool for building more practical and effective machine learning systems."
Poster,Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples,https://ICML.cc//virtual/2025/poster/43915,"chengqian gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu","The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: *Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity*. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce *Selective DPO*, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16\% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, surpassing a series of DPO variants with different algorithmic adjustments. These results together illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO","Large language model (LLM) alignment is not a *more data is always better* game. We discover that examples are learned in a consistent order---across different runs and training data---reflecting an intrinsic difficulty tied to model capacity (quantified by validation loss), and that the hardest slice---those lying beyond the model’s reach---actually degrades alignment performance. Dropping these hardest cases and training only on the rest is a tiny change but a big win. On the AlpacaEval 2 benchmark, this cut-down curriculum raises the model’s win rate by 9–16 points, beating a host of more complicated DPO variants. The lesson is clear: tune a model on what it can realistically learn."
Poster,Prior Knowledge Guided Neural Architecture Generation,https://ICML.cc//virtual/2025/poster/46429,"Jingrong Xie, Han Ji, Yanan Sun","Automated architecture design methods, especially neural architecture search, have attracted increasing attention. However, these methods naturally need to evaluate numerous candidate architectures during the search process, thus computationally extensive and time-consuming. In this paper, we propose a prior knowledge guided neural architecture generation method to generate high-performance architectures without any search and evaluation process. Specifically, in order to identify valuable prior knowledge for architecture generation, we first quantify the contribution of each component within an architecture to its overall performance. Subsequently, a diffusion model guided by prior knowledge is presented, which can easily generate high-performance architectures for different computation tasks. Extensive experiments on new search spaces demonstrate that our method achieves superior accuracy over state-of-the-art methods. For example, we only need $0.004$ GPU Days to generate architecture with $76.1\%$ top-1 accuracy on ImageNet and $97.56\%$ on CIFAR-10. Furthermore, we can find competitive architecture for more unseen search spaces, such as TransNAS-Bench-101 and NATS-Bench, which demonstrates the broad applicability of the proposed method.","We try to design architectures automatically for different tasks. However, it is hard to train a model that can generate architectures automatically with few computation costs. In this paper, we learn some high-performance architectures to obtain prior knowdge. And with this knowledge, we can train a model that can generate optimal architectures. In many datasets, our methods show good performance."
Poster,Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting,https://ICML.cc//virtual/2025/poster/44722,"Jan Schuchardt, Mina Dalirrooyfard, Jed Guzelkabaagac, Anderson Schneider, Yuriy Nevmyvaka, Stephan Günnemann","Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time series specific tasks like forecasting, since they rely on the *privacy amplification* attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this *structured subsampling* to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.","Differentially Private Stochastic Gradient Descent (DP-SGD) is the standard method for training machine learning models on sensitive data with strong formal privacy guarantees. The core principle underlying these strong privacy guarantees is amplification by subsampling: Training on randomly sampled subsets is much more private than training on an entire set of input-label pairs. But what if our training data is not simply an unstructured set, but composed of sequentially structured data like natural language or time series? What if there are no explicit labels, and we are instead training our model to predict the next sentence or the next 24 hours? We answer this question by deriving formal privacy guarantees for models that predict future information based on observed context information. In particular, we analyze the interplay of sampling sequences from a dataset, sampling shorter subsequences from these sequences, and splitting them into context and ground-truth for training. Using time series forecasting as a testbed, we experimentally demonstrate that our tight privacy guarantees enables private training on sequential data while retaining high model utility."
Poster,Privacy Amplification Through Synthetic Data: Insights from Linear Regression,https://ICML.cc//virtual/2025/poster/45176,"Clément Pierquin, Aurélien Bellet, Marc Tommasi, Matthieu Boussard","Synthetic data inherits the differential privacy guarantees of the model used to generate it. Additionally, synthetic data may benefit from privacy amplification when the generative model is kept hidden. While empirical studies suggest this phenomenon, a rigorous theoretical understanding is still lacking. In this paper, we investigate this question through the well-understood framework of linear regression. First, we establish negative results showing that if an adversary controls the seed of the generative model, a single synthetic data point can leak as much information as releasing the model itself. Conversely, we show that when synthetic data is generated from random inputs, releasing a limited number of synthetic data points amplifies privacy beyond the model's inherent guarantees. We believe our findings in linear regression can serve as a foundation for deriving more general bounds in the future.","When data holders generate synthetic data, they aim to protect individual privacy. Current methods typically involve training a model to replicate sensitive data while preserving privacy, and then releasing this model so users can generate an arbitrary amount of synthetic data. Empirical evidence suggests that keeping the model parameters hidden might offer better privacy protection than revealing them, but this has not been theoretically proven.To explore this, we studied the privacy guarantees of synthetic data generation in a simplified scenario: linear regression. Our analysis reveals that randomizing the inputs of the synthetic data generation process amplifies privacy protection, but if an adversary controls the inputs, such amplification disappears.We believe that our findings can serve as a foundation for the analysis of more complex and widely-used generative models, potentially strengthening privacy protection of synthetic data in practical scenarios."
Poster,Privacy Attacks on Image AutoRegressive Models,https://ICML.cc//virtual/2025/poster/46325,"Antoni Kowalczuk, Jan Dubiński, Franziska Boenisch, Adam Dziedzic","Image AutoRegressive generation has emerged as a new powerful paradigm with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher generation speed.However, the privacy risks associated with IARs remain unexplored, raising concerns regarding their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to the ones of DMs as reference points. Concretely, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images (with a True Positive Rate at False Positive Rate = 1% of 86.38% vs. 6.38% for DMs with comparable attacks). We leverage our novel MIA to provide dataset inference (DI) for IARs, and show that it requires as few as 6 samples to detect dataset membership (compared to 200 for DI in DMs), confirming a higher information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-*d*30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are *empirically* significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. We release the code at https://github.com/sprintml/privacy_attacks_against_iars for reproducibility.","People constantly try to find better ways of generating images using AI models. Recently, a new class of image generative models was introduced. These novel models generate images in the same way chatbots like ChatGPT generate text, treating images like sequences of words. Previously, it was common for models to generate images by iteratively removing noise, which was slow. In our work we compare these two classes of models from the perspective of data privacy. It often happens that these models, trained on vast amounts of data, leak private information. They might replicate images from their training data, or leak other types of sensitive information, for example, if they were trained on medical data.Although the new models are faster, scale better, and generate high quality images, we show that they leak significantly more information than their predecessors. In some cases they tend to leak orders of magnitude more private data, sometimes replicating hundreds of images from their training sets."
Poster,Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation,https://ICML.cc//virtual/2025/poster/45123,"Roie Reshef, Kfir Levy","This paper addresses the challenge of achieving Differential Privacy (DP) in Federated Learning (FL) under the partial-participation setting, where each machine participates in only some of training rounds.While earlier work achieved optimal performance and efficiency in full-participation scenarios, these methods could not extend effectively to cases with partial-participation.Our approach addresses this gap by introducing a novel noise-cancellation mechanism that ensures privacy without compromising convergence rates or computational efficiency.We analyze our method within the Stochastic Convex Optimization (SCO) framework and demonstrate that it achieves optimal performance for both homogeneous and heterogeneous data distributions.This work broadens the applicability of DP in FL, providing a practical and efficient solution for privacy-preserving learning in distributed systems with partial participation.","Machine Learning is a novel technique.Machines can offer data to a server, and the server uses this data to learn how to solve a task for future data.Ideally, the data given should not affect the result, but in practice, it does.By looking at the result, you can infer what the data given by the machine was, which is a privacy breach.To protect their data, the machine can distort it before sending it to the server, the more distortion we use, the more private the data become, but the less good the result is.Our research is about creating an algorithm that can get more privacy with less distortion and proving its optimality and effectiveness."
