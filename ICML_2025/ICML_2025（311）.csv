type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation,https://ICML.cc//virtual/2025/poster/45579,"Gwen Yidou-Weng, Benjie Wang, Guy Van den Broeck","As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either post-train LMs for each new attribute—expensive and inflexible—or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce **TRACE** (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable *probabilistic* reasoning and lightweight *control*. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMM’s predicted futures. This EAP is then used to reweigh the LM’s next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art detoxification results with only 20% decoding overhead, yields 76 low-resource personalized LMs within seconds, and seamlessly extends to composite attributes.","AI language models are powerful, but getting them to follow rules can be tricky. How do you make sure an AI stays polite, or create a chatbot that sounds like Taylor Swift? Current methods for controlling AI are often like having to rewire an entire skyscraper just to change the lightbulb in one office—they're slow, expensive, and impractical for each new task.We developed TRACE, a new technique that acts like a fast, simple ""crystal ball"" for the AI. At every word it writes, TRACE uses a simplified map of language to peek into thousands of potential future sentences. It checks the odds that a sentence will break a rule (like ""be non-toxic"") and uses that foresight to guide the AI's word choices in the present.This approach works. TRACE sets a new standard for preventing toxic language with very little slowdown. And because it's so adaptable, you could teach it a new personality in seconds—letting you finally create that Taylor Swift bot that actually sounds like her. It can even combine complex rules, like asking for a political speech that is also strictly non-toxic."
Poster,TraceGrad: a Framework Learning Expressive SO(3)-equivariant Non-linear Representations for Electronic-Structure Hamiltonian Prediction,https://ICML.cc//virtual/2025/poster/43826,"Shi Yin, Xinyang Pan, fengyan wang, Lixin He","We propose a  framework to combine strong non-linear expressiveness with strict SO(3)-equivariance in prediction of the electronic-structure Hamiltonian, by exploring the mathematical relationships between SO(3)-invariant and SO(3)-equivariant quantities and their representations. The proposed framework, called **TraceGrad**,  first constructs theoretical SO(3)-invariant **trace** quantities derived from the Hamiltonian targets, and use these invariant quantities as supervisory labels to guide the learning of high-quality SO(3)-invariant features. Given that SO(3)-invariance is preserved under non-linear operations, the learning of invariant features can extensively utilize non-linear mappings, thereby fully capturing the non-linear patterns inherent in physical systems. Building on this, we propose a **grad**ient-based mechanism to induce SO(3)-equivariant encodings of various degrees from the learned SO(3)-invariant features. This mechanism can incorporate powerful non-linear expressive capabilities into SO(3)-equivariant features with correspondence of physical dimensions to the regression targets, while theoretically preserving equivariant properties, establishing a strong foundation for predicting electronic-structure Hamiltonian. Experimental results on eight challenging benchmark databases demonstrate that our method achieves state-of-the-art performance in Hamiltonian prediction.","Electronic structure calculations are essential for understanding electron behavior in condensed matter systems and for predicting properties such as conductivity, magnetism, optical response, and chemical reactivity. They are widely used in the fields of materials science, chemistry, and energy research. At their core lies the solution of the Hamiltonian matrices, which yield key physical quantities such as orbital energies, band structures, and electronic wavefunctions. We propose **TraceGrad**, a novel deep learning methodology that combines strict 3D rotational equivariance with strong non-linear expressiveness. It achieves state-of-the-art accuracy in predicting electronic-structure Hamiltonians and related properties, offering great potential to advance research on computational physics and chemistry."
Poster,Tracking Most Significant Shifts in Infinite-Armed Bandits,https://ICML.cc//virtual/2025/poster/46066,"Joe Suk, Jung-hun Kim","We study an infinite-armed bandit problem where actions' mean rewards are initially sampled from a _reservoir distribution_. Most prior works in this setting focused on stationary rewards (Berry et al., 1997; Wang et al., 2008; Bonald and Proutiere, 2013; Carpentier and Valko, 2015) with the more challenging adversarial/non-stationary variant only recently studied in the context of rotting/decreasing rewards (Kim et al., 2022; 2024). Furthermore, optimal regret upper bounds were only achieved using parameter knowledge of non-stationarity and only known for certain regimes of regularity of the reservoir. This work shows the first parameter-free optimal regret bounds while also relaxing these distributional assumptions. We also study a natural notion of _significant shift_ for this problem inspired by recent developments in finite-armed MAB (Suk & Kpotufe, 2022). We show that tighter regret bounds in terms of significant shifts can be adaptively attained. Our enhanced rates only depend on the rotting non-stationarity and thus exhibit an interesting phenomenon for this problem where rising non-stationarity does not factor into the difficulty of non-stationarity.","We study the multi-armed bandit problem (used e.g. for clinical trials or recommender systems) where one chooses from infinitely many options, or _arms_, whose rewards change over time. Past work on infinite-armed bandits mostly assumed stable reward models or needed to know in advance how rewards would change. On the other hand, changing reward models are well-studied for the simpler finite-armed bandit problem.Our work introduces the first algorithm that automatically tunes itself to changing rewards without needing prior knowledge about the changes. We provide mathematical proofs that our method achieves the best possible performance guarantees (measured in a worst-case sense) in terms of the number or amount of changes in rewards. We also show how to focus only on meaningful changes in rewards, leading to faster learning when there are few changes in rewards which are _significantly_ harmful. For this, we introduce a new notion of significant changes for the infinite-armed setting, inspired by previous works in the finite-armed analogue of the problem."
Poster,Tracking The Best Expert Privately,https://ICML.cc//virtual/2025/poster/44999,"Hilal Asi, Vinod Raman, Aadirupa Saha","We design differentially private algorithms for the problem of prediction with expert advice under dynamic regret, also known as tracking the best expert. Our work addresses three natural types of adversaries, stochastic with shifting distributions, oblivious, and adaptive, and designs algorithms with sub-linear regret for all three cases. In particular, under a shifting stochastic adversary where the distribution may shift $S$ times, we provide an $\epsilon$-differentially private algorithm whose expected dynamic regret is at most $O\left( \sqrt{S T \log (NT)} + \frac{S \log (NT)}{\epsilon}\right)$, where $T$ and $N$ are the time horizon and number of experts, respectively. For oblivious adversaries, we give a reduction from dynamic regret minimization to static regret minimization, resulting in an upper bound of $O\left(\sqrt{S T \log(NT)} + \frac{S T^{1/3}\log(T/\delta) \log(NT)}{\epsilon ^{2/3}}\right)$ on the expected dynamic regret, where $S$ now denotes the allowable number of switches of the best expert. Finally, similar to static regret, we establish a fundamental separation between oblivious and adaptive adversaries for the dynamic setting:  while our algorithms show that sub-linear regret is achievable for oblivious adversaries in the high-privacy regime $\epsilon \le \sqrt{S/T}$,  we show that any $(\epsilon, \delta)$-differentially private algorithm must suffer linear dynamic regret under adaptive adversaries for $\epsilon \le \sqrt{S/T}$. Finally, to complement this lower bound, we give an $\epsilon $-differentially private algorithm that attains sub-linear dynamic regret under adaptive adversaries whenever $\epsilon \gg \sqrt{S/T}$.","In many online systems—like stock trading platforms or recommendation engines—decisions must adapt to changing environments. Traditionally, algorithms compare their performance to the best fixed decision in hindsight, but this can fail in dynamic settings where the ""best choice"" shifts over time. This paper studies how to design learning algorithms that track the best changing expert while also preserving user privacy. Specifically, we introduce differentially private algorithms that can adapt to such changes across three adversarial environments: stochastic (with shifting distributions), oblivious (fixed losses), and adaptive (strategic responses). Our algorithms achieve strong performance—called sublinear dynamic regret—in each setting while ensuring sensitive user data is protected. We also prove sharp limits: for example, learning becomes impossible if the privacy level is too strict under adaptive adversaries. This work is the first to comprehensively tackle the challenge of private, adaptive decision-making in changing environments."
Poster,Tractable Transformers for Flexible Conditional Generation,https://ICML.cc//virtual/2025/poster/44368,"Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck","Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines.","Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines."
Poster,"Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions",https://ICML.cc//virtual/2025/poster/45990,"Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, Sitan Chen","In recent years, masked diffusion models (MDMs) have emerged as a promising alternative approach for generative modeling over discrete domains. Compared to autoregressive models (ARMs), MDMs trade off complexity at training time with flexibility at inference time. At training time, they must learn to solve an exponentially large number of infilling problems, but at inference time, they can decode tokens in essentially arbitrary order. In this work we closely examine these two competing effects. On the training front, we theoretically and empirically demonstrate that MDMs indeed train on computationally intractable subproblems compared to their autoregressive counterparts. On the inference front, we show that a suitable strategy for adaptively choosing the token decoding order significantly enhances the capabilities of MDMs, allowing them to sidestep hard subproblems. On logic puzzles like Sudoku, we show that adaptive inference can boost solving accuracy in pretrained MDMs from $<7$\% to $\approx 90$\%, even outperforming ARMs that were explicitly trained via teacher forcing to learn the right order of decoding.","Standard language models write strictly left-to-right, while newer “masked diffusion” models can fill in blanks in any order—but so far, they lag. We pinpoint the bottleneck: during training, they face an exponential number of fill-in-the-mask subproblems, many of which are mathematically intractable, so learning gets hard. We found the flaw isn’t in the model itself, but in how we let it answer. At test time, we can choose which blank to reveal first, so we use a simple rule: pick the spot where the model is most confident. This one-line tweak catapults Sudoku accuracy from 7% to nearly 90% and brings similar leaps on Zebra puzzles and text-quality checks. Bottom line: training these models is tough, but smart decoding turns them into powerful, order-agnostic reasoners—no extra training required."
Poster,Training a Generally Curious Agent,https://ICML.cc//virtual/2025/poster/45106,"Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Rahman, Zico Kolter, Jeff Schneider, Russ Salakhutdinov","Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present **Paprika**, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, Paprika teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with Paprika can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.","Many AI agents struggle to gather information efficiently when they face new environments, because they lack general strategies for exploration and trying new things.Our method, **Paprika**, addresses this through a two-stage fine-tuning process: we first let a language model “play” in many simulated tasks to collect diverse examples of trial and error, then use a preference-based scoring method to teach the model which experiences led to success. Since the model sees many different tasks at the same time, it is incentivized to learn a general problem-solving strategy rather than memorizing each task individually. To make this experience gathering more efficient, Paprika orders these tasks by their “learning potential”—that is, how much each one teaches the model about intelligent and efficient exploration—and focuses on the most informative tasks first. This procedure is similar to how human students learn from a curriculum of increasing difficulty relative to the student’s knowledge.In experiments, models fine-tuned with Paprika transfer these decision-making skills to entirely unseen tasks with no extra training. By shifting the main challenge from costly model updates to smart data selection, Paprika paves the way for AI systems that can autonomously tackle novel, sequential decision-making problems with minimal human intervention."
Poster,Training Deep Learning Models with Norm-Constrained LMOs,https://ICML.cc//virtual/2025/poster/46586,"Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, Volkan Cevher","In this work, we study optimization methods that leverage the linear minimization oracle (LMO) over a norm-ball. We propose a new stochastic family of algorithms that uses the LMO to adapt to the geometry of the problem and, perhaps surprisingly, show that they can be applied to unconstrained problems. The resulting update rule unifies several existing optimization methods under a single framework. Furthermore, we propose an explicit choice of norm for deep architectures, which, as a side benefit, leads to the transferability of hyperparameters across model sizes. Experimentally, we demonstrate significant speedups on nanoGPT training without any reliance on Adam. The proposed method is memory-efficient, requiring only one set of model weights and one set of gradients, which can be stored in half-precision.","Modern deep learning models are usually trained using algorithms that adapt during training to the structure of the data. In this work, we propose a new family of training methods that, instead, adapt in advance to the model’s structure—using mathematical tools that respect how neural networks are built. Our method leads to faster training, requires less memory, and avoids the need for commonly used algorithms like the Adam optimizer. It also allows settings like the learning rate to be reused across different model sizes, making it easier to scale up models. We demonstrate that our method can train large models more efficiently, including popular architectures like GPT and vision transformers."
Poster,Training Diffusion-based Generative Models with Limited Data,https://ICML.cc//virtual/2025/poster/45514,"Zhaoyu Zhang, Yang Hua, Guanxiong Sun, Hui Wang, Seán McLoone","Diffusion-based generative models (diffusion models) often require a large amount of data to train a score-based model that learns the score function of the data distribution through denoising score matching. However, collecting and cleaning such data can be expensive, time-consuming, and even infeasible. In this paper, we present a novel theoretical insight for diffusion models that two factors, i.e., the denoiser function hypothesis space and the number of training samples, can affect the denoising score matching error of all training samples. Based on this theoretical insight, it is evident that minimizing the total denoising score matching error is challenging within the denoiser function hypothesis space in existing methods, when training diffusion models with limited data. To address this, we propose a new diffusion model called Limited Data Diffusion (LD-Diffusion), which consists of two main components: a compressing model and a novel mixed augmentation with fixed probability (MAFP) strategy. Specifically, the compressing model can constrain the complexity of the denoiser function hypothesis space and MAFP can effectively increase the training samples by providing more informative guidance than existing data augmentation methods in the compressed hypothesis space. Extensive experiments on several datasets demonstrate that LD-Diffusion can achieve better performance compared to other diffusion models. Codes are available at https://github.com/zzhang05/LD-Diffusion.","Diffusion models are a type of AI model that can generate realistic images, sounds, or other data by learning from large datasets.  However, training these models usually requires a lot of clean and high-quality data, which can be difficult or expensive to collect.  In this paper, we explore why diffusion models struggle with limited data: the model’s ability to learn is limited both by the amount of training data and the complexity of the denoiser function hypothesis space during training.  Based on this insight, we introduce a new method called LD-Diffusion.  Our approach includes two innovations: a compressing model that constrains the complexity of the denoiser function hypothesis space, and a new data augmentation technique that helps the model see more useful variations of the data.  Our experiments show that LD-Diffusion performs better than existing diffusion models when training data is scarce.  The code for our method is available online."
Poster,Training Dynamics of In-Context Learning in Linear Attention,https://ICML.cc//virtual/2025/poster/44810,"Yedi Zhang, Aaditya Singh, Peter Latham, Andrew Saxe","While attention-based models have demonstrated the remarkable ability of in-context learning (ICL), the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show that the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show that the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we provide a theoretical description of how ICL abilities evolve during gradient descent training of linear attention, revealing abrupt acquisition or progressive improvements depending on how the key and query are parametrized.","Modern AI models like large language models exhibit a remarkable ability known as in-context learning (ICL) -- they can solve unseen tasks just by seeing a few examples in the input prompt. While we observed the ICL ability in trained models, we don't really understand how they acquire it during training. We take a step toward answering this question by theoretically analyzing a simplified version of these models, called linear attention. We show that the way these models acquire ICL depends on how they are set up: in some cases, learning happens all at once -- the model makes no progress for a long time and then suddenly acquires the ability, like a eureka moment; in other cases, learning is progressive -- the model improves step by step, steadily building up its ICL ability.Our findings help explain the different learning curves seen when training AI models, and how model parameterization affects the way they learn."
