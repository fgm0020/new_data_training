type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More",https://ICML.cc//virtual/2025/poster/44951,"Geonhui Yoo, Minhak Song, Chulhee Yun","When training deep neural networks with gradient descent, sharpness often increases---a phenomenon known as *progressive sharpening*---before saturating at the *edge of stability*. Although commonly observed in practice, the underlying mechanisms behind progressive sharpening remain poorly understood. In this work, we study this phenomenon using a minimalist model: a deep linear network with a single neuron per layer. We show that this simple model effectively captures the sharpness dynamics observed in recent empirical studies, offering a simple testbed to better understand neural network training. Moreover, we theoretically analyze how dataset properties, network depth, stochasticity of optimizers, and step size affect the degree of progressive sharpening in the minimalist model. We then empirically demonstrate how these theoretical insights extend to practical scenarios. This study offers a deeper understanding of sharpness dynamics in neural network training, highlighting the interplay between depth, training data, and optimizers.","When training neural networks, researchers have observed a consistent pattern: models become increasingly “sharp,” meaning more sensitive to small changes, before reaching a stable state. This process, called progressive sharpening, is common in practice but not yet well understood.In our study, we investigate this phenomenon using a highly simplified model: a neural network with just one neuron per layer. Remarkably, this minimal setup replicates the sharpening behavior seen in much larger networks, making it a valuable tool for theoretical analysis.We use this setup to study how sharpening is influenced by factors such as training data difficulty, network depth, and randomness in the training process. We also develop new mathematical tools that explain when and why sharpening occurs, and we show that these predictions remain valid in more realistic settings.By providing theoretical foundations for a widely observed yet puzzling phenomenon, our work helps deepen the understanding of neural network dynamics and can guide the design of more reliable learning algorithms."
Poster,Understanding Synthetic Context Extension via Retrieval Heads,https://ICML.cc//virtual/2025/poster/44638,"Xinyu Zhao, Fangcong Yin, Greg Durrett","Long-context LLMs are increasingly in demand for applications such as retrieval-augmented generation. To defray the cost of pretraining LLMs over long contexts, recent work takes an approach of synthetic context extension: fine-tuning LLMs with synthetically-generated long-context data. However, it remains unclear how and why this synthetic context extension imparts abilities for downstream long-context tasks. In this paper, we investigate fine-tuning on synthetic data for three long-context tasks that require retrieval and reasoning. We vary the realism of ""needle'' concepts to be retrieved and diversity of the surrounding ""haystack'' context, from using LLMs to construct synthetic documents to using templated relations and creating symbolic datasets. Although models trained on synthetic data underperform models trained on the real data, the impacts of both training settings can be understood via a shared feature of the attention computation, retrieval heads (Wu et al., 2024). The retrieval heads learned from synthetic data have high overlap with retrieval heads learned on real data. Furthermore, there is a strong correlation between the recall of heads learned and the downstream performance of a model, allowing us to interpret and predict the performance of models trained in different settings. Our results shed light on how to interpret synthetic data fine-tuning performance and how to approach creating better data for learning real-world LLM capabilities over long contexts.","Increasingly, we expect large language models to process long documents,   but obtaining training data for such “long context” tasks is very costly. A popular approach involves automatically constructing training data based on templates. Yet it is unclear how and why training on such “synthetic data” works when its contents are very different from realistic long-context tasks.To study this, we explore three long context tasks that require locating and reasoning over information (the “needle”) found in the input documents (the “haystack”). We varied the realism of the “needle” sentences to be located and the diversity of the surrounding “haystack” context to synthesize training datasets of different complexity and naturalness. We examined the inner workings of LLMs trained on these different versions by analyzing “retrieval heads”, the model components responsible for locating the correct information within the context to generate answers. We found that even when trained on unrealistic-looking synthetic data, models developed very similar retrieval heads to those that emerged when trained on realistic data. The overlap of these heads is also highly correlated with real task performance. This link allows model developers to forecast success early, filter ineffective synthetic data, and create better training setups for creating long-context LLMs."
Poster,Understanding the difficulties of posterior predictive estimation,https://ICML.cc//virtual/2025/poster/45146,"Abhinav Agrawal, Justin Domke","Predictive posterior densities (PPDs) are essential in approximate inference for quantifying predictive uncertainty and comparing inference methods. Typically, PPDs are estimated by simple Monte Carlo (MC) averages. In this paper, we expose a critical under-recognized issue: the signal-to-noise ratio (SNR) of the simple MC estimator can sometimes be extremely low, leading to unreliable estimates. Our main contribution is a theoretical analysis demonstrating that even with exact inference, SNR can decay rapidly with an increase in (a) the mismatch between training and test data, (b) the dimensionality of the latent space, or (c) the size of test data relative to training data. Through several examples, we empirically verify these claims and show that these factors indeed lead to poor SNR and unreliable PPD estimates (sometimes, estimates are off by hundreds of nats even with a million samples). While not the primary focus, we also explore an adaptive importance sampling approach as an illustrative way to mitigate the problem, where we learn the proposal distribution by maximizing a variational proxy to the SNR. Taken together, our findings highlight an important challenge and provide essential insights for reliable estimation.","Posterior predictive distribution (PPD) is an important quantity in Bayesian inference as it is used for making predictions and comparing performance of inference methods. In this work, we uncover that naive methods of estimating PPD can be unreliable. Our primary contribution is a theoretical analysis to understand what factors lead to poor estimation when using the naive method. Based on this analysis, we propose a new technique to estimate PPD and then demonstrate how this proposed method can vastly improve the quality of estimates while using fewer resources (time and compute)."
Poster,Understanding the Emergence of Multimodal Representation Alignment,https://ICML.cc//virtual/2025/poster/46488,"Megan Tjandrasuwita, Chanakya Ekbote, Liu Ziyin, Paul Pu Liang","Multimodal representation learning is fundamentally about transforming incomparable modalities into comparable representations. While prior research has primarily focused on *explicitly* aligning these representations through targeted learning objectives and model architectures, a recent line of work has found that independently trained unimodal models of increasing scale and performance can become *implicitly* aligned with each other. These findings raise fundamental questions regarding the emergence of aligned representations in multimodal learning. Specifically: (1) when and why does alignment emerge implicitly? and (2) is alignment a reliable indicator of performance? Through a comprehensive empirical investigation, we demonstrate that both the emergence of alignment and its relationship with task performance depend on several critical data characteristics. These include, but are not necessarily limited to, the degree of similarity between the modalities and the balance between redundant and unique information they provide for the task. Our findings suggest that alignment may not be universally beneficial; rather, its impact on performance varies depending on the dataset and task. These insights can help practitioners determine whether increasing alignment between modalities is advantageous or, in some cases, detrimental to achieving optimal performance.","When does aligning different types of data, like images and text, help improve machine learning models? In our research, ""aligning"" means making different types of data, like images and text, compatible so the model can understand and process them in a similar way. Recent work has shown that models trained separately on each data type can naturally align as they become more advanced. This raises important questions: when and why does alignment emerge on its own, and does it always lead to better performance?To answer these questions, we conducted a comprehensive empirical investigation, examining factors in the data that influence the emergence of alignment and its relationship with performance on a task, such as detecting sarcasm in a video. Our findings show that alignment doesn’t always benefit performance; in fact, its impact depends on the dataset and the task at hand. This insight is important for practitioners, as it helps them determine when trying to align different data types might improve their model’s performance or when it could potentially be detrimental."
Poster,Understanding the Forgetting of (Replay-based) Continual Learning via Feature Learning: Angle Matters,https://ICML.cc//virtual/2025/poster/46371,"Hongyi Wan, Shiyuan Ren, Wei Huang, Miao Zhang, Xiang Deng, Yixin Bao, Liqiang Nie","Continual learning (CL) is crucial for advancing human-level intelligence, but its theoretical understanding, especially regarding factors influencing forgetting, is still relatively limited. This work aims to build a unified theoretical framework for understanding CL using feature learning theory. Different from most existing studies that analyze forgetting under linear regression model or lazy training, we focus on a more practical two-layer convolutional neural network (CNN) with polynomial ReLU activation for sequential tasks within a signal-noise data model. Specifically, we theoretically reveal how the angle between task signal vectors influences forgetting that: *acute or small obtuse angles lead to benign forgetting, whereas larger obtuse angles result in harmful forgetting*. Furthermore, we demonstrate that the replay method alleviates forgetting by expanding the range of angles corresponding to benign forgetting. Our theoretical results suggest that mid-angle sampling, which selects examples with moderate angles to the prototype, can enhance the replay method's ability to mitigate forgetting. Experiments on synthetic and real-world datasets confirm our theoretical results and highlight the effectiveness of our mid-angle sampling strategy.","How can we help AI systems learn continuously without forgetting what they already know? This challenge, known as catastrophic forgetting, limits AI’s ability to learn like humans. The problem is especially complex in realistic neural networks that learn features from data, and not well understood theoretically. We built a simple yet practical model to study this, using a two-layer neural network that processes tasks one by one. We found that forgetting depends on how similar the tasks are — measured by the angle between their core signals. Tasks that are moderately similar cause less forgetting, while highly different tasks lead to more. Replay methods, which mix past examples during training, help by expanding the range of safe task similarities. Based on this, we propose a smarter replay strategy: mid-angle sampling, which selects examples of moderate similarity. Our theory and experiments show that this method reduces forgetting and improves learning stability. This insight can help design AI systems that better retain knowledge over time."
Poster,Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra,https://ICML.cc//virtual/2025/poster/46566,"Raphael Meyer, William Swartworth, David Woodruff","We study the computational model where we can access a matrix $\mathbf{A}$ only by computing matrix-vector products $\mathbf{A}\mathrm{x}$ for vectors of the form $\mathrm{x} = \mathrm{x}_1 \otimes \cdots \otimes \mathrm{x}_q$.    We prove exponential lower bounds on the number of queries needed to estimate various properties, including the trace and the top eigenvalue of $\mathbf{A}$.    Our proofs hold for all adaptive algorithms, modulo a mild conditioning assumption on the algorithm's queries.    We further prove that algorithms whose queries come from a small alphabet (e.g., $\mathrm{x}_i \in \\{\pm1\\}^n$) cannot test if $\mathbf{A}$ is identically zero with polynomial complexity, despite the fact that a single query using Gaussian vectors solves the problem with probability 1.    In steep contrast to the non-Kronecker case, this shows that sketching $\mathbf{A}$ with different distributions of the same subguassian norm can yield exponentially different query complexities.    Our proofs follow from the observation that random vectors with Kronecker structure have exponentially smaller inner products than their non-Kronecker counterparts.","Scientific problems in areas like quantum physcis or quantum information science often involve large matrices that a stored in some compressed format (like an ""MPO"" or ""PEPS""). The quantum physicists and information scientists are interested in computing some linear-algebraic property of the matrix, like the top eigenvalue, von Neumann entropy, or something in that vein. Because these compressed formats are tensor-structured, the algorithms we design are often equivalent to methods that compute matrix-vector products between this large quantum matrix and vectors that have tensor (Kronecker) structure. Theoretically, the methods that operate in this ""Kronecker matrix-vector oracle"" require computing exponentially many matrix-vector products in the worst case, though it had not been shown that such an expensive cost is necessary to pay.We tackle this problem by using tools from the high-dimensional statistics and oracle complexity literatures, proving that in many regimes and under extremely mild conditions that algorithms which operate by computing only Kronecker matrix-vector products must in fact compute exponentially many such products to recover reliable calculations of linear algebraic properties.Our results show that if you want to compute linear-algebraic properties of really big tensor-structured matrices, as we do in these quantum information settings, then we must do one of two things:1. Find an alternative computational approach beyond the Kronecker matrix-vector product.2. Make strong assumptions about the properties of the input matrix, yielding a sort of ""beyond worst-case analysis"""
Poster,Understanding the Limits of Deep Tabular Methods with Temporal Shift,https://ICML.cc//virtual/2025/poster/46434,"Haorun Cai, Han-Jia Ye","Deep tabular models have demonstrated remarkable success on i.i.d. data, excelling in a variety of structured data tasks. However, their performance often deteriorates under temporal distribution shifts, where trends and periodic patterns are present in the evolving data distribution over time.In this paper, we explore the underlying reasons for this failure in capturing temporal dependencies. We begin by investigating the training protocol, revealing a key issue in how the data is split for model training and validation.While existing approaches typically use temporal ordering for splitting, we show that even a random split significantly improves model performance. By accounting for reducing training lag and validation bias to achieve better generalization ability, our proposed splitting protocol offers substantial improvements across a variety of methods.Furthermore, we analyses how temporal data affects deep tabular representations, uncovering that these models often fail to capture crucial periodic and trend information. To address this gap, we introduce a plug-and-play temporal embedding based on Fourier series expansion to learn and incorporate temporal patterns, offering an adaptive approach to handle temporal shifts.Our experiments demonstrate that this temporal embedding, combined with the improved splitting strategy, provides a more effective and robust framework for learning from temporal tabular data.","This paper aims to advance the field of machine learning by addressing the critical challenge of temporal distribution shifts in tabular data, which frequently occur in real-world applications. The proposed temporal training protocol and temporal embedding method offer practical improvements for deploying existing tabular models in open environments."
Poster,Understanding the Logic of Direct Preference Alignment through Logic,https://ICML.cc//virtual/2025/poster/46481,"Kyle Richardson, Vivek Srikumar, Ashish Sabharwal","Recent direct preference alignment algorithms (DPA), such as DPO, have shown great promise in aligning large language models to human preferences. While this has motivated the development of many new variants of the original DPO loss, understanding the differences between these recent proposals, as well as developing new DPA loss functions, remains difficult given the lack of a technical and conceptual framework for reasoning about the underlying semantics of these algorithms. In this paper, we attempt to remedy this by formalizing DPA losses in terms of discrete reasoning problems. Specifically, we ask: Given an existing DPA loss, can we systematically derive a symbolic program that characterizes its semantics? We propose a novel formalism for characterizing preference losses for single model and reference model based approaches, and identify symbolic forms for a number of commonly used DPA variants. Further, we show how this formal view of preference learning sheds new light on both the size and structure of the DPA loss landscape, making it possible to not only rigorously characterize the relationships between recent loss proposals but also to systematically explore the landscape and derive new loss functions from first principles. We hope our framework and findings will help provide useful guidance to those working on human AI alignment.","We study a core problem in the development of large language models such as ChatGPT called preference alignment. This involves training models to mimic the preferences of certain agents (e.g., preferences related to how language models should behave given certain inputs, such as avoiding generating offensive content).  Our specific aim is to better understand the underlying algorithms used to do preference aligment by formalizing these algorithms in terms of symbolic programs (i.e., the kinds of formal representations familiar from traditional computer science). Such programs help us to formally reason about existing algorithms, as well as derive new algorithms from first principles."
Poster,Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism,https://ICML.cc//virtual/2025/poster/44411,"Aviv Bick, Eric Xing, Albert Gu","State-space models (SSMs) offer efficient alternatives to Transformers for long sequences, but their fixed-size recurrent state limits capability on algorithmic tasks, such as retrieving past context.In this work, we examine how in-context retrieval operates in Transformer- and SSM-based language models and find that both rely on a Gather-and-Aggregate (G&A) mechanism:a Gather Head extracts relevant information from context, which an Aggregate Head integrates into representation.In both architectures, G&A concentrates in a few heads, forming bottlenecks even for simple retrieval.For example, disabling a single Gather or Aggregate Head in a pruned Llama-3.1-8B impairs retrieving the correct answer letter in MMLU, reducing its accuracy from 66% to 25%.Moreover, this retrieval bottleneck can obscure knowledge demands of tasks as the pruned model succeeds on MMLU with functioning G&A heads yet fails on other knowledge benchmarks.The bottleneck similarly extends to tasks where SSMs typically underperform, like GSM8K, BBH, and dialogue.We show that SSMs' retrieval challenges manifest in these heads, creating smoother attention patterns instead of the sharp transitions effective G&A requires.Thus, the Transformer-SSM retrieval gap exists in just a few heads, rather than the entire language model.% Result 3: Analyzing Hybrid modelsThis suggests a unified explanation for Transformer vs. SSM performance gap while showing how to merge their strengths.We find that pretrained hybrid models, where SSMs are combined with attention layers, delegate the role of Aggregate Heads to attention.Similarly, replacing a single G&A head in a pretrained SSM with an attention variant boosts retrieval and benchmark scores.","Modern AI systems are powered by language models that learn to understand and generate text by analyzing large amounts of written data. Most successful models today rely on complex attention mechanisms that can look back over everything they’ve seen so far, but this comes with high computational cost. Newer, more efficient models use a compact form of memory, which makes them faster and lighter—but often at the expense of accuracy, especially on tasks that require recalling earlier parts of the input.Our study investigates this gap in ability and finds that both types of models—despite their architectural differences—use a similar strategy to handle retrieval: one part identifies the relevant information, and another integrates it into the model’s final response. We call this the “gather-and-aggregate” mechanism.We show that this retrieval process is handled by just a few key components. If those are disrupted, performance on challenging tasks drops sharply. This helps explain why efficient models underperform and offers a practical solution: combining their strengths with just a few attention-based components can significantly improve results.Our findings provide insight into how language models retrieve information and point to ways to design more efficient and capable AI systems."
Poster,Understanding the Statistical Accuracy-Communication Trade-off in Personalized Federated Learning with Minimax Guarantees,https://ICML.cc//virtual/2025/poster/45549,"Xin Yu, Zelin He, Ying Sun, Lingzhou Xue, Runze Li","Personalized federated learning (PFL) offers a flexible framework for aggregating information across distributed clients with heterogeneous data. This work considers a personalized federated learning setting that simultaneously learns global and local models. While purely local training has no communication cost, collaborative learning among the clients can leverage shared knowledge to improve statistical accuracy, presenting an accuracy-communication trade-off in personalized federated learning. However, the theoretical analysis of how personalization quantitatively influences sample and algorithmic efficiency and their inherent trade-off is largely unexplored. This paper makes a contribution towards filling this gap, by providing a quantitative characterization of the personalization degree on the tradeoff. The results further offer theoretical insights for choosing the personalization degree. As a side contribution, we establish the minimax optimality in terms of statistical accuracy for a widely studied PFL formulation. The theoretical result is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.","Personalized federated learning (PFL) has gained increasing attention due to the need for adapting global models to heterogeneous client data. However, achieving both high accuracy and communication efficiency and characterizing their trade-off under varying degrees of personalization remain largely unexplored in PFL.In this work, we theoretically analyze the trade-off between statistical accuracy and communication cost under the influence of personalization degree. With a minimax theoretical guarantee, we show the benefit of PFL in generalization performance. These results offer practical guidance for designing communication-efficient personalized federated learning algorithms.To the best of our knowledge, this is the first theoretical study to characterize the generalization performance of PFL while explicitly quantifying the trade-off between statistical accuracy and communication efficiency. Our analysis provides broad insights that inform the choice of personalization degree for a wide range of PFL algorithms to achieve high statistical accuracy and communication efficiency."
