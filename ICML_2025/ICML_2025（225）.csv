type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Polynomial-Time Approximability of Constrained Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44736,Jeremy McMahan,"We study the computational complexity of approximating general constrained Markov decision processes. Our primary contribution is the design of a polynomial time $(0,\epsilon)$-additive bicriteria approximation algorithm for finding optimal constrained policies across a broad class of recursively computable constraints, including almost-sure, chance, expectation, and their anytime variants. Matching lower bounds imply our approximation guarantees are optimal so long as $P \neq NP$. The generality of our approach results in answers to several long-standing open complexity questions in the constrained reinforcement learning literature. Specifically, we are the first to prove polynomial-time approximability for the following settings: policies under chance constraints, deterministic policies under multiple expectation constraints, policies under non-homogeneous constraints (i.e., constraints of different types), and policies under constraints for continuous-state processes.","In many sequential decision-making settings, agents must obey multiple constraints of different forms. For example, an autonomous vehicle may be required to make consistent, deterministic, decisions that obeys 1) an anytime constraint on fuel consumption to ensure the destination is reached, 2) a expectation constraint on the CO2 consumption to minimize impact on the environment, and 3) a chance constraint that manages the risk of wear and tear on the vehicle throughout the route. We capture such problems through a general cost-criterion Markov Decision Process setting. Our main contribution is a general algorithm for these problems with strong theoretical guarantees. The existence of our algorithm provides the first ever provable approximation guarantees for many popular settings, answering several open questions, some of which have been open for 25 years."
Poster,Polynomial Time Learning Augmented Algorithms for NP-hard Permutation Problems,https://ICML.cc//virtual/2025/poster/46599,"Evripidis Bampis, Bruno Escoffier, Dimitris Fotakis, Panagiotis Patsilinakos, Michalis Xefteris","We consider a learning augmented framework for NP-hard permutation problems. The algorithm has access to predictions telling, given a pair $u,v$ of elements, whether $u$ is before $v$ or not in an optimal solution. Building on the work of Braverman and Mossel (SODA 2008), we show that for a class of optimization problems  including scheduling, network design and other graph permutation problems, these predictions allow to solve them in polynomial time with high probability, provided that predictions are true with probability at least $1/2+\epsilon$. Moreover, this can be achieved with a parsimonious access to the predictions.","Many important tasks in computing, such as scheduling jobs or designing efficient networks, require finding the best way to arrange a list of items. Unfortunately, solving such ordering problems is often extremely hard, even for powerful computers.In this work, we show how predictions from a machine learning model can make these problems much easier to solve. The idea is simple: the model gives us a guess, for any two items, about which one should come first in a best-possible solution. Even if these guesses are only slightly better than random (just a bit more than 50% accurate), we can use them to find the best-possible solution efficiently.The key insight is that we don’t need perfect predictions, just a slightly helpful advice. We also need to ask the model about very few pairs, which keeps the process efficient. This approach combines the strengths of algorithms and machine learning to tackle problems that are otherwise out of reach."
Poster,POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval,https://ICML.cc//virtual/2025/poster/44047,"Yaoyang Liu, Junlin Li, Yinjun Wu, Zhen Chen","Although Multi-Vector Retrieval (MVR) has achieved the state of the art on many information retrieval (IR) tasks, its performance highly depends on how to decompose queries into smaller pieces, say phrases or tokens. However, optimizing query decomposition for MVR performance is not end-to-end differentiable. Even worse, jointly solving this problem and training the downstream retrieval-based systems, say RAG systems could be highly inefficient. To overcome these challenges, we propose Performance-Oriented Query Decomposer (POQD), a novel query decomposition framework for MVR. POQD leverages one LLM for query decomposition and searches the optimal prompt with an LLM-based optimizer. We further propose an end-to-end training algorithm to alternatively optimize the prompt for query decomposition and the downstream models. This algorithm can achieve superior MVR performance at a reasonable training cost as our theoretical analysis suggests. POQD can be integrated seamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented Generation (RAG) systems. Extensive empirical studies on representative RAG-based QA tasks show that POQD outperforms existing query decomposition strategies in both retrieval performance and end-to-end QA accuracy. POQD is available at https://github.com/PKU-SDS-lab/POQD-ICML25.","In this work, we aim to enhance the end-to-end performance of retrieval-based systems, say RAG systems, through enhancing their retrieval process. To achieve this, we employ one emerging technique called multi-vector retrieval, which decomposes queries and retrieved data into fine-grained pieces, and then evaluates a new score function to evaluate the overall query-data similarities. Although this technique has been quite effective, we discovered that how to decompose queries into appropriate granularities matters. Hence, we proposed a novel method to optimize the way of decomposing queries such that the end-to-end RAG performance could be optimized. Both theoretical and empirical results can demonstrate the effectiveness and efficiency of our method. This method could be widely used to enhance the performance of arbitrary RAG systems in a lightweight manner."
Poster,POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization,https://ICML.cc//virtual/2025/poster/44002,"Batuhan K. Karaman, ishmam zabir, Alon Benhaim, Vishrav Chaudhary, Mert Sabuncu, Xia Song","Achieving both high safety and high usefulness simultaneously in large language models has become a critical challenge in recent years.Models often exhibit unsafe behavior or adopt an overly cautious approach leading to frequent overrefusal of benign prompts, which reduces their usefulness. A major factor underlying these behaviors is how the models are finetuned and aligned, particularly the nature and extent of the data used.In this work, we examine how overgenerating finetuning data with advanced teacher models (e.g., GPT-4o)—covering both general-purpose and toxic prompts—affects safety and usefulness in instruction-following language models.Additionally, we present POROver, an alignment strategy designed for models that are highly safe but prone to overrefusal. POROver employs preference optimization algorithms and leverages completions from an advanced teacher model to reduce overrefusals while maintaining safety.Our results show that overgenerating completions for general-purpose prompts significantly boosts safety with only a minimal impact on usefulness. Specifically, the F1 score calculated between safety and usefulness increases from 74.4% to 91.8% because of a substantial rise in safety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1% to 57.6% while preserving safety. Finally, applying POROVer increases usefulness further—from 57.6% to 82.1%—while keeping safety at comparable levels.","Large language models are powerful but often face a tough tradeoff: they either say things they should not (unsafe) or refuse to answer even harmless questions (not useful). Our paper explores how to make these models both safer and more helpful. We found that giving models more examples from smarter AI systems, such as GPT-4o, improves their behavior. We also introduce a new strategy called POROver, which aligns cautious models to make them less likely to say ""no"" unnecessarily, while still keeping them safe. Our work offers a practical path toward building AI systems that are both trustworthy and actually helpful in real-world use."
Poster,Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models,https://ICML.cc//virtual/2025/poster/44682,"Daiki Chijiwa, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito, Susumu Takeuchi","While foundation models have been exploited for various expert tasks with their fine-tuned parameters, any foundation model will be eventually outdated due to its old knowledge or limited capability, and thus should be replaced by a new foundation model. Subsequently, to benefit from its latest knowledge or improved capability, the new foundation model should be fine-tuned on each task again, which incurs not only the additional training cost but also the maintenance cost of the task-specific data. Existing work address this problem by inference-time tuning, i.e., modifying the output probability from the new foundation model by the outputs from the old foundation model and its fine-tuned model, which involves an additional inference cost by the latter two models. In this paper, we explore a new fine-tuning principle (which we call portable reward tuning; PRT) that reduces the inference cost by its nature, based on the reformulation of fine-tuning as the reward maximization with Kullback-Leibler regularization. Specifically, instead of fine-tuning parameters of the foundation models, PRT trains the reward model explicitly through the same loss as in fine-tuning. During inference, the reward model can be used with any foundation model (with the same set of vocabularies or labels) through the formulation of reward maximization. Experimental results, including both vision and language models, show that the PRT-trained model can achieve comparable accuracy with less inference cost, in comparison to the existing work of inference-time tuning.","As foundation models are frequently updated to incorporate newer data or improved architectures, users are often required to fine-tune each new model for their specific tasks, which is both time-consuming and costly.This paper introduces **Portable Reward Tuning** (PRT), a new framework that trains an external reward model rather than directly tuning the foundation model itself. This reward model can then be reused with any compatible foundation model, allowing users to transfer the benefits of prior fine-tuning without additional training.Experiments on both vision and language tasks demonstrate that PRT achieves accuracy on par with previous methods, but with lower computational cost and simpler deployment. This approach could make it much easier and cheaper to keep AI systems up-to-date as foundational models rapidly evolve, helping users to benefit from AI advancements with reduced operating expense."
Poster,Position: AI Agents Need Authenticated Delegation,https://ICML.cc//virtual/2025/poster/40172,"Tobin South, Samuele Marro, Thomas Hardjono, Robert Mahari, Cedric Whitney, Alan Chan, Alex Pentland","The rapid deployment of autonomous AI agents creates urgent challenges in the areas of authorization, accountability, and access control in task delegation. This position paper argues that authenticated and auditable delegation of authority to AI agents is a critical component of mitigating practical risks and unlocking the value of agents.  To support this argument, we examine how existing web authentication and authorization protocols, as well as natural language interfaces to common access control mechanisms, can be extended to enable secure authenticated delegation of authority to AI agents. By extending OAuth 2.0 and OpenID Connect with agent-specific credentials and using transparent translation of natural language permissions into robust scoping rules across diverse interaction modalities, we outline how authenticated delegation can be achieved to enable clear chains of accountability while maintaining compatibility with established authentication and web infrastructure for immediate compatibility. This work contributes to ensuring that agentic AI systems perform only appropriate actions. It argues for prioritizing delegation infrastructure as a key component of AI agent governance and provides a roadmap for achieving this.","This paper argues that we need a clear system for ""authenticated delegation."" This means when an AI agent interacts with websites, services, or even other AI agents, those entities can confidently verify:- Who authorized the AI: Confirming it's acting on your behalf, not rogue.- What the AI is allowed to do: Limiting its actions to the specific task you delegated.We explore how existing internet security technologies, like those used when you log into websites and grant apps access to your data, can be adapted for AI agents."
Poster,Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation,https://ICML.cc//virtual/2025/poster/40139,"D. Sculley, William Cukierski, Phil Culliton, Sohier Dane, Maggie Demkin, Ryan Holbrook, Addison Howard, Paul Mooney, Walter Reade, Meg Risdal, Nate Keating","In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of *leakage* and *contamination* are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.","Evaluating Generative AI (GenAI) models like ChatGPT and Gemini poses unique challenges that traditional machine learning benchmarking methods don’t address well. It’s hard to know whether a model truly understands a task or has simply ""memorized"" information from its training data. GenAI models need to be tested on novel tasks to reliably measure their capabilities, like humans tested on exams they haven’t seen before.These models are exposed to internet-scale data, sometimes including common research benchmarks, which exacerbates how hard it is to test using novel tasks. In response to this issue—known as ""contamination” or “leakage”—the AI research community urgently needs new, more reliable ways to measure GenAI capabilities.We argue AI Competitions, such as those hosted on platforms like Kaggle, offer two key advantages. First, they provide a continuous stream of novel tasks for testing GenAI model capabilities. Second, they are specifically designed to mitigate leakage risks by restricting network access and leveraging parallel development by thousands of independent teams.Given these benefits, we propose that AI Competitions should be adopted as the ""gold standard"" for rigorously evaluating GenAI models. This is crucial for the AI industry, as effective evaluation is essential for guiding development and improvement of GenAI models across diverse applications."
Poster,Position: AI Evaluation Should Learn from How We Test Humans,https://ICML.cc//virtual/2025/poster/40146,"Yan Zhuang, Qi Liu, Zachary Pardos, Patrick Kyllonen, Jiyun Zu, Zhenya Huang, Shijin Wang, Enhong Chen","As AI systems continue to evolve, their rigorous evaluation becomes crucial for their development and deployment. Researchers have constructed various large-scale benchmarks to determine their capabilities, typically against a gold-standard test set and report metrics averaged across all items. However, this static evaluation paradigm increasingly shows its limitations, including high evaluation costs, data contamination, and the impact of low-quality or erroneous items on evaluation reliability and efficiency. In this Position, drawing from human psychometrics, we discuss a paradigm shift from static evaluation methods to adaptive testing. This involves estimating the characteristics or value of each test item in the benchmark, and tailoring each model's evaluation instead of relying on a fixed test set. This paradigm provides robust ability estimation, uncovering the latent traits underlying a model’s observed scores. This position paper analyze the current possibilities, prospects, and reasons for adopting psychometrics in AI evaluation. We argue that *psychometrics, a theory originating in the 20th century for human assessment, could be a powerful solution to the challenges in today's AI evaluations*.","As AI systems, especially LLMs, continue to advance, rigorous and trustworthy evaluation becomes increasingly critical for their development and deployment. Today, AI is typically assessed using large collections of test questions called benchmarks, and each model is scored based on the average of all its answers. But this paradigm has growing problems: it's expensive, time-consuming, and often includes poor-quality or redundant questions. These issues can distort the results and make it harder to trust what the scores really mean. In this paper, we propose a new approach inspired by how human abilities are measured: adaptive testing from the field of psychometrics. Instead of treating every question equally, we estimate how useful or difficult each one is and dynamically adjust the test for each AI system. This leads to a fairer, faster, and more accurate way to measure an AI’s true capabilities. We argue that this shift in evaluation paradigm will not only save time and resources, but also give us deeper insights into what modern AI systems can and can't do."
Poster,Position: AI Safety Must Embrace an Antifragile Perspective,https://ICML.cc//virtual/2025/poster/40133,"Ming Jin, Hyunin Lee","This position paper contends that modern AI research must adopt an antifragile perspective on safety---one in which the system's capacity to handle rare or out-of-distribution (OOD) events adapts and expands over repeated exposures. Conventional static benchmarks and single-shot robustness tests overlook the reality that environments evolve and that models, if left unchallenged, can drift into maladaptation (e.g., reward hacking, over-optimization, or atrophy of broader capabilities). We argue that an antifragile approach---Rather than striving to rapidly reduce current uncertainties, the emphasis is on leveraging those uncertainties to better prepare for potentially greater, more unpredictable uncertainties in the future---is pivotal for the long-term reliability of open-ended ML systems. In this position paper, we first identify key limitations of static testing, including scenario diversity, reward hacking, and over-alignment. We then explore the potential of dynamic, antifragile solutions to manage rare events. Crucially, we advocate for a fundamental recalibration of the methods used to measure, benchmark, and continually improve AI safety over the long term, complementing existing robustness approaches by providing ethical and practical guidelines towards fostering an antifragile AI safety community.","**Problem:** Current AI safety approaches test systems once and declare them robust, but real-world environments constantly evolve with new threats, such as new attack methods, unexpected user behaviors, and environmental changes that weren't anticipated during development.**Solution:** We propose ``antifragile'' AI safety, inspired by biological immune systems that get stronger after exposure to threats. Instead of hoping our initial safety tests cover everything, we design AI systems that continuously learn from new failures and stress-test themselves in safe environments. When a system encounters an unexpected problem, it doesn't just patch that specific issue---it uses the experience to become more robust against similar future threats.**Impact:** This approach could prevent catastrophic AI failures by ensuring systems improve from every new challenge they encounter, rather than becoming brittle over time. Instead of playing an endless game of whack-a-mole with new vulnerabilities, we can build AI that evolves to handle tomorrow's unknown threats. This is crucial as AI systems become more powerful and are deployed in critical areas like healthcare, infrastructure, and finance where unexpected failures could have severe consequences."
Poster,Position: AI Safety should prioritize the Future of Work,https://ICML.cc//virtual/2025/poster/40166,"Sanchaita Hazra, Bodhisattwa Prasad Majumder, Tuhin Chakrabarty","Current efforts in AI safety prioritize filtering harmful content, preventing manipulation of human behavior, and eliminating existential risks in cybersecurity or biosecurity. While pressing, this narrow focus overlooks critical human-centric considerations that shape the long-term trajectory of a society. In this position paper, we identify the risks of overlooking the impact of AI on the future of work and recommend comprehensive transition support towards the evolution of meaningful labor with human agency. Through the lens of economic theories, we highlight the intertemporal impacts of AI on human livelihood and the structural changes in labor markets that exacerbate income inequality. Additionally, the closed-source approach of major stakeholders in AI development resembles rent-seeking behavior through exploiting resources, breeding mediocrity in creative labor, and monopolizing innovation. To address this, we argue in favor of a robust international copyright anatomy supported by implementing collective licensing that ensures fair compensation mechanisms for using data to train AI models. We strongly recommend a pro-worker framework of global AI governance to enhance shared prosperity and economic justice while reducing technical debt.","Current AI safety research focuses mainly on preventing misuse, harmful outputs, or hypothetical future risks like rogue AI. It overlooks a more immediate and pressing issue: how generative AI disrupts the job market and undermines meaningful human labor. Automation driven by AI is accelerating job loss, disproportionately affecting lower-income and less-resourced communities. We propose that AI safety should prioritize the future of work by integrating labor market concerns into its core agenda. We highlight a policy framework to support displaced workers, promote equitable access to AI resources, and implement fair remuneration for creators whose data trains AI models. We also recommend technical safeguards, such as AI-generated content detection and watermarking, to preserve trust in widespread AI use across critical domains."
