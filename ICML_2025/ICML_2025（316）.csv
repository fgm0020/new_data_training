type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory,https://ICML.cc//virtual/2025/poster/45844,"Dominik Fuchsgruber, Tom Wollschläger, Johannes Bordne, Stephan Günnemann","While uncertainty estimation for graphs recently gained traction, most methods rely on homophily and deteriorate in heterophilic settings.    We address this by analyzing message passing neural networks from an information-theoretic perspective and developing a suitable analog to data processing inequality to quantify information throughout the model's layers. In contrast to non-graph domains, information about the node-level prediction target can *increase* with model depth if a node's features are semantically different from its neighbors.     Therefore, on heterophilic graphs, the latent embeddings of an MPNN each provide different information about the data distribution - different from homophilic settings.    This reveals that considering all node representations simultaneously is a key design principle for epistemic uncertainty estimation on graphs beyond homophily.     We empirically confirm this with a simple post-hoc density estimator on the joint node embedding space that provides state-of-the-art uncertainty on heterophilic graphs. At the same time, it matches prior work on homophilic graphs without explicitly exploiting homophily through post-processing.","Previous uncertainty estimation methods for Graph Neural Networks (GNNs) heavily rely on homophily which means that edges predominantly exist between semantically similar nodes. We investigate how uncertainty can be quantified for heterophilic graphs where nodes with different semantics are connected.We use Mutual Information (MI) to measure and track the informativeness of a node's features and those of nodes it connects to. This way, we derive an analog to the well-known Data Processing Inequality that applies to all Message Passing Neural Networks (MPNNs), a broad family of GNNs. In contrast to domains with independent data, our analysis shows that a node's embedding in the MPNN can get more informative the deeper it is in the MPNN. The amount of additional information is governed by how heterophilic the node's neighbors are. This motivates estimating uncertainty on these graphs from jointly considering all embeddings of a node the GNN provides. In practice, this strategy outperforms existing methods for heterophilic graphs and is competitive on homophilic graphs.Our research provides an information-theoretic background for MPNNs and uses this to bridge the gap between reliable Machine Learning and non-homophilic graphs."
Poster,Uncertainty Quantification for LLM-Based Survey Simulations,https://ICML.cc//virtual/2025/poster/44103,"Chengpiao Huang, Yuhang Wu, Kaizheng Wang","We investigate the use of large language models (LLMs) to simulate human responses to survey questions, and perform uncertainty quantification to gain reliable insights. Our approach converts imperfect, LLM-simulated responses into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.","Researchers and companies increasingly use large language models (LLMs) to simulate human responses to surveys, in economics, social science, and market research. An LLM can generate hundreds of responses in minutes, yet those synthetic responses rarely match real human opinions perfectly. How, then, can we trust these simulations to give a realistic picture of what actual people think? We tackle this problem by converting imperfect LLM simulations into a reliable confidence interval for human responses. Our method identifies a simulation sample size under which the LLM’s bias is naturally covered by the confidence interval. This sample size reveals how many humans the LLM effectively represents, and measures how well the LLM simulations align with real human responses. Importantly, our method works for any LLM, no matter how advanced or imperfect.Our real-data experiments show that for social opinion surveys, existing LLMs’ responses represent at most 60 randomly selected people in the general U.S. population, while for middle-school math questions, the LLMs can barely mimic the responses from 10 real students."
Poster,Unconstrained Robust Online Convex Optimization,https://ICML.cc//virtual/2025/poster/43582,"Jiujia Zhang, Ashok Cutkosky","This paper addresses online learning with ''corrupted'' feedback. Our learner is provided with potentially corrupted gradients $\tilde g_t$ instead of the ''true'' gradients $g_t$. We make no assumptions about how the corruptions arise: they could be the result of outliers, mislabeled data, or even malicious interference. We focus on the difficult  ``unconstrained'' setting in which our algorithm must maintain low regret with respect to any comparison point $u \in \mathbb{R}^d$. The unconstrained setting is significantly more challenging as existing algorithms suffer extremely high regret  even with very tiny amounts of corruption (which is not true in the case of a bounded domain). Our algorithms guarantee regret $ \|u\|G (\sqrt{T} + k) $ when $G \ge \max_t \|g_t\|$ is known, where $k$ is a measure of the total amount of corruption. When $G$ is unknown we incur an extra additive penalty of $(\|u\|^2+G^2) k$.","Modern machine learning systems often rely on feedback to learn over time, but what happens if that feedback is wrong or misleading? For example, an algorithm might learn from mislabeled data, noisy measurements, or even adversarial attacks. Existing study have addressed such problem when domain is constrained, but this assumption is not true in many real-world situations. In this work, we show the case of unconstrained domain, an appropriate regularization could address the problem. This research lays a theoretical foundation for learning in the unconstrained domain through corrupted feedback."
Poster,Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning,https://ICML.cc//virtual/2025/poster/45477,"Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora Kreacic, Yifan Li, Xiang Yue, Bo Li, Vamsi Potluru, Pan Li, Eli Chien","Large Language Models (LLMs) embed sensitive, human-generated data, prompting the need for unlearning methods. Although certified unlearning offers strong privacy guarantees, its restrictive assumptions make it unsuitable for LLMs, giving rise to various heuristic approaches typically assessed through empirical evaluations. These standard evaluations randomly select data for removal, apply unlearning techniques, and use membership inference attacks (MIAs) to compare unlearned models against models retrained without the removed data. However, to ensure robust privacy protections for every data point, it is essential to account for scenarios in which certain data subsets face elevated risks. Prior research suggests that outliers, particularly including data tied to minority groups, often exhibit higher memorization propensity which indicates they may be more difficult to unlearn. Building on these insights, we introduce a complementary, minority-aware evaluation framework to highlight blind spots in existing frameworks. We substantiate our findings with carefully designed experiments, using canaries with personally identifiable information (PII) to represent these minority subsets and demonstrate that they suffer at least 20\% higher privacy leakage across various unlearning methods, MIAs, datasets, and LLM scales. Our proposed minority-aware evaluation framework marks an essential step toward more equitable and comprehensive assessments of LLM unlearning efficacy.","Large language models (LLMs) are trained on vast amounts of human-generated data, including sensitive information such as phone numbers, emails, and other personal details. When these models memorize such information, it can lead to privacy risks, especially for minority groups whose data may be more unique or less represented in the training set. Yet, existing methods for removing sensitive data from LLMs, called LLM unlearning, usually test on randomly selected data and overlook these high-risk cases.To tackle this problem, we created a new way to evaluate unlearning methods that pays special attention to data from minority groups. We designed experiments using “canaries”, i.e., small, carefully chosen test cases with personal information like phone numbers, and found that across various unlearning techniques and models, data from minority groups consistently suffers about 20% more privacy leakage. Our work highlights the importance of designing fairer and more robust privacy evaluations for LLMs. By sharing our code and framework, we hope to help researchers and developers build safer AI systems that offer stronger privacy protections for everyone, not just the majority."
Poster,Understanding and Improving Length Generalization in Recurrent Models,https://ICML.cc//virtual/2025/poster/46587,"Ricardo Buitrago Ruiz, Albert Gu","Recently, recurrent models such as state space models and linear attention have become popular due to their linear complexity in the sequence length. Thanks to their recurrent nature, in principle they can process arbitrarily long sequences, but their performance sometimes drops considerably beyond their training context lengths---i.e. they fail to length generalize.In this work, we provide comprehensive empirical and theoretical analysis to support the \textit{unexplored states hypothesis}, which posits that models fail to length generalize when during training they are only exposed to a limited subset of the distribution of all \textit{attainable} states (i.e. states that would be attained if the recurrence was applied to long sequences).Furthermore, we investigate simple training interventions that aim to increase the coverage of the states that the model is trained on, e.g. by initializing the state with Gaussian noise or with the final state of a different input sequence. With only 500 post-training steps ($\sim 0.1\%$ of the pre-training budget), these interventions enable length generalization for sequences that are orders of magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and show improved performance in long context tasks, thus presenting a simple and efficient way to enable robust length generalization in general recurrent models.","Recently, recurrent models have emerged as alternative deep learning architectures with strong performance in many areas such as text, vision, and audio. Their main advantage over the widely used Transformers is their ability to process long sequences more efficiently. However, in practice, their performance on long sequences is as poor as that of Transformers, so this remains an unrealized potential. In this work, we study why recurrent models, despite theoretical ability to process arbitrarily long sequences, fail to achieve good performance. Additionally, we propose simple and inexpensive interventions that enable good performance on long sequences, allowing them to process sequences more than 64 times longer than before and thus realize their advantage over Transformers. Finally, we show that these interventions also lead to improved performance on complex tasks that require long-context reasoning, such as answering a question whose answer is hidden in a very long context."
Poster,Understanding and Mitigating Memorization in Diffusion Models for Tabular Data,https://ICML.cc//virtual/2025/poster/45667,"Zhengyu Fang, Zhimeng Jiang, Huiyuan Chen, Xiao Li, Jing Li","Tabular data generation has attracted significant research interest in recent years, with the tabular diffusion models greatly improving the quality of synthetic data. However, while memorization—where models inadvertently replicate exact or near-identical training data—has been thoroughly investigated in image and text generation, its effects on tabular data remain largely unexplored. In this paper, we conduct the first comprehensive investigation of memorization phenomena in diffusion models for tabular data. Our empirical analysis reveals that memorization appears in tabular diffusion models and increases with larger training epochs. We further examine the influence of factors such as dataset sizes, feature dimensions, and different diffusion models on memorization. Additionally, we provide a theoretical explanation for why memorization occurs in tabular diffusion models. To address this issue, we propose TabCutMix, a simple yet effective data augmentation technique that exchanges randomly selected feature segments between random same-class training sample pairs. Building upon this, we introduce TabCutMixPlus, an enhanced method that clusters features based on feature correlations and ensures that features within the same cluster are exchanged together during augmentation. This clustering mechanism mitigates out-of-distribution (OOD) generation issues by maintaining feature coherence. Experimental results across various datasets and diffusion models demonstrate that TabCutMix effectively mitigates memorization while maintaining high-quality data generation. Our code is available at https://github.com/fangzy96/TabCutMix.","Generating synthetic tabular data, such as medical records or financial logs, is important when real data cannot be shared due to privacy concerns. Diffusion models have recently achieved strong results in generating realistic tabular data. However, while memorization, where models unintentionally copy training data, has been carefully studied in image and text generation, it remains largely unexamined in tabular data.We present the first comprehensive study of memorization in tabular diffusion models. We find that memorization does occur and becomes more severe with longer training. We also analyze how factors such as dataset size, number of features, and model design affect memorization. To address this issue, we propose a method called TabCutMix that mixes parts of features between samples from the same class. We further introduce TabCutMixPlus, which groups related features and exchanges them together to preserve the structure of the data better.Our findings reveal a hidden privacy risk in tabular data generation. The proposed methods reduce memorization while keeping the generated data useful. This makes diffusion-based tabular data generation safer and more suitable for applications in healthcare, finance, and other sensitive domains."
Poster,Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes,https://ICML.cc//virtual/2025/poster/45941,"Dongjae Jeon, Dueun Kim, Albert No","In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term.","Generative models like Stable Diffusion can produce impressive images, but they sometimes memorize parts of their training data. This raises privacy concerns, especially when the training data contains sensitive content.We study this memorization through a geometric perspective, showing that it emerges in regions where the model’s predictions are unusually sharp. By analyzing the Hessian of the log probability, we quantify this sharpness and demonstrate that it enables early detection of memorization during the generation process.We also provide a theoretical explanation for Wen et al.'s popular detection metric, showing that it captures sharpness differences between conditional and unconditional predictions. Building on this insight, we develop an enhanced metric by amplifying these sharpness signals using the Hessian.Based on our sharpness framework, we introduce SAIL, a method that reduces memorization by selecting smoother initial noise during sampling. SAIL operates entirely at inference time and requires no changes to the model or prompt.Our work offers both theoretical insight and a practical tool for improving the safety of generative models."
Poster,Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models,https://ICML.cc//virtual/2025/poster/43778,"Shuoyuan Wang, Sharon Li, Hongxin Wei","Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we demonstrate that existing prompt tuning methods usually lead to a trade-off of calibration between base and new classes: the cross-entropy loss used in standard fine-tuning (e.g., CoOp) causes overconfidence in new classes by increasing textual label divergence, whereas regularization-based tuning (e.g., KgCoOp) maintains the confidence level but results in underconfidence in base classes due to the improved accuracy. Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR) to ensure the confidence calibration on both base and new classes after fine-tuning. In particular, we propose to minimize the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary. In effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes. Extensive experiments demonstrate that DOR can enhance the calibration performance of current fine-tuning methods on base and new classes.","When vision-language foundational models like CLIP are fine-tuned for specific tasks, they can sometimes become too sure or not sure enough about their predictions. This can make them less dependable for important real-world uses, like in healthcare or autonomous vehicles, where knowing how confident a model should be is key to safety. Our study found that current fine-tuning methods either make the model overconfident in new classes or not confident enough in familiar ones. To solve this, we proposed a new method called Dynamic Outlier Regularization (DOR). DOR carefully balances how the model handles both new and familiar classes, ensuring it stays reliable without being overly certain. Our experiments show that DOR makes CLIP’s predictions more accurate and trustworthy for real-world applications."
Poster,Understanding Chain-of-Thought in LLMs through Information Theory,https://ICML.cc//virtual/2025/poster/45723,"Jean-Francois Ton, Muhammad Faaiz Taufiq, Yang Liu","Large Language Models (LLMs) have shown impressive performance in complex reasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing models to break down problems into manageable sub-tasks. However, existing CoT evaluation techniques either require annotated CoT data or fall short of accurately assessing intermediate reasoning steps, leading to high rates of false positives. In this paper, we formalize CoT reasoning in LLMs through an information-theoretic lens. Specifically, our framework quantifies the `information gain' at each reasoning step, enabling the identification of failure modes in LLMs without the need for expensive annotated datasets. We demonstrate the efficacy of our approach through extensive experiments on toy arithmetic, GSM8K and PRM800k datasets, where it significantly outperforms existing outcome-based methods by providing more accurate insights into model performance on individual tasks.","**Problem:** When AI language models solve complex problems, they break them down into step-by-step reasoning called ""Chain-of-Thought."" However, we currently have no reliable way to identify which specific steps in this reasoning process are incorrect without expensive human annotation of every step. Existing methods often give false alarms, incorrectly flagging correct reasoning steps as wrong.**Solution:** We developed a mathematical framework using information theory to automatically detect reasoning errors. Our method measures how much useful information each reasoning step contributes toward the correct final answer. When a step fails to add meaningful information (or even reduces confidence in the correct answer), this signals an error in the model's reasoning process. We train a separate ""supervisor"" model that can assess whether each step brings the AI closer to the right solution.**Impact:** Our approach can pinpoint exactly where AI reasoning goes wrong without requiring humans to manually check every step, making it much more practical and cost-effective than current methods. This enables researchers and developers to identify specific weaknesses in AI reasoning systems and improve them more efficiently. As AI systems become more complex and are used in critical applications, having reliable tools to evaluate and improve their reasoning processes becomes essential for building trustworthy AI."
Poster,Understanding Complexity in VideoQA via Visual Program Generation,https://ICML.cc//virtual/2025/poster/46385,"Cristobal Eyzaguirre, Igor Vasiljevic, Achal Dave, Jiajun Wu, Rareș Ambruș, Thomas Kollar, Juan Carlos Niebles, Pavel Tokmakov","We propose a data-driven approach to analyzing query complexity in Video Question Answering (VideoQA). Previous efforts in benchmark design have relied on human expertise to design challenging questions, yet we experimentally show that humans struggle to predict which questions are difficult for machine learning models. Our automatic approach leverages recent advances in code generation for visual question answering, using the complexity of generated code as a proxy for question difficulty. We demonstrate that this measure correlates significantly better with model performance than human estimates. To operationalize this insight, we propose an algorithm for estimating question complexity from code. It identifies fine-grained primitives that correlate with the hardest questions for any given set of models, making it easy to scale to new approaches in the future. Finally, to further illustrate the utility of our method, we extend it to automatically generate complex questions, constructing a new benchmark that is 1.9 times harder than the popular NExT-QA.","In this paper we tackle the problem of estimating how hard it is for video models to answer some questions about videos. This is important because not all questions are equally difficult, and we're interested in understanding the reasons for why models succeed at correctly answering some questions and fail at answering others.We also show that humans and VideoLLMs are bad at estimating question difficulty. Instead, we use existing models to generate computer code that represents how to answer each question, and then show that the complexity of the code is a good proxy to estimate how hard the question really is for models.Our most effective approach is to train a model (CodePlexity) that takes the code as input and uses it to estimate the question complexity. Finally, we show that the resulting model can be used to create a new benchmark for video QA: we generate questions and then filter out the easy ones."
