type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Position: Human Baselines in Model Evaluations Need Rigor and Transparency (With Recommendations & Reporting Checklist),https://ICML.cc//virtual/2025/poster/40115,"Kevin Wei, Patricia Paskov, Sunishchal Dev, Michael Byun, Anka Reuel, Xavier Roberts-Gaal, Rachel Calcott, Evie Coxon, Chinmay Deshpande","**In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end.** Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve ""super-human"" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: [https://github.com/kevinlwei/human-baselines](https://github.com/kevinlwei/human-baselines).","Advanced AI systems are more and more able to perform complex, realistic, and profitable tasks. How can we meaningfully figure out if AI systems can perform these tasks as well as humans—and how much better or worse are they? We looked at how other disciplines like psychology, economics, and political science measure differences between groups of humans, and based on what these disciplines do, we wrote guidelines for comparing AI and human performance. We then looked at AI studies that made human vs. AI performance comparisons, and we found that most comparisons aren't very trustworthy. For instance, many studies don't compare AI systems with enough humans, or they are really comparing humans and AIs on different tasks under the hood.This research will help improve our understanding of what AIs can do when compared to what humans can do. That understanding is important not just to AI researchers, but also to companies and users who want to know where AI excels and fails, as well as to policymakers thinking about how AI can be dangerous or how AI can affect jobs. We hope that our research can lead to better AI research, AI usage, and AI policy."
Poster,Position: Humanity Faces Existential Risk from Gradual Disempowerment,https://ICML.cc//virtual/2025/poster/40107,"Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud","This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios commonly discussed in AI safety. We analyze how even incremental improvements in AI capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As AI increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human preferences that often arise from societal systems' reliance on human participation to function. Furthermore, AI systems may amplify existing misalignments with human preferences by optimizing these systems more powerfully. These distortions across domains may be mutually reinforcing: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This analysis suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.","AI risk scenarios usually portray a relatively sudden loss of human control to AIs, outmaneuvering individual humans and human institutions, due to a sudden increase in AI capabilities, or a coordinated betrayal. However, we argue that even an incremental increase in AI capabilities, without any coordinated power-seeking, poses a substantial risk of eventual human disempowerment. This loss of human influence will be centrally driven by having more competitive machine alternatives to humans in almost all societal functions, such as economic labor, decision making, artistic creation, and even companionship.A gradual loss of control of our own civilization might sound implausible. Hasn't technological disruption usually improved aggregate human welfare? We argue that the alignment of societal systems with human interests has been stable only because of the necessity of human participation for thriving economies, states, and cultures. Once this human participation gets displaced by more competitive machine alternatives, our institutions' incentives for growth will be untethered from a need to ensure human flourishing. Decision-makers at all levels will soon face pressures to reduce human involvement across labor markets, governance structures, cultural production, and even social interactions. Those who resist these pressures will eventually be displaced by those who do not.Still, wouldn't humans notice what's happening and coordinate to stop it? Not necessarily. What makes this transition particularly hard to resist is that pressures on each societal system bleed into the others. For example, we might attempt to use state power and cultural attitudes to preserve human economic power. However, the economic incentives for companies to replace humans with AI will also push them to influence states and culture to support this change, using their growing economic power to shape both policy and public opinion, which will in turn allow those companies to accrue even greater economic power.Once AI has begun to displace humans, existing feedback mechanisms that encourage human influence and flourishing will begin to break down. For example, states funded mainly by taxes on AI profits instead of their citizens' labor will have little incentive to ensure citizens' representation. This could occur at the same time as AI provides states with unprecedented influence over human culture and behavior, which might make coordination amongst humans more difficult, thereby further reducing humans' ability to resist such pressures. We describe these and other mechanisms and feedback loops in more detail in this work.Though we provide some proposals for slowing or averting this process, and survey related discussions, we emphasize that no one has a concrete plausible plan for stopping gradual human disempowerment and methods of aligning individual AI systems with their designers' intentions are not sufficient. Because this disempowerment would be global and permanent, and because human flourishing requires substantial resources in global terms, it could plausibly lead to human extinction or similar outcomes."
Poster,Position: In-House Evaluation Is Not Enough. Towards Robust Third-Party Evaluation and Flaw Disclosure for General-Purpose AI,https://ICML.cc//virtual/2025/poster/40170,"Shayne Longpre, Kevin Klyman, Ruth Elisabeth Appel, Sayash Kapoor, Rishi Bommasani, Michelle Sahar, Sean McGregor, Avijit Ghosh, Borhane Blili-Hamelin, Nathan Butters, Alondra Nelson, Amit Elazari, Andrew Sellars, Casey Ellis, Dane Sherrets, Dawn Song, Harley Geiger, Ilona Cohen, Lauren McIlvenny, Madhulika Srikumar, Mark Jaycox, Markus Anderljung, Nadine Johnson, Nicholas Carlini, Nicolas Miailhe, Nik Marda, Peter Henderson, Rebecca Portnoff, Rebecca Weiss, Victoria Westerhoff, Yacine Jernite, Rumman Chowdhury, Percy Liang, Arvind Narayanan","The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped, lagging far behind more established fields like software security. Based on a collaboration between experts from the fields of software security, machine learning, law, social science, and policy, we identify key gaps in the evaluation and reporting of flaws in GPAI systems. We call for three interventions to advance system safety. First, we propose using standardized AI flaw reports and rules of engagement for researchers in order to ease the process of submitting, reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system providers adopt broadly-scoped flaw disclosure programs, borrowing from bug bounties, with legal safe harbors to protect researchers. Third, we advocate for the development of improved infrastructure to coordinate distribution of flaw reports across the many stakeholders who may be impacted. These interventions are increasingly urgent, as evidenced by the prevalence of jailbreaks and other flaws that can transfer across different providers' GPAI systems. By promoting robust reporting and coordination in the AI ecosystem, these proposals could significantly improve the safety, security, and accountability of GPAI systems.","The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped, lagging far behind more established fields like software security. Based on a collaboration between experts from the fields of software security, machine learning, law, social science, and policy, we identify key gaps in the evaluation and reporting of flaws in GPAI systems. We call for three interventions to advance system safety. First, we propose using standardized AI flaw reports and rules of engagement for researchers in order to ease the process of submitting, reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system providers adopt broadly-scoped flaw disclosure programs, borrowing from bug bounties, with legal safe harbors to protect researchers. Third, we advocate for the development of improved infrastructure to coordinate distribution of flaw reports across the many stakeholders who may be impacted. These interventions are increasingly urgent, as evidenced by the prevalence of jailbreaks and other flaws that can transfer across different providers' GPAI systems. By promoting robust reporting and coordination in the AI ecosystem, these proposals could significantly improve the safety, security, and accountability of GPAI systems."
Poster,Position: Iterative Online-Offline Joint Optimization is Needed to Manage Complex LLM Copyright Risks,https://ICML.cc//virtual/2025/poster/40114,"Yanzhou Pan, Jiayi Chen, Jiamin Chen, Zhaozhuo Xu, Denghui Zhang","The infringement risks of LLMs have raised significant copyright concerns across different stages of the model lifecycle. While current methods often address these issues separately, this position paper argues that the LLM copyright challenges are inherently connected, and independent optimization of these solutions leads to theoretical bottlenecks. Building on this insight, we further argue that managing LLM copyright risks requires a systemic approach rather than fragmented solutions. In this paper, we analyze the limitations of existing methods in detail and introduce an iterative online-offline joint optimization framework to effectively manage complex LLM copyright risks. We demonstrate that this framework offers a scalable and practical solution to mitigate LLM infringement risks, and also outline new research directions that emerge from this perspective.","Large language models (LLMs) can unintentionally reproduce copyrighted content, raising serious legal and ethical concerns. However, current methods to address these risks focus on isolated stages—like training or output filtering—without considering how these stages interact.We show that the fragmented approach has inherent limitations and propose a unified, joint optimization framework that coordinates online and offline copyright risk controls throughout the LLM lifecycle.This unified framework enables more effective risk mitigation and better aligns with real-world deployment needs. It not only offers a scalable solution to manage legal exposure but also opens new research directions for building AI systems that are both powerful and compliant with copyright law."
Poster,Position: It Is Time We Test Neural Computation In Vitro,https://ICML.cc//virtual/2025/poster/40174,"Frithjof Gressmann, Ashley Chen, Lily Xie, Nancy Amato, Lawrence Rauchwerger","Recent advances in bioengineering have enabled the creation of biological neural networks in vitro, significantly reducing the cost, ethical hurdles, and complexity of experimentation with genuine biological neural computation. In this position paper, we argue that this trend offers a unique and timely opportunity to put our understanding of neural computation to the test. By designing artificial neural networks that can interact and control living neural systems, it is becoming possible to validate computational models beyond simulation and gain empirical insights to help unlock more robust and energy-efficient next-generation AI systems. We provide an overview of key technologies, challenges, and principles behind this development and describe strategies and opportunities for novel machine learning research in this emerging field. We also discuss implications and fundamental questions that could be answered as this technology advances, exemplifying the longer-term impact of increasingly sophisticated in vitro neural networks.","Recent advances in neuroscience and artificial intelligence (AI) have allowed us to interact and experiment with lab-grown, small brain-like networks of living neurons. We are now at a point where experimenting with these systems is growing easier and more practical. In this position paper, we argue that now is the time to advance experimentation with and understanding of these biological neural networks. By using AI models to interact with and even control these living neurons, this could help learn how to build smarter and more energy-efficient AI. We explain the main technologies, challenges, and potential directions for using these living neurons. We also explore the significant questions that this research could help answer and the long-term implications of these advancements."
Poster,Position: Language model developers should report train-test overlap,https://ICML.cc//virtual/2025/poster/40154,"Andy Zhang, Kevin Klyman, Yifan Mai, Yoav Levine, Yian Zhang, Rishi Bommasani, Percy Liang","Language models are extensively evaluated, but correctly interpreting evaluation results requires knowledge of train-test overlap, which refers to the extent to which the language model is trained on the very data it is being tested on. The public currently lacks adequate information about train-test overlap: most models have no public train-test overlap statistics, and third parties cannot directly measure train-test overlap since they do not have access to the training data. To make this clear, we document the practices of 30 models, finding that just 9 models report train-test overlap: 4 models release training data under open-source licenses, enabling the community to directly measure train-test overlap, and 5 models publish their train-test overlap methodology and statistics. By engaging with language model developers, we provide novel information about train-test overlap for three additional models. Overall, this position paper argues that language model developers should publish train-test overlap statistics and/or training data whenever they report evaluation results on public test sets. We hope our work increases transparency into train-test overlap to increase the community-wide trust in model evaluations.","Language models are computer programs that learn to understand and generate human language by studying huge amounts of text, like books or websites. To see how well they work, we test them with tasks like answering questions or finishing sentences. But to understand the results of these tests, we need to know how much of the test material that the model has seen during its training—this is called *train-test overlap*.The trouble is, we often don’t know if there’s overlap because most model creators don’t share this detail. In a review of 30 language models, only 9 provided details: 4 shared their training data openly for others to check, and 5 explained how they looked for overlap. This gap makes it tough to interpret the test results.Here our position is that model developers should always report train-test overlap details—either by sharing training data or reporting statistics on train-test overlap. This openness would help us trust language model tests more."
Poster,Position: Lifetime tuning is incompatible with continual reinforcement learning,https://ICML.cc//virtual/2025/poster/40153,"Golnaz Mesbahi, Parham Mohammad Panahi, Olya Mastikhina, Steven Tang, Martha White, Adam White","In continual RL we want agents capable of never-ending learning, and yet our evaluation methodologies do not reflect this. The standard practice in RL is to assume unfettered access to the deployment environment for the full lifetime of the agent. For example, agent designers select the best performing hyperparameters in Atari by testing each for 200 million frames and then reporting results on 200 million frames. In this position paper, we argue and demonstrate the pitfalls of this inappropriate empirical methodology: lifetime tuning. We provide empirical evidence to support our position by testing DQN and SAC across several of continuing and non-stationary environments with two main findings: (1) lifetime tuning does not allow us to identify algorithms that work well for continual learning---all algorithms equally succeed; (2) recently developed continual RL algorithms outperform standard non-continual algorithms when tuning is limited to a fraction of the agent's lifetime. The goal of this paper is to provide an explanation for why recent progress in continual RL has been mixed and motivate the development of empirical practices that better match the goals of continual RL.","In continual reinforcement learning (RL) we want agents capable of never-ending learning, and yet our evaluation methodologies do not reflect this. The standard practice in RL is to assume unfettered access to the deployment environment for the full lifetime of the agent. For example, agent designers select the best performing hyperparameters (configurable parameters) in Atari by testing each for 200 million frames and then reporting results on 200 million frames. In this position paper, we argue and demonstrate the pitfalls of this inappropriate empirical methodology: \emph{lifetime tuning}. We provide empirical evidence to support our position by testing DQN and SAC (two popular algorithms) across several continuing and non-stationary environments with two main findings: (1) lifetime tuning does not allow us to identify algorithms that work well for continual learning---all algorithms equally succeed; (2) recently developed continual RL algorithms outperform standard non-continual algorithms when tuning is limited to a fraction of the agent's lifetime. The goal of this paper is to provide an explanation for why recent progress in continual RL has been mixed and motivate the development of empirical practices that better match the goals of continual RL."
Poster,Position: LLMs Need a Bayesian Meta-Reasoning Framework for More Robust and Generalizable Reasoning,https://ICML.cc//virtual/2025/poster/40142,"Hanqi Yan, Linhai Zhang, Jiazheng Li, Zhenyi Shen, Yulan He","Large language models (LLMs)  excel in many reasoning tasks but continue to face significant challenges, such as lack of robustness in reasoning, struggling with cross-task generalization, and inefficiencies in scaling up reasoning capabilities. Current training paradigms, including next-token prediction and reinforcement learning from human feedback, often fall short in adaptability to diverse reasoning tasks. Existing approaches, such as prompt optimization and iterative output refinement, offer performance improvement, but can be inefficient and lack effective generalization. To overcome these limitations, this position paper argues for a transformative shift in how LLMs approach reasoning. Drawing inspiration from cognitive science, particularly meta-reasoning theories such as Dual-Process Theory and Metacognitive Reasoning, we propose a Bayesian meta-reasoning framework for LLMs. Our approach integrates self-awareness, monitoring, evaluation, regulation, and meta-reflection, to enhance LLMs' ability to refine reasoning strategies and generalize across tasks. We revisit existing LLM reasoning methods, identify key challenges, and suggest  directions for future research.","Large language models (LLMs) are getting better at solving different reasoning problems. But they still stumble when asked to reason clearly, adapt to new kinds of problems, or explain their thinking. Why does this happen, and how can we fix it? In this work, we take inspiration from how humans reason — especially from psychology theories about how people monitor and adjust their own thinking. We argue that LLMs should be trained not just to give answers, but also to reflect on how they’re reasoning, much like a person might double-check their logic or change strategies when something feels off. We introduce a new framework that helps LLMs become more self-aware and adaptive by borrowing ideas from cognitive science, such as the “dual-process theory” of fast vs. slow thinking. This framework encourages models to evaluate their own thought processes, regulate their responses, and learn how to generalize their reasoning across a wide range of tasks. Our goal is to spark a shift in how AI systems learn to reason — moving from static answering machines toward dynamic thinkers that can flexibly solve problems and explain their own reasoning. We've also shared tools and resources to help researchers explore this new approach."
Poster,Position: LLM Social Simulations Are a Promising Research Method,https://ICML.cc//virtual/2025/poster/40125,"Jacy Anthis, Ryan Liu, Sean Richardson, Austin Kozlowski, Bernard Koch, Erik Brynjolfsson, James Evans, Michael Bernstein","Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.","In recent years, artificial intelligence (AI) systems have become much more powerful and humanlike. This has led many researchers to test using AI systems, particularly large language models (LLMs) such as ChatGPT and Gemini, to simulate human research subjects in studies of human behavior. However, many researchers remain skeptical of this approach, and there have not been many applications of LLM social simulations beyond initial testing and proof of concept work.In this paper, we argue that five challenges (diversity, bias, sycophancy, alienness, and generalization) stand in the way of widespread use of LLM social simulations. These are significant challenges, but we see exciting opportunities for progress on each. Our argument builds on a literature review of studies run to date and related work. We identify promising directions, including context-rich prompting and fine-tuning LLMs with social science datasets.We believe that LLM social simulations can already be used for exploratory research and building new scientific theories. More widespread use in more applications may soon be possible. Researchers should prioritize developing conceptual models—better ways to make sense of these “digital minds”—and evaluations of simulations so that we can track AI capabilities over time. Accurate and verifiable LLM social simulations can help humanity navigate technological and social change, and they can provide data to train safe and beneficial AI systems."
Poster,Position: Machine Learning Models Have a Supply Chain Problem,https://ICML.cc//virtual/2025/poster/40099,"Sarah Meiklejohn, Hayden Blauzvern, Mihai Maruseac, Spencer Schrock, Laurent Simon, Ilia Shumailov","Powerful machine learning (ML) models are now readily available online, which creates exciting possibilities for users who lack the deep technical expertise or substantial computing resources needed to develop them. On the other hand, this type of open ecosystem comes with many risks. In this paper, we argue that the current ecosystem for open ML models contains significant *supply-chain* risks, some of which have been exploited already in real attacks. These include an attacker replacing a model with something malicious (e.g., malware), or a model being trained using a vulnerable version of a framework or on restricted or poisoned data. We then explore how Sigstore, a solution designed to bring transparency to open-source software supply chains, can be used to bring transparency to open ML models, in terms of enabling model publishers to sign their models and prove properties about the datasets they use.","Powerful machine learning models are now easy to find and download online. This is exciting because people who aren't tech experts or don't have powerful computers can still use them. On the other hand, sharing these models so openly comes with downsides. Specifically, our paper highlights how there are big security risks that come from downloading and using models we don't know much about, similar to downloading and using computer programs from unsafe sources. For example, someone could replace a good model with a harmful one (like a computer virus), or might have built their model in an unsafe way or using data that is bad or that they weren't supposed to use.We suggest using something called Sigstore to make this process safer. Sigstore adds transparency to models by helping people digitally ""sign"" the models they create in a way that proves they're legitimate and shows where the data that was used to create the model came from."
