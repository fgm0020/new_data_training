type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,EmoGrowth: Incremental Multi-label Emotion Decoding with Augmented Emotional Relation Graph,https://ICML.cc//virtual/2025/poster/45725,"Kaicheng Fu, Changde Du, Jie Peng, Kunpeng Wang, Shuangchen Zhao, Xiaoyu Chen, Huiguang He","Emotion recognition systems face significant challenges in real-world applications, where novel emotion categories continually emerge and multiple emotions often co-occur. This paper introduces multi-label fine-grained class incremental emotion decoding, which aims to develop models capable of incrementally learning new emotion categories while maintaining the ability to recognize multiple concurrent emotions. We propose an Augmented Emotional Semantics Learning (AESL) framework to address two critical challenges: past- and future-missing partial label problems. AESL incorporates an augmented Emotional Relation Graph (ERG) for reliable soft label generation and affective dimension-based knowledge distillation for future-aware feature learning. We evaluate our approach on three datasets spanning brain activity and multimedia domains, demonstrating its effectiveness in decoding up to 28 fine-grained emotion categories. Results show that AESL significantly outperforms existing methods while effectively mitigating catastrophic forgetting. Our code is available at https://github.com/ChangdeDu/EmoGrowth.","How can AI systems continuously learn new emotions while recognizing multiple emotions at the same time? Traditional emotion recognition models struggle with this because they either forget past emotions or fail to adapt to new ones.Our work introduces AESL, a method that solves this by (1) using an ""Emotion Relation Graph"" to connect different emotions and fill in missing labels, and (2) incorporating psychological knowledge to help the system prepare for future emotions. Surprisingly, our approach not only handles 28+ fine-grained emotions but also avoids forgetting old ones—something previous methods couldn’t achieve.This breakthrough means AI can better understand complex, real-world emotions, improving applications like mental health support and human-computer interaction. It also challenges the assumption that emotion recognition must trade off flexibility for stability."
Poster,Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection,https://ICML.cc//virtual/2025/poster/45356,"Zhipeng Wei, Yuqi Liu, N. Benjamin Erichson","Jailbreaking techniques trick Large Language Models (LLMs) into producing restricted output, posing a potential threat. One line of defense is to use another LLM as a Judge to evaluate the harmfulness of generated text. However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens. This alters the embeddings of the entire sequence, reducing detection accuracy and allowing harmful content to be misclassified as safe. In this paper, we introduce Emoji Attack, a novel strategy that amplifies existing jailbreak prompts by exploiting token segmentation bias. Our method leverages in-context learning to systematically insert emojis into text before it is evaluated by a Judge LLM, inducing embedding distortions that significantly lower the likelihood of detecting unsafe content. Unlike traditional delimiters, emojis also introduce semantic ambiguity, making them particularly effective in this attack. Through experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack substantially reduces the unsafe prediction rate, bypassing existing safeguards.","Large Language Models (LLMs) are not only used to generate text but also to screen it for safety. Before a message goes public, a specialized ""Judge LLM"" often checks whether the content is harmful—such as hate speech, violence, or misinformation—and blocks it if necessary. However, our research reveals a subtle flaw in how these Judge LLMs understand language. LLMs process text by breaking it into small pieces called tokens. If you change where those breaks occur (even without altering the actual words) you can confuse the model’s understanding. We found that inserting emojis 😊🔥💀 into a generated response is a surprisingly effective way to do this. Emojis don’t just split words into unusual token fragments, they also introduce ambiguity. For example, 🔥 might mean something is exciting or literally on fire, 💀 could signal humor or real-world danger, and 😊 may soften the tone of a toxic message, misleading the model into thinking it’s harmless. We call this method the Emoji Attack. By weaving emojis into generated text, this strategy tricks Judge LLMs into overlooking dangerous content. In tests on ten advanced moderation models, the presence of emojis reduced their ability to flag unsafe messages by over 14%, especially when combined with existing jailbreak techniques. As emojis become more common in everyday communication, this research highlights a key vulnerability in AI safety systems. Developers must build defenses that can reliably interpret meaning, even when messages are masked in subtle, emoji-filled ways. 🤖🔍"
Poster,Emotional Face-to-Speech,https://ICML.cc//virtual/2025/poster/45920,"Jiaxin Ye, Boyuan Cao, Hongming Shan","How much can we infer about an emotional voice solely from an expressive face? This intriguing question holds great potential for applications such as virtual character dubbing and aiding individuals with expressive language disorders. Existing face-to-speech methods offer great promise in capturing identity characteristics but struggle to generate diverse vocal styles with emotional expression. In this paper, we explore a new task, termed *emotional face-to-speech*, aiming to synthesize emotional speech directly from expressive facial cues. To that end, we introduce  **DEmoFace**, a novel generative framework that leverages a discrete diffusion transformer (DiT) with curriculum learning, built upon a multi-level neural audio codec. Specifically, we propose multimodal DiT blocks to dynamically align text and speech while tailoring vocal styles based on facial emotion and identity. To enhance training efficiency and generation quality, we further introduce a coarse-to-fine curriculum learning algorithm for multi-level token processing. In addition, we develop an enhanced predictor-free guidance to handle diverse conditioning scenarios, enabling multi-conditional generation and disentangling complex attributes effectively. Extensive experimental results demonstrate that DEmoFace generates more natural and consistent speech compared to baselines, even surpassing speech-driven methods. Demos of DEmoFace are shown at our project https://demoface.github.io.","How much can we infer about a person’s authentic vocal style—including their voice timbre and emotional prosody—just by observing their facial expressions? This intriguing question has wide-ranging applications, from dubbing virtual characters to assisting individuals with expressive language disorders. To that end, we introduce DEmoFace, a novel system that generates emotional speech based only on visual information with diverse expression. Specifically,  DEmoFace is built upon an advanced diffusion framework, conditioned on visual inputs for acoustic characteristics and text inputs for semantic content. We further develop a multi-conditional guidance mechanism to improve the fidelity to multimodal conditions. Extensive experimental results demonstrate that DEmoFace produces speech with greater naturalness and vocal style consistency compared to existing approaches,  serving as a foundation for multimodal personalized text-to-speech systems."
Poster,Empirical Design in Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46712,"Andrew Patterson, Samuel F Neumann, Martha White, Adam White","Empirical design in reinforcement learning is no small task. Running good experiments requires attention to detail and at times significant computational resources. While compute resources available per dollar have continued to grow rapidly, so have the scale of typical experiments in reinforcement learning. It is now common to benchmark agents with millions of parameters against dozens of tasks, each using the equivalent of 30 days of experience. The scale of these experiments often conflict with the need for statistical evidence, especially when comparing algorithms. Recent studies have highlighted how popular algorithms are sensitive to hyperparameter settings and implementation details, and that common empirical practice leads to weak statistical evidence (Machado et al., 2018; Henderson et al., 2018).This manuscript represents both a call to action, and a comprehensive resource for how to do good experiments in reinforcement learning. In particular, we cover: the statistical assumptions underlying common performance measures, how to properly characterize performance variation and stability, hypothesis testing, special considerations for comparing multiple agents, baseline and illustrative example construction, and how to deal with hyperparameters and experimenter bias. Throughout we highlight common mistakes found in the literature and the statistical consequences of those in example experiments. The objective of this document is to provide answers on how we can use our unprecedented compute to do good science in reinforcement learning, as well as stay alert to potential pitfalls in our empirical design.",
Poster,Empirical Privacy Variance,https://ICML.cc//virtual/2025/poster/44071,"Yuzheng Hu, Fan Wu, Ruicheng Xian, Yuhang Liu, Lydia Zakynthinou, Pritish Kamath, Chiyuan Zhang, David Forsyth","We propose the notion of empirical privacy variance and study it in the context of differentially private fine-tuning of language models. Specifically, we show that models calibrated to the same $(\varepsilon, \delta)$-DP guarantee using DP-SGD with different hyperparameter configurations can exhibit significant variations in empirical privacy, which we quantify through the lens of memorization. We investigate the generality of this phenomenon across multiple dimensions and discuss why it is surprising and relevant. Through regression analysis, we examine how individual and composite hyperparameters influence empirical privacy. The results reveal a no-free-lunch trade-off: existing practices of hyperparameter tuning in DP-SGD, which focus on optimizing utility under a fixed privacy budget, often come at the expense of empirical privacy. To address this, we propose refined heuristics for hyperparameter selection that explicitly account for empirical privacy, showing that they are both precise and practically useful. Finally, we take preliminary steps to understand empirical privacy variance. We propose two hypotheses, identify limitations in existing techniques like privacy auditing, and outline open questions for future research.","Training large language models (LLMs) while protecting the privacy of the data they learn from is a significant challenge. A popular technique called differential privacy (DP) offers strong theoretical guarantees, but we found a surprising issue: even when models are trained with the same level of theoretical privacy protection using a common method (DP-SGD), they can leak significantly different amounts of private information in practice. Our research introduces the concept of ""empirical privacy variance"" to measure this difference in practical privacy under the same theoretical privacy guarantee. We show that how you set the training parameters in DP-SGD has a big, and often overlooked, impact on practical privacy. Standard ways of picking these parameters focus on making the model more useful while adhering to a theoretical privacy budget; we show that this practice unfortunately makes the model ""remember"" more about the training data than necessary, undermining practical privacy. To address this, we propose new strategies for choosing training parameters that consider this practical privacy alongside model performance. These strategies are shown to be effective in producing models that are not only theoretically private but also offer better practical privacy. Our work highlights the gap between theoretical and practical privacy, and calls for a careful reflection of the prevalent usage of DP in LLMs and beyond."
Poster,Empowering World Models with Reflection for Embodied Video Prediction,https://ICML.cc//virtual/2025/poster/44044,"Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-Min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, Yike Guo","Video generation models have made significant progress in simulating future states, showcasing their potential as world simulators in embodied scenarios. However, existing models often lack robust understanding, limiting their ability to perform multi-step predictions or handle Out-of-Distribution (OOD) scenarios.  To address this challenge, we propose the Reflection of Generation (RoG), a set of intermediate reasoning strategies designed to enhance video prediction.  It leverages the complementary strengths of pre-trained vision-language and video generation models, enabling them to function as a world model in embodied scenarios. To support RoG, we introduce Embodied Video Anticipation Benchmark(EVA-Bench), a comprehensive benchmark that evaluates embodied world models across diverse tasks and scenarios, utilizing both in-domain and OOD datasets. Building on this foundation, we devise a world model, Embodied Video Anticipator (EVA), that follows a multistage training paradigm to generate high-fidelity video frames and apply an autoregressive strategy to enable adaptive generalization for longer video sequences. Extensive experiments demonstrate the efficacy of EVA in various downstream tasks like video generation and robotics, thereby paving the way for large-scale pre-trained models in real-world video prediction applications. The video demos are available at https://sites.google.com/view/icml-eva.","Imagine a robot that can look at a video and predict what happens next — not just the next frame, but a whole sequence of actions. This kind of prediction is essential for intelligent machines to safely interact with the real world, whether it’s a kitchen robot or a self-driving car. But current video models often struggle when faced with complex tasks or unfamiliar situations.To solve this, we introduced a new method called Reflection of Generation (RoG). This approach helps the model “reflect” — that is, reason through intermediate steps — before making predictions. We combine strengths from two powerful technologies: video generation and vision-language models (like those used in image captioning).We also built EVA-Bench, a large-scale benchmark that helps evaluate how well these models work in both common and unusual environments. Our new model, EVA, uses this system to generate longer, more accurate video predictions.This work brings us closer to building machines that can understand and anticipate the physical world, with applications in robotics, virtual assistants, and immersive simulations."
Poster,Empower Structure-Based Molecule Optimization with Gradient Guided Bayesian Flow Networks,https://ICML.cc//virtual/2025/poster/46071,"Keyue Qiu, Yuxuan Song, Jie Yu, Hongbo Ma, Ziyao Cao, Zhilong Zhang, Yushuai Wu, Mingyue Zheng, Hao Zhou, Wei-Ying Ma","Structure-based molecule optimization (SBMO) aims to optimize molecules with both continuous coordinates and discrete types against protein targets.A promising direction is to exert gradient guidance on generative models given its remarkable success in images, but it is challenging to guide discrete data and risks inconsistencies between modalities.To this end, we leverage a continuous and differentiable space derived through Bayesian inference, presenting Molecule Joint Optimization (MolJO), the gradient-based SBMO framework that facilitates joint guidance signals across different modalities while preserving SE(3)-equivariance.We introduce a novel backward correction strategy that optimizes within a sliding window of the past histories, allowing for a seamless trade-off between explore-and-exploit during optimization.MolJO achieves state-of-the-art performance on CrossDocked2020 benchmark (Success Rate 51.3\%, Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success Rate compared to the gradient-based counterpart, and 2x ``Me-Better'' Ratio as much as 3D baselines. Furthermore, we extend MolJO to a wide range of optimization settings, including multi-objective optimization and challenging tasks in drug design such as R-group optimization and scaffold hopping, further underscoring its versatility.Code is available at https://github.com/AlgoMole/MolCRAFT.","**Problem**: Designing molecules for new drugs is challenging because we need to optimize both the molecule’s shape (where atoms are positioned, continuous) and their chemical components (what types of atoms to use, discrete). Traditional approaches struggle to effectively and smoothly guide both aspects together.  **Solution**: We developed MolJO, a framework that creates a unified, flexible “workspace” where a molecule’s 3D structure and chemical parts can be optimized together. Think of it as a molecular sculptor that adjusts both the sculpture’s shape and material in real time. A novel “memory window” technique balances exploring new designs and refining existing ones.**Impact**: - 4× better success rate than previous gradient-based methods (51.3%).  - Produces molecules with stronger binding (-9.05 docking score) and safer chemical profiles (SA score 0.78).  - Adapts to practical tasks like R-group design and scaffold hopping."
Poster,Enabling Optimal Decisions in Rehearsal Learning under CARE Condition,https://ICML.cc//virtual/2025/poster/44293,"Wen-Bo Du, Hao-Yi Lei, Lue Tao, Tian-Zuo Wang, Zhi-Hua Zhou","In the field of machine learning (ML), an essential type of decision-related problem is known as AUF (Avoiding Undesired Future): if an ML model predicts an undesired outcome, how can decisions be made to prevent it? Recently, a novel framework called *rehearsal learning* has been proposed to address the AUF problem. Despite its utility in modeling uncertainty for decision-making, it remains unclear *under what conditions* and *how* optimal actions that maximize the *AUF probability* can be identified. In this paper, we propose *CARE* (CAnonical REctangle), a condition under which the maximum AUF probability can be achieved. Under the CARE condition, we present a projection-Newton algorithm to select actions and prove that the algorithm achieves superlinear convergence to the optimal one. Besides, we provide a generalization method for adopting the algorithm to AUF scenarios beyond the CARE condition. Finally, we demonstrate that a closed-form solution exists when the outcome is a singleton variable, substantially reducing the time complexity of decision-making. Experiments validate the effectiveness and efficiency of our method.","Machine learning models sometimes predict undesirable future outcomes, like a drone risking package loss. The challenge is deciding how to act—adjusting the drone's flight, for instance—to prevent this, especially when real-world tests are costly or risky and the exact impact of actions is unclear.Our work offers a novel method for better decision-making. We define a *CARE (CAnonical REctangle)* condition allowing precise calculation of actions that maximize the chance of good outcomes. We developed an efficient algorithm for this and a way to adapt it if *CARE* isn't fully met. For single-outcome scenarios, we provide a direct formula for the best action.Our research offers a more reliable way to act on undesirable predictions. By directly maximizing the probability of a positive result, our method enables more effective actions than prior approaches. Its efficiency, particularly the direct formula for simpler cases, makes it practical for real-world systems to better avoid negative outcomes and achieve desired results."
Poster,ENAHPool: The Edge-Node Attention-based Hierarchical Pooling for Graph Neural Networks,https://ICML.cc//virtual/2025/poster/43473,"Zhehan Zhao, Lu Bai, Lixin Cui, Ming Li, Ziyu Lyu, Lixiang Xu, Yue Wang, Edwin Hancock","Graph Neural Networks (GNNs) have emerged as powerful tools for graph learning, and one key challenge arising in GNNs is the development of effective pooling operations for learning meaningful graph representations. In this paper, we propose a novel Edge-Node Attention-based Hierarchical Pooling (ENAHPool) operation for GNNs. Unlike existing cluster-based pooling methods that suffer from ambiguous node assignments and uniform edge-node information aggregation, ENAHPool assigns each node exclusively to a cluster and employs attention mechanisms to perform weighted aggregation of both node features within clusters and edge connectivity strengths between clusters, resulting in more informative hierarchical representations. To further enhance the model performance, we introduce a Multi-Distance Message Passing Neural Network (MD-MPNN) that utilizes edge connectivity strength information to enable direct and selective message propagation across multiple distances, effectively mitigating the over-squashing problem in classical MPNNs. Experimental results demonstrate the effectiveness of the proposed method.","Graph-based structured data is prevalent in real world applications. How to accurately identify their categories is always a challenging problem arising in existing researches. The aim of this work is to propose a novel Edge-Node Attention-based Hierarchical Pooling method for graph classification. This approach can hierarchically assign the nodes into different clusters, and further adaptively aggregate both the node features within clusters as well as the edge connectivity strengths between clusters. As a result, the proposed approach is able to capture more informative hierarchical representations, enhancing the classification performance."
Poster,EncryptedLLM: Privacy-Preserving Large Language Model Inference via GPU-Accelerated Fully Homomorphic Encryption,https://ICML.cc//virtual/2025/poster/45395,"Leo de Castro, Daniel Escudero, Adya Agrawal, Antigoni Polychroniadou, Manuela Veloso","As large language models (LLMs) become more powerful, the computation required to run these models is increasingly outsourced to a third-party cloud. While this saves clients' computation, it risks leaking the clients' LLM queries to the cloud provider. Fully homomorphic encryption (FHE) presents a natural solution to this problem: simply encrypt the query and evaluate the LLM homomorphically on the cloud machine. The result remains encrypted and can only be learned by the client who holds the secret key. In this work, we present a GPU-accelerated implementation of FHE and use this implementation to benchmark an encrypted GPT-2 forward pass, with runtimes over $200\times$ faster than the CPU baseline. We also present novel and extensive experimental analysis of approximations of LLM activation functions to maintain accuracy while achieving this performance.","Large language models (LLMs) are typically deployed in cloud environments. To use these models, the user's data must be sent to an external cloud machine. For sensitive queries (e.g., topics related to healthcare or finance), this represents a major security concern. This work improves the efficiency of techniques to privately evaluate models over sensitive queries. This allows users to safely send their query to a cloud machine and receive the model output without allowing the cloud to learn anything about their data. The main underlying tool is an advanced cryptography primitive called fully homomorphic encryption (FHE), and a technical contribution of this work is a new GPU-accelerated implementation of FHE. We also develop methods to evaluate LLMs using FHE while preserving the quality of the model outputs."
