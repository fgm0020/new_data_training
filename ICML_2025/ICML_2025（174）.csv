type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models,https://ICML.cc//virtual/2025/poster/45191,"Parshin Shojaee, Ngoc Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa Doan, Chandan Reddy","Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect actual discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorization, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods on LLM-SRBench, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy.These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.","Scientists have long sought to discover mathematical equations that explain how the natural world works, from gravity to climate patterns. Recently, researchers have been testing whether large language models (LLMs) can help in finding these scientific models by using their vast knowledge. However, current tests of these discovery frameworks are flawed because they are based on benchmarks with well-known equations that LLMs might have simply memorized during training. In this work, we created LLM-SRBench, a challenging new benchmark with 239 difficult problems across four scientific domains, addressing the memorization issue of current benchmarks. Our benchmark includes two types of challenges: problems that disguise familiar physics equations in unfamiliar mathematical forms (LSR-Transform), and completely synthetic problems that require genuine reasoning from data rather than recalling memorized facts (LSR-Synth).When we tested several leading LLM-based discovery frameworks on our benchmark, even the best performer only solved about one-third of the problems correctly. This reveals that current frameworks are far from being able to truly discover scientific equations on its own. Our benchmark provides researchers with a more honest way to measure progress in LLM-assisted scientific discovery, helping guide future breakthroughs in this emerging field."
Poster,LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations,https://ICML.cc//virtual/2025/poster/46274,"Anian Ruoss, Fabio Pardo, Harris Chan, Bonnie Li, Vlad Mnih, Tim Genewein","In this paper, we present a benchmark to pressure-test today’s frontier models’ multimodal decision-making capabilities in the very long-context regime (up to one million tokens) and investigate whether these models can learn from large numbers of expert demonstrations in their context. We evaluate the performance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0 Flash Experimental, GPT-4o, o1-mini, o1-preview, and o1 as policies across a battery of simple interactive decision-making tasks: playing tic-tac-toe, chess, and Atari, navigating grid worlds, solving crosswords, and controlling a simulated cheetah. We study increasing amounts of expert demonstrations in the context — from no demonstrations to 512 full episodes. Across our tasks, models rarely manage to fully reach expert performance, and often, presenting more demonstrations has little effect. Some models steadily improve with more demonstrations on a few tasks. We investigate the effect of encoding observations as text or images and the impact of chain-of-thought prompting. To help quantify the impact of other approaches and future innovations, we open source our benchmark that covers the zero-, few-, and many-shot regimes in a unified evaluation.","We evaluate how well large language models (LLMs), such as Gemini, ChatGPT, and Claude, can perform in simulated interactive environments (e.g., moving a player to a target in a 2D world) and games (e.g., tic-tac-toe, chess, Atari). Performing well on these tasks requires cognitive skills that are not typically assessed by many standard LLM benchmarks and are relevant to, for example, real-life robotics applications. We test how well LLMs perform in two types of settings: one, how well LLMs do when the only source of information is the data produced by interacting with the environment. And two, whether and how this changes when they are given lots of examples from an expert that they could (in principle) imitate. Our results show that most models did not achieve expert-level performance and that providing additional examples often does not significantly improve their performance. Accordingly, our results indicate that LLMs are not yet fully ready for use in these domains. We share our testbed publicly, allowing other researchers to build upon our work and measure whether and to what extent new methods enhance the capabilities of LLMs."
Poster,LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models,https://ICML.cc//virtual/2025/poster/44395,"Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, Sergey Levine","Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. Even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, the emergence of complex skills such as persuasion, and long-horizon strategic behavior, such as in the context of games. Enabling this requires the community to develop reliable reinforcement learning algorithms for training LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework for getting started on multi-turn RL with offline value-based and online policy-based RL methods. Our benchmark consists of 3 Interactive Dialogue tasks and 5 RL Capability tests for a total of 8 tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.","Large language models like ChatGPT are great at writing fluent text, but they often struggle when it comes to making decisions over multiple steps—like holding a strategic conversation, asking good follow-up questions, or planning ahead in a game. This is because these models are not trained to act like agents with goals that unfold over time. Our research tackles this by combining language models with reinforcement learning (RL), a method where agents learn by trial and error to make better decisions, and are rewarded for making good decisions. But to build truly smart and goal-directed language agents, we first need good scenarios to test our agents and reliable ways to measure progress.This is why we built LMRL-Gym, a new benchmark suite that helps researchers train and evaluate language models in tasks that require multiple turns of interaction. These include strategic dialogues, open-ended games, and other challenges where a model has to think ahead and improve over time. By sharing these tasks and training tools openly, we aim to accelerate progress toward language agents that are not just good at generating text, but can act with purpose and improve through experience."
Poster,LOB-Bench: Benchmarking Generative AI for Finance - an Application to Limit Order Book Data,https://ICML.cc//virtual/2025/poster/46058,"Peer Nagy, Sascha Frey, Kang Li, Bidipta Sarkar, Svitlana Vyetrenko, Stefan Zohren, Anisoara Calinescu, Jakob Foerster","While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms. To address this, we present **LOB-Bench**, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format. Our framework  measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation. The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network. Lastly, LOB-Bench contains ""market impact metrics"", i.e. the cross-correlations and price response functions for specific events in the data. We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes.","High-frequency trading data is an interesting task to consider for models which aim to generate sequential events, i.e. where a given generated piece of data depends on all the previously generated data. There are a number of models which attempt to generate this kind of high-frequency financial data using different approaches, but it is very difficult to compare them. This paper aims to provide a series of evaluations to allow for the comparison of data generated by different models. The benchmark consists of three main parts. The first considers different features that can be measured from a sequence of generated events. For example, the number of orders to buy or a sell a stock in a time period. Measuring this feature across a large number of sequences allows for the construction of a distribution, in practice a histogram. This distribution can be constructed for both real and generated data, and metrics can be applied to measure how similar or different these distributions are. We also consider the case, where a neural network itself learns the distinguishing features of the data. Secondly, we consider the price impact of different order types. A sign that a generative model is able to reproduce sequences well is if the arrival of a given order type moves the price in an expected way, on average, over some time-period. Finally, we use generated data to see how this affects the learning of a trend forecasting task. We compare both cutting-edge models for sequence generation with more traditional models and find that the newer models outperform the traditional ones. Having access to this sort of benchmark is very important as it allows researchers to compare how good their models are in this application in a standardised way."
Poster,Local Identifying Causal Relations in the Presence of Latent Variables,https://ICML.cc//virtual/2025/poster/45459,"Zheng Li, Zeyu Liu, Feng Xie, Hao Zhang, Chunchen LIU, zhi geng","We tackle the problem of identifying whether a variable is the cause of a specified target using observational data. State-of-the-art causal learning algorithms that handle latent variables typically rely on identifying the global causal structure, often represented as a partial ancestral graph (PAG), to infer causal relationships. Although effective, these approaches are often redundant and computationally expensive when the focus is limited to a specific causal relationship. In this work, we introduce novel local characterizations that are necessary and sufficient for various types of causal relationships between two variables, enabling us to bypass the need for global structure learning. Leveraging these local insights, we develop efficient and fully localized algorithms that accurately identify causal relationships from observational data. We theoretically demonstrate the soundness and completeness of our approach. Extensive experiments on benchmark networks and real-world datasets further validate the effectiveness and efficiency of our method.","In this work, we introduce novel local characterizations that are necessary and sufficient for various types of causal relationships between two variables, enabling us to bypass the need for global structure learning. Leveraging these local insights, we develop efficient and fully localized algorithms that accurately identify causal relationships from observational data. We theoretically demonstrate the soundness and completeness of our approach."
Poster,Locality Preserving Markovian Transition for Instance Retrieval,https://ICML.cc//virtual/2025/poster/45178,"Jifei Luo, Wenzheng Wu, Hantao Yao, Lu Yu, Changsheng Xu","Diffusion-based re-ranking methods are effective in modeling the data manifolds through similarity propagation in affinity graphs. However, positive signals tend to diminish over several steps away from the source, reducing discriminative power beyond local regions. To address this issue, we introduce the Locality Preserving Markovian Transition (LPMT) framework, which employs a long-term thermodynamic transition process with multiple states for accurate manifold distance measurement. The proposed LPMT first integrates diffusion processes across separate graphs using Bidirectional Collaborative Diffusion (BCD) to establish strong similarity relationships. Afterwards, Locality State Embedding (LSE) encodes each instance into a distribution for enhanced local consistency. These distributions are interconnected via the Thermodynamic Markovian Transition (TMT) process, enabling efficient global retrieval while maintaining local effectiveness. Experimental results across diverse tasks confirm the effectiveness of LPMT for instance retrieval.","In image retrieval systems, the exclusive reliance on basic distance metrics such as the commonly used Euclidean distance and Cosine similarity between raw image features often results in suboptimal retrieval performance, as such features may fail to adequately capture the high-level semantics inherent in images. Nevertheless, we observe that feature representations of semantically similar images, even when initially distant in the feature space, often reside on a low-dimensional manifold, forming a smooth and continuous trajectory rather than appearing as isolated points.We leverage the manifold assumption to improve image retrieval through a novel diffusion-based manifold ranking method that uncovers the intrinsic structure of the feature space. To address the issue of information loss, where positive signals tend to vanish after several steps of diffusion from the source, our method first represents each image as a probability distribution shaped by its local neighborhood structure. The distance between images is then measured as the minimal transition cost required to transform one distribution into another along a multi-step path, with each transition restricted to a local region. This design enables robust information propagation across the manifold while preserving local semantic consistency.Our research proposes a versatile solution that can be seamlessly integrated as a post-processing module to enhance a wide range of image retrieval systems. Furthermore, by uncovering deeper, manifold-based similarities, our approach can also benefit other machine learning algorithms that require a nuanced understanding of complex data structures."
Poster,Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning,https://ICML.cc//virtual/2025/poster/45958,"Kyowoon Lee, Jaesik Choi","Recent advances in diffusion-based generative modeling have demonstrated significant promise in tackling long-horizon, sparse-reward tasks by leveraging offline datasets. While these approaches have achieved promising results, their reliability remains inconsistent due to the inherent stochastic risk of producing infeasible trajectories, limiting their applicability in safety-critical applications. We identify that the primary cause of these failures is inaccurate guidance during the sampling procedure, and demonstrate the existence of manifold deviation by deriving a lower bound on the guidance gap. To address this challenge, we propose *Local Manifold Approximation and Projection* (LoMAP), a *training-free* method that projects the guided sample onto a low-rank subspace approximated from offline datasets, preventing infeasible trajectory generation. We validate our approach on standard offline reinforcement learning benchmarks that involve challenging long-horizon planning. Furthermore, we show that, as a standalone module, LoMAP can be incorporated into the hierarchical diffusion planner, providing further performance enhancements.","Artificial intelligence (AI) is increasingly used to plan tasks that require multiple steps, like navigating robots through complex environments. But a common problem arises: AI planners sometimes suggest unrealistic or impossible paths, which can be dangerous or ineffective.We propose a simple approach called LoMAP that helps AI planners stay closer to realistic outcomes. Instead of allowing the AI to drift away from feasible solutions, our method regularly checks and gently corrects its plans to ensure they remain practical and safe.By testing LoMAP on tasks such as robot navigation and movement control, we found it significantly reduces the number of unrealistic solutions generated. This improvement helps make AI-driven decision-making more reliable, especially in safety-critical situations."
Poster,Local Pan-privacy for Federated Analytics,https://ICML.cc//virtual/2025/poster/45560,"Vitaly Feldman, Audra McMillan, Guy Rothblum, Kunal Talwar","Pan-privacy was proposed by Dwork et al. (2010) as an approach to designing a private analytics system that retains its privacy properties in the face of intrusions that expose the system's internal state. Motivated by Federated telemetry applications, we study {\em local pan-privacy}, where privacy should be retained under repeated unannounced intrusions {\em on the local state}. We consider the problem of monitoring the count of an event in a federated system, where event occurrences on a local device should be hidden even from an intruder on that device. We show that under reasonable constraints, the goal of providing information-theoretic differential privacy under intrusion is incompatible with collecting telemetry information. We then show that this problem can be solved in a scalable way using standard cryptographic primitives.","In this work, we define and study a formal notion of privacy in the federated setting, which allows for privacy to be retained under intrusions on device."
Poster,LOCATE 3D: Real-World Object Localization via Self-Supervised Learning in 3D,https://ICML.cc//virtual/2025/poster/45895,"Paul McVay, Sergio Arnaud, Ada Martin, Arjun Majumdar, Krishna Murthy Jatavallabhula, Phillip Thomas, Ruslan Partsey, Daniel Dugas, Abha Gejji, Alexander Sax, Vincent-Pierre Berges, Mikael Henaff, Ayush Jain, Ang Cao, Ishita Prasad, Mrinal Kalakrishnan, Michael Rabbat, Nicolas Ballas, Mahmoud Assran, Oleksandr Maksymets, Aravind Rajeswaran, Franziska Meier","We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like ""the small coffee table between the sofa and the lamp."" LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model. Code, models and dataset can be found at the project website: locate3d.atmeta.com","For robot assistants to become commonplace and perform household tasks alongside humans, we will need to communicate via natural language. This means that the robot will need to be able to differentiate objects based on object names, descriptions, and spatial relationships to successfully ‘put away the pillows on the bed’. This work describes a system that is able to locate objects in 3D space from natural language input.Given the limited amount of 3D data with objects and descriptions, this work uses a three-fold approach. First, we leverage image models to incorporate knowledge from labeled and unlabeled 2D data. Second, we develop a new algorithm to learn from unlabeled 3D data. Finally, we develop a new approach to learn from labeled 3D data. Additionally, we release additional labeled 3D data. This system achieves state of the art performance on existing benchmarks that measure location accuracy in 3D based on natural language descriptions. We also show the system performs well in robotic use-cases."
Poster,Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing,https://ICML.cc//virtual/2025/poster/44270,"Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan Cheng, Lijie Hu, Di Wang","The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve knowledge with implicit subject information from deeper MLP layers, unlike single-hop tasks, which rely on shallow layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers with single-hop edit prompts, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further incorporates multi-hop editing prompts to locate and modify knowledge across different stages of reasoning. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, overcoming the limitations of previous locate-then-edit methods.","While previous locate-then-edit methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge in LLM. To solve this problem, leveraging tools in mechanistic interpretability, we first identify the mechanism and then propose IFMET to edit both shallow and deep MLP layers. Our IFMET significantly improves performance on multi-hop factual recall tasks, overcoming the limitations of previous locate-then-edit methods."
