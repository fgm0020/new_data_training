type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,PEINR: A Physics-enhanced Implicit Neural Representation for High-Fidelity Flow Field Reconstruction,https://ICML.cc//virtual/2025/poster/46640,"Liming Shen, Liang Deng, Chongke Bi, Yu Wang, Xinhai Chen, Yueqing Wang, Jie Liu","Implicit neural representation (INR) has now been thrust into the limelight with its flexibility in high-fidelity flow field reconstruction tasks. However, the lack of standard benchmarking datasets and the grid independence assumption for INR-based methods hinder progress and adoption in real-world simulation scenarios. Moreover, naive adoptions of existing INR frameworks suffer from limited accuracy in capturing fine-scale structures and spatiotemporal dynamics. Tacking these issues, we first introduce HFR-Beach, a 5.4 TB public large-scale CFD dataset with 33,600 unsteady 2D and 3D vector fields for reconstructing high-fidelity flow fields. We further present PEINR, a physics-enhanced INR framework, to enrich the flow fields by concurrently enhancing numerical-precision and grid-resolution. Specifically, PEINR is mainly composed of physical encoding and transformer-based spatiotemporal fuser (TransSTF). Physical encoding decouples temporal and spatial components, employing Gaussian coordinate encoding and localized encoding techniques to capture the nonlinear characteristics of spatiotemporal dynamics and the stencil discretization of spatial dimensions, respectively. TransSTF fuses both spatial and temporal information via transformer for capturing long-range temporal dependencies. Qualitative and quantitative experiments and demonstrate that PEINR outperforms state-of-the-art INR-based methods in reconstruction quality.","High-fidelity simulations of fluid dynamics are essential for understanding complex physical phenomena in science and engineering. A new class of machine learning models, called Implicit Neural Representations (INRs), can reconstruct these fluid flow fields with impressive detail. However, INR models face major limitations: they often struggle with real-world data, which is imbalanced across space and time, and there is currently no standardized dataset to benchmark progress in this field.To address these challenges, our work contributes two major innovations. First, we release HFR-Bench, a large-scale public dataset containing over 33,000 high-resolution 2D and 3D flow fields. This gives researchers a shared foundation to compare their models and spur further advancements. Second, we introduce PEINR, a novel physics-informed INR model. It encodes spatial and temporal information using techniques inspired by physics and uses transformer-based neural networks to better capture dynamic flow behaviors over time.Our approach leads to significantly better reconstructions than existing methods, particularly in challenging scenarios with complex, time-varying flows. This work sets the stage for more reliable and physically grounded fluid simulations powered by AI."
Poster,Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data,https://ICML.cc//virtual/2025/poster/45886,"Jeonghye Kim, Yongjae Shin, Whiyoung Jung, Sunghoon Hong, Deunsol Yoon, Youngchul Sung, Kanghoon Lee, Woohyung Lim","Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task.","How can we prevent AI from overestimating the value of data it hasn't seen?AI learns the value of actions from the data it has observed, but this understanding is limited to the training distribution. When it encounters situations just beyond that range, especially if the data ends on an upward trend, it may mistakenly assume that unseen actions will have even higher value. In reality, such areas are often unpredictable, and overestimating their value can lead to poor or unsafe decisions.This paper aims to address the problem of incorrect extrapolation by encouraging the AI to naturally lower its value estimates for actions outside the data boundary. This helps prevent overly optimistic predictions. To this end, it proposes two key techniques:- **Reward scaling**: By increasing the scale of rewards and applying normalization, the AI learns to more clearly distinguish in-distribution actions from those outside. This reduces the influence of training signals on out-of-distribution actions, helping to keep predictions in those areas lower.- **Penalizing infeasible actions**: Actions that are far from the feasible region or clearly unrealistic are explicitly assigned low values, encouraging the values of actions outside the data range to be pulled downward naturally.These two strategies are combined in a simple yet effective algorithm called PARS.PARS outperforms existing methods across a wide range of tasks and helps the AI make safer, more reliable decisions in complex environments. It also performs well when the AI continues learning from new data. PARS allows smooth adaptation without losing the stability gained from prior training."
Poster,PENCIL: Long Thoughts with Short Memory,https://ICML.cc//virtual/2025/poster/46351,"Chenxiao Yang, Nati Srebro, David McAllester, Zhiyuan Li","While state-of-the-art LLMs have demonstrated great promise of using long Chains-of-Thought (CoT) to boost reasoning, scaling it up to more challenging problems is fundamentally limited by suboptimal memory usage — intermediate computations accumulate indefinitely in context even no longer needed for future thoughts. We introduce PENCIL, which incorporates a novel reduction mechanism into the autoregressive generation process that recursively clean up intermediate thoughts based on patterns learned from training. By alternately generating and erasing, PENCIL can think deeper to solve harder problems using shorter context and less computes. Empirically, for example, we demonstrate PENCIL with a small 25M-parameter transformer and 2048 context length solves Einstein's puzzle — a task that challenges much larger models like GPT-4. Theoretically, we prove PENCIL can perform universal efficient computation by simulating any Turing machines with optimal time and space complexity, and thus can solve arbitrary computable tasks that are otherwise intractable for vanilla CoT.","We propose PENCIL, a new LLM reasoning approach that generates and erases thoughts, enabling longer and deeper thinking with shorter context. Theoretically, PENCIL is Turing-complete with optimal space and time complexity, and thus can solve arbitrary computable problems efficiently."
Poster,PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion,https://ICML.cc//virtual/2025/poster/45889,"Sophia Tang, Yinuo Zhang, Pranam Chatterjee, PhD","We present **PepTune**, a multi-objective discrete diffusion model for simultaneous generation and optimization of therapeutic peptide SMILES. Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid peptide structures with a novel bond-dependent masking schedule and invalid loss function. To guide the diffusion process, we introduce **Monte Carlo Tree Guidance (MCTG)**, an inference-time multi-objective guidance algorithm that balances exploration and exploitation to iteratively refine Pareto-optimal sequences. MCTG integrates classifier-based rewards with search-tree expansion, overcoming gradient estimation challenges and data sparsity. Using PepTune, we generate diverse, chemically-modified peptides simultaneously optimized for multiple therapeutic properties, including target binding affinity, membrane permeability, solubility, hemolysis, and non-fouling for various disease-relevant targets. In total, our results demonstrate that MCTG for masked discrete diffusion is a powerful and modular approach for multi-objective sequence design in discrete state spaces.","Peptides — short protein-like drugs such as GLP-1 receptor agonists (think Ozempic and Wegovy) — have transformed treatment for diabetes and obesity. But what if we could create Ozempic-like therapies to treat cancer, reproductive, or neurodegenerative disease? PepTune moves us toward that goal. Unlike traditional methods that depend on a protein’s 3D structure, PepTune works directly from the target sequence, enabling peptide design for disease-driving proteins that are often flexible and disordered. Our model generates entirely new peptides that not only bind their targets but also meet multiple drug-like criteria — such as solubility, low toxicity, and membrane permeability — critical for clinical success. At the heart of PepTune is a powerful multi-objective algorithm that combines a generative model with a search strategy to learn which sequences optimally balance these competing properties. The result: peptide drugs that are not only effective, but also developable, allowing us to make better, safer therapies faster."
Poster,Perception in Reflection,https://ICML.cc//virtual/2025/poster/44894,"Yana Wei, Liang Zhao, Kangheng Lin, En Yu, Yuang Peng, Runpei Dong, Jianjian Sun, Haoran Wei, Zheng Ge, Xiangyu Zhang, Vishal Patel","We present a perception in reflection paradigm designed to transcend the limitations of current large vision-language models (LVLMs), which are expected yet often fail to achieve perfect perception initially. Specifically, we propose Reflective Perception (RePer), a dual-model reflection mechanism that systematically alternates between policy and critic models, enables iterative refinement of visual perception. This framework is powered by Reflective Perceptual Learning (RPL), which reinforces intrinsic reflective capabilities through a methodically constructed visual reflection dataset and reflective unlikelihood training Comprehensive experimental evaluation demonstrates RePer's quantifiable improvements in image understanding, captioning precision, and hallucination reduction. Notably, RePer achieves strong alignment between model attention patterns and human visual focus, while RPL optimizes fine-grained and free-form preference alignment. These advancements establish perception in reflection as a robust paradigm for future multimodal agents, particularly in tasks requiring complex reasoning and multi-step manipulation. Project Page: [https://weiyana.github.io/Perception-in-Reflection](https://weiyana.github.io/Perception-in-Reflection)","AI systems that interpret images and generate text often make mistakes, such as describing objects that are not present or overlooking important visual details. These issues are especially common in models that combine vision and language, which struggle to align what they see with how they describe it. To address this, we developed a method called RePer that helps vision-language models reflect on their own errors and improve through step-by-step corrections. RePer learns from feedback, adjusting its responses over multiple rounds, much like how people revise their thinking. This approach trains models to better align their visual focus with human attention and generate more accurate image descriptions. We also introduced a new benchmark that evaluates whether a model’s understanding of images matches how humans perceive them. In both automated and human evaluations, RePer consistently outperforms existing models. Our work shows that adding reflection and feedback to AI systems can significantly enhance their reliability and interpretability. This brings us closer to building AI tools that see and describe the world in ways people can understand and trust."
Poster,Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting,https://ICML.cc//virtual/2025/poster/44339,"Hongbi ZHOU, Zhangkai NI","3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a perceptual sensitivity-adaptive distribution to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: https://github.com/eezkni/Perceptual-GS","Efficient and high-quality 3D scene reconstruction remains a significant challenge in computer vision. We investigate whether theories related to human perception can guide the trade-off between quality and efficiency in 3D scene reconstruction.Building on the theory that the Human Visual System is highly sensitive to complex textures but less so to smooth areas, we introduce Perceptual-GS. This approach ensures that the reconstructed 3D scenes align with characteristics of human perception. Our experiments show that Perceptual-GS significantly outperforms existing methods in reconstruction quality, particularly in perceptual quality, while also achieving superior efficiency in terms of both storage and computation.Moreover, when combined with other techniques, our method demonstrates enhanced effectiveness, highlighting its potential for a broader range of 3D reconstruction applications."
Poster,Perceptually Constrained Precipitation Nowcasting Model,https://ICML.cc//virtual/2025/poster/45973,"Wenzhi Feng, Xutao Li, Zhe Wu, Kenghong Lin, Demin Yu, Yunming Ye, Yaowei Wang","Most current precipitation nowcasting methods aim to capture the underlying spatiotemporal dynamics of precipitation systems by minimizing the mean square error (MSE). However, these methods often neglect effective constraints on the data distribution, leading to unsatisfactory prediction accuracy and image quality, especially for long forecast sequences. To address this limitation, we propose a precipitation nowcasting model incorporating perceptual constraints. This model reformulates precipitation nowcasting as a posterior MSE problem under such constraints. Specifically, we first obtain the posteriori mean sequences of precipitation forecasts using a precipitation estimator. Subsequently, we construct the transmission between distributions using rectified flow. To enhance the focus on distant frames, we design a frame sampling strategy that gradually increases the corresponding weights. We theoretically demonstrate the reliability of our solution, and experimental results on two publicly available radar datasets demonstrate that our model is effective and outperforms current state-of-the-art models.","Most Current precipitation nowcasting methods mainly focus on reducing prediction errors to improve performance. However, they often overlook how precipitation is actually distributed, which can lead to blurry forecast images and lower accuracy — especially for longer time periods.To tackle this problem, we introduce a new approach that adds perceptual constraints to the forecasting process. This helps the model not only reduce errors, but also better understand and reproduce how precipitation really looks and behaves. We first predict the general trend of future rainfall, and then use a rectified flow model to adjust the results so they better match real-world patterns. We also design a strategy that encourages the model to pay more attention to frames further in the future. Experiments on several public radar datasets show that our method produces clearer, more accurate forecasts compared to existing approaches."
Poster,Peri-LN: Revisiting Normalization Layer in the Transformer Architecture,https://ICML.cc//virtual/2025/poster/44675,"Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Kang Min Yoo","Selecting a layer normalization (LN) strategy that stabilizes training and speeds convergence in Transformers remains difficult, even for today’s large language models (LLM). We present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformers. Until recently, Pre-LN and Post-LN have long dominated practices despite their limitations in large-scale training. However, several open-source models have recently begun silently adopting a third strategy without much explanation. This strategy places normalization layer **peripherally** around sublayers, a design we term **Peri-LN**. While Peri-LN has demonstrated promising performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis delineates the distinct behaviors of LN strategies, showing how each placement shapes activation variance and gradient propagation. To validate our theoretical insight, we conduct extensive experiments on Transformers up to $3.2$B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement of LN.","Training today’s large language models is a bit like building a very tall tower of blocks: unless each layer is carefully aligned, the whole structure can wobble or even collapse. One of the “alignment tools” engineers use is **layer normalization**, which keeps the numbers inside the model from drifting too high or too low. Most builders put this tool either **before** or **after** each layer, but both choices have hidden drawbacks—one can weaken the learning signal, while the other can let problematically large numbers sneak through.Our study shines a spotlight on a quieter third option, where we wrap each layer **both before and after** with normalization—an arrangement we call **Peri-LN** (“peri” meaning “around”). By rigorously comparing all three setups across models with up to 3 billion parameters, we show that Peri-LN keeps calculations balanced, prevents training crashes. This simple change could make future language models more reliable, cheaper to train, and accessible to more research groups—helping the field progress without wasting massive computing resources."
Poster,Peripheral Memory for LLMs: Integration of Sequential Memory Banks with Adaptive Querying,https://ICML.cc//virtual/2025/poster/46386,"Songlin Zhai, Yuan Meng, Yongrui Chen, Yiwei Wang, Guilin Qi","Large Language Models (LLMs) have revolutionized various natural language processing tasks with their remarkable capabilities. However, challenges persist in effectively integrating new knowledge into LLMs without compromising their performance, particularly in the Large Language Models (LLMs) have revolutionized various natural language processing tasks with their remarkable capabilities. However, a challenge persists in effectively processing new information, particularly in the area of long-term knowledge updates without compromising model performance. To address this challenge, this paper introduces a novel memory augmentation framework that conceptualizes memory as a peripheral component (akin to physical RAM), with the LLM serving as the information processor (analogous to a CPU). Drawing inspiration from RAM architecture, we design memory as a sequence of memory banks, each modeled using Kolmogorov-Arnold Network (KAN) to ensure smooth state transitions. Memory read and write operations are dynamically controlled by query signals derived from the LLMs' internal states, closely mimicking the interaction between a CPU and RAM. Furthermore, a dedicated memory bank is used to generate a mask value that indicates the relevance of the retrieved data, inspired by the sign bit in binary coding schemes. The retrieved memory feature is then integrated as a prefix to enhance the model prediction. Extensive experiments on knowledge-based model editing validate the effectiveness and efficiency of our peripheral memory.","Large language models (AI systems like ChatGPT) struggle to update their knowledge efficiently - adding new facts often requires expensive retraining or causes performance drops. Existing memory systems for these AI models are rigidly tied to their core architecture, making them hard to scale, share between different models, or adapt to new tasks. We designed a new ""plug-and-play"" memory system inspired by how computers separate processors (CPUs) from memory (RAM). Our system: (1) Works as a standalone memory bank that any language model can access. (2) Uses smarter storage units to handle conflicting updates. (3) Includes a quality-check filter to prevent irrelevant/outdated information from affecting responses."
Poster,Permutation-based Rank Test in the Presence of Discretization and Application in Causal Discovery with Mixed Data,https://ICML.cc//virtual/2025/poster/45077,"Xinshuai Dong, Ignavier Ng, Boyang Sun, Haoyue Dai, Guangyuan Hao, Shunxing Fan, Peter Spirtes, Yumou Qiu, Kun Zhang","Recent advances have shown that statistical tests for the rank of cross-covariance matrices play an important role in causal discovery. These rank tests include partial correlation tests as special cases and provide further graphical information about latent variables. Existing rank tests typically assume that all the continuous variables can be perfectly measured, and yet, in practice many variables can only be measured after discretization. For example, in psychometric studies, the continuous level of certain personality dimensions of a person can only be measured after being discretized into order-preserving options such as disagree, neutral, and agree. Motivated by this, we propose Mixed data Permutation-based Rank Test (MPRT), which properly controls the statistical errors even when some or all variables are discretized. Theoretically, we establish the exchangeability and estimate the asymptotic null distribution by permutations; as a consequence, MPRT can effectively control the Type I error in the presence of discretization while previous methods cannot. Empirically, our method is validated by extensive experiments on synthetic data and real-world data to demonstrate its effectiveness as well as applicability in causal discovery (code will be available at https://github.com/dongxinshuai/scm-identify).","Many scientific fields, such as psychometrics and econometrics, encounter a challenge: certain continuous variables can only be measured using order-preserving discrete options (e.g., ""disagree,"" ""neutral,"" ""agree""). This raises a crucial question: how can we examine causal relationships between variables when observations are discretized, or when some variables are continuous and others are discrete (mixed data)?To address this, we developed a valid statistical test for causal discovery specifically designed for mixed data. Our approach utilizes data permutation to derive the asymptotic null distribution, effectively controlling statistical errors. This work is a significant step towards applying causal discovery methods in real-world scientific knowledge discovery."
