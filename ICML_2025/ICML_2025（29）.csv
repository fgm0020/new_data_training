type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Automated Benchmark Generation for Repository-Level Coding Tasks,https://ICML.cc//virtual/2025/poster/43922,"Konstantinos Vergopoulos, Mark Müller, Martin Vechev","Code Agent development is an extremely active research area, where a reliable performance metric is critical for tracking progress and guiding new developments. This demand is underscored by the meteoric rise in popularity of SWE-Bench -- a benchmark that challenges code agents to generate patches addressing GitHub issues given the full repository as context. The correctness of generated patches is then evaluated by executing a human-written test suite extracted from the repository after the issue's resolution.However, constructing benchmarks like SWE-Bench requires substantial manual effort to set up historically accurate execution environments for testing. Crucially, this severely limits the number of considered repositories, e.g., just 12 for SWE-Bench. Considering so few repositories, selected for their popularity runs the risk of leading to a distributional mismatch, i.e., the measured performance may not be representative of real-world scenarios running the riks of misguiding development efforts. In this work, we address this challenge and introduce SetUpAgent, a fully automated system capable of historically accurate dependency setup, test execution, and result parsing. Using SetUpAgent, we generate two new datasets: (i) SWEE-Bench an extended version of SWE-Bench encompassing hundreds of repositories, and (ii) SWA-Bench a benchmark focusing on applications rather than libraries. Comparing these datasets to SWE-Bench with respect to their characteristics and code agent performance, we find significant distributional differences, including lower issue description quality and detail level, higher fix complexity, and most importantly up to 60% lower agent success rates.","(1) To evaluate the performance of Code Agents, diverse and up-to-date benchmarks are critical. However, creating such benchmarks and corresponding evaluation pipelines is challenging and time-consuming. (2) We develop a fully automated process of creating such benchmarks from a list of GitHub repositories, using an Agent to set up the evaluation pipeline. (3) This allowed us to create two new and more diverse benchmarks for Code Agents, demonstrating that some approaches and models perform noticeably worse on these new benchmarks."
Poster,Automated Hypothesis Validation with Agentic Sequential Falsifications,https://ICML.cc//virtual/2025/poster/44356,"Kexin Huang, Ying Jin, Ryan Li, Michael Li, Emmanuel J Candes, Jure Leskovec","Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose POPPER, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, POPPER validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate POPPER on six domains including biology, economics, and sociology. POPPER delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, POPPER achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation. POPPER is freely available at https://github.com/snap-stanford/POPPER.","Hypotheses — educated guesses about how the world works — are the foundation of scientific discovery. But many real-world hypotheses are abstract and hard to test directly. With AI systems like ChatGPT now generating large numbers of scientific hypotheses, the problem is growing: How do we know which ones are actually true? We introduce a new tool called Popper, named after the philosopher Karl Popper, who believed that the best way to test an idea is to try to prove it wrong. Popper is an AI system that automatically designs and runs experiments to challenge a given hypothesis. If the hypothesis can survive these rigorous tests, it becomes more trustworthy. Our system uses powerful language models as agents to create and carry out these tests — across fields as diverse as biology, economics, and sociology. Unlike traditional tools, it ensures results are statistically sound and avoids false positives. Popper can validate hypotheses as well as human scientists but does so 10 times faster. By making hypothesis testing faster and more reliable, it helps scale up scientific discovery in the age of AI."
Poster,Automated Red Teaming with GOAT: the Generative Offensive Agent Tester,https://ICML.cc//virtual/2025/poster/44754,"Maya Pavlova, Erik Brinkman, Krithika Iyer, Vítor Albiero, Joanna Bitton, Hailey Nguyen, Cristian Canton, Ivan Evtimov, Aaron Grattafiori","Red teaming aims to assess how large language models (LLMs) can produce content that violates norms, policies, and rules set forth during their safety training. However, most existing automated methods in literature are not representative of the way common users exploit the multi-turn conversational nature of AI models. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vuLnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general purpose model in a way that encourages reasoning through the choices of methods available, the current target model’s response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 96% against smaller models such as Llama 3.1 8B, and 91% against Llama 3.1 70B and 94% for GPT-4o when evaluated against larger models on the JailbreakBench dataset.","Red teaming is a way of stress-testing whether large language models (LLMs) can produce content that violates norms, policies, and rules. This helps inform model creators if safety gaps exist and should be remedied before the model is made broadly available and is a critical part of the so-called ""training"" process. Red teaming can be either performed manually by humans or automated with adversarial algorithms. However, some of the existing algorithms do not fully capture the way humans perform this testing as they never simulate conversations and only come up with a single adversarial question to the model. This work shows a simple-yet-effective method that allows automation to better simulate human red teamers. It does so by using another language model and instructing it to carry out an adversarial conversation with the model being tested. Our research contributes to the safety and responsible development of future AI systems by identifying vulnerabilities so that they can be remedied. By conducting controlled research to uncover these issues now, we proactively mitigate risks that could otherwise emerge during real-world deployments."
Poster,Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios,https://ICML.cc//virtual/2025/poster/44366,"xihong yang, Siwei Wang, Fangdi Wang, Jiaqi Jin, Suyuan Liu, Yue Liu, En Zhu, Xinwang Liu, Yueming Jin","Leveraging the powerful representation learning capabilities, deep multi-view clustering methods have demonstrated reliable performance by effectively integrating multi-source information from diverse views in recent years. Most existing methods rely on the assumption of clean views. However, noise is pervasive in real-world scenarios, leading to a significant degradation in performance. To tackle this problem, we propose a novel multi-view clustering framework for the automatic identification and rectification of noisy data, termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly identification problem using GMM. We then design a hybrid rectification strategy to mitigate the adverse effects of noisy data based on the identification results. Furthermore, we introduce a noise-robust contrastive mechanism to generate reliable representations. Additionally, we provide a theoretical proof demonstrating that these representations can discard noisy information, thereby improving the performance of downstream tasks. Extensive experiments on six benchmark datasets demonstrate that AIRMVC outperforms state-of-the-art algorithms in terms of robustness in noisy scenarios. The code of AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github.","Modern AI systems often group data by leveraging multiple views or perspectives—such as different sensor readings or feature sets—to discover underlying patterns, a process known as multi-view clustering. In practice, however, one or more of these views can be corrupted by noise—faulty sensors, labeling errors, or environmental interference—which can dramatically degrade clustering quality.To address this challenge, we introduce **AIRMVC**, a novel framework that first treats noise detection as an anomaly-identification problem using Gaussian Mixture Models. Once noisy samples are flagged, a hybrid rectification strategy repairs the corrupted views by selectively correcting their data. We further enhance robustness with a noise-resistant contrastive learning mechanism.We also provide a theoretical guarantee: the representations generated by AIRMVC can discard noisy information, thereby improving the performance of downstream tasks. Extensive experiments on six benchmark datasets demonstrate that AIRMVC consistently outperforms state-of-the-art multi-view clustering algorithms, offering a more resilient solution for real-world noisy environments."
Poster,Automatically Interpreting Millions of Features in Large Language Models,https://ICML.cc//virtual/2025/poster/45936,"Gonçalo Paulo, Alex Mallen, Caden Juang, Nora Belrose","While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which can be more easily interpretable. However, SAEs can have millions of distinct latents, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language interpretations for SAE latents using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of interpretations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a latent, which we find explains latents that are not recalled by existing methods. We propose guidelines for generating better interpretations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. Our code is available at https://github.com/EleutherAI/delphi.","Interpretability of large language models remains a open problem. A recent advance in mechanistic interpretability has been the development of sparse autoencoders, a type of explainer model, used to decompose the high dimensional latent space of the base models. In this work we investigate the feasibility of using other large language models to generate explanations for this decompostion and present different methods to evaluate the explanations generated."
Poster,Automatic Differentiation of Optimization Algorithms with Time-Varying Updates,https://ICML.cc//virtual/2025/poster/43618,"Sheheryar Mehmood, Peter Ochs","Numerous optimization algorithms have a time-varying update rule thanks to, for instance, a changing step size, momentum parameter or, Hessian approximation. Often, such algorithms are used as solvers for the lower-level problem in bilevel optimization, and are unrolled when computing the gradient of the upper-level objective. In this paper, we apply unrolled or automatic differentiation to a time-varying iterative process and provide convergence (rate) guarantees for the resulting derivative iterates. We then adapt these convergence results and apply them to proximal gradient descent with variable step size and FISTA when solving partly-smooth problems. We test the convergence (rates) of these algorithms numerically through several experiments. Our theoretical and numerical results show that the convergence rate of the algorithm is reflected in its derivative iterates.","Optimization problems involve finding solutions that minimize costs or losses, or maximize profits. Optimization algorithms help computers find such solutions by improving their guesses step by step. Many of these algorithms become more effective by changing how they update their guesses over time, for example, by adjusting the length of a step they take. This flexibility allows them to improve the guess more quickly, that is, in fewer steps.In many real-world problems, the objective we are optimizing depends on external parameters. For instance, a company might want to minimize production costs, but those costs depend on raw material prices or market demand. As these parameters vary, both the optimal solution and the algorithm’s intermediate guesses may also vary. Understanding how these variations happen is essential, and this is captured mathematically using derivatives.In this paper, we study how the variation in an optimization algorithm’s guess relates to the variation in the true solution. In fact, we show that, in certain cases, the derivative of the algorithm’s guess itself serves as a good approximation of the derivative of the solution. Our results have applications in areas such as Meta Learning and Hyperparameter Optimization."
Poster,Automatic Reward Shaping from Confounded Offline Data,https://ICML.cc//virtual/2025/poster/45757,"Mingxuan Li, Junzhe Zhang, Elias Bareinboim","Reward shaping has been demonstrated to be an effective technique for accelerating the learning process of reinforcement learning (RL) agents. While successful in empirical applications, the design of a good shaping function is less well understood in principle and thus often relies on domain expertise and manual design. To overcome this limitation, we propose a novel automated approach for designing reward functions from offline data, possibly contaminated with the unobserved confounding bias.We propose to use causal state value upper bounds calculated from offline datasets as a conservative optimistic estimation of the optimal state value, which is then used as state potentials in Potential-Based Reward Shaping (PBRS). When applying our shaping function to a model-free learner based on UCB principles, we show that it enjoys a better gap-dependent regret bound than the learner without shaping. To the best of our knowledge, this is the first gap-dependent regret bound for PBRS in model-free learning with online exploration.Simulations support the theoretical findings.","Reinforcement learning (RL) is a type of machine learning where agents learn to make decisions by receiving rewards for their actions. A popular way to help agents learn faster is through reward shaping, which gives them extra feedback to guide their learning. However, designing this extra feedback often requires human expertise and is hard to do automatically.In this work, we introduce a new method that automatically designs helpful reward signals using only existing data—without needing expert knowledge. Our approach carefully adjusts rewards based on estimates of how good different situations are, even when the data might be biased or incomplete. We prove that this method helps agents learn more efficiently and provide strong mathematical guarantees to support this. Experiments confirm that our method works well in practice."
Poster,AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML,https://ICML.cc//virtual/2025/poster/44029,"Patara Trirat, Wonyong Jeong, Sung Ju Hwang","Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes *AutoML-Agent*, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. *AutoML-Agent* takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that *AutoML-Agent* achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.","Building machine learning systems can be difficult, especially for people without a background in AI. Many existing tools still require coding and technical skills, making it hard for non-experts to use them. This paper tackles that problem by creating a tool called AutoML-Agent, which helps users build AI systems just by describing their task in plain language.AutoML-Agent works like a team of smart assistants. Each assistant (or ""agent"") has a special role: some focus on understanding the data, others on choosing the right model, tuning it, and writing the code. The system takes in a user's request, such as ""Build a model to detect spam messages,"" and automatically plans, builds, tests, and delivers a ready-to-use AI model.To make this process reliable and efficient, AutoML-Agent combines smart planning strategies with checks at each step to avoid mistakes. The result is a powerful tool that builds high-performing AI systems with minimal human effort, opening the door for more people to create AI solutions in their own fields."
Poster,Autonomy-of-Experts Models,https://ICML.cc//virtual/2025/poster/46286,"Ang Lv, Ruobing Xie, Yining Qian, Songhao Wu, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan","Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and learning. To address this, we propose Autonomy-of-Expert (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.","Modern large language models often use a technique called ""Mixture-of-Experts"" (MoE), where only a portion of the model is activated for each input, saving time and resources. Typically, a separate module called a ""router"" decides which parts of the model—namely, the experts—to activate. However, the separation between the router and the experts can reduce training efficiency, cause imbalanced workloads, and lower overall performance. In our work, we introduce a new method called ""Autonomy-of-Experts,"" in which each expert decides for itself whether to activate, based on its activation scale (i.e., how useful it expects to be). This removes the need for a router and leads to improved performance."
Poster,Auto-reconfiguration for Latency Minimization in CPU-based DNN Serving,https://ICML.cc//virtual/2025/poster/45995,"Ankit Bhardwaj, Amar Phanishayee, Deepak Narayanan, Ryan Stutsman","In this paper, we investigate how to push the performance limits of serving Deep Neural Network (DNN) models on CPU-based servers. Specifically, we observe that while intra-operator parallelism across multiple threads is an effective way to reduce inference latency, it provides diminishing returns. Our primary insight is that instead of running a single instance of a model with all available threads on a server, running multiple instances each with smaller batch sizes and fewer threads for intra-op parallelism can provide lower inference latency. However, the right configuration is hard to determine manually since it is workload- (DNN model and batch size used by the serving system) and deployment-dependent (number of CPU cores on server). We present Packrat, a new serving system for online inference that given a model and batch size (𝐵) algorithmically picks the optimal number of instances (𝑖), the number of threads each should be allocated (𝑡), and the batch sizes each should operate on (𝑏) that minimizes latency. Packrat is built as an extension to TorchServe and supports online reconfigurations to avoid serving downtime. Averaged across a range of batch sizes, Packrat improves inference latency by 1.43× to 1.83× on a range of commonly used DNNs.","Minimizing CPU-based inference latency for a given workload is challenging. Pure inter- and intra-op parallelism results in sub-optimal latency. Moreover, the best configuration depends on the model and the CPU hardware. Packrat solves this using an automated approach that combines selective profiling, an optimizer that estimates the performance of unprofiled configurations and suggests configurations tominimize latency, and performs online reconfigurations to avoid serving downtime. Collectively, these let Packrat realize latency and throughput speedups of 1.43×to 1.83× averaged across batch sizes on a range of common DNNs."
