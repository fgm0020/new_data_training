type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space Guided Diffusion,https://ICML.cc//virtual/2025/poster/46072,"Xingpei Ma, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Qiang Zhang, Shunsi Zhang","Recent diffusion-based talking face generation models have demonstrated impressive potential in synthesizing videos that accurately match a speech audio clip with a given reference identity. However, existing approaches still encounter significant challenges due to uncontrollable factors, such as inaccurate lip-sync, inappropriate head posture and the lack of fine-grained control over facial expressions. In order to introduce more face-guided conditions beyond speech audio clips, a novel two-stage training framework Playmate is proposed to generate more lifelike facial expressions and talking faces. In the first stage, we introduce a decoupled implicit 3D representation along with a meticulously designed motion-decoupled module to facilitate more accurate attribute disentanglement and generate expressive talking videos directly from audio cues. Then, in the second stage, we introduce an emotion-control module to encode emotion control information into the latent space, enabling fine-grained control over emotions and thereby achieving the ability to generate talking videos with desired emotion. Extensive experiments demonstrate that Playmate not only outperforms existing state-of-the-art methods in terms of video quality, but also exhibits strong competitiveness in lip synchronization while offering improved flexibility in controlling emotion and head pose. The code will be available at https://github.com/Playmate111/Playmate.","Recent talking face generation models can create videos that match a given speech audio clip to a specific person, but they still face challenges like inaccurate lip-sync, unnatural head positions, and limited control over facial expressions. To solve these issues, a new method called Playmate has been developed. This two-stage framework aims to produce more realistic facial expressions and talking faces.In the first stage, Playmate uses a special 3D representation and a motion-decoupled module to better separate and accurately generate facial attributes from audio. In the second stage, it adds an emotion-control feature, allowing for precise adjustments of emotions in the generated videos. Tests show that Playmate surpasses current top methods in video quality and lip-sync accuracy, while also offering greater flexibility in controlling emotions and head poses."
Poster,PlaySlot: Learning Inverse Latent Dynamics for Controllable Object-Centric Video Prediction and Planning,https://ICML.cc//virtual/2025/poster/45399,"Angel Villar-Corrales, Sven Behnke","Predicting future scene representations is a crucial task for enabling robots to understand andinteract with the environment. However, most existing methods rely on videos and simulationswith precise action annotations, limiting their ability to leverage the large amount of avail-able unlabeled video data. To address this challenge, we propose PlaySlot, an object-centricvideo prediction model that infers object representations and latent actions from unlabeledvideo sequences. It then uses these representations to forecast future object states and videoframes. PlaySlot allows the generation of multiple possible futures conditioned on latent actions,which can be inferred from video dynamics, provided by a user, or generated by a learned actionpolicy, thus enabling versatile and interpretable world modeling. Our results show that PlaySlotoutperforms both stochastic and object-centric baselines for video prediction across different environments. Furthermore, we show that our inferred latent actions can be used to learn robot behaviors sample-efficiently from unlabeled videodemonstrations. Videos and code are available on our project website.","Robots need to understand what’s happening around them and guess what might happen next. Most current methods need lots of detailed instructions to learn this, which takes a lot of time and effort. Our method, called PlaySlot, learns just by watching videos---like how people can learn by observing. It figures out what the important objects are, how they move, and then imagines what could happen next. This helps robots learn to do tasks even when we don’t give them step-by-step directions. It’s more flexible, easier to use, and works better than other tools."
Poster,Point Cloud Dataset Distillation,https://ICML.cc//virtual/2025/poster/45095,"Deyu Bo, Xinchao Wang","This study introduces dataset distillation (DD) tailored for 3D data, particularly point clouds. DD aims to substitute large-scale real datasets with a small set of synthetic samples while preserving model performance. Existing methods mainly focus on structured data such as images. However, adapting DD for unstructured point clouds poses challenges due to their diverse orientations and resolutions in 3D space. To address these challenges, we theoretically demonstrate the importance of matching rotation-invariant features between real and synthetic data for 3D distillation. We further propose a plug-and-play point cloud rotator to align the point cloud to a canonical orientation, facilitating the learning of rotation-invariant features by all point cloud models. Furthermore, instead of optimizing fixed-size synthetic data directly, we devise a point-wise generator to produce point clouds at various resolutions based on the sampled noise amount. Compared to conventional DD methods, the proposed approach, termed DD3D, enables efficient training on low-resolution point clouds while generating high-resolution data for evaluation, thereby significantly reducing memory requirements and enhancing model scalability. Extensive experiments validate the effectiveness of DD3D in shape classification and part segmentation tasks across diverse scenarios, such as cross-architecture and cross-resolution settings.","This work addresses the challenge of training deep models on large 3D point cloud datasets by distilling them into a compact, synthetic core that preserves task performance. To overcome the variability of point cloud orientations, we introduce a theoretical analysis showing that aligning rotation-invariant features between real and distilled samples is critical. Building on this insight, we develop a plug-and-play rotator module that automatically reorients all point clouds into a shared canonical frame, ensuring consistent feature learning across rotations.Rather than directly optimizing a fixed set of synthetic point clouds, we design a lightweight, point-wise generator that produces clouds at arbitrary resolutions by modulating input noise. This flexibility allows models to train efficiently on low-resolution data while still enabling high-resolution evaluation, substantially reducing memory usage without sacrificing accuracy.Extensive experiments on shape classification and part segmentation demonstrate that our method, DD3D, matches or outperforms existing distillation approaches under diverse architectures and resolution settings. These results validate DD3D’s ability to deliver state-of-the-art performance with a fraction of the data and computational resources typically required."
Poster,Point-Level Topological Representation Learning on Point Clouds,https://ICML.cc//virtual/2025/poster/43458,"Vincent P. Grande, Michael Schaub","Topological Data Analysis (TDA) allows us to extract powerful topological and higher-order information on the global shape of a data set or point cloud.Tools like Persistent Homology give a single complex description of the global structure of the point cloud.However, common machine learning applications like classification require point-level information and features.In this paper, we bridge this gap and propose a novel method to extract node-level topological features from complex point clouds using discrete variants of concepts from algebraic topology and differential geometry.We verify the effectiveness of these topological point features (TOPF) on both synthetic and real-world data and study their robustness under noise and heterogeneous sampling.","Many datasets take the form of a list of points in space.Previous work has focussed on describing the ""shape"" of these so-called point clouds.We now propose a novel method to describe the relationship between each individual point and the global shape of the point cloud.To do this, we leverage insights from disciplines of Pure Mathematics called Algebraic Topology and Differential GeometryFinally, we apply our method to data from Biology and Physics and compare it with other methods."
Poster,Pointwise Information Measures as Confidence Estimators in Deep Neural Networks: A Comparative Study,https://ICML.cc//virtual/2025/poster/45545,"Shelvia Wongso, Rohan Ghosh, Mehul Motani","Estimating the confidence of deep neural network predictions is crucial for safe deployment in high-stakes applications. While softmax probabilities are commonly used, they are often poorly calibrated, and existing calibration methods have been shown to be detrimental to failure prediction. In this paper, we propose using information-theoretic measures to estimate prediction confidence in a post-hoc manner, without modifying network architecture or training. Specifically, we compare three pointwise information (PI) measures: pointwise mutual information (PMI), pointwise V-information (PVI), and the recently proposed pointwise sliced mutual information (PSI). These measures are theoretically grounded in their relevance to predictive uncertainty, with properties such as invariance, convergence rates, and sensitivity to geometric attributes like margin and intrinsic dimensionality. Through extensive experiments on benchmark computer vision models and datasets, we find that PVI consistently outperforms or matches existing baselines in post-hoc confidence estimation, excelling over PMI and PSI in both failure prediction and calibration. This aligns with our theoretical insights, which suggest that PVI offers the most balanced trade-offs. Finally, we show that replacing softmax outputs with PVI in existing confidence estimation methods can further enhance performance.","Deep neural networks often produce overconfident predictions, which can be dangerous in high-stakes applications like healthcare or autonomous driving. While softmax probabilities are commonly used to estimate confidence, they are often poorly calibrated, and many existing methods actually make failure prediction worse. In this work, we explore three information-theoretic measures: (1) Pointwise Mutual Information (PMI), (2) Pointwise V-Information (PVI), and (3) Pointwise Sliced Mutual Information (PSI), as post-hoc tools for estimating confidence, without changing the model’s architecture or training. We analyze how these measures behave under different data transformations and geometric properties like margin and dimensionality. Our experiments on standard computer vision benchmarks show that PVI consistently outperforms PMI, PSI, and existing confidence estimation methods. These findings suggest that PVI offers a more balanced and reliable way to assess model confidence, especially for detecting when the model is likely to fail."
Poster,PoisonBench: Assessing Language Model Vulnerability to Poisoned Preference Data,https://ICML.cc//virtual/2025/poster/46610,"Tingchen Fu, Mrinank Sharma, Phil Torr, Shay Cohen, David Krueger, Fazl Barez","Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 22 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not always enhance resilience against poisoning attacks and the influence on model resilience varies among different model suites. (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.","When training AI models to align with human preferences, malicious actors can ""poison"" the training data—sneaking in hidden biases or harmful content. This manipulation can cause models to generate unsafe or unintended outputs while appearing normal, posing serious risks. We introduce PoisonBench, a benchmark to test how vulnerable AI models are to such attacks. We evaluated 22 popular models across realistic scenarios, uncovering key weaknesses: larger models aren’t always more resilient, attacks scale predictably with poison levels, and their effects can spread to untriggered scenarios. Our findings expose critical flaws in current AI training methods, urging the development of stronger defenses to prevent misuse and ensure safer, more reliable AI systems."
Poster,PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented Generation based Large Vision-Language Models,https://ICML.cc//virtual/2025/poster/46373,"Chenyang Zhang, Xiaoyu Zhang, Jian Lou, KAI WU, Zilong Wang, Xiaofeng Chen","Vision-Language Retrieval-Augmented Generation (VLRAG) systems have been widely applied to Large Vision-Language Models (LVLMs) to enhance their generation ability. However, the reliance on external multimodal knowledge databases renders VLRAG systems vulnerable to malicious poisoning attacks. In this paper, we introduce PoisonedEye, the first knowledge poisoning attack designed for VLRAG systems. Our attack successfully manipulates the response of the VLRAG system for the target query by injecting only one poison sample into the knowledge database. To construct the poison sample, we follow two key properties for the retrieval and generation process, and identify the solution by satisfying these properties. Besides, we also introduce a class query targeted poisoning attack, a more generalized strategy that extends the poisoning effect to an entire class of target queries. Extensive experiments on multiple query datasets, retrievers, and LVLMs demonstrate that our attack is highly effective in compromising VLRAG systems.","AI systems have gained the ability to ""see"" and ""describe"", such as explaining what is in a picture or answering questions about an image. These systems often use external knowledge bases to improve their performance. However, this reliance on outside information can be a security risk. If the knowledge base is tampered with by attackers, the system's answers may become incorrect or even mislead users.In this paper, we introduce PoisonedEye, the first attack designed specifically to exploit these vision-language systems. Our method allows an attacker to manipulate the system's response to a specific question by simply inserting one malicious data point into the knowledge base. As a result, when a user asks the specific question, the system will give a wrong or harmful answer that was planned by the attacker.This research reminds us that many current AI systems that rely on external knowledge might be vulnerable to attacks. We need to pay more attention to securing these systems and improving their defenses."
Poster,PokéChamp: an Expert-level Minimax Language Agent,https://ICML.cc//virtual/2025/poster/45207,"Seth Karten, Andy Nguyen, Chi Jin","We introduce PokéChamp, a minimax agent powered by Large Language Models (LLMs) for Pokémon battles. Built on a general framework for two-player competitive games, PokéChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate PokéChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76\% against the best existing LLM-based bot and 84\% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, PokéChamp consistently outperforms the previous best LLM-based bot, Pokéllmon powered by GPT-4o, with a 64\% win rate. PokéChamp attains a projected Elo of 1300-1500 on the Pokémon Showdown online ladder, placing it among the top 30\%-10\% of human players. In addition, this work compiles the largest real-player Pokémon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. This work establishes Pokémon as a benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multi-agent problems. Videos, code, and dataset are available online.","PokéChamp is an artificial-intelligence player that battles in the popular strategy game Pokémon. Instead of being hand-coded with rigid rules, PokéChamp taps into the broad knowledge already stored inside large language models—the same technology that powers advanced chatbots. We give the language model three jobs: suggest promising moves, guess what the opponent might do next, and judge which future positions look best. With those ingredients it “thinks ahead” much like a skilled human, but without any extra training specific to Pokémon.We tested PokéChamp in the game’s most competitive online format (Generation 9 OverUsed). Using the state-of-the-art GPT-4o model, it won about 80% of matches against the strongest existing bots and reached an Elo rating in the top 10–30% of human players on the public ladder. Even when we swapped in a much smaller, openly available model, PokéChamp still beat the previous best language-model bot most of the time.To support research beyond our own agent, we collected and cleaned the largest public Pokémon battle dataset so far—over three million matches, including half a million high-level games. From this trove we built new skill puzzles and benchmarks and also patched the open-source game engine so future systems can be tested more reliably.By showing that language models can guide strategic search in a fast, partially hidden, multi-step game, PokéChamp offers a template for building versatile AI teammates and opponents in many other competitive settings."
Poster,Policy Design for Two-sided Platforms with Participation Dynamics,https://ICML.cc//virtual/2025/poster/43919,"Haruka Kiyohara, Fan Yao, Sarah Dean","In two-sided platforms (e.g., video streaming or e-commerce), viewers and providers engage in interactive dynamics: viewers benefit from increases in provider populations, while providers benefit from increases in viewer population. Despite the importance of such “population effects” on long-term platform health, recommendation policies do not generally take the participation dynamics into account. This paper thus studies the dynamics and recommender policy design on two-sided platforms under the population effects for the first time. Our control- and game-theoretic findings warn against the use of the standard “myopic-greedy” policy and shed light on the importance of provider-side considerations (i.e., effectively distributing exposure among provider groups) to improve social welfare via population growth. We also present a simple algorithm to optimize long-term social welfare by taking the population effects into account, and demonstrate its effectiveness in synthetic and real-data experiments. Our experiment code is available at https://github.com/sdean-group/dynamics-two-sided-market.","In two-sided platforms (e.g., video streaming or e-commerce), viewers and providers engage in interactive dynamics: viewers benefit from increases in provider populations, while providers benefit from increases in viewer population. Such participation dynamics and their effect on the viewer and provider benefits should be important for long-term platform health.This paper studies the dynamics and recommender algorithm design on two-sided platforms under the aforementioned ""population effects"". Our findings warn against the use of myopic-benefit-seeking algorithms and shed light on the importance of provider-side considerations (i.e., effectively distributing exposure among provider groups) to improve social welfare via population growth.Based on the analyses, we also present a simple algorithm to optimize long-term social welfare by taking the population effects into account, and demonstrate its effectiveness in synthetic and real-data experiments."
Poster,Policy Filtration for RLHF to Mitigate Noise in Reward Models,https://ICML.cc//virtual/2025/poster/45609,"Chuheng Zhang, Wei Shen, Li Zhao, Xuyun Zhang, Xiaolong Xu, Wanchun Dou, Jiang Bian","While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination ($R^2$) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH).","Recently, RLHF has become a key technique in LLM, and PPO is the dominate RL algroithm used for industrial-tier LLMs. Since we are optimizing the LLM to follow the value of humans which cannot be precisely described by a precise math formulaiton, researchers train a large reward model based on a few human annotations and use this model to further guide the learning of LLMs. This introduces an issue to the PPO algorithm which is originally proposed to optimized under the guidance of low-noise signals. Therefore, our motivation is to explore techniques that alleviate the problem raised by noisy rewards.Following this idea, we first monitor the fidelity of the reward signals given by the reward model. If we can find some indicators that are correlated with the fidelity of the reward signal, we can filter the samples based on this indicator to increase the signal-to-noise ratio during the training and thus improves the performance. Luckily, we find that the reward value itself is a good indicator - samples with highest and lowest rewards are more relieable.Then, we conduct experiments using the found sample filtering strategy and find this technique works pretty well in a wide range of tasks, ranging from math reasoning and code generation to even knowledge and instruction following tasks. Our work is mostly based on empirical obseravtion and experiments, and we will go deeper to reveal theoretical insight behind our method in the future work."
