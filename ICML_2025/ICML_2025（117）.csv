type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on LoRA with Parallel Control,https://ICML.cc//virtual/2025/poster/43595,"Chi Zhang, REN Lianhai, Jingpu Cheng, Qianxiao Li","The LoRA method has achieved notable success in reducing GPU memory usage by applying low-rank updates to weight matrices. Yet, one simple question remains: can we push this reduction even further? Furthermore, is it possible to achieve this while improving performance and reducing computation time? Answering these questions requires moving beyond the conventional weight-centric approach. In this paper, we present a state-based fine-tuning framework that shifts the focus from weight adaptation to optimizing forward states, with LoRA acting as a special example. Specifically, state-based tuning introduces parameterized perturbations to the states within the computational graph, allowing us to control states across an entire residual block. A key advantage of this approach is the potential to avoid storing large intermediate states in models like transformers. Empirical results across multiple architectures—including ViT, RoBERTa, LLaMA2-7B, and LLaMA3-8B—show that our method further reduces memory consumption and computation time while simultaneously improving performance. Moreover, as a result of memory reduction, we explore the feasibility to train 7B/8B models on consumer-level GPUs like Nvidia 3090, without model quantization. The code is available at an anonymous GitHub repository","Are LoRA methods only about low-rank updates to model weights? We challenge this foundational view in this paper. Inspired by ideas from control theory—where systems are often adjusted by changing their internal states rather than their structure—we propose a new approach. Instead of updating the model’s weights, we update the information (or “states”) flowing through the model as it makes predictions. You can think of this like rerouting the flow of water through a system of pipes, rather than replacing the pipes themselves.This method treats the model as a computational graph and introduces controlled tweaks (called ""parameterized perturbations"") to how the data flows through this graph. Our approach includes LoRA as a special case, but goes further by allowing us to control larger parts of the model, like entire residual blocks.One major benefit is improved memory efficiency. Since we don’t need to store as many large intermediate values during training, we can now train big models (like those with 7 to 8 billion parameters) on consumer-grade GPUs such as the Nvidia 3090—without quantization."
Poster,FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training,https://ICML.cc//virtual/2025/poster/46134,"Filipp Zmushko, Aleksandr Beznosikov, Martin Takac, Samuel Horváth","With the increase in the number of parameters in large language models, the training process increasingly demands larger volumes of GPU memory. A significant portion of this memory is typically consumed by the optimizer state. To overcome this challenge, recent approaches such as low-rank adaptation (LoRA), low-rank gradient projection (GaLore), and blockwise optimization (BAdam) have been proposed. However, in all these algorithms, the *effective rank of the weight updates remains low-rank*, which can lead to a substantial loss of information from the gradient. This loss can be critically important, especially during the pre-training stage. In this paper, we introduce FRUGAL (**F**ull-**R**ank **U**pdates with **G**r**A**dient sp**L**itting), a new memory-efficient optimization framework. FRUGAL leverages gradient splitting to perform low-dimensional updates using advanced algorithms (such as Adam), while updates along the remaining directions are executed via state-free methods like SGD or signSGD. Our framework can be integrated with various low-rank update selection techniques, including GaLore and BAdam. We provide theoretical convergence guarantees for our framework when using SGDM for low-dimensional updates and SGD for state-free updates. Additionally, our method consistently outperforms concurrent approaches, achieving state-of-the-art results in pre-training and fine-tuning tasks while balancing memory efficiency and performance metrics.","As language models grow larger, training them requires enormous amounts of computer memory. Much of this memory is used by the optimization algorithm that guides the learning process. To address this problem, researchers have developed techniques like LoRA, GaLore, and BAdam that reduce memory usage by limiting updates to only certain parts of the model.However, these methods have a key limitation: they only make *low-rank updates at each step*, meaning they lose important information from the learning signal. This information loss can hurt performance, especially when training models from scratch.In this paper, we introduce 𝙵𝚁𝚄𝙶𝙰𝙻 (**F**ull-**R**ank **U**pdates with **G**r**A**dient sp**L**itting), a new approach that solves this problem. Our method splits the learning signal into two parts: important directions get updated using sophisticated algorithms like Adam (de-facto optimization algorithm in Deep Learning) , while the remaining directions use simpler, memory-efficient methods. This way, we keep all the information while still saving memory.We  mathematically prove that out approach will converge to good solutions. In experiments, our method consistently outperforms other methods on both training new models and adapting existing ones, achieving the best results while using memory efficiently."
Poster,FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation,https://ICML.cc//virtual/2025/poster/45763,"Srijith Nair, Michael Lin, Peizhong Ju, Amirreza Talebi, Elizabeth Bentley, Jia (Kevin) Liu","Collaborative training methods like Federated Learning (FL) and Split Learning(SL) enable distributed machine learning without sharing raw data. However, FL assumes clients can train entire models, which is infeasible for large-scalemodels.In contrast, while SL alleviates the client memory constraint in FL by offloading most training to the server, it increases network latency due to its sequential nature. Other methods address the conundrum by using local loss functions for parallel client-side training to improve efficiency, but they lack server feedback and potentially suffer poor accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation Gradient Estimation), a new federated split learning algorithm that estimates server-side gradient feedback via auxiliary models.These auxiliary models periodically adapt to emulate server behavior on localdatasets. We show that FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is the number of communication rounds.This result matches FedAvg, while significantly reducing communication costs andclient memory requirements. Our empirical results also verify that it outperforms existing state-of-the-artFSL methods, offering both communication efficiency and accuracy.","With the impressive performance of recently developed AI models like ChatGPT, there is a clear need for multiple computers or devices to take part in collaboratively training big models.  Although many small devices like smartphones or smaller laptop computers have a lot of useful data to train large AI models, they are unable to participate in training because they have much lower resources like GPU, memory, etc.  We design an algorithm that can allow such low-resource devices to take part in training large models without breaching the privacy of their data, with whatever resources they can provide for doing so. Our method and its future extensions can greatly shape the way we train AI models today, by enabling collaboration between many small-resource players with useful data, like academic institutions, hospitals, small firms, etc., while preserving privacy standards."
Poster,FSTLLM: Spatio-Temporal LLM for Few Shot Time Series Forecasting,https://ICML.cc//virtual/2025/poster/44031,"Yue Jiang, Yile Chen, Xiucheng Li, Qin Chao, SHUAI LIU, Gao Cong","Time series forecasting fundamentally relies on accurately modeling complex interdependencies and shared patterns within time series data. Recent advancements, such as Spatio-Temporal Graph Neural Networks (STGNNs) and Time Series Foundation Models (TSFMs), have demonstrated promising results by effectively capturing intricate spatial and temporal dependencies across diverse real-world datasets. However, these models typically require large volumes of training data and often struggle in data-scarce scenarios. To address this limitation, we propose a framework named Few-shot Spatio-Temporal Large Language Models (FSTLLM), aimed at enhancing model robustness and predictive performance in few-shot settings. FSTLLM leverages the contextual knowledge embedded in Large Language Models (LLMs) to provide reasonable and accurate predictions. In addition, it supports the seamless integration of existing  forecasting models to further boost their predicative capabilities. Experimental results on real-world datasets demonstrate the adaptability and consistently superior performance of FSTLLM over major baseline models by a significant margin. Our code is available at: https://github.com/JIANGYUE61610306/FSTLLM.","In this study, we propose a framework named Few-shot Spatio-Temporal Large Language Models (FSTLLM), aimed at enhancing model robustness and predictive performance in few-shot time series forecasting. FSTLLM leverages the contextual knowledge embedded in Large Language Models (LLMs) to provide reasonable and accurate predictions. In addition, it supports the seamless integration of existing  forecasting models to further boost their predicative capabilities."
Poster,Fully Dynamic Embedding into $\ell_p$ Spaces,https://ICML.cc//virtual/2025/poster/44420,"Kiarash Banihashem, Xiang Chen, MohammadTaghi Hajiaghayi, Sungchul Kim, Kanak Mahadik, Ryan A Rossi, Tong Yu","Metric embeddings are fundamental in machine learning, enabling similarity search, dimensionality reduction, and representation learning. They underpin modern architectures like transformers and large language models, facilitating scalable training and improved generalization.Theoretically, the classic problem in embedding design is mapping arbitrary metrics into $\ell_p$ spaces while approximately preserving pairwise distances. We study this problem in a fully dynamic setting, where the underlying metric is a graph metric subject to edge insertions and deletions.Our goal is to maintain an efficient embedding after each update.We present the first fully dynamic algorithm for this problem, achieving $O(\log(n))^{2q} O(\log(nW))^{q-1}$ expected distortion with $O(m^{1/q + o(1)})$ update time and $O(q \log(n) \log(nW))$ query time, where $q \ge 2$ is an integer parameter.","Embeddings are a key technique in machine learning. They turn data—like words or nodes in a network—into vectors, allowing algorithms to reason about distances and similarities. Embeddings power many modern systems, including large language models, where they help represent meaning in a way machines can work with.We study this problem from a theoretical angle by modeling data as a graph and designing embeddings that preserve distances even as the graph changes over time. Specifically, we focus on the fully dynamic setting, where edges can be both inserted and deleted. Prior work could only handle limited cases, like when edge weights only increase. We give the first algorithm that efficiently maintains a low-distortion embedding into $\ell_p$ space under fully dynamic updates. We also prove that certain stronger guarantees are impossible to achieve efficiently, which shows our result is nearly the best one can hope for."
Poster,Fully Dynamic Euclidean Bi-Chromatic Matching in Sublinear Update Time,https://ICML.cc//virtual/2025/poster/43715,"Gramoz Goranci, Peter Kiss, Neel Patel, Martin Seybold, Eva Szilagyi, Da Wei Zheng","We consider the Euclidean bi-chromatic matching problem in the dynamic setting, where the goal is to efficiently process point insertions and deletions while maintaining a high-quality solution. Computing the minimum cost bi-chromatic matching is one of the core problems in geometric optimization that has found many applications, most notably in estimating Wasserstein distance between two distributions. In this work, we present the first fully dynamic algorithm for Euclidean bi-chromatic matching with sublinear update time. For any fixed $\varepsilon > 0$, our algorithm achieves $O(1/\varepsilon)$-approximation and handles updates in $O(n^{\varepsilon})$ time.  Our experiments show that our algorithm enables effective monitoring of the distributional drift in the Wasserstein distance on real and synthetic data sets, while outperforming the runtime of baseline approximations by orders of magnitudes.","Consider the following problem: you are given the same number of red and blue points on a plane, and you need to pair (match) these two sets by drawing a line between each red-blue pair. Your goal is to use as little of your pencil as possible - the total length of all the lines should be as short as possible. Surprisingly, this simple-sounding problem is closely connected to an important task in machine learning: deciding how similar two data sets are. However, data sets, like our red and blue points, can change over time, meaning that new red or blue points may appear or disappear.We designed a fast algorithm that, after each such change, quickly returns a pairing that is almost as good as the best possible matching. You might wonder why our algorithm doesn't always return the absolute best pairing. Interestingly, we proved that, in a certain sense, it's theoretically impossible to design a fast algorithm that always finds the best pairing - or even one that is much better than the one our algorithm produces."
Poster,Fully Heteroscedastic Count Regression with Deep Double Poisson Networks,https://ICML.cc//virtual/2025/poster/46290,"Spencer Young, Porter Jenkins, Longchao Da, Jeffrey Dotson, Hua Wei","Neural networks capable of accurate, input-conditional uncertainty representation are essential for real-world AI systems. Deep ensembles of Gaussian networks have proven highly effective for continuous regression due to their ability to flexibly represent aleatoric uncertainty via unrestricted heteroscedastic variance, which in turn enables accurate epistemic uncertainty estimation. However, no analogous approach exists for $\textit{count}$ regression, despite many important applications. To address this gap, we propose the Deep Double Poisson Network (DDPN), a novel neural discrete count regression model that outputs the parameters of the Double Poisson distribution, enabling arbitrarily high or low predictive aleatoric uncertainty for count data and improving epistemic uncertainty estimation when ensembled. We formalize and prove that DDPN exhibits robust regression properties similar to heteroscedastic Gaussian models via learnable loss attenuation, and introduce a simple loss modification to control this behavior. Experiments on diverse datasets demonstrate that DDPN outperforms current baselines in accuracy, calibration, and out-of-distribution detection, establishing a new state-of-the-art in deep count regression.","Many real-world problems involve predicting counts — like how many people are in a photo or how many items will sell tomorrow. But it’s not enough to just predict a number; we also need to know how confident the model is in its prediction. Most existing tools struggle to provide reliable uncertainty for count data, especially when that uncertainty varies across inputs. In this work, we introduce a new neural network model that not only predicts counts accurately, but also captures how uncertain those predictions are — adapting to each situation. Our approach sets a new standard for reliability in count-based forecasting tasks, with strong performance across vision, medical, and retail datasets. This makes it easier to trust AI systems in settings where stakes are high and decisions depend on more than just a number."
Poster,FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch,https://ICML.cc//virtual/2025/poster/44948,"Virginia Aglietti, Ira Ktena, Jessica Schrouff, Eleni Sgouritsa, Francisco Ruiz, Alan Malek, Alexis Bellot, Silvia Chiappa","The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations. The best-performing AFs can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices. This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings. Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a number of evaluations for a limited set of objective functions. We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks. We show how FunBO identifies AFs that generalize well both in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms.","When tackling complex optimization problems where each experiment is expensive—like finding the best recipe for a new material or tuning a sophisticated AI model—we often use a method called Bayesian Optimization. This method smartly decides what to try next using a ""guide"" called an acquisition function. However, the best guide can differ greatly from one problem to another, making it tricky to choose the right one. We've developed FunBO, a new approach that employs Large Language Models (LLMs) to automatically discover novel and effective acquisition functions. Instead of relying on pre-defined guides, FunBO gets a LLM to write new ones as small pieces of computer code. This makes the discovered guides transparent, easy to understand, and simple to integrate into existing systems. Our experiments show FunBO can find guides that accelerate the discovery of optimal solutions for a wide variety of challenges, often outperforming standard approaches and helping to reduce the cost of searching for optimal solutions."
Poster,Functional Alignment Can Mislead: Examining Model Stitching,https://ICML.cc//virtual/2025/poster/44458,"Damian Smith, Harvey Mannering, Antonia Marcu","A common belief in the representational comparison literature is that if two representations can be functionally aligned, they must capture similar information. In this paper we focus on model stitching and show that models can be functionally aligned, but represent very different information. Firstly, we show that discriminative models with very different biases can be stitched together. We then show that models trained to solve entirely different tasks on different data modalities, and even clustered random noise, can be successfully stitched into MNIST or ImageNet-trained models. We end with a discussion of the wider impact of our results on the community's current beliefs. Overall, our paper draws attention to the need to correctly interpret the results of such functional similarity measures and highlights the need for approaches that capture informational similarity.","With the widespread adoption of AI, a better understanding of neural networks is becoming a priority. Researchers can measure how *well* neural networks perform a task, but would also like to understand *how* different networks perform the task. A useful tool that might help shed light on the inner workings of neural networks would be one that meaningfully determines when two networks are different. In this work, we focus on a technique called stitching. Informally, stitching strongly focuses on what decision a network makes. We argue that this focus can abstract away from how the network makes that decision. As a result, stitching can consider two networks to be similar if they make the same decision but based on very different evidence (or rules). This becomes problematic since not all rules are equally likely to hold when networks are deployed in the wild. Thus, networks can have qualitatively different performance in the real world but can falsely be considered similar if compared using approaches such as stitching. In our paper we create a series of carefully designed experiments to illustrate this and encourage the deep learning community to consider such settings when evaluating other network comparison tools."
Poster,Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces,https://ICML.cc//virtual/2025/poster/45562,"Tyler Ingebrand, Adam Thorpe, Ufuk Topcu","A central challenge in transfer learning is designing algorithms that can quickly adapt and generalize to new tasks without retraining. Yet, the conditions of when and how algorithms can effectively transfer to new tasks is poorly characterized. We introduce a geometric characterization of transfer in Hilbert spaces and define three types of inductive transfer: interpolation within the convex hull, extrapolation to the linear span, and extrapolation outside the span. We propose a method grounded in the theory of function encoders to achieve all three types of transfer. Specifically, we introduce a novel training scheme for function encoders using least-squares optimization, prove a universal approximation theorem for function encoders, and provide a comprehensive comparison with existing approaches such as transformers and meta-learning on four diverse benchmarks. Our experiments demonstrate that the function encoder outperforms state-of-the-art methods on four benchmark tasks and on all three types of transfer.","Artificial intelligence (AI) systems are great at automating tasks—like recognizing images—but they usually rely on large, carefully collected datasets. That means they struggle with tasks that aren't well represented in the data. This paper introduces a new way to understand and model this limitation using a mathematical framework. It also presents a new method, called the function encoder, which outperforms existing techniques on several benchmarks."
