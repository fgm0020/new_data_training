type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Mixture of Hidden-Dimensions: Not All Hidden-States’ Dimensions are Needed in Transformer,https://ICML.cc//virtual/2025/poster/45793,"Yilong Chen, Junyuan Shang, Zhenyu Zhang, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang","Transformer models encounter inefficiency when scaling hidden dimensions due to the uniform expansion of parameters. When delving into the sparsity of hidden dimensions, we observe that only a small subset of dimensions are highly activated, where some dimensions are commonly activated across tokens, and some others uniquely activated for individual tokens. To leverage this, we propose MoHD (Mixture of Hidden Dimensions), a sparse architecture that combines shared sub-dimensions for common features and dynamically routes specialized sub-dimensions per token. To address the potential information loss from sparsity, we introduce activation scaling and group fusion mechanisms. MoHD efficiently expands hidden dimensions with minimal computational increases, outperforming vanilla Transformers in both parameter efficiency and task performance across 10 NLP tasks. MoHD achieves 1.7% higher performance with 50% fewer activatied parameters and 3.7% higher performance with 3× total parameters expansion at constant activated parameters cost. MoHD offers a new perspective for scaling the model, showcasing the potential of hidden dimension sparsity.","Transformer models are powerful tools used widely in tasks like translation, text generation, and understanding language. However, making these models bigger often means inefficiently adding many unnecessary parameters because they uniformly enlarge their internal structure. We discovered that Transformers actually don’t need all of their internal ""dimensions"" (think of these as pathways for information). Instead, only some dimensions are regularly active: some are used by many words, while others are specifically important for certain words.To utilize this observation, we developed a method called Mixture of Hidden Dimensions (MoHD). MoHD smartly selects which dimensions to activate: it shares some dimensions across many words and dynamically assigns specialized dimensions for specific words. To avoid losing important information from activating fewer dimensions, MoHD uses techniques to boost signals and merge dimensions effectively. Our experiments show that MoHD significantly improves model performance and efficiency, achieving better results while using far fewer active parameters. This makes Transformer models faster and more powerful, demonstrating a smarter way to scale up language technology."
Poster,Mixture of Lookup Experts,https://ICML.cc//virtual/2025/poster/43620,"Shibo Jie, Yehui Tang, Kai Han, Yitong Li, Duyu Tang, Zhi-Hong Deng, Yunhe Wang","Mixture-of-Experts (MoE) activates only a subset of experts during inference, allowing the model to maintain low inference FLOPs and latency even as the parameter count scales up. However, since MoE dynamically selects the experts, all the experts need to be loaded into VRAM. Their large parameter size still limits deployment, and offloading, which load experts into VRAM only when needed, significantly increase inference latency. To address this, we propose Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in both communication and VRAM usage. In MoLE, the experts are Feed-Forward Networks (FFNs) during training, taking the output of the embedding layer as input. Before inference, these experts can be re-parameterized as lookup tables (LUTs) that retrieves expert outputs based on input ids, and offloaded to storage devices. Therefore, we do not need to perform expert computations during inference. Instead, we directly retrieve the expert's computation results based on input ids and load them into VRAM, and thus the resulting communication overhead is negligible. Experiments show that, with the same FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models and significantly faster than MoE with experts offloading, while maintaining performance on par with MoE. Code: https://github.com/JieShibo/MoLE.","The Mixture of Experts (MoE) architecture is a mainstream design for today’s large language models (LLMs). When generating a word, an MoE model activates only a small subset of its numerous expert modules. However, a key challenge arises: although only a few experts are used at each step, all experts typically need to reside in GPU memory. This makes it difficult to deploy such models on devices with limited memory, such as smartphones or personal PCs.A common workaround is to store experts on lower-tier memory, such as disk, and load the required experts into GPU memory on demand. While this approach reduces memory usage, it incurs significant data transfer overhead, which drastically slows down inference.To address this, we propose a new architecture that transforms the computation of experts into a lookup process. This design allows all computations to be completed without loading the experts into GPU memory, thereby reducing memory usage while avoiding the latency caused by large-scale parameter transfers."
Poster,ML$^2$-GCL: Manifold Learning Inspired Lightweight Graph Contrastive Learning,https://ICML.cc//virtual/2025/poster/44332,"Jianqing Liang, Zhiqiang Li, Xinkai Wei, Yuan Liu, Zhiqiang Wang","Graph contrastive learning has attracted great interest as a dominant and promising self-supervised representation learning approach in recent years. While existing works follow the basic principle of pulling positive pairs closer and pushing negative pairs far away, they still suffer from several critical problems, such as the underlying semantic disturbance brought by augmentation strategies, the failure  of GCN in capturing long-range dependence, rigidness and inefficiency of node sampling techniques. To address these issues, we propose Manifold Learning Inspired Lightweight Graph Contrastive Learning (ML$^2$-GCL), which inherits the merits of both manifold learning and GCN. ML$^2$-GCL avoids the potential risks of semantic disturbance with only one single view. It achieves global nonlinear structure recovery from locally linear fits, which can make up for the defects of GCN. The most amazing advantage is about the lightweight due to its closed-form solution of positive pairs weights and removal of pairwise distances calculation. Theoretical analysis proves the existence of the optimal closed-form solution. Extensive empirical results on various benchmarks and evaluation protocols demonstrate effectiveness and lightweight of ML$^2$-GCL. We release the code at https://github.com/a-hou/ML2-GCL.","This paper proposes Manifold Learning Inspired Lightweight Graph Contrastive Learning, achieving both effectiveness and lightweight."
Poster,"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",https://ICML.cc//virtual/2025/poster/44904,"Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li","Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce **MME-CoT**, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level.Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: *1)* Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; *2)* CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and *3)* Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases.We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs.","This paper introduces MME-CoT, a new benchmark for testing whether AI systems that process both text and images can reason through problems step-by-step, rather than just checking final answers. Testing 16 AI models across math, science, and visual reasoning tasks, the researchers found that while self-correcting models generally perform better, step-by-step reasoning often hurts performance on simple visual tasks (AI ""overthinks"" easy problems) and about 40% of self-correction attempts don't actually help solve problems, revealing important flaws in how current AI systems think."
Poster,MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization,https://ICML.cc//virtual/2025/poster/44599,"Kangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng Wang, Huaxiu Yao","The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately addressed clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. In response, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by 14.2% and 51.7% on the Med-VQA and report generation tasks, respectively. Our code are available in https://github.com/aiming-lab/MMedPO}{https://github.com/aiming-lab/MMedPO.","Modern AI tools that look at both medical images and text—like X-rays and doctors’ notes—are helping improve healthcare. But they still make mistakes. One big problem is that these systems often rely too heavily on text and don’t pay enough attention to the actual medical images. This can lead to errors, like describing a disease that isn’t there.To fix this, we created a new training method called MMedPO. It helps the AI learn to better balance what it sees in the images with what it reads. We do this by giving the AI examples where it makes common medical mistakes, like overlooking a tumor or giving a believable but incorrect answer. Then, we score how medically important each example is, so the AI learns to focus on the right things.Our results show that this method makes AI much more accurate when answering medical questions or writing reports. We’ve also made our tools freely available for others to use."
Poster,MMInference: Accelerating Pre-filling for Long-Context Visual Language Models via Modality-Aware Permutation Sparse Attention,https://ICML.cc//virtual/2025/poster/44144,"Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu","The integration of long-context capabilities with visual understanding unlocks unprecedented potential for Vision Language Models (VLMs). However, the quadratic attention complexity during the pre-filling phase remains a significant obstacle to real-world deployment. To overcome this limitation, we introduce MMInference (Multimodality Million tokens Inference), a dynamic sparse attention method that accelerates the prefilling stage for long-context multi-modal inputs. First, our analysis reveals that the temporal and spatial locality of video input leads to a unique sparse pattern, the Grid pattern. Simultaneously, VLMs exhibit markedly different sparse distributions across different modalities. We introduce a permutation-based method to leverage the unique Grid pattern and handle modality boundary issues. By offline search the optimal sparse patterns for each head, MMInference constructs the sparse distribution dynamically based on the input. We also provide optimized GPU kernels for efficient sparse computations. Notably, MMInference integrates seamlessly into existing VLM pipelines without any model modifications or fine-tuning. Experiments on multi-modal benchmarks-including Video QA, Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while maintaining accuracy. Our code is available at https://ama.ms/MMInference.","We propose MMInference, a dynamic sparse attention method that accelerates prefilling for long-context multi-modal inputs. Our analysis identifies a Grid pattern induced by the spatiotemporal locality of video inputs and highlights modality-specific sparsity in VLMs. MMInference uses a permutation-based approach to align with the Grid pattern and resolve modality boundaries, dynamically constructing sparse layouts via offline head-wise search. We also provide optimized GPU kernels for efficient sparse computation.In tests using state-of-the-art AI systems on video and image tasks, MMInference sped up the input processing step by over 8× on million-token-long inputs—while still getting the right answers. This helps bring AI models closer to real-world use in areas like long-video understanding, AI tutoring, and multi-modal research tools."
Poster,MM-RLHF: The Next Step Forward in Multimodal LLM Alignment,https://ICML.cc//virtual/2025/poster/45124,"Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, xue wang, Yibo Hu, Bin Wen, Tingting Gao, Zhang Zhang, Fan Yang, Di ZHANG, Liang Wang, Rong Jin","Existing efforts to align multimodal large language models (MLLMs) with human preferences have only achieved progress in narrow areas, such as hallucination reduction, but remain limited in practical applicability and generalizability. To this end, we introduce **MM-RLHF**, a dataset containing **120k** fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality.  Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce the **Critique-Based Reward Model**, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms.  Additionally, we propose **Dynamic Reward Scaling**, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs.  Our approach is rigorously evaluated across **10** distinct dimensions, encompassing **27** benchmarks, with results demonstrating significant and consistent improvements in model performance (Figure.1).","We are proud to open-source **MM-RLHF**, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. This release includes:- A **high-quality MLLM alignment dataset** (120K samples, created by over 50 experts over two months, including ratings and manual annotations across eight dimensions.).- A **strong Critique-Based MLLM reward model** which is trained on human annotations, achieving state-of-the-art (SOTA) performance on public benchmarks.- A **novel alignment algorithm MM-DPO**, effectively integrates reward signals to improve the data efficiency of DPO training..- **Two new benchmarks** designed for the reward model and multimodal safety, addressing gaps in existing benchmarks in these areas.Our dataset and algorithms enable consistent performance improvements across **10 dimensions** and **27 benchmarks** for open-source MLLMs."
Poster,Modalities Contribute Unequally: Enhancing Medical Multi-modal Learning through Adaptive Modality Token Re-balancing,https://ICML.cc//virtual/2025/poster/45524,"Jie Peng, Jenna Ballard, Mohan Zhang, Sukwon Yun, Jiayi Xin, Qi Long, Yanyong Zhang, Tianlong Chen","Medical multi-modal learning requires an effective fusion capability of various heterogeneous modalities.One vital challenge is how to effectively fuse modalities when their data quality varies across different modalities and patients.For example, in the TCGA benchmark, the performance of the same modality can differ between types of cancer. Moreover, data collected at different times, locations, and with varying reagents can introduce inter-modal data quality differences ($i.e.$, $\textbf{Modality Batch Effect}$).In response, we propose ${\textbf{A}}$daptive ${\textbf{M}}$odality Token Re-Balan${\textbf{C}}$ing ($\texttt{AMC}$), a novel top-down dynamic multi-modal fusion approach.The core of $\texttt{AMC}$ is to quantify the significance of each modality (Top) and then fuse them according to the modality importance (Down).Specifically, we access the quality of each input modality and then replace uninformative tokens with inter-modal tokens, accordingly.The more important a modality is, the more informative tokens are retained from that modality.The self-attention will further integrate these mixed tokens to fuse multi-modal knowledge.Comprehensive experiments on both medical and general multi-modal datasets demonstrate the effectiveness and generalizability of $\texttt{AMC}$.","**Motivation:**Medical research and care often rely on combining different modalities (like medical images, genetic information, and patient records). However, these “multimodal” datasets face a critical challenge: data quality varies widely between different types of data and between patients. Traditional methods struggle to handle these inconsistencies, especially in complex medical scenarios where data types (like genes and pathology scans) are very different and their relevance can change.**Key Insight:**Not all data types (“modalities”) are equally useful for every patient or task. Some modalities might be highly informative for a specific case, while others are unreliable or irrelevant. Instead of treating all modalities the same, we need to dynamically weigh their importance and focus on the most trustworthy information.**Our Solution:**  We propose a new approach called AMC to address these challenges. 1. **Assess Modality Importance (“Top” Step):**  First, the model evaluates how useful each type of data (e.g., MRI scans vs. blood test results) is for the specific task, like cancer diagnosis. It does this by identifying which data types contain the clearest, most relevant information for the patient or condition at hand.2. **Replace Unreliable Data with Useful Insights (“Down” Step):**  For each data type, the model then filters out uninformative or noisy parts and replaces them with insights from more reliable data types. For example, if a patient’s genetic data is of poor quality, the model might rely more on their imaging data instead. This “re-balancing” ensures the model focuses on the most trustworthy information from all available sources.3. **Smart Fusion with a Customized Model:**  We design a flexible, efficient model (similar to those used in language apps like chatbots) to combine the re-balanced data. This model includes features to improve accuracy and interpretability, making it suitable for medical use where trust and clarity are essential.**Impact:** Tests on real-world medical datasets showed that AMC performs better than traditional methods when data quality varies. Key benefits include:**More Reliable Diagnoses and Predictions:**  By focusing on high-quality data, the model makes more accurate decisions, which is critical for treatments and personalized care.**Flexibility Across Medical Fields:**  AMC works well for diverse tasks, from Alzheimer’s research to cancer subtype analysis, showing its broad utility in healthcare.**Interpretability:**  The model’s ability to quantify which data types are most important helps doctors and researchers understand why it makes certain decisions, building trust in AI-driven medical tools.This approach could pave the way for more robust, adaptable AI in healthcare, improving how we use diverse data to better understand and treat diseases."
Poster,"MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding",https://ICML.cc//virtual/2025/poster/46210,"Zhicheng Zhang, Wuyou Xia, Chenxi Zhao, Zhou Yan, Xiaoqiang Liu, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di ZHANG, Jufeng Yang","Multimodal large language models (MLLMs) recently showed strong capacity in integrating data among multiple modalities, empowered by generalizable attention architecture. Advanced methods predominantly focus on language-centric tuning while less exploring multimodal tokens mixed through attention, posing challenges in high-level tasks that require fine-grained cognition and emotion understanding. In this work, we identify the attention deficit disorder problem in multimodal learning, caused by inconsistent cross-modal attention and layer-by-layer decayed attention activation. To address this, we propose a novel attention mechanism, termed MOdular Duplex Attention (MODA), simultaneously conducting the inner-modal refinement and inter-modal interaction. MODA employs a correct-after-align strategy to effectively decouple modality alignment from cross-layer token mixing. In the alignment phase, tokens are mapped to duplex modality spaces based on the basis vectors, enabling the interaction between visual and language modality. Further, the correctness of attention scores is ensured through adaptive masked attention, which enhances the model's flexibility by allowing customizable masking patterns for different modalities. Extensive experiments on 21 benchmark datasets verify the effectiveness of MODA in perception, cognition, and emotion tasks.","Imagine trying to understand a scene where both text and images are involved — like reading a description of a movie while watching a scene. Current computer models, called multimodal large language models (MLLMs), can process both types of information. However, they often struggle to pay equal attention to the text and the images, causing them to miss important details.We identified a key issue in how these models mix the information from different sources, which affects their ability to understand complex tasks like recognizing emotions or making sense of complicated scenarios. To fix this, we introduced a new attention mechanism called MODA. It helps the model better focus on both the text and the images by improving how they interact and align with each other. Think of it like fine-tuning the way you read and watch at the same time to better understand the full picture.Our new approach has been tested on a variety of tasks, from recognizing emotions to understanding images and text, and it works better than previous methods. This improvement can help create smarter AI systems that can understand the world more like humans do, across different types of media."
Poster,Model-Based Exploration in Monitored Markov Decision Processes,https://ICML.cc//virtual/2025/poster/45819,"Alireza Kazemipour, Matthew Taylor, Michael Bowling","A tenet of reinforcement learning is that the agent always observes rewards. However, this is not true in many realistic settings, e.g., a human observer may not always be available to provide rewards, sensors may be limited or malfunctioning, or rewards may be inaccessible during deployment. Monitored Markov decision processes (Mon-MDPs) have recently been proposed to model such settings. However, existing Mon-MDP algorithms have several limitations: they do not fully exploit the problem structure, cannot leverage a known monitor, lack worst-case guarantees for ""unsolvable"" Mon-MDPs without specific initialization, and offer only asymptotic convergence proofs. This paper makes three contributions.  First, we introduce a model-based algorithm for Mon-MDPs that addresses these shortcomings. The algorithm employs two instances of model-based interval estimation: one to ensure that observable rewards are reliably captured, and another to learn the minimax-optimal policy. Second, we empirically demonstrate the advantages. We show faster convergence than prior algorithms in more than four dozen benchmarks, and even more dramatic improvements when the monitoring process is known. Third, we present the first finite-sample bound on performance. We show convergence to a minimax-optimal policy even when some rewards are never observable.","Learning from trial-and-error is the most natural form of learning in humans. We learn by our mistakes and successes when they were not clear from the outset. This idea is at the core of artificial intelligence (AI). Artificial agents should be able to try different possibilities and learn from the outcomes. This approach is an alternative to being explicitly shown beforehand on how the task at hand should be accomplished. However, in many real-world scenarios the agents cannot have access to feedback to learn from, e.g., imagine the human supervisor leaving because of their time-constraints. This possibility has been overlooked in the past for developing AI methods for sequential decision-making. Our solution to this challenge was to drive a cautious behaviour when encountering unknowns. Our algorithm uses a classical approach to incentivize the agent to explore the world and try different possibilities to learn. But, if the agent recognized that some behaviour does not lead to feedback, then due to lack of knowledge it becomes cautious about them. This caution provides a worst-case guarantee and the agent has preemptively avoided the most catastrophic outcome. At the same time, if the agent recognized that everything can be learned, it never becomes prematurely cautious and try different actions and possibilities to learn as much as needed from them."
