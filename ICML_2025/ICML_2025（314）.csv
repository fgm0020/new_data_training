type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,TSP: A Two-Sided Smoothed Primal-Dual Method for Nonconvex Bilevel Optimization,https://ICML.cc//virtual/2025/poster/45160,Songtao Lu,"Extensive research has shown that a wide range of machine learning problems can be formulated as bilevel optimization, where two levels of learning processes intertwine through distinct sets of optimization variables. However, prevailing approaches often impose stringent assumptions, such as strong convexity of the lower-level loss function or uniqueness of the optimal solution, to enable algorithmic development and convergence analysis. However, these assumptions tend to be overly restrictive in real-world scenarios. In this work, we explore a recently popularized Moreau envelope based reformulation of bilevel optimization problems, accommodating nonconvex objective functions at both levels. We propose a stochastic primal-dual method that incorporates smoothing on both sides, capable of finding Karush-Kuhn-Tucker solutions for this general class of nonconvex bilevel optimization problems. A key feature of our algorithm is its ability to dynamically weigh the lower-level problems, enhancing its performance, particularly in stochastic learning scenarios. Numerical experiments underscore the superiority of our proposed algorithm over existing penalty-based methods in terms of both the convergence rate and the test accuracy.","Many machine learning problems involve two interconnected tasks, which can be formulated as bilevel optimization—for example, tuning a model while it’s still learning. Solving this class of problems are challenging, especially when both tasks are complex and may have multiple optimal solutions. We propose a new, efficient algorithm that handles such realistic, nonconvex settings without relying on the strict assumptions common in existing methods. By smoothing both levels and adopting a primal-dual optimization approach, our method finds high-quality solutions effectively, even in stochastic environments. Experiments show that our approach outperforms existing methods in both speed and accuracy, making it a powerful tool for solving advanced machine learning problems."
Poster,TS-SNN: Temporal Shift Module for Spiking Neural Networks,https://ICML.cc//virtual/2025/poster/44594,"Kairong Yu, Tianqing Zhang, Qi Xu, Gang Pan, Hongwei Wang","Spiking Neural Networks (SNNs) are increasingly recognized for their biological plausibility and energy efficiency, positioning them as strong alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing applications. SNNs inherently process temporal information by leveraging the precise timing of spikes, but balancing temporal feature utilization with low energy consumption remains a challenge. In this work, we introduce Temporal Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel Temporal Shift (TS) module to integrate past, present, and future spike features within a single timestep via a simple yet effective shift operation. A residual combination method prevents information loss by integrating shifted and original features. The TS module is lightweight, requiring only one additional learnable parameter, and can be seamlessly integrated into existing architectures with minimal additional computational cost. TS-SNN achieves state-of-the-art performance on benchmarks like CIFAR-10 (96.72\%), CIFAR-100 (80.28\%), and ImageNet (70.61\%) with fewer timesteps, while maintaining low energy consumption. This work marks a significant step forward in developing efficient and accurate SNN architectures.","Spiking Neural Networks (SNNs) are a type of brain-inspired computing system that mimics how neurons communicate through electrical pulses. They are energy-efficient and well-suited for tasks like sensor data, where timing matters. However, existing SNNs often struggle to balance accuracy and energy use when handling time-sensitive information.In this work, we developed a simple yet powerful module called the Temporal Shift Module (TS-SNN) to improve SNNs’ performance. Our module acts like a ""time window,"" combining past, present, and future information within a single processing step. By adding just one adjustable parameter and using minimal extra computation, TS-SNN retains critical details while keeping energy costs low.Tests on image recognition benchmarks showed TS-SNN achieves top-tier accuracy—like 96.7% on CIFAR-10 and 70.6% on ImageNet—while using fewer processing steps than traditional methods. This breakthrough brings us closer to creating AI systems that are both highly accurate and energy-efficient, paving the way for smarter devices in applications like robotics or wearable tech. We hope this inspires further innovations in brain-like computing systems"
Poster,TtBA: Two-third Bridge Approach for Decision-Based Adversarial Attack,https://ICML.cc//virtual/2025/poster/43631,"Feiyang Wang, Xingquan Zuo, Hai Huang, Gang Chen","A key challenge in black-box adversarial attacks is the high query complexity in hard-label settings, where only the top-1 predicted label from the target deep model is accessible. In this paper, we propose a novel normal-vector-based method called Two-third Bridge Attack (TtBA). A innovative bridge direction is introduced which is a weighted combination of the current unit perturbation direction and its unit normal vector, controlled by a weight parameter $k$. We further use binary search to identify $k=k_\text{bridge}$, which has identical decision boundary as the current direction. Notably, we observe that $k=2/3 k_\text{bridge}$ yields a near-optimal perturbation direction, ensuring the stealthiness of the attack. In addition, we investigate the critical importance of local optima during the perturbation direction optimization process and propose a simple and effective approach to detect and escape such local optima. Experimental results on MNIST, FASHION-MNIST, CIFAR10, CIFAR100, and ImageNet datasets demonstrate the strong performance and scalability of our approach. Compared to state-of-the-art non-targeted and targeted attack methods, TtBA consistently delivers superior performance across most experimented datasets and deep learning models. Code is available at https://anonymous.4open.science/r/TtBA-6ECF.","Deep learning systems, such as those used in image recognition, can be easily fooled by tiny, carefully crafted changes called adversarial attacks. These changes are often invisible to the human eye but can cause models to make incorrect decisions. In decision-based black box attack settings, where only the final label (such as “cat” or “dog”) output by the model is visible, generating such attacks becomes especially difficult and requires many attempts. Our research introduces a method called the *Two-third Bridge Attack* (TtBA), which significantly reduces the number of attempts needed to successfully fool a model. We propose a novel metric, $k_\text{bridge}$, to capture the shape of a model’s decision boundary and discover that using $2/3k_\text{bridge}$ leads to an effective attack. This metric also helps detect when the attack is stuck in a suboptimal region and guides it toward better attacks. By uncovering these vulnerabilities, our work contributes to developing more robust and trustworthy AI systems that are safer for real-world use."
Poster,TTFSFormer: A TTFS-based Lossless Conversion of Spiking Transformer,https://ICML.cc//virtual/2025/poster/44159,"Lusen Zhao, Zihan Huang, Ding Jianhao, Zhaofei Yu","ANN-to-SNN conversion has emerged as a key approach to train Spiking Neural Networks (SNNs), particularly for Transformer architectures, as it maps pre-trained ANN parameters to SNN equivalents without requiring retraining, thereby preserving ANN accuracy while eliminating training costs. Among various coding methods used in ANN-to-SNN conversion, time-to-first-spike (TTFS) coding, which allows each neuron to at most one spike, offers significantly lower energy consumption. However, while previous TTFS-based SNNs have achieved comparable performance with convolutional ANNs, the attention mechanism and nonlinear layers in Transformer architectures remains a challenge by existing SNNs with TTFS coding. This paper proposes a new neuron structure for TTFS coding that expands its representational range and enhances the capability to process nonlinear functions, along with detailed designs of nonlinear neurons for different layers in Transformer. Experimental results on different models demonstrate that our proposed method can achieve high accuracy with significantly lower energy consumption. To the best of our knowledge, this is the first work to focus on converting Transformer to SNN with TTFS coding.","Spiking Neural Networks (SNNs) offer a promising alternative to Artificial Neural Networks (ANNs) due to their energy efficiency and biological plausibility. A widely used approach to build SNNs is converting from pretrained ANNs, which avoids the cost of training from scratch. As Transformers have shown their strong ability in complex tasks, several rate-based Transformer-to-SNN conversion methods have been proposed. However, the nonlinearity of Transformers poses a major challenge in preserving accuracy.We propose a new method that uses time-to-first-spike (TTFS) coding, where each neuron fires only once, making full use of the time information provided by single spikes. To handle the complex operators in Transformers, we introduce a novel neuron model and adapt it for different layers. We also provide a detailed analysis showing that the conversion can be theoretically lossless.Our method first apply TTFS coding to Transformer-to-SNN conversion, achieve comparable performance to ANN while greatly reduce the energy cost. Our work offers a novel way for converting complex architectures into SNNs, providing a possible way to further unlock the full potential of SNNs."
Poster,TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs,https://ICML.cc//virtual/2025/poster/43504,"Felipe Nuti, Tim Franzmeyer, Joao Henriques","Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. However, a way to quantitatively and systematically analyze its effect on individual outputs is still lacking.In this work, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. Our method takes into account the model's intermediate hidden states, giving a more fine-grained insight into the effects of fine-tuning than a simple comparison of the final outputs of pre-trained and fine-tuned models.We introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component.Empirically, we find that one can steer model behavior and performance by up- or down-scaling the fine-tuning component during the forward pass.Motivated by this finding and our theoretical analysis, we define the Tuning Contribution ($\mathrm{TuCo}$) in terms of the ratio of the magnitudes fine-tuning component and the pre-training component.We find that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces the Tuning Contribution, and that $\mathrm{TuCo}$ is consistently lower on prompts where the attacks succeed compared to ones where they do not. This suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of these attacks.In short, $\mathrm{TuCo}$ enables the quantitative study of how fine-tuning influences model behavior and safety, and vice-versa.","AI writing tools like ChatGPT first learn from vast collections of internet text, and afterwards are trained to follow instructions and safety rules. But it's hard to know how much each stage — learning from the internet versus learning to follow instructions — contributes to any single reply, making it difficult to quantitatively analyse how the AI works and behaves.We introduce a way to peek at the AI's internal signals as it answers each question and split each reply into two contributions: from internet data versus from instruction data. From that split, we compute the Tuning Contribution (TuCo), a simple percentage that shows how much the instructions data shaped the response (for example, ""30% tuning contribution""), compared to the internet data.TuCo can help researchers spot when the AI's instruction learning phase has less effect than intended, letting the AI go into ""unfamiliar"" territory for which it does not have instructions. It can reveal hidden blind spots — like trick prompts that quietly undermine safeguards — and can guide teams in strengthening defences. It can also point out questions where tweaks barely help, so developers can refine their training data and make AI systems more reliable."
Poster,TUMTraf VideoQA: Dataset and Benchmark for Unified Spatio-Temporal Video Understanding in Traffic Scenes,https://ICML.cc//virtual/2025/poster/44900,"Xingcheng Zhou, Konstantinos Larintzakis, Hao Guo, Walter Zimmer, Mingyu Liu, Hu Cao, Jiajie Zhang, Venkatnarayanan Lakshminarasimhan, Leah Strand, Alois Knoll","We present TUMTraf VideoQA, a novel dataset and benchmark designed for spatio-temporal video understanding in complex roadside traffic scenarios. The dataset comprises 1,000 videos, featuring 85,000 multiple-choice QA pairs, 2,300 object captioning, and 5,700 object grounding annotations, encompassing diverse real-world conditions such as adverse weather and traffic anomalies. By incorporating tuple-based spatio-temporal object expressions, TUMTraf VideoQA unifies three essential tasks—multiple-choice video question answering, referred object captioning, and spatio-temporal object grounding—within a cohesive evaluation framework. We further introduce the TraffiX-Qwen baseline model, enhanced with visual token sampling strategies, providing valuable insights into the challenges of fine-grained spatio-temporal reasoning. Extensive experiments demonstrate the dataset’s complexity, highlight the limitations of existing models, and position TUMTraf VideoQA as a robust foundation for advancing research in intelligent transportation systems. The dataset and benchmark are publicly available to facilitate further exploration.","Understanding complex traffic scenes is essential for developing intelligent transportation systems. Yet, most existing AI benchmarks focus on either simple driving environments or isolated tasks, limiting progress in real-world applications.To address this gap, we introduce TUMTraf VideoQA, a new dataset featuring 1,000 real-world roadside videos and over 85,000 multiple-choice questions. It includes detailed annotations for describing and locating objects over time and uniquely combines three tasks: video question answering, referred object captioning, and spatio-temporal grounding within one benchmark. We also present TraffiX-Qwen, a strong baseline that performs well on these tasks and reveals key limitations of current models, particularly in fine-grained spatio-temporal reasoning.TUMTraf VideoQA provides a challenging, unified benchmark to drive next-generation models in traffic understanding and intelligent transportation systems."
Poster,Tuning LLM Judge Design Decisions for 1/1000 of the Cost,https://ICML.cc//virtual/2025/poster/44657,"David Salinas, Omar Swelam, Frank Hutter","Evaluating Large Language Models (LLMs) often requires costly human annotations. To address this, LLM-based judges have been proposed, which compare the outputs of two LLMs enabling the ranking of models without human intervention.  While several approaches have been proposed, many confounding factors are present between different papers. For instance the model, the prompt and other hyperparameters are typically changed at the same time making apple-to-apple comparisons challenging.In this paper, we propose to systematically analyze and tune the hyperparameters of LLM judges. To alleviate the high cost of evaluating a judge, we propose to leverage multi-objective multi-fidelity which allows to find judges that trades accuracy for cost and also reduce significantly the cost of the search. Our method identifies judges that not only outperform existing benchmarks in accuracy and cost-efficiency but also utilize open-weight models, ensuring greater accessibility and reproducibility.","Comparing different AI language models requires human experts to evaluate their responses—a costly and slow process. A cheaper alternative consists in using AI models themselves as judges to compare other AI systems. Think of it as having one AI referee determine which of two AI players performed better at a task.However, previous research has been inconsistent, like comparing apples to oranges. Different studies used different AI judges, instructions, and settings all at once, making it impossible to know what actually works best.This paper shows how to tune systematically different AI judge design decisions. We propose a method that finds judges offering the best balance between accuracy and cost—identifying which AI judges are both reliable and affordable to run.In particular, we find AI judges that outperform existing methods while using publicly available models, which we hope can help to make research using and based on AI judge more open."
Poster,Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization,https://ICML.cc//virtual/2025/poster/45373,"Kyurae Kim, Zuheng Xu, Jacob Gardner, Trevor Campbell","The performance of sequential Monte Carlo (SMC) samplers heavily depends on the tuning of the Markov kernels used in the path proposal. For SMC samplers with unadjusted Markov kernels, standard tuning objectives, such as the Metropolis-Hastings acceptance rate or the expected-squared jump distance, are no longer applicable. While stochastic gradient-based end-to-end optimization algorithms have been explored for tuning SMC samplers, they often incur excessive training costs, even for tuning just the kernel step sizes. In this work, we propose a general adaptation framework for tuning the Markov kernels in SMC samplers by minimizing the incremental Kullback-Leibler (KL) divergence between the proposal and target paths. For step size tuning, we provide a gradient- and tuning-free algorithm that is generally applicable for kernels such as Langevin Monte Carlo (LMC). We further demonstrate the utility of our approach by providing a tailored scheme for tuning kinetic LMC used in SMC samplers. Our implementations are able to obtain a full schedule of tuned parameters at the cost of a few vanilla SMC runs, which is a fraction of gradient-based approaches.","Sequential Monte Carlo (SMC) is a popular algorithm in statistics, physics, and machine learning for numerically evaluating high-dimensional integrals over probability distributions. It is particularly relevant for the purpose of comparing scientific hypotheses within the Bayesian framework, a process known as Bayesian model comparison.In practice, however, SMC tends to be difficult to use due to the abundance of tunable parameters. Furthermore, SMC is an iterative algorithm, where each step comes with its own set of tunable parameters. This is particularly problematic for certain types of SMC samplers that internally use ""unadjusted MCMC kernels."" Previous automatic tuning approaches had to rely on end-to-end optimization, which is particularly expensive. These methods attempt to tune all of the parameters at once by performing stochastic gradient descent. As a result, they requires running SMC many times and involve higher-order derivatives in the process. In this work, we propose an approach to tuning SMC samplers that is much cheaper and does not involve higher-order derivatives. The key idea is that we break up the tuning problem into multiple subproblems, one for each step of SMC. Each subproblem can be easily solved online without having to involve derivatives.Our proposed scheme significantly reduces the computational cost of using SMC samplers. On some problems, it even improves their statistical accuracy compared to end-to-end optimization approaches."
Poster,Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations,https://ICML.cc//virtual/2025/poster/44304,"Lee Cohen, Connie Hong, Jack Hsieh, Judy Hanwen Shen","In an era of increasingly capable foundation models, job seekers are turning to generative AI tools to enhance their application materials. However, unequal access to and knowledge about generative AI tools can harm both employers and candidates by reducing the accuracy of hiring decisions and giving some candidates an unfair advantage. To address these challenges, we introduce a new variant of the strategic classification framework tailored to manipulations performed using large language models, accommodating varying levels of manipulations and stochastic outcomes. We propose a ""two-ticket"" scheme, where the hiring algorithm applies an additional manipulation to each submitted resume and considers this manipulated version together with the original submitted resume. We establish theoretical guarantees for this scheme, showing improvements for both the fairness and accuracy of hiring decisions when the true positive rate is maximized subject to a no false positives constraint. We further generalize this approach to an $n$-ticket scheme and prove that hiring outcomes converge to a fixed, group-independent decision, eliminating disparities arising from differential LLM access. Finally, we empirically validate our framework and the performance of our two-ticket scheme on real resumes using an open-source resume screening tool.","Job seekers are increasingly using AI tools like ChatGPT to improve their resumes and cover letters. However, this creates unfairness because not everyone has equal access to these AI tools or knows how to use them effectively. This means some candidates get an unfair advantage, while employers struggle to make accurate hiring decisions because they can't tell which applications were AI-enhanced. We developed a ""two-ticket"" approach to level the playing field. Instead of just looking at the resume a candidate submits, our proposed algorithm automatically creates a second, AI-enhanced version of every resume. Then it evaluates both versions together - the original and the AI-improved one. This way, all candidates effectively get the same AI assistance, regardless of whether they had access to these tools themselves. We proved mathematically that this approach makes hiring both fairer and more accurate. By giving everyone the same AI boost, we eliminate the advantage that comes from unequal access to technology. We also tested this on real resumes using actual resume screening software and found it works in practice. Our work addresses a growing concern about how AI might increase inequality in job markets while helping employers make better hiring decisions."
Poster,TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories,https://ICML.cc//virtual/2025/poster/43566,"Honghua Dong, Jiacheng Yang, Xun Deng, Yuhe Jiang, Gennady Pekhimenko, Fan Long, Xujie Si","Type inference for dynamic languages like Python is a persistent challenge in software engineering. While large language models (LLMs) have shown promise in code understanding, their type inference capabilities remain underexplored. We introduce `TypyBench`, a benchmark designed to evaluate LLMs' type inference across entire Python repositories. `TypyBench` features two novel metrics: `TypeSim`, which captures nuanced semantic relationships between predicted and ground truth types, and `TypeCheck`, which assesses type consistency across codebases. Our evaluation of various LLMs on a curated dataset of 50 high-quality Python repositories reveals that, although LLMs achieve decent `TypeSim` scores, they struggle with complex nested types and exhibit significant type consistency errors. These findings suggest that future research should shift focus from improving type similarity to addressing repository-level consistency. `TypyBench` provides a foundation for this new direction, offering insights into model performance across different type complexities and usage contexts. Our code and data are available at \href{https://github.com/typybench/typybench}.","Figuring out the specific data types used in flexible programming languages like Python can be a real headache for software developers. While the powerful AI models known as LLMs are good at understanding code, we didn't know how well they could handle this specific task on a large scale.To find out, we created TypyBench, a new test to see how accurately these AIs can predict data types across entire software projects. We developed two new ways to measure their performance: one that checks if the predicted type is close in meaning to the correct one, and another that verifies if the AI's predictions are consistent throughout the code.Our tests on 50 high-quality Python projects revealed that while the AIs are pretty good at guessing the general meaning of types, they often make mistakes with more complicated ones and create inconsistencies within the same project. This shows that future efforts should focus on making AI predictions more consistent, and TypyBench provides the perfect tool to guide this research."
