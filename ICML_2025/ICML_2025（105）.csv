type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Faster Stochastic Optimization with Arbitrary Delays via Adaptive Asynchronous Mini-Batching,https://ICML.cc//virtual/2025/poster/44661,"Amit Attia, Ofir Gaash, Tomer Koren","We consider the problem of asynchronous stochastic optimization, where an optimization algorithm makes updates based on stale stochastic gradients of the objective that are subject to an arbitrary (possibly adversarial) sequence of delays. We present a procedure which, for any given $q \in (0,1]$, transforms any standard stochastic first-order method to an asynchronous method with convergence guarantee depending on the $q$-quantile delay of the sequence. This approach leads to convergence rates of the form $O(\tau_q/qT+\sigma/\sqrt{qT})$ for non-convex and $O(\tau_q^2/(q T)^2+\sigma/\sqrt{qT})$ for convex smooth problems, where $\tau_q$ is the $q$-quantile delay, generalizing and improving on existing results that depend on the average delay. We further show a method that automatically adapts to all quantiles simultaneously, without any prior knowledge of the delays, achieving convergence rates of the form $O(\inf_{q} \tau_q/qT+\sigma/\sqrt{qT})$ for non-convex and $O(\inf_{q} \tau_q^2/(q T)^2+\sigma/\sqrt{qT})$ for convex smooth problems. Our technique is based on asynchronous mini-batching with a careful batch-size selection and filtering of stale gradients.","Training modern machine learning models often involves huge datasets and running computations in parallel across many computing units. But when these systems update their models, they sometimes use outdated (or “stale”) information because different units report back at different times. This delay can slow down learning and degrade performance.We developed new methods that allow training algorithms to effectively reduce the impact of delays by making fewer, more meaningful updates using the most relevant parts of the delayed computations. Instead of relying on the average delay, which might be sensitive to a few very slow responses, our methods adapt to how often certain delays occur. This shift can lead to faster and more stable training.Our methods can be applied to many standard training algorithms with little to no modification, and they scale naturally with increasing parallelism, making them compelling options for large-scale systems. By adjusting how and when updates are made, they make better use of the available computations, even when some are delayed. As a result, our methods can help machine learning models learn faster and more reliably in real-world computing environments."
Poster,Fast Estimation of Partial Dependence Functions using Trees,https://ICML.cc//virtual/2025/poster/45692,"Jinyang Liu, Tessa Steensgaard, Marvin N. Wright, Niklas Pfister, Munir Hiabu","Many existing interpretation methods are based on Partial Dependence (PD) functions that, for a pre-trained machine learning model, capture how a subset of the features affects the predictions by averaging over the remaining features. Notable methods include Shapley additive explanations (SHAP) which computes feature contributions based on a game theoretical interpretation and PD plots (i.e., 1-dim PD functions) that capture average marginal main effects. Recent work has connected these approaches using a functional decomposition and argues that SHAP values can be misleading since they merge main and interaction effects into a single local effect. However, a major advantage of SHAP compared to other PD-based interpretations has been the availability of fast estimation techniques, such as `TreeSHAP`. In this paper, we propose a new tree-based estimator, `FastPD`, which efficiently estimates arbitrary PD functions. We show that `FastPD` consistently estimates the desired population quantity -- in contrast to path-dependent `TreeSHAP` which is inconsistent when features are correlated. For moderately deep trees, `FastPD` improves the complexity of existing methods from quadratic to linear in the number of observations. By estimating PD functions for arbitrary feature subsets, `FastPD` can be used to extract PD-based interpretations such as SHAP, PD plots and higher-order interaction effects.","Many powerful AI systems work by combining hundreds of simple “if-then” rules to make predictions—but these systems are often opaque, making it difficult to understand why they reach a particular decision. To address this, we introduce FastPD, an efficient algorithm that breaks down a model’s output into understandable parts, showing how each input factor and their combinations contribute to a prediction. FastPD runs in time that grows linearly with the amount of data, instead of quadratically, by reusing calculations across data points, and its explanations become exact as more background data are used. By cutting explanation times and clearly revealing how each factor and its interactions drive a prediction, FastPD empowers analysts, regulators, and everyday users to trust, debug, and manage AI-driven decisions more effectively."
Poster,Fast Exact Unlearning for In-Context Learning Data for LLMs,https://ICML.cc//virtual/2025/poster/45148,"Andrei Muresanu, Anvith Thudi, Michael Zhang, Nicolas Papernot","Modern machine learning models are expensive to train, and there is a growing concern about the challenge of retroactively removing specific training data. Achieving exact unlearning in deep learning pipelines—producing models as if certain data had never been included in training—remains an open problem. In this paper, we revisit exact unlearning in deep learning and show that for large language models (LLMs) we can efficiently exactly unlearn ``fine-tuning data"" (the data used to adapt a pre-trained model). This follows from two observations. First, we can use in-context learning to adapt the LLM to the fine-tuning dataset instead of SGD based algorithms. Second, we show that accurate in-context learning can be done with quantized k-means, which allows for effectively constant time unlearning operations. Our evaluation shows that this unlearning recipe has similar performance to fine-tuning alternatives, but vastly reduces the unlearning costs. Our study also highlights the need for new measures of unlearning cost when adapting the learning algorithm to have faster unlearn operations.","After deploying a model, it may become necessary to ""unlearn"" some of the original training data. Exactly unlearning training data has been expensive for deep learning, and in this paper we showed that it can be efficient when adapting a pre-trained LLM to a task. This followed from observing that a sometimes effective learning algorithm is pre-pending training examples to the prompt given to an LLM. We studied ways of unlearning this selection of examples, and found we could do so with costs independent of the model and dataset size. We also observed all past efforts to making unlearning faster also increased inference cost, and proposed new metrics to capture this trade-off."
Poster,Fast Incomplete Multi-view Clustering by Flexible Anchor Learning,https://ICML.cc//virtual/2025/poster/45017,"Yalan Qin, Guorui Feng, Xinpeng Zhang","Multi-view clustering aims to improve the final performance by taking advantages of complementary and consistent information of all views. In real world, data samples with partially available information are common and the issue regarding the clustering for incomplete multi-view data is inevitably raised. To deal with the partial data with large scales, some fast clustering approaches for incomplete multi-view data have been presented. Despite the significant success, few of these methods pay attention to learning anchors with high quality in a unified framework for incomplete multi-view clustering, while ensuring the scalability for large-scale incomplete datasets. In addition, most existing approaches based on incomplete multi-view clustering ignore to build the relation between anchor graph and similarity matrix in symmetric nonnegative matrix factorization and then directly conduct graph partition based on the anchor graph to reduce the space and time consumption. In this paper, we propose a novel fast incomplete multi-view clustering method for the data with large scales, termed Fast Incomplete Multi-view clustering by flexible anchor Learning (FIML), where graph construction, anchor learning and graph partition are simultaneously integrated into a unified framework for fast incomplete multi-view clustering. To be specific, we learn a shared anchor graph to guarantee the consistency among multiple views and employ a adaptive weight coefficient to balance the impact for each view. The relation between anchor graph and similarity matrix in symmetric nonnegative matrix factorization can also be built, i.e., each entry in the anchor graph can characterize the similarity between the anchor and original data sample. We then adopt an alternative algorithm for solving the formulated problem. Experiments conducted on different datasets confirm the superiority of FIML compared with other clustering methods for incomplete multi-view data.","Data samples with partially available information are common and the issue regarding the clustering for incomplete multi-view data is inevitably raised in real world. To deal with the partial data with large scales, some fast clustering approaches for incomplete multi-view data have been given. Despite the significant success, few of these methods pay attention to learning representative data samples with high quality in a unified framework for incomplete multi-view clustering, while ensuring the scalability for large-scale incomplete datasets. In this paper, we propose a novel fast incomplete multi-view clustering method for the data with large scales, termed Fast Incomplete Multi-view clustering by flexible anchor Learning (FIML), where graph construction, representative data samples learning and graph partition are simultaneously integrated into a unified framework for fast incomplete multi-view clustering. Experiments conducted on different datasets confirm the superiority of FIML compared with other clustering methods for incomplete multi-view data."
Poster,Fast Inference with Kronecker-Sparse Matrices,https://ICML.cc//virtual/2025/poster/45258,"Antoine Gonon, Léon Zheng, Pascal Carrivain, TUNG LE","Kronecker-sparse (KS) matrices—whose supports are Kronecker products of identity and all-ones blocks—underpin the structure of Butterfly and Monarch matrices and offer the promise of more efficient models. However, existing GPU kernels for KS matrix multiplication suffer from high data movement costs, with up to 50% of time spent on memory-bound tensor permutations. We propose a fused, output-stationary GPU kernel that eliminates these overheads, reducing global memory traffic threefold. Across 600 KS patterns, our kernel achieves in FP32 a median speedup of x1.4 and lowers energy consumption by 15%. A simple heuristic based on KS pattern parameters predicts when our method outperforms existing ones. We release all code at [github.com/PascalCarrivain/ksmm](https://github.com/PascalCarrivain/ksmm), including a PyTorch-compatible *KSLinear* layer, and demonstrate in FP32 end-to-end latency reductions of up to 22% in ViT-S/16 and 16% in GPT-2 medium.","Today’s AI models are costly to run because they spend most of their time and energy multiplying huge tables of numbers. One way to speed this up is to use tables with lots of zeros, since multiplying by zero does nothing and can be skipped. If we know where the zeros are in advance, we can organize the work to skip them efficiently—but how we skip them matters.In this work, we focus on a promising zero pattern that has been widely studied. We show that, even with this pattern, the fastest current Graphical Processing Unit (GPU) methods waste up to half their time just shuffling numbers around before doing the real calculations. While this shuffling is meant to help (to reorganize the table in big chunks that can be skipped), our work reveals that it is unnecessary, and we propose a faster workaround. By keeping the table data in place and changing how the GPU handles the work, we skip the zeros without extra shuffling. Our method runs up to 40% faster and uses 15% less energy across 600 test cases. We provide open-source code for both GPUs and CPUs, plus a one-line rule of thumb to tell when it helps. Plugged into large AI models like Transformers, it cuts total running time by up to 22%, paving the way for faster, cheaper AI."
Poster,Fast Large Language Model Collaborative Decoding via Speculation,https://ICML.cc//virtual/2025/poster/45230,"Jiale Fu, Yuchu Jiang, Junkai Chen, Jiaming Fan, Xin Geng, Xu Yang","Large Language Model (LLM) collaborative decoding techniques improve output quality by combining the outputs of multiple models at each generation step, but they incur high computational costs. In this paper, we introduce **Collaborative decoding via Speculation (CoS)**, a novel framework that accelerates collaborative decoding without compromising performance. Inspired by Speculative Decoding—where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel, our approach builds on two key insights: (1) the verification distribution can be the combined distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to collaboration among *n* models and theoretically prove that CoS is never slower than standard collaborative decoding, typically achieving faster speed. Extensive experiments demonstrate CoS is **1.11x–2.23x** faster than standard collaborative decoding without compromising generation quality. Our code is available at https://github.com/Kamichanw/CoS/.","Large language models (LLMs), like ChatGPT, generate responses by predicting one word (or *token*) at a time based on the input. A natural idea is that instead of using only one LLM to guess the next token, we can combine the guesses from several LLMs to get more accurate and reliable results. We refer to this class of methods as *LLM collaborative decoding*. However, since it needs to run several models for each token, the time it takes becomes *n* times longer, which makes it hard to use in real situations.To fix this problem, we propose a new framework: **Collaborative Decoding via Speculation (CoS)**. CoS can speed up any type of collaborative decoding—such as model ensemble, contrastive decoding, or decoding-time realignment—while still keeping the same high-quality output.Also, CoS does not need any training, added parameters, or extra calculation. This means it can be used directly to replace current ways of doing LLM collaborative decoding. Because of this, CoS has strong potential and value for real-world use."
Poster,Fast Min-$\epsilon$ Segmented Regression using Constant-Time Segment Merging,https://ICML.cc//virtual/2025/poster/43642,"Ansgar Lößer, Max Schlecht, Florian Schintke, Joel Witzke, Matthias Weidlich, Björn Scheuermann","Segmented regression is a statistical method that approximates a function $f$ by a piecewise function $\hat{f}$ using noisy data samples.*Min-$\epsilon$* approaches aim to reduce the regression function's mean squared error (MSE) for a given number of $k$ segments.An optimal solution for *min-$\epsilon$* segmented regression is found in $\mathcal{O}(n^2)$ time (Bai & Perron, 1998; Yamamoto & Perron, 2013) for $n$ samples. For large datasets, current heuristics improve time complexity to $\mathcal{O}(n\log{n})$ (Acharya et al., 2016) but can result in large errors, especially when exactly $k$ segments are used.We present a method for *min-$\epsilon$* segmented regression that combines the scalability of top existing heuristic solutions with a statistical efficiency similar to the optimal solution. This is achieved by using a new method to merge an initial set of segments using precomputed matrices from samples, allowing both merging and error calculation in constant time.Our approach, using the same samples and parameter $k$, produces segments with up to 1,000 times lower MSE compared to Acharya et al. (2016) in about 100 times less runtime on data sets over $10^4$ samples.","A well-known and fundamental technique of machine learning and statistical analysis is regression. Given a dataset of samples with a known input value and a correlated measured result, it is possible to derive a function that approximates the correlation between these two variables as closely as possible. This can be used to either derive knowledge about the underlying data or to predict the output value for unseen input values. In some use cases, the dataset contains ordered samples and suddenly changes behavior at a certain point. Correctly detecting these breakpoints is quite challenging. Current state-of-the-art algorithms are either exceedingly compute-heavy with a growing number of samples or result in a significantly worse regression function.In this paper, we present and evaluate a new algorithm for segmented regression. In our evaluation, this new approach needed much fewer computational resources -- even compared to the other heuristics -- while resulting in regressions very close to the optimal solution, without creating additional breakpoints. Given that more data enables more precise models, we think that our approach enables faster analysis and much more precise models in many different fields, including time-series analysis, ecology, econometrics, and gene analysis."
Poster,Fast Tensor Completion via Approximate Richardson Iteration,https://ICML.cc//virtual/2025/poster/44964,"Mehrdad Ghadiri, Matthew Fahrbach, Yunbum Kook, Ali Jadbabaie","We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD). Many TD algorithms use fast alternating minimization methods to solve _highly structured_ linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions). However, such algebraic structure is often lost in TC regression problems, making direct extensions unclear.  This work proposes a novel _lifting_ method for approximately solving TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods. We analyze the convergence rate of our approximate Richardson iteration-based algorithm, and our empirical study shows that it can be 100x faster than direct methods for CP completion on real-world tensors.","We tackle the challenge of filling in missing entries in large, multi‑dimensional data arrays—like predicting unknown pixels in a video or missing ratings in a recommendation system—by leveraging techniques that decompose the data into small components. Although extremely fast and efficient algorithms exist for fully observed data, exploiting its inherent structure, that structure is lost when some entries are missing. Our work introduces a “lifting” trick that recovers this structure, enabling us to use these fast algorithms as black‑box subroutines. In this way, we decompose the data into small building blocks and accurately recover the missing entries. On real‑world datasets, our approach runs up to 100x faster than standard techniques—potentially accelerating everything from medical imaging to machine‑learning pipelines."
Poster,Fast Video Generation with Sliding Tile Attention,https://ICML.cc//virtual/2025/poster/45139,"Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, Hao Zhang","Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 950 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being \emph{hardware-efficient}. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79\% MFU -- 7.17× faster than prior art methods. On the leading video DiT model, Hunyuan, it accelerates attention by 1.6–10x over FlashAttention-3, yielding a 1.36–3.53× end-to-end speedup with no or minimum quality loss.","Creating high-quality videos using AI models requires a huge amount of computation. For example, generating just a 5-second high-resolution video using current methods can take over 16 minutes—even on powerful hardware. The main bottleneck is a process called ""attention,"" which helps the AI understand relationships between different parts of the video but is extremely slow.In this work, we introduce a new technique called Sliding Tile Attention that makes this process dramatically faster. Instead of treating every part of the video equally, our method takes advantage of a simple observation: nearby frames and pixels often contain similar information. By focusing only on these local areas and processing them in larger chunks (or ""tiles""), we eliminate a lot of unnecessary work.Our method works right out of the box with existing video AI models and cuts the video generation time nearly in half—without hurting quality. When further fine-tuned, it can make video generation up to 3.5× faster, opening the door to faster and more practical AI-powered video tools."
Poster,FDGen: A Fairness-Aware Graph Generation Model,https://ICML.cc//virtual/2025/poster/46368,"Zichong Wang, Wenbin Zhang","Graph generation models have shown significant potential across various domains. However, despite their success, these models often inherit societal biases, limiting their adoption in real-world applications. Existing research on fairness in graph generation primarily addresses structural bias, overlooking the critical issue of feature bias. To address this gap, we propose FDGen, a novel approach that defines and mitigates both feature and structural biases in graph generation models. Furthermore, we provide a theoretical analysis of how bias sources in graph data contribute to disparities in graph generation tasks. Experimental results on four real-world datasets demonstrate that FDGen outperforms state-of-the-art methods, achieving notable improvements in fairness while maintaining competitive generation performance.","We often use computer models to generate graphs, which are structures that show how different items or entities connect to each other (for example, friendships in a social network). Unfortunately, these models can pick up hidden biases from the data, leading to unfairness. Our work introduces a new method called FDGen that addresses two types of bias in these models: feature bias (where certain attributes of the nodes are treated unfairly) and structural bias (where connections are unfairly formed). We also explain how different sources of bias in the data can lead to these problems. Our experiments on real-world datasets show that FDGen makes the generated graphs fairer while still producing high-quality results."
