type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Grokking at the Edge of Linear Separability,https://ICML.cc//virtual/2025/poster/44651,"Alon Beck, Noam Levi, Yohai Bar-Sinai","We investigate the phenomenon of grokking -- delayed generalization accompanied by non-monotonic test loss behavior -- in a simple binary logistic classification task, for which ""memorizing"" and ""generalizing"" solutions can be strictly defined. Surprisingly, we find that grokking arises naturally even in this minimal model when the parameters of the problem are close to a critical point, and provide both empirical and analytical insights into its mechanism.Concretely, by appealing to the implicit bias of gradient descent, we show that logistic regression can exhibit grokking when the training dataset is nearly linearly separable from the origin and there is strong noise in the perpendicular directions.The underlying reason is that near the critical point, ""flat"" directions in the loss landscape with nearly zero gradient cause training dynamics to linger for arbitrarily long times near quasi-stable solutions before eventually reaching the global minimum.Finally, we highlight similarities between our findings and the recent literature, strengthening the conjecture that grokking generally occurs in proximity to the interpolation threshold, reminiscent of critical phenomena often observed in physical systems.","We investigate why a strange training pattern called “grokking” occurs in very simple machine learning models. Grokking happens when a model at first manages to fit the training data perfectly but fails to generalize (i.e., doesn’t perform well on new data). After more training, it suddenly figures out how to generalize properly.We discovered that grokking happens in simple yes/no (binary) classification tasks when the training data is almost perfectly separated by a straight line (hyperplane), but not quite. This causes the model to get stuck for a long time before finally learning to generalize. We believe this is similar to how physical systems behave near phase transitions, which is really interesting."
Poster,Grokking Beyond the Euclidean Norm of Model Parameters,https://ICML.cc//virtual/2025/poster/45887,"Pascal Jr Tikeng Notsawo, Guillaume Dumas, Guillaume Rabusseau","Grokking refers to a delayed generalization following overfitting when optimizing artificial neural networks with gradient-based methods. In this work, we demonstrate that grokking can be induced by regularization, either explicit or implicit.  More precisely, we show that when there exists a model with a property $P$ (e.g., sparse or low-rank weights) that generalizes on the problem of interest, gradient descent with a small but non-zero regularization of $P$ (e.g., $\ell_1$ or nuclear norm regularization) results in grokking.  This extends previous work showing that small non-zero weight decay induces grokking. Moreover, our analysis shows that over-parameterization by adding depth makes it possible to grok or ungrok without explicitly using regularization, which is impossible in shallow cases.  We further show that the $\ell_2$ norm is not a reliable proxy for generalization when the model is regularized toward a different property $P$, as the $\ell_2$ norm grows in many cases where no weight decay is used, but the model generalizes anyway. We also show that grokking can be amplified solely through data selection, with any other hyperparameter fixed.","Sometimes when learning, children do not seem to understand something at first — they simply mimic what they see. But after enough repetition, something clicks: they suddenly ""get it"" and can apply the idea in new situations. The same thing can happen with artificial intelligence (AI). AI models often start by memorizing the training examples. Yet, after a surprisingly long time, they begin to understand the underlying patterns and solve problems they have never seen before. This sudden shift is called **grokking**.Our research investigates why grokking happens and how to influence it. We find that it is not just about the model’s architecture — grokking also depends on the kind of simplicity (**regularization**) enforced during training, such as using fewer connections (**sparsity**) or a simpler internal structure (**low-rankness**). In some cases, we even show that grokking is necessary for a model to reach an optimal solution. However, when simplicity is enforced, training can take significantly longer. There is a tradeoff: small regularization can improve generalization, but requires more training time. Our results provide a way to manage this tradeoff based on the resources available and the kind of behavior we want from the model.These insights help explain why some AI systems require much more training than expected to reach deep understanding — and how we can guide them more effectively."
Poster,Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers,https://ICML.cc//virtual/2025/poster/44180,"Roman Abramov, Felix Steinbauer, Gjergji Kasneci","Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns -- yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95--100\% accuracy on 2WikiMultiHopQA -- substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.","Many real-world questions require linking facts across multiple sources, like determining a person’s birth year by first identifying their spouse and then tracing their birth date. Current AI models struggle with this because they tend to memorize isolated facts without understanding how they connect.We tackled this by adding synthetic links to the training data of language models. These artificial connections aren’t always factual, but they force the models to focus on linking facts instead of just memorizing them. Surprisingly, the ""fake"" connections don’t harm the models; instead, they push them to develop their own reasoning pathways.Our experiments showed that this approach helps AI models go beyond memorization, enabling them to answer complex, multi-step questions with impressive accuracy. This could lead to more powerful and trustworthy AI systems in many different fields."
Poster,GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs,https://ICML.cc//virtual/2025/poster/45967,"Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han","Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with substantial costs due to its compromises in their general functionality, leading to a notorious trade-off between unlearning and retention. It motivates this paper to explore enhanced unlearning schemes that can mitigate this trade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an improved framework that regulates the directions of gradient updates during the unlearning procedure such that their side impacts on other, unrelated responses can be minimized. GRU is easy and general to implement, demonstrating practical effectiveness across a variety of well-established unlearning benchmarks.","Large language models (LLMs) are typically trained on web-scaled corpora, which carry the risk of learning private and harmful knowledge. This knowledge can be further exposed to users, raising many legal and policy-related concerns. These issues have motivated recent studies on LLM unlearning, which aim to remove such harmful knowledge from model parameterization—a pursuit with notable academic and industrial significance.However, in practice, unlearning often severely impacts the performance of unrelated responses, thereby diminishing the overall model utility. This highlights a notorious trade-off between removing harmful knowledge and retaining overall performance. To address this issue, we propose Gradient Rectified Unlearning (GRU), a method that controls model updates by eliminating gradient directions that negatively impact overall performance. Such an insight is formalized as a constrained optimization problem, which has a closed-form solution that can be implemented effectively. We conduct extensive experiments on several widely used benchmarks, demonstrating that GRU can effectively eliminate targeted knowledge while preserving the overall model capabilities. Our results show the reliability of GRU to mitigate the trade-off between unlearning and retention, serving as a promising method that warrants further study."
Poster,GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models,https://ICML.cc//virtual/2025/poster/44465,"Zhaohong Huang, Yuxin Zhang, JingJing Xie, Fei Chao, Rongrong Ji","Recent advances in test-time adaptation (TTA) for Vision-Language Models (VLMs) have garnered increasing attention, particularly through the use of multiple augmented views of a single image to boost zero-shot generalization. Unfortunately, existing methods fail to strike a satisfactory balance between performance and efficiency, either due to excessive overhead of tuning text prompts or unstable benefits from handcrafted, training-free visual feature enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias), an efficient and effective TTA paradigm that incorporates two learnable biases during TTA, unfolded as the global bias and spatial bias. Particularly, the global bias captures the global semantic features of a test image by learning consistency across augmented views, while spatial bias learns the semantic coherence between regions in the image’s spatial visual representation. It is worth highlighting that these two sets of biases are directly added to the logits outputed by the pretrained VLMs, which circumvent the full backpropagation through VLM that hinders the efficiency of existing TTA methods. This endows GS-Bias with extremely high efficiency while achieving state-of-the-art performance on 15 benchmark datasets. For example, it achieves a 2.23% improvement over TPT in cross-dataset generalization and a 2.72% improvement in domain generalization, while requiring only 6.5% of TPT's memory usage on ImageNet.","Test-time adaptation (TTA) allows vision-language models to handle new, unseen images without any additional training. It typically works by generating different versions of the same image and letting the model learn from them. Existing methods either learn custom text prompts for each image or aggregate various visual features to adapt to new inputs. However, these approaches can be slow, memory-intensive, or produce unstable results. To address this, our paper proposes GS-Bias, a simple yet reliable method that subtly adjusts the model’s predictions without altering its original reasoning process. It introduces two lightweight “hints”: one helps the model form a global understanding of the image, while the other guides it to focus more precisely on local regions. This small modification leads to significant improvements, making the model faster, more efficient, and more accurate."
Poster,G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration,https://ICML.cc//virtual/2025/poster/45361,"Samuel Holt, Max Ruiz Luyten, Antonin Berthon, Mihaela van der Schaar","Constructing robust simulators is essential for asking ""what if?"" questions and guiding policy in critical domains like healthcare and logistics. However, existing methods often struggle, either failing to generalize beyond historical data or, when using Large Language Models (LLMs), suffering from inaccuracies and poor empirical alignment. We introduce **G-Sim**, a hybrid framework that automates simulator construction by synergizing LLM-driven structural design with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop to propose and refine a simulator's core components and causal relationships, guided by domain knowledge. This structure is then grounded in reality by estimating its parameters using flexible calibration techniques. Specifically, G-Sim can leverage methods that are both **likelihood-free** and **gradient-free** with respect to the simulator, such as **gradient-free optimization** for direct parameter estimation or **simulation-based inference** for obtaining a posterior distribution over parameters. This allows it to handle non-differentiable and stochastic simulators. By integrating domain priors with empirical evidence, G-Sim produces reliable, causally-informed simulators, mitigating data-inefficiency and enabling robust system-level interventions for complex decision-making.","Making smart decisions for complex systems, like managing hospital capacity or a company's supply chain, often requires asking ""what if...?"" questions. We rely on computer simulations to explore different scenarios, but building accurate ones is a major challenge. Current methods often fail in two ways: some are stuck in the past, unable to predict new situations they haven't seen in the data, while others that use creative AI like Large Language Models (LLMs) can be unreliable and invent details that don't match reality. Our work, G-Sim, introduces a hybrid approach that gets the best of both worlds. First, we use an LLM's vast knowledge to sketch a basic blueprint of the system, outlining its main parts and how they connect. Then, in a crucial second step, we use flexible algorithms to automatically tune this blueprint, adjusting all its specific numbers until the simulation's behavior accurately matches real-world data. If the simulation is still flawed, G-Sim identifies the problem and asks the LLM to propose a better blueprint, repeating the process until the model is right. This method results in trustworthy digital replicas of complex systems. It empowers decision-makers—from hospital administrators to city planners—to safely and reliably test the consequences of their choices before implementing them in the real world, leading to better, safer, and more informed strategies."
Poster,GSM-$\infty$: How Do your LLMs Behave over Infinitely Increasing Reasoning Complexity and Context Length?,https://ICML.cc//virtual/2025/poster/44121,"Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, Beidi Chen","Recently, long-context large language models (LLMs) have shown strong performance in information retrieval and long-document QA. However, to tackle the most challenging intellectual problems, LLMs must reason effectively in long and complex contexts (e.g., frontier mathematical research). Studying how LLMs handle increasing reasoning complexity and context length is essential, yet existing benchmarks lack a solid basis for quantitative evaluation. Inspired by the abstraction of GSM-8K problems as computational graphs—and the ability to introduce noise by adding unnecessary nodes and edges—we develop a grade-school math problem generator capable of producing arithmetic problems with infinite difficulty and context length under fine-grained control. Using our newly synthesized GSM-$\infty$ benchmark, we comprehensively evaluate existing LLMs. We find a consistent sigmoid decline in reasoning performance as complexity increases, along with a systematic inference scaling trend: exponentially increasing inference computation yields only linear performance gains. These findings underscore the fundamental limitations of current long-context LLMs and the key challenges in scaling reasoning capabilities. Our GSM-$\infty$ benchmark provides a scalable and controllable testbed for systematically studying and advancing LLM reasoning in long and complex contexts.","Do we really need LLMs to have long-context ability? Retrieval Augmented Generation (RAG) is a cheap-to-build alternative that is seemingly powerful for general long-context tasks, while a long context window tends to require big companies millions of dollars to train. In the paper, we discover that on most existing long-context benchmarks, through comprehensive evaluation, RAG is on par or even surpasses the performance of long-context LLMs. However, context-level methods alone are not sufficient for agents that one day will be capable of contributing to frontier scientific discovery. We clearly need a much more difficult long-context benchmark.In the paper, we first develop a mapping between an abstract computation graph and natural language problems. Then, by randomly perturbing the computation graphs' generation, we can map to different natural language math problems. The difficulty of a problem is defined as the number of essential steps required to solve the particular problem. Besides, the computation graph can be extended to include unnecessary nodes. Benefitting from the tight semantic connections to the essential core graph, the added noise is RAG-insolvable. Empirically, we show that the added noise is hard to filter by RAG.By using the generator, we are able to generate a large quantity of problems guaranteed to be correctly labeled with controllable reasoning complexity and context length of the problems. We named the suite of problems generated as GSM-Infinite. We comprehensively evaluate LLMs on GSM-Infinite. We discovered that LLM performance decay follows a sigmoid pattern, alongside other insights revealed from our studies."
Poster,"GTR: A General, Multi-View, and Dynamic Framework for Trajectory Representation Learning",https://ICML.cc//virtual/2025/poster/44569,"Xiangheng Wang, Ziquan Fang, Chenglong Huang, Danlei Hu, Lu Chen, Yunjun Gao","Trajectory representation learning aims to transform raw trajectory data into compact and low-dimensional vectors that are suitable for downstream analysis. However, most existing methods adopt either a free-space view or a road-network view during the learning process, which limits their ability to capture the complex, multi-view spatiotemporal features inherent in trajectory data. Moreover, these approaches rely on task-specific model training, restricting their generalizability and effectiveness for diverse analysis tasks. To this end, we propose GTR, a general, multi-view, and dynamic Trajectory Representation framework built on a pre-train and fine-tune architecture. Specifically, GTR introduces a multi-view encoder that captures the intrinsic multi-view spatiotemporal features. Based on the pre-train and fine-tune architecture, we provide the spatio-temporal fusion pre-training with a spatio-temporal mixture of experts to dynamically combine spatial and temporal features, enabling seamless adaptation to diverse trajectory analysis tasks. Furthermore, we propose an online frozen-hot updating strategy to efficiently update the representation model, accommodating the dynamic nature of trajectory data. Extensive experiments on two real-world datasets demonstrate that GTR consistently outperforms 15 state-of-the-art methods across 6 mainstream trajectory analysis tasks. All source code and data are available at https://github.com/ZJU-DAILY/GTR.","Smartphones and GPS devices constantly record our movements, generating massive amounts of trajectory data—sequences of location points that reveal how people and vehicles move through cities. This data holds great potential, helping urban planners optimize traffic, navigation apps suggest faster routes, and researchers study mobility patterns. However, current methods analyze trajectories from just a single perspective—either raw GPS points or road maps—missing the bigger picture. They also require retraining for each new task, making them inefficient for real-life development.We present GTR, a unified and more flexible way to process trajectory data. GTR combines multiple perspectives, looking at both free movement (like GPS traces) and road networks to better understand travel patterns. It’s designed as a general-purpose tool, meaning it can adapt to different tasks—such as predicting travel times, finding similar routes, or even trajectory data generation, without needing full retraining each time. Additionally, GTR keeps improving over time, automatically adjusting to new movement data, which is crucial for real-world applications where traffic and travel behaviors constantly change. In evaluations, GTR outperforms 15 SOTA methods across 6 common trajectory analysis tasks, proving its versatility and superiority."
Poster,Guarantees of a Preconditioned Subgradient Algorithm for Overparameterized Asymmetric Low-rank Matrix Recovery,https://ICML.cc//virtual/2025/poster/45824,"Paris Giampouras, HanQin Cai, Rene Vidal","In this paper, we focus on a matrix factorization-based approach for robust recovery of low-rank  asymmetric matrices from corrupted measurements. We propose an  Overparameterized Preconditioned Subgradient Algorithm (OPSA) and provide, for the first time in the literature, linear convergence rates independent of the rank of the sought asymmetric matrix in the presence of gross corruptions. Our work goes beyond existing results in preconditioned-type approaches addressing their current limitation, i.e., the lack of convergence guarantees in the case of asymmetric matrices of unknown rank. By applying our approach to (robust) matrix sensing, we highlight its merits when the measurement operator satisfies a mixed-norm restricted isometry property. Lastly, we present extensive numerical experiments that validate our theoretical results and demonstrate the effectiveness of our approach for different levels of overparameterization and corruption from outliers.","This  paper focuses on a problem that appears in many areas like image processing, data science, and machine learning: how to recover a clean and complete version of a matrix when some of the data is  corrupted. To tackle this, we propose a new method called OPSA. OPSA is designed to work even when the structure of the original matrix is not fully known and when the data is badly corrupted. What's special about our method is that:- It can handle cases where the matrix isn't nicely balanced (asymmetric),- It doesn’t require knowing in advance how complex the original matrix is (its “rank”),- It still works well even if the corruption in the data is pretty severe, no matter how complicated the matrix is.We have  tested our method in various challenging scenarios, and it consistently performed well. We also backed up our claims with mathematical proofs to show that our approach is not only practical but also theoretically sound."
Poster,GuardAgent: Safeguard LLM Agents via Knowledge-Enabled Reasoning,https://ICML.cc//virtual/2025/poster/46569,"Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, Dawn Song, Bo Li","The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security. In this paper, we propose GuardAgent, the first guardrail agent to protect target agents by dynamically checking whether their actions satisfy given safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution. By performing the code execution, GuardAgent can deterministically follow the safety guard request and safeguard target agents. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety policies for web agents. We show that GuardAgent effectively moderates the violation actions for different types of agents on these two benchmarks with over 98% and 83%guardrail accuracies, respectively. Project page: https://guardagent.github.io/","As AI agents grow more powerful, they’re increasingly used in sensitive areas like healthcare and web automation. But how can we ensure they follow safety rules, such as avoiding private patient data or unsafe websites? Traditional safeguards fall short because they focus on filtering text, not regulating actions. We introduce GuardAgent, a system that interprets safety rules and generates code to monitor and block unsafe behavior. It uses a large language model and draws on past examples to adapt to new tasks. Tested in healthcare and web scenarios, GuardAgent achieved high accuracy and shows promise as a flexible, low-overhead safety layer for responsible AI use."
