type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,TimeBase: The Power of Minimalism in Efficient Long-term Time Series Forecasting,https://ICML.cc//virtual/2025/poster/45815,"Qihe Huang, Zhengyang Zhou, Kuo Yang, Zhongchao Yi, Xu Wang, Yang Wang","Long-term time series forecasting (LTSF) has traditionally relied on  large parameters to capture extended temporal dependencies, resulting in substantial computational costs and inefficiencies in both memory usage and processing time. However, time series data, unlike high-dimensional images or text, often exhibit temporal pattern similarity and  low-rank structures, especially in long-term horizons.  By leveraging this structure, models can be guided to focus on more essential, concise temporal data, improving both accuracy and computational efficiency. In this paper, we introduce TimeBase, an ultra-lightweight network  to harness the power of minimalism in LTSF.  TimeBase 1) extracts core basis temporal components and 2) transforms traditional point-level forecasting into efficient segment-level forecasting, achieving optimal utilization of both data and parameters. Extensive experiments on diverse  real-world datasets show that TimeBase achieves remarkable efficiency and secures competitive forecasting performance. Additionally, TimeBase can also serve as a very effective plug-and-play complexity reducer for any patch-based forecasting models. Code is available at \url{https://github.com/hqh0728/TimeBase}.","Long-term time series forecasting (LTSF) is important for areas like weather, energy, and finance. But current methods often rely on large, complex models that are slow and costly.We introduce TimeBase, a lightweight forecasting model that makes predictions using only the most essential patterns in the data. Instead of forecasting each time point separately, it predicts meaningful segments, reducing computation while improving accuracy.TimeBase works well across many real-world datasets and can also simplify existing models by acting as a plug-in. This makes accurate long-term forecasting faster and more efficient, even in resource-limited settings."
Poster,TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting,https://ICML.cc//virtual/2025/poster/43973,"Peiyuan Liu, Beiliang Wu, Yifan Hu, Naiqi Li, Tao Dai, Jigang Bao, Shutao Xia","Non-stationarity poses significant challenges for multivariate time series forecasting due to the inherent short-term fluctuations and long-term trends that can lead to spurious regressions or obscure essential long-term relationships. Most existing methods either eliminate or retain non-stationarity without adequately addressing its distinct impacts on short-term and long-term modeling. Eliminating non-stationarity is essential for avoiding spurious regressions and capturing local dependencies in short-term modeling, while preserving it is crucial for revealing long-term cointegration across variates. In this paper, we propose TimeBridge, a novel framework designed to bridge the gap between non-stationarity and dependency modeling in long-term time series forecasting. By segmenting input series into smaller patches, TimeBridge applies Integrated Attention to mitigate short-term non-stationarity and capture stable dependencies within each variate, while Cointegrated Attention preserves non-stationarity to model long-term cointegration across variates. Extensive experiments show that TimeBridge consistently achieves state-of-the-art performance in both short-term and long-term forecasting. Additionally, TimeBridge demonstrates exceptional performance in financial forecasting on the CSI 500 and S&P 500 indices, further validating its robustness and effectiveness. Code is available at https://github.com/Hank0626/TimeBridge.","Forecasting things like stock trends or weather patterns can be tricky, especially when the data we use keeps changing over time. These changes — known as non-stationarity — include short-term ups and downs as well as long-term trends, and both can confuse traditional forecasting methods.Our new method, called TimeBridge, is designed to deal with this challenge in a smarter way. Instead of treating all changes the same, it looks at short-term and long-term patterns separately. For the short term, it cleans out the noise to focus on recent, reliable patterns. For the long term, it keeps important trends so the model can understand deeper relationships across different types of data.In tests, TimeBridge consistently made more accurate predictions than existing tools. It also did especially well on real-world financial data like the CSI 500 and S&P 500 stock indices, showing its strength in both research and practical use."
Poster,TimeDART: A Diffusion Autoregressive Transformer for Self-Supervised Time Series Representation,https://ICML.cc//virtual/2025/poster/43701,"Daoyu Wang, Mingyue Cheng, Zhiding Liu, Qi Liu","Self-supervised learning has garnered increasing attention in time series analysis for benefiting various downstream tasks and reducing reliance on labeled data. Despite its effectiveness, existing methods often struggle to comprehensively capture both long-term dynamic evolution and subtle local patterns in a unified manner. In this work, we propose \textbf{TimeDART}, a novel self-supervised time series pre-training framework that unifies two powerful generative paradigms to learn more transferable representations. Specifically, we first employ a causal Transformer encoder, accompanied by a patch-based embedding strategy, to model the evolving trends from left to right. Building on this global modeling, we further introduce a denoising diffusion process to capture fine-grained local patterns through forward diffusion and reverse denoising. Finally, we optimize the model in an autoregressive manner. As a result, TimeDART effectively accounts for both global and local sequence features in a coherent way.We conduct extensive experiments on public datasets for time series forecasting and classification. The experimental results demonstrate that TimeDART consistently outperforms previous compared methods, validating the effectiveness of our approach.Our code is available at \url{https://github.com/Melmaphother/TimeDART}.","TimeDART: Helping Computers Better Understand Time-Series DataMany crucial types of data, like stock prices, weather patterns, or medical readings, unfold over time. Understanding these time-series data deeply is key to predicting future trends or identifying anomalies. However, current computer methods often struggle to simultaneously capture both the long-term overall changes and the subtle local details within these complex datasets.To address this challenge, we developed TimeDART, a new machine learning framework. TimeDART cleverly unifies two powerful generative techniques. It first models evolving trends like reading a story from left to right, capturing the global flow. Then, through a ""denoising"" process, it meticulously identifies fine-grained local patterns that might otherwise be overlooked. This dual approach allows TimeDART to learn more comprehensive and transferable representations of time series.Our extensive experiments show that TimeDART consistently outperforms previous methods on various tasks, including forecasting future values and classifying different types of time series. This means TimeDART can help computers analyze time-based data more accurately, leading to more reliable applications in fields like finance, healthcare, and environmental monitoring."
Poster,TimeFilter: Patch-Specific Spatial-Temporal Graph Filtration for Time Series Forecasting,https://ICML.cc//virtual/2025/poster/46502,"Yifan Hu, Guibin Zhang, Peiyuan Liu, Disen Lan, Naiqi Li, Dawei Cheng, Tao Dai, Shutao Xia, Shirui Pan","Time series forecasting methods generally fall into two main categories: Channel Independent (CI) and Channel Dependent (CD) strategies. While CI overlooks important covariate relationships, CD captures all dependencies without distinction, introducing noise and reducing generalization. Recent advances in Channel Clustering (CC) aim to refine dependency modeling by grouping channels with similar characteristics and applying tailored modeling techniques. However, coarse-grained clustering struggles to capture complex, time-varying interactions effectively. To address these challenges, we propose TimeFilter, a GNN-based framework for adaptive and fine-grained dependency modeling. After constructing the graph from the input sequence, TimeFilter refines the learned spatial-temporal dependencies by filtering out irrelevant correlations while preserving the most critical ones in a patch-specific manner. Extensive experiments on 13 real-world datasets from diverse application domains demonstrate the state-of-the-art performance of TimeFilter. The code is available at https://github.com/TROUBADOUR000/TimeFilter.","Forecasting future values in time series data is challenging because of the complex dependencies within the data. Traditional forecasting methods often struggle to balance capturing interdependencies among different data channels and avoiding irrelevant or noisy relationships.We propose a new method called TimeFilter to address these challenges. It constructs a graph-based framework that adapts to the dependencies between different time periods and data channels. TimeFilter segments the input time series into patches and constructs a spatial-temporal graph, then filters out irrelevant correlations through patch-specific filtering, preserving only the most critical dependencies.Experiments on multiple real-world datasets show that TimeFilter performs better than existing methods, making it a promising advancement for predicting future trends in various fields"
Poster,TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning,https://ICML.cc//virtual/2025/poster/44741,"Ron Shapira Weber, shahar benishay, Andrey Lavrinenko, Shahaf E. Finder, Oren Freifeld","Fast and scalable alignment of time series is a fundamental challenge in many domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor scalability and sensitivity to noise. We introduce TimePoint, a self-supervised method that dramatically accelerates DTW-based alignment while typically improving alignment accuracy by learning keypoints and descriptors from synthetic data. Inspired by 2D keypoint detection but carefully adapted to the unique challenges of 1D signals, TimePoint leverages efficient 1D diffeomorphisms, which effectively model nonlinear time warping,  to generate realistic training data. This adaptation, along with fully convolutional and wavelet convolutional architectures, enables the extraction of informative keypoints and descriptors. Applying DTW to these sparse representations yields major speedups and typically higher alignment accuracy than standard DTW applied to the full signals. Despite being trained solely on synthetic data, TimePoint generalizes well to real-world time series. Extensive experiments demonstrate that TimePoint consistently achieves faster and more accurate alignments than standard DTW, making it a scalable solution for time-series analysis. Our code is available at https://github.com/BGU-CS-VIL/TimePoint.","TimePoint is a new method for aligning time series faster and more accurately. Instead of comparing every point in two signals, TimePoint learns to find the most important ones and describe them in a way that makes matching easier. It learns from synthetic examples and generalizes well to real data. To do this, we created a large synthetic dataset designed specifically to teach the model how to handle realistic patterns and timing changes. When combined with traditional alignment tools, TimePoint leads to big speedups and often better results. leads to big speedups and often better results."
Poster,TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state,https://ICML.cc//virtual/2025/poster/43851,"Xiaowen Ma, Zhen-Liang Ni, Shuai Xiao, Xinghao Chen","In long-term time series forecasting, different variables often influence the target variable over distinct time intervals, a challenge known as the multi-delay issue. Traditional models typically process all variables or time points uniformly, which limits their ability to capture complex variable relationships and obtain non-trivial time representations. To address this issue, we propose TimePro, an innovative Mamba-based model that constructs variate- and time-aware hyper-states. Unlike conventional approaches that merely transfer plain states across variable or time dimensions, TimePro preserves the fine-grained temporal features of each variate token and adaptively selects the focused time points to tune the plain state. The reconstructed hyper-state can perceive both variable relationships and salient temporal information, which helps the model make accurate forecasting. In experiments, TimePro performs competitively on eight real-world long-term forecasting benchmarks with satisfactory linear complexity. Code is available at https://github.com/xwmaxwma/TimePro.","In long-term time series forecasting, different variables often influence the target variable over distinct time intervals, a challenge known as the multi-delay issue.  We design the model for the multi-delay issue thus improving the prediction performance.We propose the TimePro from the most recent Mamba. Specifically, we design a variable- and time-aware hyper-state that senses both variable relationships and significant temporal information within variables to effectively solve the multi-latency issue.Our model TimePro can significantly improve the performance of multivariate time series forecasting. In particular, it also possesses good efficiency. We have also verified that TimePro can be used as a powerful and efficient predictor  in some real-world industrial scenarios ."
Poster,Time Series Representations with Hard-Coded Invariances,https://ICML.cc//virtual/2025/poster/45216,"Thibaut Germain, Chrysoula Kosma, Laurent Oudre","Automatically extracting robust representations from large and complex time series data is becoming imperative for several real-world applications. Unfortunately, the potential of common neural network architectures in capturing invariant properties of time series remains relatively underexplored. For instance, convolutional layers often fail to capture underlying patterns in time series inputs that encompass strong deformations, such as trends. Indeed, invariances to some deformations may be critical for solving complex time series tasks, such as classification, while guaranteeing good generalization performance.To address these challenges, we mathematically formulate and technically design efficient and hard-coded *invariant convolutions* for specific group actions applicable to the case of time series.We construct these convolutions by considering specific sets of deformations commonly observed in time series, including *scaling*, *offset shift*, and *trend*.We further combine the proposed invariant convolutions with standard convolutions in single embedding layers, and we showcase the layer capacity to capture complex invariant time series properties in several scenarios.","Time series data, such as physiological signals, often contain distortions such as baseline wander, which can mislead the training of neural networks. For example, a long-term trend can mask periodic signal patterns, causing convolutional networks to not correctly model the data. To address this, we propose a mathematical framework that uses group actions for time series to model how certain deformations affect time series data. Subsequently, we propose deformation-free representations of time series. It allows neural networks to learn representations inherently robust to common time series distortions, such as offset shift and linear trend. Combined into standard convolutional layers, these deformation-invariant representations can improve the network’s robustness across tasks like classification, anomaly detection, and transfer learning."
Poster,TimeStacker: A Novel Framework with Multilevel Observation for Capturing Nonstationary Patterns in Time Series Forecasting,https://ICML.cc//virtual/2025/poster/46428,"Qinglong Liu, Cong Xu, Wenhao Jiang, Kaixuan Wang, Lin Ma, Haifeng Li","Real-world time series inherently exhibit significant non-stationarity, posing substantial challenges for forecasting. To address this issue, this paper proposes a novel prediction framework, TimeStacker, designed to overcome the limitations of existing models in capturing the characteristics of non-stationary signals. By employing a unique stacking mechanism, TimeStacker effectively captures global signal features while thoroughly exploring local details. Furthermore, the framework integrates a frequency-based self-attention module, significantly enhancing its feature modeling capabilities. Experimental results demonstrate that TimeStacker achieves outstanding performance across multiple real-world datasets, including those from the energy, finance, and weather domains. It not only delivers superior predictive accuracy but also exhibits remarkable advantages with fewer parameters and higher computational efficiency.","Due to the time-frequency uncertainty principle, the frequency of a non-stationary signal at a specific moment cannot be precisely determined. This raises the question of how the temporal evolution of frequency in such signals can be effectively captured.We propose the TimeStacker framework, which captures the temporal evolution of frequency in non-stationary signals by combining windows of different sizes.Our study introduces a new perspective on time series forecasting from a time-frequency variation perspective, achieving better performance while using fewer parameters."
Poster,TimeStep Master: Asymmetrical Mixture of Timestep LoRA Experts for Versatile and Efficient Diffusion Models in Vision,https://ICML.cc//virtual/2025/poster/46207,"Shaobin Zhuang, Yiwei Guo, Yanbo Ding, Kunchang Li, Xinyuan Chen, Yaohui Wang, Fangyikang Wang, Ying Zhang, Chen Li, Yali Wang","Diffusion models have driven the advancement of vision generation over the past years. However, it is often difficult to apply these large models in downstream tasks, due to massive fine-tuning cost. Recently, Low-Rank Adaptation (LoRA) has been applied for efficient tuning of diffusion models. Unfortunately, the capabilities of LoRA-tuned diffusion models are limited, since the same LoRA is used for different timesteps of the diffusion process. To tackle this problem, we introduce a general and concise TimeStep Master (TSM) paradigm with two key fine-tuning stages. In the fostering stage (1-stage), we apply different LoRAs to fine-tune the diffusion model at different timestep intervals. This results in different TimeStep LoRA experts that can effectively capture different noise levels. In the assembling stage (2-stage), we design a novel asymmetrical mixture of TimeStep LoRA experts, via core-context collaboration of experts at multi-scale intervals. For each timestep, we leverage TimeStep LoRA expert within the smallest interval as the core expert without gating, and use experts within the bigger intervals as the context experts with time-dependent gating. Consequently, our TSM can effectively model the noise level via the expert in the finest interval, and adaptively integrate contexts from the experts of other scales, boosting the versatility of diffusion models. To show the effectiveness of our TSM paradigm, we conduct extensive experiments on three typical and popular LoRA-related tasks of diffusion models, including domain adaptation, post-pretraining, and model distillation. Our TSM achieves the state-of-the-art results on all these tasks, throughout various model structures (UNet, DiT and MM-DiT) and visual data modalities (Image, Video), showing its remarkable generalization capacity.","Diffusion models create pictures and videos by gently turning noise into clear scenes. Fine-tuning these large models for new jobs usually costs lots of time and computer power. We introduce TimeStep Master, a lightweight add-on that lets the model learn small, specialized tweaks for different moments in the creation process, then smartly combines them. This approach keeps quality high while cutting training effort and works for many kinds of images, videos, and model types. In tests, it beat previous methods at adapting to new styles, improving older models, and shrinking big models into faster, smaller ones."
Poster,Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time,https://ICML.cc//virtual/2025/poster/44775,"Duc Anh Nguyen, Ernesto Araya, Adalbert Fono, Gitta Kutyniok","Recent years have seen significant progress in developing spiking neural networks (SNNs) as a potential solution to the energy challenges posed by conventional artificial neural networks (ANNs). However, our theoretical understanding of SNNs remains relatively limited compared to the ever-growing body of literature on ANNs. In this paper, we study a discrete-time model of SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as discrete-time LIF-SNNs, a widely used framework that still lacks solid theoretical foundations. We demonstrate that discrete-time LIF-SNNs realize piecewise constant functions defined on polyhedral regions, and more importantly, we quantify the network size required to approximate continuous functions. Moreover, we investigate the impact of latency (number of time steps) and depth (number of layers) on the complexity of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis highlights the importance of latency and contrasts these networks with ANNs that use piecewise linear activation functions. Finally, we present numerical experiments to support our theoretical findings.","Spiking neural networks (SNNs) are gaining increasing interest as a potentially more energy-efficient alternative to traditional artificial neural networks. However, our theoretical understanding of what SNNs can actually do—such as the types of problems they can solve and the functions they can represent—remains limited. In our work, we take a first step toward addressing these questions by applying tools and concepts commonly used in deep learning research. This foundational insight helps pave the way for better understanding and potentially more effective designs of SNNs in the future."
