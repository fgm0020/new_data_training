type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)",https://ICML.cc//virtual/2025/poster/40104,"Yoonsoo Nam, Seok Hyeong Lee, Clémentine Dominé, Yeachan Park, Charles London, Wonyl Choi, Niclas Göring, Seungjai Lee","In physics, complex systems are often simplified into minimal, solvable models that retain only the core principles. In machine learning, layerwise linear models (e.g., linear neural networks) act as simplified representations of neural network dynamics. These models follow the dynamical feedback principle, which describes how layers mutually govern and amplify each other's evolution. This principle extends beyond the simplified models, successfully explaining a wide range of dynamical phenomena in deep neural networks, including neural collapse, emergence, lazy and rich regimes, and grokking. In this position paper, we call for the use of layerwise linear models retaining the core principles of neural dynamical phenomena to accelerate the science of deep learning.","Neural collapse, emergence, scaling laws, the lazy and rich regimes, and grokking are widely studied phenomena in deep neural networks. While these behaviors are often attributed to complex interactions between architecture, data, and non-linear activations, we propose a unifying explanation based on gradient dynamics in layerwise models. Notably, these models lack non-linear activations, highlighting that the layerwise structure alone is a powerful yet underappreciated characteristic of deep neural networks. In this position paper, we argue that focusing on analytically tractable, layerwise models can not only explain existing phenomena but also uncover new insights, accelerating the scientific understanding of deep learning."
Poster,Position: Spectral GNNs Rely Less on Graph Fourier Basis than Conceived,https://ICML.cc//virtual/2025/poster/40152,"Yuhe Guo, Huayi Tang, Jiahong Ma, Hongteng Xu, Zhewei Wei","Spectral graph learning builds upon two foundations: Graph Fourier basis as its theoretical cornerstone,with polynomial approximation to enable practical implementation. While this framework has led to numerous successful designs, we argue that its effectiveness might stem from mechanisms different from its theoretical foundations. In this paper, we identify two fundamental issues that challenge our current understanding: (1) The graph Fourier basis $\mathbf{U}$ (eigenvectors of the normalized graph Laplacian) faces too many questions to truly serve its intended role, particularly in preserving its semantic properties of Fourier analysis; (2) The limitations preventing expressive filters are not merely practical constraints, but fundamental barriers that naturally protect stability and generalization. Importantly, the two issues entangle with each other. The second obscured the first: the natural avoidance of complex filters has prevented us from fully confronting the questions about $\mathbf{U}$'s role as a Fourier basis. This observation leads to our position: the effectiveness of spectral GNNs relies less on Graph Fourier basis than originally conceived, or,  in other words, **spectral GNNs might not be so spectral**.   The position leads us to at least two potential research interests: to incorporate a more semantically meaningful graph dictionary except for $\mathbf{U}$, and to re-examine the theoretical role of the introduced polynomial techniques.","Have you ever heard of the Fourier basis? Its striking ability to represent global oscillations at different frequencies is nothing short of impressive. On graphs, researchers have tried to harness a similar idea by defining a “graph Fourier basis”—essentially transplanting the classical Fourier concept onto network structures. They treat it as a powerful tool for analyzing signals on graphs.**There are plenty of reasons to be fascinated by the graph Fourier basis.** In graph neural networks, practitioners first encode the graph as a Laplacian matrix and then seek ways to exploit its eigenvectors—i.e., the graph Fourier basis. What’s clever is that they often combine this approach with polynomial approximation techniques, which bypass the costly full spectral decomposition yet still allow the graph Fourier basis to be used in practice.**But is the graph Fourier basis really as useful as everyone assumes?** When we visualized these basis vectors on a 3D mesh of a horse, we observed that many of them clearly no longer exhibited the hallmark global oscillations (We also did other analysis). This led us to ask: **where does the belief come from that “the graph Fourier basis is semantically meaningful, just like the classical Fourier basis”?** We uncovered several factors: an unquestioning trust in mathematical analogies (even when those analogies jump too far), the influence of high-profile research directions, and a tendency to overgeneralize from familiar concepts.At the same time, we realized that polynomial approximation—used as the vehicle for employing the graph Fourier basis—naturally prevents the ""graph Fourier atoms"" from being revealed. In order to ensure basic stability and generalization, we rarely use polynomials that are complex enough.  Therefore, our position paper takes a step back to reflect on how these technical developments have unfolded. It argues that we need to rethink our reliance on the graph Fourier basis and examine what we have achieved and why are the networks working."
Poster,Position: Stop treating `AGI' as the north-star goal of AI research,https://ICML.cc//virtual/2025/poster/40183,"Borhane Blili-Hamelin, Christopher Graziul, Leif Hancox-Li, Hananel Hazan, El-Mahdi El-Mhamdi, Avijit Ghosh, Katherine Heller, Jacob Metcalf, Fabricio Murai, Eryk Salvaggio, Andrew Smart, Todd Snider, Mariame Tighanimine, Talia Ringer, Margaret Mitchell, Shiri Dori-Hacohen","The AI research community plays a vital role in shaping the scientific, engineering, and societal goals of AI research. In this position paper, we argue that focusing on the highly contested topic of 'artificial general intelligence' ('AGI') undermines our ability to choose effective goals. We identify six key traps---obstacles to productive goal setting---that are aggravated by AGI discourse: Illusion of Consensus, Supercharging Bad Science, Presuming Value-Neutrality, Goal Lottery, Generality Debt, and Normalized Exclusion. To avoid these traps, we argue that the AI research community needs to (1) prioritize specificity in scientific, engineering, and societal goals, (2) center pluralism about multiple worthwhile approaches to multiple valuable goals, and (3) foster innovation through greater inclusion of disciplines and communities. Therefore, the AI research community needs to stop treating ``AGI'' as the north-star goal of AI research.","Debates about artificial general intelligence (AGI) miss the forest for the trees. What AGI means is highly contested. That makes debates slippery. What are we debating? What’s at stake? Are we having the right debate? We argue the debate researchers should keep having is about the scientific, engineering, and societal goals of the field. We examine six key traps that hinder setting worthwhile goals: Illusion of Consensus, Supercharging Bad Science, Presuming Value-Neutrality, Goal Lottery, Generality Debt, and Normalized Exclusion. We then elaborate how AGI narratives aggravate each. We offer three recommendations for addressing these traps. Specificity: Prioritize specific language for scientific, engineering, and societal goals. Pluralism: Articulate many worthwhile scientific, engineering, and societal goals and many possible paths to fulfilling them. Inclusion: Greater inclusion of communities and disciplines in shaping the goals of AI research is beneficial to innovation.Drawing on high-quality work by AGI proponents, we examine an alternative view we find especially strong: Can’t the obstacles be overcome through better definitions of AGI? We provide three reasons to favor our position and reject this alternative. First, if specificity and pluralism are key solutions, we should avoid unifying north-star goals altogether. Second, the research community has a responsibility to help distinguish hype from reality. No matter how well or poorly defined, AGI has acquired a cultural significance that undermines this responsibility. Third, if the AI community still wants to work on consensus on a high-level goal, that goal should be benefiting and supporting people. Debating AGI keeps communities focused on trees instead of this worthwhile forest."
Poster,Position: Strong Consumer Protection is an Inalienable Defense for AI Safety in the United States,https://ICML.cc//virtual/2025/poster/40112,Serena Booth,"Consumer protection laws are designed to protect consumers from unethical business practices. In this position paper, I argue that these laws serve an emergent dual purpose: if appropriately enforced and strengthened, consumer protection laws can serve as an inalienable defense for AI safety. These laws are well established and can be enforced and strengthened to incentivize businesses to design and deploy safer AI systems. This position runs counter to two prevailing trends in AI policy. The first alternative position is that AI safety requires an entirely new set of focused laws to protect humanity's prosperity. Though I find these efforts valuable, I argue that such focused laws are both hard to write and easy to skirt. The second alternative position is that consumer protection is nothing more than red tape; I argue that existing laws dating back many decades have already reigned in some nefarious business practices related to the development and deployment of AI, and that the litigious society of the United States is well-positioned to use consumer protection laws to encourage new AI safety guardrails. This paper takes a tour of some existing consumer protection laws in the United States and their effects on the development and use of AI systems. This paper also calls to enforce and preserve these laws in a rapidly changing, de-regulatory political landscape.","Consumer protection laws are designed to protect consumers from unethical business practices. In this position paper, I argue that these laws serve an emergent dual purpose: if appropriately enforced and strengthened, consumer protection laws can serve as an inalienable defense for AI safety. These laws are well established and can be enforced and strengthened to incentivize businesses to design and deploy safer AI systems. This position runs counter to two prevailing trends in AI policy. The first alternative position is that AI safety requires an entirely new set of focused laws to protect humanity's prosperity. Though I find these efforts valuable, I argue that such focused laws are both hard to write and easy to skirt. The second alternative position is that consumer protection is nothing more than red tape; I argue that existing laws dating back many decades have already reigned in some nefarious business practices related to the development and deployment of AI, and that the litigious society of the United States is well-positioned to use consumer protection laws to encourage new AI safety guardrails. This paper takes a tour of some existing consumer protection laws in the United States and their effects on the development and use of AI systems. This paper also calls to enforce and preserve these laws in a rapidly changing, de-regulatory political landscape."
Poster,Position: Supervised Classifiers Answer the Wrong Questions for OOD Detection,https://ICML.cc//virtual/2025/poster/40136,"Yucen Li, Daohan Lu, Polina Kirichenko, Shikai Qiu, Tim G. J. Rudner, C. Bayan Bruss, Andrew Wilson","To detect distribution shifts and improve model safety, many out-of-distribution (OOD) detection methods rely on the predictive uncertainty or features of supervised models trained on in-distribution data. In this position paper, we critically re-examine this popular family of OOD detection procedures, and we argue that these methods are fundamentally answering the wrong questions for OOD detection. There is no simple fix to this misalignment, since a classifier trained only on in-distribution classes cannot be expected to identify OOD points; for instance, a cat-dog classifier may confidently misclassify an airplane if it contains features that distinguish cats from dogs, despite generally appearing nothing alike. We find that uncertainty-based methods incorrectly conflate high uncertainty with being OOD, while feature-based methods incorrectly conflate far feature-space distance with being OOD. We show how these pathologies manifest as irreducible errors in OOD detection and identify common settings where these methods are ineffective. Additionally, interventions to improve OOD detection such as feature-logit hybrid methods, scaling of model and data size, epistemic uncertainty representation, and outlier exposure also fail to address this fundamental misalignment in objectives.","To improve model safety, there have been many methods which aim to detect whether a new input was drawn from a different distribution compared to the inputs that the model has seen during training. These methods often rely on the features or the uncertainties of a model. In this paper, we argue that the methods which only rely on the original model's features and uncertainties are not able to accurately detect whether the input is from a new distribution. We show that current methods wrongly assume that models have high confidence on inputs drawn from the same distribution and have low confidence on inputs drawn from different distributions; however, a model trained to distinguish cats from dogs may confidently mislabel an airplane as a dog if the airplane and dog have a few shared traits, despite generally appearing nothing alike. Current methods also assume that only the features of the inputs drawn from different distributions will be far from the features of the inputs that the model has seen during training, which is also incorrect. In this paper, we identify common settings where these methods are ineffective, and we also show how many popular methods also fail to address these pathologies."
Poster,Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards,https://ICML.cc//virtual/2025/poster/40108,"Jaeho Kim, Yunseok Lee, Seulki Lee","The peer review process in major artificial intelligence (AI) conferences faces unprecedented challenges with the surge of paper submissions (exceeding 10,000 submissions per venue), accompanied by growing concerns over review quality and reviewer responsibility. This position paper argues for **the need to transform the traditional one-way review system into a bi-directional feedback loop where authors evaluate review quality and reviewers earn formal accreditation, creating an accountability framework that promotes a sustainable, high-quality peer review system.** The current review system can be viewed as an interaction between three parties: the authors, reviewers, and system (i.e., conference), where we posit that all three parties share responsibility for the current problems. However, issues with authors can only be addressed through policy enforcement and detection tools, and ethical concerns can only be corrected through self-reflection. As such, this paper focuses on reforming reviewer accountability with systematic rewards through two key mechanisms: (1) a two-stage bi-directional review system that allows authors to evaluate reviews while minimizing retaliatory behavior, (2) a systematic reviewer reward system that incentivizes quality reviewing. We ask for the community's strong interest in these problems and the reforms that are needed to enhance the peer review process.","Currently, most AI-related conferences (e.g., ICML, NeurIPS, ICLR, KDD) utilize the OpenReview system for peer reviewing, where 4-6 fellow researchers (i.e., reviewers) are assigned to a paper as judges. These judges provide reviews and scoring for the papers. Authors have the chance to correct misconceptions through rebuttals during the discussion phase. A decision (acceptance or rejection) on the paper is made based on the reviews and scores by the meta-reviewer (head of reviewers).However, over the past few years, complaints about peer review quality have increased (e.g., most likely due to the rising volume of paper submissions and LLM advancement). There are concerning signs that some reviewers may be using LLMs to generate reviews, resulting in superficial or irresponsible feedback that frustrates authors. Unfortunately, authors do not have effective defense mechanisms against these irresponsible reviews in the current review process. This trend poses a serious threat to AI conferences, as it undermines the reputation and credibility of papers accepted at these prestigious venues.In this position paper, we argue that we should **(1) implement an author feedback system, where authors can rate reviewers based on review quality and paper comprehension, and (2) provide short-term and long-term motivations to reviewers using digital badges and reviewer impact scores.** We also propose several detailed implementation strategies to protect both authors and reviewers.We hope that our position paper will generate strong interest within the community regarding these problems and the reforms needed to enhance the peer review process."
Poster,Position: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process,https://ICML.cc//virtual/2025/poster/40116,Jing Yang,"The rapid growth of submissions to top-tier Artificial Intelligence (AI) and Machine Learning (ML) conferences has prompted many venues to transition from closed to open review platforms. Some have fully embraced open peer reviews, allowing public visibility throughout the process, while others adopt hybrid approaches, such as releasing reviews only after final decisions or keeping reviews private despite using open peer review systems. In this work, we analyze the strengths and limitations of these models, highlighting the growing community interest in transparent peer review. To support this discussion, we examine insights from Paper Copilot ([papercopilot.com](https://papercopilot.com/)), a website launched two years ago to aggregate and analyze AI / ML conference data while engaging a global audience. The site has attracted over 200,000 early-career researchers, particularly those aged 18–34 from 177 countries, many of whom are actively engaged in the peer review process. \textit{Drawing on our findings, this position paper advocates for a more transparent, open, and well-regulated peer review aiming to foster greater community involvement and propel advancements in the field.","The way research papers are reviewed at top AI and machine learning conferences has a big impact on what ideas are shared and recognized. But the current review system is facing serious problems: there are too many papers, reviews are often hidden from public view, and decisions can feel inconsistent or unfair—especially to early-career researchers.To better understand and improve this process, we created Paper Copilot, a website that tracks and visualizes how papers are reviewed across conferences. Since its launch, over 200,000 people from 177 countries have used it—mostly young researchers. We found that conferences with more open and transparent review systems attract much more community interest and trust. These systems also lead to more thoughtful discussions between reviewers and authors.Our research shows that making the review process more open—while still protecting privacy—could lead to fairer, more rigorous evaluations. We argue that the AI research community should adopt more consistent and transparent peer review practices, not just to improve fairness, but to better serve the global community pushing this field forward."
Poster,Position: The Categorization of Race in ML is a Flawed Premise,https://ICML.cc//virtual/2025/poster/40122,"Miriam Doh, Benedikt Höltgen, Piera Riccio, Nuria Oliver","This position paper critiques the reliance on rigid racial taxonomies in machine learning, exposing their U.S.-centric nature and lack of global applicability—particularly in Europe, where race categories are not commonly used. These classifications oversimplify racial identity, erasing the experiences of mixed-race individuals and reinforcing outdated essentialist views that contradict the social construction of race. We suggest research agendas in machine learning that move beyond categorical variables to better address discrimination and social inequality.","Many machine learning tools and bias-detection methods still rely on fixed race groups—White, Black, Asian—borrowed from U.S. census labels. This position paper argues that such broad categories flatten complex identities, erase mixed-race experiences, and treat race as if it were a biological fact, which risks embedding stereotypes into everything from hiring software to image processing.Instead of using those rigid labels, the paper recommends dropping categorical race variables and focusing on the real traits that drive discrimination in each context—skin tone, facial features, spoken language, nationality, and other locally relevant characteristics. Because the attributes that matter vary by setting, it calls for a participatory process: working directly with affected communities and domain experts to choose the right mix of traits for each application.By shifting away from simplistic race labels toward flexible, multi-dimensional assessments, discrimination can be more accurately detected and mitigated. This move promises models that are both more equitable and more attuned to the rich diversity of human identities."
Poster,Position: The Future of Bayesian Prediction Is Prior-Fitted,https://ICML.cc//virtual/2025/poster/40175,"Samuel Gabriel Müller, Arik Reuter, Noah Hollmann, David Rügamer, Frank Hutter","Training neural networks on randomly generated artificial datasets yields Bayesian models that capture the prior defined by the dataset-generating distribution.Prior-data Fitted Networks (PFNs) are a class of methods designed to leverage this insight.In an era of rapidly increasing computational resources for pre-training and a near stagnation in the generation of new real-world data in many applications, PFNs are poised to play a more important role across a wide range of applications.They enable the efficient allocation of pre-training compute to low-data scenarios.Originally applied to small Bayesian modeling tasks, the field of PFNs has significantly expanded to address more complex domains and larger datasets. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference, leveraging amortized learning to tackle data-scarce problems. We thus believe they are a fruitful area of research. In this position paper, we explore their potential and directions to address their current limitations.","The way increasing computational resources available for neural network training, e.g., in data centers, are commonly used is by training neural networks for longer.This does require substantial amounts of data at some point, though, as training on the same data too many times does not tend to yield improvements for neural networks.We advocate for a particular method (PFNs) to use the increasing compute resources available to improve performance of neural networks in domains with little data available.The way this is done is that training is not actually performed on the real-world data available but instead on synthetic data that is randomly generated.The neural network is then only conditioned on the real-world data, similar to how ChatGPT is conditioned on your question, and yields predictions on the fly."
Poster,Position: The Most Expensive Part of an LLM *should* be its Training Data,https://ICML.cc//virtual/2025/poster/40151,"Nikhil Kandpal, Colin Raffel","Training a state-of-the-art Large Language Model (LLM) is an increasingly expensive endeavor due to growing computational, hardware, energy, and engineering demands. Yet, an often-overlooked (and seldom paid) expense is the human labor behind these models' training data. Every LLM is built on an unfathomable amount of human effort: trillions of carefully written words sourced from books, academic papers, codebases, social media, and more.  This position paper aims to assign a monetary value to this labor and argues that the most expensive part of producing an LLM \emph{should} be the compensation provided to training data producers for their work. To support this position, we study 64 LLMs released between 2016 and 2024, estimating what it would cost to pay people to produce their training datasets from scratch. Even under highly conservative estimates of wage rates, the costs of these models' training datasets are $10$-$1000$ times larger than the costs to train the models themselves, representing a significant financial liability for LLM providers. In the face of the massive gap between the value of training data and the lack of compensation for its creation, we highlight and discuss research directions that could enable fairer practices in the future.","Training a modern Large Language Model (LLM) is an incredibly expensive endeavor due to the cost of specialized hardware, energy required to run that hardware, and the enormous engineering labor needed to architect large-scale training systems. However, an often overlooked (and seldom paid) expense is the human labor behind these models' training data. Every LLM is built on an unfathomable amount of human effort: trillions of carefully written words sourced from books, academic papers, codebases, social media, and more.  This position paper aims to assign a monetary value to this labor and argues that the most expensive part of producing an LLM \emph{should} be the compensation provided to training data producers for their work. To support this position, we study 64 LLMs released between 2016 and 2024, estimating what it would cost to pay people to produce their training datasets from scratch. Even under highly conservative estimates of wage rates, the costs of these models' training datasets are $10$-$1000$ times larger than the costs to train the models themselves, representing a significant financial liability for LLM providers. In the face of the massive gap between the value of training data and the lack of compensation for its creation, we highlight and discuss research directions that could enable fairer practices in the future."
