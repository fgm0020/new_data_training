type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Putnam-AXIOM: A Functional & Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs,https://ICML.cc//virtual/2025/poster/44232,"Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno de Moraes Dumont, Sanmi Koyejo","Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving $>$ 90% accuracy, and are increasingly compromised by training-set contamination.We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables, and constants.The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed.On the Original set, OpenAI's o1-preview – the strongest evaluated model – scores 41.9%, but its accuracy drops by 19.6 % (46.8% relative decrease) on the paired Variations.The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals.These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement (""boxed"") accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations.Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs.Data and evaluation code are publicly available athttps://github.com/brando90/putnam-axiom.","Modern AI chatbots can already pass many school-level math tests, so researchers need harder ways to see whether these systems are truly “thinking” or just repeating what they have seen online.  We built a new test set called **Putnam-AXIOM** by collecting 522 tough questions from the famous Putnam university math contest.  To make sure the bots have not memorized the answers from the internet, we also wrote a computer program that automatically rewrites each problem—changing numbers, variable names, and wording—while keeping the difficulty the same.  This lets us generate endless fresh versions that the bots have never encountered.When we ran twenty leading AI models on our test, every one of them scored lower on the rewritten versions than on the originals; the best model’s score fell from 42 % to 22 %.  This gap suggests that today’s systems still rely heavily on memorization.  Finally, we introduce a simple scoring method that checks how well a model follows each step of a correct solution, not just the final answer.  All data, code, and evaluation tools are publicly released so others can track future progress in genuine mathematical reasoning."
Poster,Puzzle: Distillation-Based NAS for Inference-Optimized LLMs,https://ICML.cc//virtual/2025/poster/45275,"Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammed Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Argov, Ran Zilberstein, Ran El-Yaniv","Large language models (LLMs) offer remarkable capabilities, yet their high inference costs restrict wider adoption.While increasing parameter counts improves accuracy, it also broadens the gap between state-of-the-art capabilities and practical deployability. We present **Puzzle**, a hardware-aware framework that accelerates the inference of LLMs while preserving their capabilities.Using neural architecture search (NAS) at a large-scale, Puzzle optimizes models with tens of billions of parameters.Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.We showcase our framework’s impact via Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B) and Llama-3.3-Nemotron-49B, two publicly available models derived from Llama-70B-Instruct. Both models achieve a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while retaining 98.4% of the original model's benchmark accuracies. These are the most accurate models supporting single H100 GPU inference with large batch sizes, despite training on 45B tokens at most, far fewer than the 15T used to train Llama-70B.Lastly, we show that lightweight alignment on these derived models allows them to surpass the parent model in specific capabilities.Our work establishes that powerful LLM models can be optimized for efficient deployment with only negligible loss in quality, underscoring that inference performance, not parameter count alone, should guide model selection.","Large language models (LLMs) are incredibly powerful but very expensive to run. Their massive size makes them difficult to deploy in real-world settings, especially when hardware resources are limited—like using just a single GPU or operating under strict memory constraints. Training a new model from scratch for each hardware setup is far too costly to be practical.We present **Puzzle**, a method that adapts existing LLMs to specific hardware and usage scenarios, making them much faster to run while preserving nearly all of their accuracy. Puzzle works by breaking the original model into smaller building blocks and exploring leaner alternatives for each one—like swapping puzzle pieces. A mathematical solver then assembles the best combination of blocks to meet performance and hardware goals, and we briefly finetune the model to ensure all the parts work smoothly together.With Puzzle, we built models that run more than twice as fast on an NVIDIA H100 GPU while maintaining over 98% of the original accuracy—and in some cases, even improving performance. Puzzle enables affordable, efficient deployment of large AI models, tailored to real-world needs without requiring massive compute budgets."
Poster,"PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models",https://ICML.cc//virtual/2025/poster/45780,"Alex Velez-Arce, Marinka Zitnik","Existing biomedical benchmarks do not provide end-to-end infrastructure for training, evaluation, and inference of models that integrate multimodal biological data and a broad range of machine learning tasks in therapeutics. We present PyTDC, an open-source machine-learning platform providing streamlined training, evaluation, and inference software for multimodal biological AI models. PyTDC unifies distributed, heterogeneous, continuously updated data sources and model weights and standardizes benchmarking and inference endpoints. This paper discusses the components of PyTDC's architecture and, to our knowledge, the first-of-its-kind case study on the introduced single-cell drug-target nomination ML task. We find state-of-the-art methods in graph representation learning and domain-specific methods from graph theory perform poorly on this task. Though we find a context-aware geometric deep learning method that outperforms the evaluated SoTA and domain-specific baseline methods, the model is unable to generalize to unseen cell types or incorporate additional modalities, highlighting PyTDC's capacity to facilitate an exciting avenue of research developing multimodal, context-aware, foundation models for open problems in biomedical AI.","Biomedical research for new treatments involves analyzing complex data from sources like genes, proteins, and clinical trials. However, combining this diverse data and using it with machine learning is difficult because existing tools don’t fully support the process or keep up with new information. Our team created PyTDC, a platform that brings together these varied data sources and offers simple tools for training, testing, and applying machine learning models. PyTDC keeps data current with an innovative design and provides a model server to access and compare cutting-edge models easily. This helps researchers and developers study diseases and treatments more effectively. By simplifying access to data and models, PyTDC could speed up the discovery of new therapies and improve our understanding of diseases, making a real difference in healthcare."
Poster,QEM-Bench: Benchmarking Learning-based Quantum Error Mitigation and QEMFormer as a Multi-ranged Context Learning Baseline,https://ICML.cc//virtual/2025/poster/45382,"Tianyi Bao, Ruizhe Zhong, Xinyu Ye, Yehui Tang, Junchi Yan","Quantum Error Mitigation (QEM) has emerged as a pivotal technique for enhancing the reliability of noisy quantum devices in the *Noisy Intermediate-Scale Quantum* (NISQ) era. Recently, machine learning (ML)-based QEM approaches have demonstrated strong generalization capabilities without sampling overheads compared to conventional methods. However, evaluating these techniques is often hindered by a lack of standardized datasets and inconsistent experimental settings across different studies. In this work, we present **QEM-Bench**, a comprehensive benchmark suite of *twenty-two* datasets covering diverse circuit types and noise profiles, which provides a unified platform for comparing and advancing ML-based QEM methods. We further propose a refined ML-based QEM pipeline **QEMFormer**, which leverages a feature encoder that preserves local, global, and topological information, along with a two-branch model that captures short-range and long-range dependencies within the circuit. Empirical evaluations on QEM-Bench illustrate the superior performance of QEMFormer over existing baselines, underscoring the potential of integrated ML-QEM strategies.","Quantum error mitigation (QEM) is crucial for enhancing the reliability of noisy NISQ devices; however, current ML-based QEM methods lack consistent benchmarks and employ heterogeneous evaluation protocols. To address this gap, we propose QEM-Bench, a comprehensive suite of twenty-two datasets that capture diverse circuit structures and noise regimes. We also introduce QEMFormer, an ML-driven framework that employs a multi-scale feature encoder to integrate local, global, and topological information, alongside a dual-branch architecture for modeling both short- and long-range circuit dependencies. Our empirical studies on QEM-Bench demonstrate that QEMFormer significantly outperforms existing baselines, establishing a unified standard for ML-based QEM research and paving the way for more accurate and scalable quantum computations."
Poster,QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search,https://ICML.cc//virtual/2025/poster/44545,"Zongyu Lin, Yao Tang, Xingcheng Yao, Da Yin, ziniu hu, Yizhou Sun, Kai-Wei Chang","Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis.","Imagine teaching a language agent not just by praising its final answer but by cheering it on at every step of a complex task. That’s what QLASS does: it automatically assigns helpful “hint scores” to each individual action, using a reasoning tree to estimate the long-term value of each choice. By offering real-time guidance on every move, QLASS helps the assistant spot promising directions, course-correct before issues escalate, enabling more focused effort. This leads to faster learning, smarter decision-making, and more reliable performance on challenging, multi-stage tasks. Even with half as many annotated examples, agents trained with QLASS maintain strong results, showing how efficiently it works with limited feedback. They also found that stepwise guidance reduces wasted steps and improves overall task success rates. QLASS gives language-powered systems a coach that highlights good moves, warns of dead ends, and keeps them on track—making AI assistants more effective, adaptable, and consistent."
Poster,QMamba: On First Exploration of Vision Mamba for Image Quality Assessment,https://ICML.cc//virtual/2025/poster/44591,"Fengbin Guan, Xin Li, Zihao Yu, Yiting Lu, Zhibo Chen","In this work, we take the first exploration of the recently popular foundation model, *i.e.,* State Space Model/Mamba, in image quality assessment (IQA), aiming at observing and excavating the perception potential in vision Mamba.  A series of works on Mamba has shown its significant potential in various fields, *e.g.,* segmentation and classification. However, the perception capability of Mamba remains under-explored. Consequently, we propose QMamba by revisiting and adapting the Mamba model for three crucial IQA tasks, *i.e.,* task-specific, universal, and transferable IQA, which reveals its clear advantages over existing foundational models, *e.g.,* Swin Transformer, ViT, and CNNs, in terms of perception and computational cost.  To improve the transferability of QMamba, we propose the StylePrompt tuning paradigm, where lightweight mean and variance prompts are injected to assist task-adaptive transfer learning of pre-trained QMamba for different downstream IQA tasks. Compared with existing prompt tuning strategies, our StylePrompt enables better perceptual transfer with lower computational cost.  Extensive experiments on multiple synthetic, authentic IQA datasets, and cross IQA datasets demonstrate the effectiveness of our proposed QMamba.","Assessing how good an image looks is surprisingly difficult for computers. Traditional models often miss small visual flaws or use too much computing power to detect them. Our research investigates a new type of AI model called a State Space Model (specifically, “Mamba”) to improve how machines measure image quality. We introduce a model called QMamba that can not only spot subtle image problems more accurately but also run much more efficiently than existing systems. To make this model work well across different types of images, we developed a lightweight tuning method called StylePrompt. This approach adapts the model to new tasks by adjusting only a few key features, avoiding the need to retrain the entire system. Our experiments show that QMamba performs strongly on a wide range of image quality tasks, including challenging cases with synthetic, real-world, and AI-generated distortions. This work could help improve visual quality in areas like photo enhancement, image compression, and AI-generated media, where accurate quality assessment is essential."
Poster,QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration,https://ICML.cc//virtual/2025/poster/44489,"HamidReza Imani, Jiaxin Peng, Peiman Mohseni, Abdolah Amirany, Tarek El-Ghazawi","The deployment of mixture-of-experts (MoE) large language models (LLMs) presents significant challenges due to their high memory demands. These challenges become even more pronounced in multi-tenant environments, where shared resources must accommodate multiple models, limiting the effectiveness of conventional virtualization techniques. This paper addresses the problem of efficiently serving multiple fine-tuned MoE-LLMs on a single GPU. We propose a serving system that employs \textit{similarity-based expert consolidation} to reduce the overall memory footprint by sharing similar experts across models. To ensure output quality, we introduce \textit{runtime partial reconfiguration}, dynamically replacing non-expert layers when processing requests from different models. As a result, our approach achieves competitive output quality while maintaining throughput comparable to serving a single model, and incurs only a negligible increase in time-to-first-token (TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using Mixtral-8x7B models demonstrate an 85\% average reduction in turnaround time compared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on Google's Switch Transformer Base-8 model with up to four variants demonstrate the scalability and resilience of our approach in maintaining output quality compared to other model merging baselines, highlighting its effectiveness.","Large language models like ChatGPT are powerful, but they require a lot of memory to run—especially when using a technique called “mixture of experts” (MoE), where different parts of the model are used depending on the task. This becomes a major challenge when multiple such models, each fine-tuned for a specific purpose, need to run on the same shared hardware, such as a single GPU.Our research introduces a more efficient way to serve several of these models together by reducing the overall memory requirement. We achieve this by identifying and sharing similar expert components between models to save memory, and dynamically swapping other parts in and out as needed to maintain response accuracy.This makes it possible to serve multiple MoE language models simultaneously, with performance close to that of running a single model and only a slight reduction in accuracy. Our system performs especially well in constrained environments, significantly reducing completion times compared to existing resource-sharing solutions. This can help make advanced AI tools more scalable and accessible—even on limited hardware."
Poster,QPRL : Learning Optimal Policies with Quasi-Potential Functions for Asymmetric Traversal,https://ICML.cc//virtual/2025/poster/44583,"Jumman Hossain, Nirmalya Roy","Reinforcement learning (RL) in real-world tasks such as robotic navigation often encounters environments with asymmetric traversal costs, where actions like climbing uphill versus moving downhill incur distinctly different penalties, or transitions may become irreversible. While recent quasimetric RL methods relax symmetry assumptions, they typically do not explicitly account for path-dependent costs or provide rigorous safety guarantees. We introduce Quasi-Potential Reinforcement Learning (QPRL), a novel framework that explicitly decomposes asymmetric traversal costs into a path-independent potential function ($\Phi$) and a path-dependent residual ($\Psi$). This decomposition allows efficient learning and stable policy optimization via a Lyapunov-based safety mechanism. Theoretically, we prove that QPRL achieves convergence with improved sample complexity of $\tilde{O}(\sqrt{T})$, surpassing prior quasimetric RL bounds of $\tilde{O}(T)$. Empirically, our experiments demonstrate that QPRL attains state-of-the-art performance across various navigation and control tasks, significantly reducing irreversible constraint violations by approximately $4\times$ compared to baselines.","Many real-world tasks—like a delivery robot climbing steep streets or a rescue drone squeezing through one-way passages—cost much more effort in one direction than the other, and some actions cannot be undone at all. Today’s learning algorithms usually ignore this imbalance, so they may choose routes that waste energy or put the robot in a trap. Our research introduces Quasi-Potential Reinforcement Learning (QPRL), a method that lets an agent recognize and plan around these one-way or “uneven-effort” situations. QPRL splits the cost of every move into two parts: a reusable “potential” map (showing, for example, how steep a hill is) and an extra penalty for truly irreversible steps (such as dropping off a ledge). A built-in safety check ensures the agent never takes a step whose future cost could exceed a small, user-set limit. Across several simulated navigation and control tasks, QPRL learns faster, reaches goals more reliably, and commits about four times fewer irreversible errors than existing approaches. These ideas could help future robots and self-driving vehicles to act more safely and efficiently whenever undoing a decision is hard or impossible."
Poster,Q-Supervised Contrastive Representation: A State Decoupling Framework for Safe Offline Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44391,"Zhihe Yang, Yunjian Xu, Yang Zhang","Safe offline reinforcement learning (RL), which aims to learn the safety-guaranteed policy without risky online interaction with environments, has attracted growing recent attention for safety-critical scenarios. However, existing approaches encounter out-of-distribution problems during the testing phase, which can result in potentially unsafe outcomes. This issue arises due to the infinite possible combinations of reward-related and cost-related states. In this work, we propose State Decoupling with Q-supervised Contrastive representation (SDQC), a novel framework that decouples the global observations into reward- and cost-related representations for decision-making, thereby improving the generalization capability for unfamiliar global observations. Compared with the classical representation learning methods, which typically require model-based estimation (e.g., bisimulation), we theoretically prove that our Q-supervised method generates a coarser representation while preserving the optimal policy, resulting in improved generalization performance. Experiments on DSRL benchmark problems provide compelling evidence that SDQC surpasses other baseline algorithms, especially for its exceptional ability to achieve almost zero violations in more than half of the tasks, while the state-of-the-art algorithm can only achieve the same level of success in a quarter of the tasks. Further, we demonstrate that SDQC possesses superior generalization ability when confronted with unseen environments.","Safe offline reinforcement learning (RL) focuses on teaching systems to make safe decisions without risky trial-and-error in real-world environments. However, existing methods often struggle when faced with unfamiliar situations during testing, which can lead to unsafe outcomes. This happens because of the infinite possible combinations of reward-related and cost-related states. To address this, we propose a noval framework called State Decoupling with Q-supervised Contrastive representation (SDQC). Our approach decouples the global observations into reward- and cost-related representations, making it easier for the system to handle new situations. Experiments show that SDQC outperforms other methods, achieving safety in more tasks and handling unseen environments better than the existing algorithms."
Poster,QT-DoG: Quantization-Aware Training for Domain Generalization,https://ICML.cc//virtual/2025/poster/45444,"Saqib Javed, Hieu Le, Mathieu Salzmann","A key challenge in Domain Generalization (DG) is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both an analytical perspective and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Code is released at: https://saqibjaved1.github.io/QT_DoG/.","Deep learning models often struggle when tested in conditions that differ from those on which they were trained. For example, a model trained to recognize objects in daytime images may not perform well at night. It is a key obstacle to making AI systems reliable in the real world, where conditions frequently change. Domain Generalization (DG) addresses this problem and aims to learn models that perform well not only on the training (source) domains but also in new, unseen (target) data distributions.In our research, we introduce Quantization-Aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization helps deep learning models become more reliable in unfamiliar settings. We demonstrate that quantization-aware training, a technique traditionally employed to reduce model size and improve inference speed, can also serve as a form of regularization. By introducing small structured noise during training, we guide the model to avoid overly sensitive solutions and instead learn more stable patterns that enhance generalization.We also combine several quantized models into an Ensemble of Quantization (EoQ) and show that EoQ outperforms many larger models and more complex training algorithms, without requiring significant additional computing power."
