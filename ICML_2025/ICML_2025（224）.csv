type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Policy Gradient with Tree Expansion,https://ICML.cc//virtual/2025/poster/43515,"Gal Dalal, Assaf Hallak, Gugan Chandrashekhar Mallika Thoppe, Shie Mannor, Gal Chechik","Policy gradient methods are notorious for having a large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax---a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance. We prove that the variance depends on the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the stronger the variance decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction. Ours is the first result to bound the gradient bias for an approximate model. In a practical implementation of SoftTreeMax we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO.","Imagine you're learning to play a video game where you need to make a sequence of moves to maximize your score. Most AI agents that learn through trial and error suffer from a major problem: their learning process is very noisy and unstable, like trying to navigate in a storm. This makes them slow to improve and requires enormous amounts of practice to get good results.Our solution, called SoftTreeMax, works like a chess player who thinks several moves ahead before deciding what to do. Instead of just considering immediate consequences, our AI looks at all possible sequences of moves over the next few steps and chooses actions based on this forward-looking analysis. We proved mathematically that this approach reduces learning noise exponentially—the further ahead the agent looks, the more stable its learning becomes. Testing on classic Atari games, our method achieved up to 5 times higher scores than standard approaches while reducing learning noise by a factor of 1,000, making AI training much more efficient and reliable across many applications."
Poster,Policy Guided Tree Search for Enhanced LLM Reasoning,https://ICML.cc//virtual/2025/poster/45503,Yang Li,"Despite their remarkable capabilities, large language models often struggle with tasks requiring complex reasoning and planning. While existing approaches like Chain-of-Thought prompting and tree search techniques show promise, they are limited by their reliance on predefined heuristics and computationally expensive exploration strategies. We propose Policy-Guided Tree Search (PGTS), a framework that combines reinforcement learning with structured tree exploration to efficiently navigate reasoning paths. Our key innovation is a learned policy that dynamically decides between expanding, branching, backtracking, or terminating exploration, eliminating the need for manual heuristics or exhaustive search. Experiments across mathematical reasoning, logical deduction, and planning benchmarks demonstrate that PGTS achieves superior reasoning performance while significantly reducing computational costs compared to existing methods. These results establish PGTS as a scalable and effective solution for tackling complex reasoning tasks with LLMs.","Large language models (LLMs) like ChatGPT can solve complex problems by generating step-by-step reasoning. However, they often waste computational effort by exploring too many unnecessary steps, even for simple tasks—a problem sometimes referred to as ""overthinking."" This inefficiency limits their practicality, especially in time-sensitive or resource-constrained applications.To address this, we introduce a new method called Policy-Guided Tree Search (PGTS). PGTS treats the language model as an environment and trains a lightweight decision-making policy to guide which reasoning steps to explore. This policy learns to prioritize the most promising paths, enabling the system to focus its effort where it matters most.Our approach builds on techniques like reinforcement learning and graph neural networks to learn this guidance policy efficiently. Unlike prior methods that blindly generate many reasoning paths or rely on handcrafted heuristics, PGTS adaptively allocates inference effort in a smarter, learned way.PGTS improves both the quality and efficiency of language model reasoning, making it faster and more accurate on challenging tasks like logic puzzles and planning problems. Ultimately, this work pushes us closer to language models that can reason more like humans—deliberate, structured, and efficient."
Poster,Policy-labeled Preference Learning: Is Preference Enough for RLHF?,https://ICML.cc//virtual/2025/poster/43946,"Taehyun Cho, Seokhun Ju, Seungyub Han, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee","To design reward that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing models using reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. To address this, we propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Additionally, we introduce a contrastive KL regularization term derived from regret-based principles to enhance sequential contrastive learning. Experiments in high-dimensional continuous control environments demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings.","When we teach AI systems to act by learning from human feedback, current approaches often assume the examples people provide are perfectly optimal—and that can lead the AI to learn the wrong lessons.We introduce policy-labeled preference learning (PPL), which treats each human example as coming from some real behavior and measures how much “regret” a person had—i.e., how they might wish they’d acted differently—instead of assuming perfection. We also add a new “contrastive” adjustment that sharpens the AI’s understanding of which choices are truly preferred.In challenging simulated control tasks, PPL yields policies that perform much better both offline and when learning on the fly. This brings us closer to AI systems that reliably follow the goals we actually care about."
Poster,Policy Optimization for CMDPs with Bandit Feedback: Learning Stochastic and Adversarial Constraints,https://ICML.cc//virtual/2025/poster/43815,"Francesco Emanuele Stradi, Anna Lunghi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti","We study online learning in constrained Markov decision processes (CMDPs) in which rewards and constraints may be either stochastic or adversarial. In such settings, stradi et al. (2024) proposed the first best-of-both-worlds algorithm able to seamlessly handle stochastic and adversarial constraints, achieving optimal regret and constraint violation bounds in both cases. This algorithm suffers from two major drawbacks. First, it only works under full feedback, which severely limits its applicability in practice. Moreover, it relies on optimizing over the space of occupancy measures, which requires solving convex optimization problems, an highly inefficient task. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with bandit feedback. Specifically, when the constraints are stochastic, the algorithm achieves $\widetilde{\mathcal{O}}(\sqrt{T})$ regret and constraint violation, while, when they are adversarial, it attains $\widetilde{\mathcal{O}}(\sqrt{T})$ constraint violation and a tight fraction of the optimal reward. Moreover, our algorithm is based on a policy optimization approach, which is much more efficient than occupancy-measure-based methods.","We introduce the first algorithm for constrained Markov decision processes (CMDPs) with bandit feedback to effectively handle both stochastic and adversarial constraints. It achieves $\widetilde{\mathcal{O}}(\sqrt{T})$ regret and constraint violation in the stochastic case. When the constraints are adversarial the algorithm attains $\widetilde{\mathcal{O}}(\sqrt{T})$ constraint violation and a tight fraction of the optimal reward. Unlike previous methods that require full feedback and rely on occupancy-measure optimization, our approach is based on efficient policy optimization."
Poster,Policy-Regret Minimization in Markov Games with Function Approximation,https://ICML.cc//virtual/2025/poster/44578,"Thanh Nguyen-Tang, Raman Arora","We study policy-regret minimization problem in dynamically evolving environments, modeled as Markov games between a learner and a strategic, adaptive opponent. We propose a general algorithmic framework that achieves the optimal $\mathcal{O}(\sqrt{T})$ policy regret for a wide class of large-scale problems characterized by an Eluder-type condition--extending beyond the tabular settings of previous work. Importantly, our framework uncovers a simpler yet powerful algorithmic approach for handling reactive adversaries, demonstrating that leveraging opponent learning in such settings is key to attaining the optimal $\mathcal{O}(\sqrt{T})$ policy regret.","In many real-world situations—from autonomous vehicles to online recommendation systems—multiple decision-makers (or agents) must interact with one another while learning how to make better choices. But when these agents have competing goals, learning can become difficult and unpredictable. In this work, we design a learning algorithm that helps a decision-maker perform well even when facing an opponent that learns and adapts over time. Unlike past work, our approach works in complex settings where the environment is too large to fully memorize and where the opponent's behavior must be approximated. We show that our algorithm learns to make good decisions over time and provides the first theoretical guarantees for success in such challenging situations. Our results could help build safer and more reliable AI systems that learn through repeated interaction."
Poster,Policy Regularization on Globally Accessible States in Cross-Dynamics Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45810,"Zhenghai Xue, Lang Feng, Jiacheng Xu, Kang Kang, xiang wen, Bo An, Shuicheng YAN","To learn from data collected in diverse dynamics, Imitation from Observation (IfO) methods leverage expert state trajectories based on thepremise that recovering expert state distributions in other dynamics facilitates policy learning in the current one. However, Imitation Learning inherently imposes a performance upper bound of learned policies. Additionally, as the environment dynamics change, certain expert states may become inaccessible, rendering their distributions less valuable for imitation. To address this, we propose a novel framework that integrates reward maximization with IfO, employing F-distance regularized policy optimization. This framework enforces constraints on globally accessible states—those with nonzero visitation frequency across all considered dynamics—mitigating the challenge posed by inaccessible states. By instantiating F-distance in different ways, we derive two theoretical analysis and develop a practical algorithm called Accessible State Oriented Policy Regularization (ASOR). ASOR serves as a general-purpose module that can be incorporated into various RL approaches, including offline RL and off-policy RL. Extensive experiments across multiple benchmarks demonstrate ASOR’s effectiveness in enhancing state-of-the-art cross-domain policy transfer algorithms, significantly improving their performance.","Modern reinforcement-learning (RL) agents often have to learn from data gathered in many different versions of the same world—think of a robot that is sometimes heavy, sometimes light, or a driving simulator where traffic density changes. Existing imitation-based methods assume that the expert visits roughly the same situations in every version, but this breaks down whenever some states become unreachable after the dynamics change. Our work pinpoints this mismatch as a key bottleneck and asks: what if we only imitate the parts of the world that stay reachable everywhere? We formalise these safe regions as globally accessible states—states that can still be visited no matter how the environment shifts. Building on this idea, we add a gentle steering force to any RL algorithm: we constrain the learner’s behaviour to stay close (in an information-theoretic F-distance) to the expert only on those accessible states while still maximising reward. The resulting plug-in module, ASOR (Accessible-State Oriented Regularisation), is implemented with a GAN-style discriminator that simply augments the reward signal—no new policy architecture required. We prove that this targeted regularisation guarantees smaller performance loss than previous alternatives, even with finite data. In practice, attaching ASOR to state-of-the-art baselines lifts scores across offline MuJoCo datasets, online robotic control, grid-world navigation and a chaotic Fall-Guys-style game, often by large margins. Because ASOR ignores misleading, now-inaccessible states, it makes agents more reliable and sample-efficient when reality refuses to stay still. In short, we turn a vulnerability—dynamics shift—into a strength, giving RL systems a principled way to focus on what they can actually reach."
Poster,Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications,https://ICML.cc//virtual/2025/poster/44245,"Maria Despoina Siampou, Jialiang Li, John Krumm, Cyrus Shahabi, Hua Lu","Encoding geospatial objects is fundamental for geospatial artificial intelligence (GeoAI) applications, which leverage machine learning (ML) models to analyze spatial information. Common approaches transform each object into known formats, like image and text, for compatibility with ML models. However, this process often discards crucial spatial information, such as the object's position relative to the entire space, reducing downstream task effectiveness. Alternative encoding methods that preserve some spatial properties are often devised for specific data objects (e.g., point encoders), making them unsuitable for tasks that involve different data types (i.e., points, polylines, and polygons). To this end, we propose Poly2Vec, a polymorphic Fourier-based  encoding approach that unifies the representation of geospatial objects, while preserving the essential spatial properties. Poly2Vec incorporates a learned fusion module that adaptively integrates the magnitude and phase of the Fourier transform for different tasks and geometries.We evaluate Poly2Vec on five diverse tasks, organized into two categories. The first empirically demonstrates that Poly2Vec consistently outperforms object-specific baselines in preserving three key spatial relationships: topology, direction, and distance. The second shows that integrating Poly2Vec into a state-of-the-art GeoAI workflow improves the performance in two popular tasks: population prediction and land use inference.","Machine learning is increasingly used to understand and reason about the world around us, including geographic data like buildings, roads, and parks. But unlike text or images, geospatial data comes in many forms: a store might be a point, a street a line, and a park a polygon. Most machine learning models aren't designed to work with this kind of diverse spatial data directly.To make it work, a common approach is to convert map objects into other formats, like images. But this conversion can distort important spatial relationships, making models less accurate. Other methods try to encode spatial features more directly, but only for one object type at a time, such as points. That makes them hard to generalize across tasks involving mixed geometries.We introduce Poly2Vec, a new method that encodes all types of geospatial object  (points, polylines, and polygons) into a unified format that machine learning models can use, without losing key spatial information. Poly2Vec uses the Fourier transform to capture important spatial properties like the shape and location of each object, and adapts to different tasks.Plugging Poly2Vec into existing workflows improves performance on real-world applications like population prediction and land use classification, demonstrating its versatility and effectiveness in GeoAI pipelines."
Poster,polybasic Speculative Decoding Through a Theoretical Perspective,https://ICML.cc//virtual/2025/poster/45669,"Ruilin Wang, Huixia Li, Yuexiao Ma, Xiawu Zheng, Fei Chao, Xuefeng Xiao, Rongrong Ji","Inference latency stands as a critical bottleneck in the large-scale deployment of Large Language Models (LLMs). Speculative decoding methods have recently shown promise in accelerating inference without compromising the output distribution. However, existing work typically relies on a dualistic draft-verify framework and lacks rigorous theoretical grounding. In this paper, we introduce a novel \emph{polybasic} speculative decoding framework, underpinned by a comprehensive theoretical analysis. Specifically, we prove a fundamental theorem that characterizes the optimal inference time for multi-model speculative decoding systems, shedding light on how to extend beyond the dualistic approach to a more general polybasic paradigm. Through our theoretical investigation of multi-model token generation, we expose and optimize the interplay between model capabilities, acceptance lengths, and overall computational cost. Our framework supports both standalone implementation and integration with existing speculative techniques, leading to accelerated performance in practice. Experimental results across multiple model families demonstrate that our approach yields speedup ratios ranging from $3.31\times$ to $4.01\times$ for LLaMA2-Chat 7B, up to $3.87 \times$ for LLaMA3-8B, up to $4.43 \times$ for Vicuna-7B and up to $3.85 \times$ for Qwen2-7B---all while preserving the original output distribution. We release our theoretical proofs and implementation code to facilitate further investigation into polybasic speculative decoding.","Large language models (LLMs) face high inference latency, hindering real-world deployment. Current speculative decoding methods, which use a draft model to propose tokens and a target model to verify them, are limited by their two-model setup and lack rigorous theoretical guidance, capping speedup potential. We propose polybasic speculative decoding, a framework employing multiple interconnected draft models guided by a new theoretical analysis. We derive optimal inference time equations, establish conditions for efficiently adding models, and prove that speculative sampling stabilizes token acceptance. Experiments across major LLMs (e.g., LLaMA, Vicuna) show speedups of 3.16–4.43× without altering output quality. The theory enables systematic model selection and system design, advancing beyond heuristic approaches. This accelerates LLMs for applications like translation, reasoning, and chatbots while preserving reliability, making high-quality AI more accessible."
Poster,PolyConf: Unlocking Polymer Conformation Generation through Hierarchical Generative Models,https://ICML.cc//virtual/2025/poster/46091,"Fanmeng Wang, Wentao Guo, Qi Ou, Hongshuai Wang, Haitao Lin, Hongteng Xu, Zhifeng Gao","Polymer conformation generation is a critical task that enables atomic-level studies of diverse polymer materials. While significant advances have been made in designing conformation generation methods for small molecules and proteins, these methods struggle to generate polymer conformations due to their unique structural characteristics.Meanwhile, the scarcity of polymer conformation datasets further limits the progress, making this important area largely unexplored.In this work, we propose PolyConf, a pioneering tailored polymer conformation generation method that leverages hierarchical generative models to unlock new possibilities.Specifically, we decompose the polymer conformation into a series of local conformations (i.e., the conformations of its repeating units), generating these local conformations through an autoregressive model, and then generating their orientation transformations via a diffusion model to assemble them into the complete polymer conformation.Moreover, we develop the first benchmark with a high-quality polymer conformation dataset derived from molecular dynamics simulations to boost related research in this area.The comprehensive evaluation demonstrates that PolyConf consistently outperforms existing conformation generation methods, thus facilitating advancements in polymer modeling and simulation.The whole work is available at https://polyconf-icml25.github.io.","In this work, we successfully unlock an important yet largely unexplored task in the context of machine learning—polymer conformation generation. Specifically, we propose PolyConf, the first tailored polymer conformation generation method that leverages hierarchical generative models to tackle this task, and further develop PolyBench, the first benchmark for polymer conformation generation that comprises a high-quality polymer conformation dataset derived from molecular dynamics simulations to overcome the data scarcity.Comprehensive experiments on the PolyBench benchmark demonstrate that our PolyConf significantly outperforms existing conformation generation methods in both quality and efficiency while maintaining superior scalability and generalization capabilities, thus facilitating advancements in polymer modeling and simulation."
Poster,Polynomial-Delay MAG Listing with Novel Locally Complete Orientation Rules,https://ICML.cc//virtual/2025/poster/46347,"Tian-Zuo Wang, Wen-Bo Du, Zhi-Hua Zhou","A maximal ancestral graph (MAG) is widely used to characterize the causal relations among observable variables in the presence of latent variables. However, given observational data, only a partial ancestral graph representing a Markov equivalence class (MEC) of MAGs is identifiable, which generally contains uncertain causal relations. Due to the uncertainties, \emph{MAG listing}, \emph{i.e.}, listing all the MAGs in the MEC, is critical for many downstream tasks. In this paper, we present the first \emph{polynomial-delay} MAG listing method, where delay refers to the time for outputting each MAG, through introducing enumerated structural knowledge in the form of \emph{singleton background knowledge (BK)}. To incorporate such knowledge, we propose the \emph{sound} and \emph{locally complete} orientation rules. By recursively introducing singleton BK and applying the rules, our method can output all and only MAGs in the MEC with polynomial delay. Additionally, while the proposed novel rules enable more efficient MAG listing, for the goal of incorporating general BK, we present two counterexamples to imply that existing rules including ours, are not yet \emph{complete}, which motivate two more rules. Experimental results validate the efficiency of the proposed MAG listing method.","Understanding cause-and-effect relations is crucial in many areas, such as medicine, biology, and economics. However, in real-world tasks, not all factors can be observed, making it difficult to reveal how each factor causally influence others. Our research tackles this challenge by developing a new method to list all possible explanations for the observed relations among the factors when some factors are unobserved. We create a faster way to generate these explanations. This helps researchers and decision-makers see every possible scenario that fits the data. By making it easier and more efficient to explore all the ways things might be connected, our work supports better, more reliable insights from data when the full picture is hidden from view."
