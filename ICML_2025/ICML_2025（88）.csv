type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Efficient First-Order Optimization on the Pareto Set for Multi-Objective Learning under Preference Guidance,https://ICML.cc//virtual/2025/poster/45381,"Lisha Chen, Quan Xiao, Ellen Fukuda, Xinyi Chen, Kun Yuan, Tianyi Chen","Multi-objective learning under user-specified preference is common in real-world problems such as multi-lingual speech recognition under fairness. In this work, we frame such a problem as a semivectorial bilevel optimization problem, whose goal is to optimize a pre-defined preference function, subject to the constraint that the model parameters are weakly Pareto optimal. To solve this problem, we convert the multi-objective constraints to a single-objective constraint through a merit function with an easy-to-evaluate gradient, and then, we use a penalty-based reformulation of the bilevel optimization problem. We theoretically establish the properties of the merit function, and the relations of solutions for the penalty reformulation and the constrained formulation. Then we propose algorithms to solve the reformulated single-level problem, and establish its convergence guarantees. We test the method on various synthetic and real-world problems. The results demonstrate the effectiveness of the proposed method in finding preference-guided optimal solutions to the multi-objective problem.","This paper addresses how to train machine learning models when there are multiple goals to achieve and the user has a specific preference for how to balance them. Instead of treating preferences as fixed weights or strict constraints, which have theoretical and practical limitations, we treat them as an extra goal to optimize -- over models that already do well across all objectives. This creates a layered decision-making problem, where we aim to find the best option based on preference, but only among solutions that already perform well on the main objectives. To simplify this nonsmooth nonconvex layered problem, we use a penalty method and a smooth approximation to the original problem. We establish the theoretical relation between the solutions of the reformulated problem and the original one. Interestingly and perhaps surprisingly, even though the stationary solutions to such problems often need second-order information, we show that first-order approaches can still be used to approximate such solutions. This serves as a foundation for developing efficient first-order algorithms to solve the problem with convergence guarantees. Experiments on real-world applications such as fairness-aware multi-lingual speech recognition show that our method is both practical and effective."
Poster,Efficient Generative Modeling with Residual Vector Quantization-Based Tokens,https://ICML.cc//virtual/2025/poster/44543,"Jaehyeon Kim, Taehong Moon, Keon Lee, Jaewoong Cho","We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based generative model for high-fidelity generation with fast sampling. RVQ improves data fidelity by increasing the number of quantization steps, referred to as depth, but deeper quantization typically increases inference steps in generative models. To address this, ResGen directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Additionally, we formulate token masking and multi-token prediction within a probabilistic framework using discrete diffusion and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256×256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models.","Scaling data and model size makes generative AI outputs more realistic, but it also stretches generation time and energy budgets. A common remedy is to encode raw data into compact codes; Residual Vector Quantization (RVQ) stands out for packing detail into short code sequences. Unfortunately, existing RVQ-based generators still slow down sampling time when deeper depths are used.We introduce ResGen, an efficient RVQ-based generative modeling framework that directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Extensive ablation studies confirm that our modeling is especially well-matched to RVQ tokens. On ImageNet 256 × 256 image generation and zero-shot text-to-speech, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared with similarly sized baseline models. This efficiency slashes GPU hours, lowers carbon cost, and opens the door to real-time, on-device generative applications."
Poster,Efficient Graph Continual Learning via Lightweight Graph Neural Tangent Kernels-based Dataset Distillation,https://ICML.cc//virtual/2025/poster/45684,"Rihong Qiu, Xinke Jiang, Yuchen Fang, Hongbin Lai, Hao Miao, Xu Chu, Junfeng Zhao, Yasha Wang","Graph Neural Networks (GNNs) have emerged as a fundamental tool for modeling complex graph structures across diverse applications.However, directly applying pretrained GNNs to varied downstream tasks without fine-tuning-based continual learning remains challenging, as this approach incurs high computational costs and hinders the development of Large Graph Models (LGMs).In this paper, we investigate an efficient and generalizable dataset distillation framework for Graph Continual Learning  (GCL) across multiple downstream tasks, implemented through a novel Lightweight Graph Neural Tangent Kernel (LIGHTGNTK).Specifically, LIGHTGNTK employs a low-rank approximation of the Laplacian matrix via Bernoulli sampling and linear association within the GNTK. This design enables efficient capture of both structural and feature relationships while supporting gradient-based dataset distillation.Additionally, LIGHTGNTK incorporates a unified subgraph anchoring strategy, allowing it to handle graph-level, node-level, and edge-level tasks under diverse input structures.Comprehensive experiments on several datasets show that LIGHTGNTK achieves state-of-the-art performance in GCL scenarios, promoting the development of adaptive and scalable LGMs.","Graphs are often be used to understand complex relationships, like how people connect in social networks or how molecules bond in chemistry. Graph Neural Networks (GNNs) are powerful models designed to process this kind of graph data and help make predictions or find patterns. However, making these GNNs work well on different tasks usually means retraining them from scratch, which takes lots of time and computing resources.To tackle this challenge, we developed a new, efficient method that lets GNNs quickly adapt to new jobs without having to start over on the entire training dataset each time. Our approach, called LIGHTGNTK, cleverly selects important parts of large graphs so the GNN model can achieve impressive performance with limited graph samples. By looking at both the structure and the features of the graphs from the perspective of the subgraph, our method can easily switch between tasks that focus on whole graphs, single points, or the connections between nodes.We tested LIGHTGNTK on a variety of real-world problems, and it performed better than current techniques most of the time. This means our work could make it much easier and cheaper to build adaptable, large-scale graph-based AI tools for science, industry, and beyond."
Poster,Efficient Heterogeneity-Aware Federated Active Data Selection,https://ICML.cc//virtual/2025/poster/44007,"Yingpeng Tang, Chao Ren, Xiaoli Tang, Sheng-Jun Huang, Lizhen Cui, Han Yu","Federated Active Learning (FAL) aims to learn an effective global model, while minimizing label queries. Owing to privacy requirements, it is challenging to design effective active data selection schemes due to the lack of cross-client query information. In this paper, we bridge this important gap by proposing the \underline{F}ederated \underline{A}ctive data selection by \underline{LE}verage score sampling (FALE) method. It is designed for regression tasks in the presence of non-i.i.d. client data to enable the server to select data globally in a privacy-preserving manner. Based on FedSVD, FALE aims to estimate the utility of unlabeled data and perform data selection via leverage score sampling. Besides, a secure model learning framework is designed for federated regression tasks to exploit supervision. FALE can operate without requiring an initial labeled set and select the instances in a single pass, significantly reducing communication overhead. Theoretical analyze establishes the query complexity for FALE to achieve constant factor approximation and relative error approximation. Extensive experiments on 11 benchmark datasets demonstrate significant improvements of FALE over existing state-of-the-art methods.","Training effective AI models usually requires large amounts of labeled data, which is costly and challenging to gather, especially when data privacy regulations restrict direct data sharing among institutions. Existing techniques often fail to efficiently select the most useful data points when data is distributed unevenly across multiple clients.To address this, we developed Federated Active data selection by Leverage score sampling (FALE), a novel method combining federated Learning with active Learning. FALE utilizes a privacy-preserving federated singular value decomposition to understand data distribution securely across different clients. Based on this analysis, FALE employs a leverage-score sampling strategy to select globally informative data points efficiently. It further securely trains a robust global AI model using these selected data points, without compromising client privacy.Our FALE significantly reduces redundant labeling efforts and enhances the accuracy of AI models in decentralized environments. Experiments on multiple benchmark datasets demonstrate that FALE consistently outperforms existing methods, achieving better performance with fewer labeled data points. Thus, FALE makes decentralized AI training more practical, efficient, and privacy-aware, with broad implications for secure and collaborative machine learning applications."
Poster,Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling,https://ICML.cc//virtual/2025/poster/46384,"Xiang Hu, Zhihao Teng, Jun Zhao, Wei Wu, Kewei Tu","Despite the success of Transformers, handling longer contexts remains challenging due to the limited length generalization and quadratic complexity of self-attention, which often requires post-training with a larger attention window, significantly increasing computational and memory costs. In this paper, we propose a novel attention mechanism based on dynamic context, Grouped Cross Attention (GCA), which can generalize to 1000 $\times$ the pre-training context length while maintaining the ability to access distant information with a constant attention window size. For a given input sequence, we split it into chunks and use each chunk to retrieve top-$k$ relevant past chunks for subsequent text generation. Specifically, unlike most previous works that use an off-the-shelf retriever, our key innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner, which adapts better to causal language models.Such a mechanism accommodates retrieved chunks with a fixed-size attention window to achieve long-range information access, significantly reducing computational and memory costs during training and inference. Experiments show that GCA-based models achieve near-perfect accuracy in passkey retrieval for 16M context lengths, which is $1000 \times$ the training length.","Enabling machines to possess human-like long-term memory is a crucial step toward creating personalized assistants capable of recalling all relevant historical interactions. However, achieving this remains an unsolved challenge. If we equate ultra-long-term memory to infinite context, the key difficulty lies in how models extrapolate from limited pre-trained contexts to vastly longer ones, while still being capable of random-accessing all contexts. This paper introduces an effective sparse attention mechanism that achieves 1000x length extrapolation, offering a promising prototype for enabling machines to develop permanent memory."
Poster,Efficient LiDAR Reflectance Compression via Scanning Serialization,https://ICML.cc//virtual/2025/poster/46532,"Jiahao Zhu, Kang You, Dandan Ding, Zhan Ma","Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods. To address this, we introduce SerLiC, a serialization-based neural compression framework to fully exploit the intrinsic characteristics of LiDAR reflectance. SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization, offering a device-centric perspective for reflectance analysis. Each point is then tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance, for effective dependencies exploration. For efficient sequential modeling, Mamba is incorporated with a dual parallelization scheme, enabling simultaneous autoregressive dependency capture and fast processing. Extensive experiments demonstrate that SerLiC attains over 2$\times$ volume reduction against the original reflectance data,  outperforming the state-of-the-art method by up to 22\% reduction of compressed bits while using only 2\% of its parameters. Moreover, a lightweight version of SerLiC achieves $\geq 10$ fps (frames per second) with just 111K parameters, which is attractive for real applications.","Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods. To address this, we introduce SerLiC, a serialization-based neural compression framework to efficiently process reflectance data. Extensive experiments demonstrate that SerLiC attains over 2$\times$ volume reduction against the original reflectance data, which is attractive for real-world applications."
Poster,Efficient Logit-based Knowledge Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment,https://ICML.cc//virtual/2025/poster/44825,"Chengting Yu, Xiaochen Zhao, Lei Liu, Shu Yang, Gaoang Wang, Erping Li, Aili Wang","Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative to traditional Artificial Neural Networks (ANNs), prized for their potential energy efficiency on neuromorphic hardware. Despite this, SNNs often suffer from accuracy degradation compared to ANNs and face deployment challenges due to fixed inference timesteps, which require retraining for adjustments, limiting operational flexibility. To address these issues, our work considers the spatio-temporal property inherent in SNNs, and proposes a novel distillation framework for deep SNNs that optimizes performance across full-range timesteps without specific retraining, enhancing both efficacy and deployment adaptability. We provide both theoretical analysis and empirical validations to illustrate that training guarantees the convergence of all implicit models across full-range timesteps. Experimental results on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance among distillation-based SNNs training methods. Our code is available at https://github.com/Intelli-Chip-Lab/snn_temporal_decoupling_distillation.","Considering the spatiotemporal characteristics of Spiking Neural Networks, we optimize logit-based knowledge distillation in deep SNNs training. The method is neat and effective, and we provide both theoretical and empirical analysis as evidence."
Poster,Efficient Long Context Fine-tuning with Chunk Flow,https://ICML.cc//virtual/2025/poster/43857,"Xiulong Yuan, Hongtao Xu, Wenting Shen, Ang Wang, Xiafei Qiu, Jie Zhang, Yuqiong Liu, Bowen Yu, Junyang Lin, Mingzhen Li, Weile Jia, Yong Li, Wei Lin","Long context fine-tuning of large language models(LLMs) involves training on datasets that are predominantly composed of short sequences and a small proportion of longer sequences. However, existing approaches overlook this long-tail distribution and employ training strategies designed specifically for long sequences. Moreover, these approaches also fail to address the challenges posed by variable sequence lengths during distributed training, such as load imbalance in data parallelism and severe pipeline bubbles in pipeline parallelism. These issues lead to suboptimal training performance and poor GPU resource utilization.To tackle these problems, we propose a chunk-centric training method named ChunkFlow. ChunkFlow reorganizes input sequences into uniformly sized chunks by consolidating short sequences and splitting longer ones. This approach achieves optimal computational efficiency and balance among training inputs. Additionally, ChunkFlow incorporates a state-aware chunk scheduling mechanism to ensure that the peak memory usage during training is primarily determined by the chunk size rather than the maximum sequence length in the dataset. Integrating this scheduling mechanism with existing pipeline scheduling algorithms further enhances the performance of distributed training.Experimental results demonstrate that, compared with Megatron-LM, ChunkFlow can be up to 4.53x faster in the long context fine-tuning of LLMs. Furthermore, we believe that ChunkFlow serves as an effective solution for a broader range of scenarios, such as long context continual pre-training, where datasets contain variable-length sequences.","Long context fine-tuning is essential to extend the ability of large language models (LLMs) to handle long texts. This process involves training on meticulously gathered datasets that are predominantly composed of short sequences and a small proportion of longer sequences (e.g., 99% of texts are short, 1% are extremely long). However, existing training methods overlook this characteristic and employ training strategies designed for long sequences, resulting in sub-optimal training efficiency. To solve this, we propose ChunkFlow, a chunk-centric training method designed for long context fine-tuning scenario. Chunkflow reorganizes input sequences into uniformly sized chunks by combining short texts and splitting long ones to achieve balanced workloads and optimal computational efficiency. Additionally, ChunkFlow also employs a state-aware chunk scheduling mechanism to ensure the peak memory usage controllable, which is primarily determined by the length of pre-defined chunk rather than the longest sequence in training dataset.Experiments show ChunkFlow speeds up long-context fine-tuning by up to 4.53x compared to state-of-the-art system Megatron-LM. ChunkFlow significantly accelerates the process of long context fine-tuning, benefiting wide downstream applications from code generation to complex question-answering."
Poster,Efficiently Access Diffusion Fisher: Within the Outer Product Span Space,https://ICML.cc//virtual/2025/poster/44763,"Fangyikang Wang, Hubery Yin, Shaobin Zhuang, Huminhao Zhu, Yinan Li, Lei Qian, Chao Zhang, Hanbin Zhao, Hui Qian, Chen Li","Recent Diffusion models (DMs) advancements have explored incorporating the second-order diffusion Fisher information (DF), defined as the negative Hessian of log density, into various downstream tasks and theoretical analysis.However, current practices typically approximate the diffusion Fisher by applying auto-differentiation to the learned score network. This black-box method, though straightforward, lacks any accuracy guarantee and is time-consuming.In this paper, we show that the diffusion Fisher actually resides within a space spanned by the outer products of score and initial data.Based on the outer-product structure, we develop two efficient approximation algorithms to access the trace and matrix-vector multiplication of DF, respectively.These algorithms bypass the auto-differentiation operations with time-efficient vector-product calculations. Furthermore, we establish the approximation error bounds for the proposed algorithms.Experiments in likelihood evaluation and adjoint optimization demonstrate the superior accuracy and reduced computational cost of our proposed algorithms.Additionally, based on the novel outer-product formulation of DF, we design the first numerical verification experiment for the optimal transport property of the general PF-ODE deduced map.","Diffusion models are a type of artificial intelligence that have been used for many tasks, like generating images. They work by gradually adding noise to data and then learning how to reverse this process to get back the original data. A key concept in these models is the diffusion Fisher information, which helps us understand how the data is spread out in the model.Current methods for calculating this information are often inaccurate and slow. In our research, we discovered that the diffusion Fisher information is related to how the initial data and the model's 'guesses' (called scores) interact. Based on this, we developed two new, faster ways to estimate important aspects of the diffusion Fisher information. These new methods are more efficient because they avoid complex calculations and instead use simpler vector operations.We also figured out how accurate these new methods are. Our experiments, such as when evaluating how well the model predicts likelihoods and in optimization tasks, showed that our new methods are not only more accurate but also save a lot of computing time. Additionally, our new understanding of the diffusion Fisher information allowed us to design the first experiment to numerically test an important property of a general map in the context of diffusion models. Overall, our work makes diffusion models more efficient and helps us better understand how they work."
Poster,Efficiently Serving Large Multimodal Models Using EPD Disaggregation,https://ICML.cc//virtual/2025/poster/44120,"Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Bai Xiaolong, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan","Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15× lower peak memory utilization), batch sizes (up to 22× larger), 10× more images per request, and 2.2× larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90–100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at [https://github.com/vbdi/epdserve](https://github.com/vbdi/epdserve).","Modern AI systems can now understand not just text, but also images 🖼️, audio 🔊, and video 🎥. These powerful tools—called **multimodal models**—power applications that can answer questions about pictures, assist with medical scans, or even analyze videos. ✨However, **running these models is slow and memory-hungry**, especially when dealing with high-resolution images or complex inputs. That’s because each model request goes through several heavy processing steps, and current systems make all those steps share the same resources—leading to traffic jams inside the computer. Our work introduces a smarter way to run these models. We _break the process into three stages_—understanding the multimodal input 🖼️, preparing the response 🧮, and generating the output ✍️—and **assign each stage its own set of specialized GPUs**. This separation avoids bottlenecks🚦and lets the system run more smoothly and efficiently. 🚀With our approach, the system can handle **10× more images**, use **15× less memory**, and respond **up to 71% faster** than current methods. This makes **advanced AI tools more practical** for real-world use—in areas like healthcare 🏥, creative work 🎨, and interactive digital assistants 🧑💻."
