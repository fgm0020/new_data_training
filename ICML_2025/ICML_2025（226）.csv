type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Position: AI Scaling: From Up to Down and Out,https://ICML.cc//virtual/2025/poster/40137,"Yunke Wang, Yanxi Li, Chang Xu","AI Scaling has traditionally been synonymous with Scaling Up, which builds larger and more powerful models. However, the growing demand for efficiency, adaptability, and collaboration across diverse applications necessitates a broader perspective. This position paper presents a holistic framework for AI scaling, encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that while Scaling Up of models faces inherent bottlenecks, the future trajectory of AI scaling lies in Scaling Down and Scaling Out. These paradigms address critical technical and societal challenges, such as reducing carbon footprint, ensuring equitable access, and enhancing cross-domain collaboration. We explore transformative applications in healthcare, smart manufacturing, and content creation, demonstrating how AI Scaling can enable breakthroughs in efficiency, personalization, and global connectivity. Additionally, we highlight key challenges, including balancing model complexity with interpretability, managing resource constraints, and fostering ethical development. By synthesizing these approaches, we propose a unified roadmap that redefines the future of AI research and application, paving the way for advancements toward Artificial General Intelligence (AGI).","Most advances in AI today come from building ever-larger models, which is a strategy known as ""Scaling Up."" But this approach faces growing challenges: it’s costly, energy-intensive, and often inaccessible to many communities and applications. Our research asks: how can we rethink AI scaling to make it more efficient, adaptable, and widely usable?In this position paper, we propose a new framework that combines three directions: Scaling Up, Scaling Down, and Scaling Out. Scaling Down makes models smaller and faster, enabling them to run on everyday devices. Scaling Out connects many specialized models or AI agents together, allowing them to collaborate like teams. This shift offers a more flexible and sustainable path forward for AI.We show how this broader approach to scaling can unlock breakthroughs in healthcare, smart factories, and online platforms like TikTok and YouTube, where AI needs to be responsive, personal, and fast. Our framework also addresses key concerns like fairness, energy use, and ethical deployment.Ultimately, we argue that the future of AI will depend not just on building bigger models, but on building better systems—ones that are distributed, efficient, and ready to work in the real world."
Poster,Position: AI's growing due process problem,https://ICML.cc//virtual/2025/poster/40138,Sunayana Rane,"AI systems are now ubiquitous in real-world decision-making. However, their use is often invisible and almost always difficult to understand for the ordinary people who now come into contact with AI regularly. As these AI-driven decision-making systems increasingly replace human counterparts, our ability to understand the reasons behind a decision, and to contest that decision fairly, is quickly being eroded. In the United States legal system due process includes the right to understand the reasons for certain major decisions and the right to openly contest those decisions. Everyone is entitled to due process under the law, and human decision-makers have been required to adhere to due process when making many important decisions that are now slowly being relegated to AI systems. Using two recent court decisions as a foundation, this paper takes the position that AI in its current form cannot guarantee due process, and therefore cannot and (should not) be used to make decisions that should be subject to due process. The supporting legal analysis investigates how the current lack of technical answers about the interpretability and causality of AI decisions, coupled with extreme trade secret protections severely limiting any exercise of the small amount of technical knowledge we do have, serve as a fatal anti-due-process combination. Throughout the analysis, this paper explains why technical researchers' involvement is vital to informing the legal process and restoring due process protections.","AI systems are now ubiquitous in real-world decision-making. However, their use is often invisible and almost always difficult to understand for the ordinary people who now come into contact with AI regularly. As these AI-driven decision-making systems increasingly replace human counterparts, our ability to understand the reasons behind a decision, and to contest that decision fairly, is quickly being eroded. In the United States legal system due process includes the right to understand the reasons for certain major decisions and the right to openly contest those decisions. Everyone is entitled to due process under the law, and human decision-makers have been required to adhere to due process when making many important decisions that are now slowly being relegated to AI systems. Using two recent court decisions as a foundation, this paper takes the position that AI in its current form cannot guarantee due process, and therefore cannot and (should not) be used to make decisions that should be subject to due process. The supporting legal analysis investigates how the current lack of technical answers about the interpretability and causality of AI decisions, coupled with extreme trade secret protections severely limiting any exercise of the small amount of technical knowledge we do have, serve as a fatal anti-due-process combination. Throughout the analysis, this paper explains why technical researchers' involvement is vital to informing the legal process and restoring due process protections."
Poster,Position: AI Should Not Be An Imitation Game: Centaur Evaluations,https://ICML.cc//virtual/2025/poster/40148,"Andreas Haupt, Erik Brynjolfsson","Benchmarks and evaluations are central to machine learning methodology and direct research in the field. Current evaluations commonly test systems in the absence of humans. This position paper argues that the machine learning community should increasingly use _centaur evaluations_, in which humans and AI jointly solve tasks. Centaur Evaluations refocus machine learning development toward human augmentation instead of human replacement, they allow for direct evaluation of human-centered desiderata, such as interpretability and helpfulness, and they can be more challenging and realistic than existing evaluations. By shifting the focus from _automation_ toward _collaboration_ between humans and AI, centaur evaluations can drive progress toward more effective and human-augmenting machine learning systems.","To make decisions on which Artificial Intelligence system (e.g., ChatGPT, Claude, or Gemini) to use for a task, we need to know which ones are good at the task at hand. Currently, many of these tasks test models on how they perform on human activities, such as solving mathematical problems, or summarization. We argue that we need to include humans in the evaluation, e.g., by letting many humans solve a writing or coding task together with different Artificial Intelligence models and comparing the outcomes."
Poster,Positional Attention: Expressivity and Learnability of Algorithmic Computation,https://ICML.cc//virtual/2025/poster/46691,"Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veličković, Kimon Fountoulakis","There is a growing interest in the ability of neural networks to execute algorithmic tasks (e.g., arithmetic, summary statistics, and sorting).The goal of this work is to better understand the role of attention in Transformers for algorithmic execution. Its importance for algorithmic execution has been studied theoretically and empirically using parallel computational models. Notably, many parallel algorithms communicate between processors solely using positional information. Inspired by this observation, we investigate how Transformers can execute algorithms using positional attention, where attention weights depend exclusively on positional encodings. We prove that Transformers with positional attention (positional Transformers) maintain the same expressivity of parallel computational models, incurring a logarithmic depth cost relative to the input length. We analyze their in-distribution learnability and explore how parameter norms in positional attention affect sample complexity. Our results show that positional Transformers introduce a learning trade-off: while they exhibit better theoretical dependence on parameter norms, certain tasks may require more layers, which can, in turn, increase sample complexity. Finally, we empirically explore the out-of-distribution performance of positional Transformers and find that they perform well in tasks where their underlying algorithmic solution relies on positional information.","Transformers, a powerful type of neural network, can perform complex tasks like solving math problems or sorting numbers. But how exactly do they achieve this? In our study, we explore the role of the attention mechanism—a core part of Transformers—by analyzing a simplified version called positional attention, where attention depends only on the positions of inputs, not their actual values. This approach reflects how some classical parallel algorithms work. We find that Transformers using positional attention can still perform complex computations and offer theoretical advantages in how they learn. Our experiments show that this simplified attention can generalize better in tasks where position matters more than content. This helps us understand when and how attention enables Transformers to solve complex tasks."
Poster,Positional Encoding meets Persistent Homology on Graphs,https://ICML.cc//virtual/2025/poster/46612,"Yogesh Verma, Amauri Souza, Vikas Garg","The local inductive bias of message-passing graph neural networks (GNNs) hampers their ability to exploit key structural information (e.g., connectivity and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged as two promising approaches to mitigate this issue. PE schemes endow GNNs with location-aware features, while PH methods enhance GNNs with multiresolution topological features. However, a rigorous theoretical characterization of the relative merits and shortcomings of PE and PH has remained elusive. We bridge this gap by establishing that neither paradigm is more expressive than the other, providing novel constructions where one approach fails but the other succeeds. Our insights inform the design of a novel learnable method, PiPE (Persistence-informed Positional Encoding), which is provably more expressive than both PH and PE. PiPE demonstrates strong performance across a variety of tasks (e.g., molecule property prediction,  graph classification, and out-of-distribution generalization), thereby advancing the frontiers of graph representation learning. Code is available at https://github.com/Aalto-QuML/PIPE","Graph Neural Networks (GNNs) are powerful tools for learning from complex data like molecules or social networks, but they often miss key structural patterns. Two popular approaches to mitigate this issue are positional encodings (PE) and persistent homology (PH)—but it’s unclear which is better. We show that PE and PH are incomparable: each can detect graph patterns that the other misses. Using this insight, we propose PiPE (Persistence-informed Positional Encoding), a new method that combines the strengths of both approaches. Our method outperforms existing techniques on real-world tasks like drug molecule property prediction, graph classification, and out-of-distribution generalization."
Poster,Position: Algebra Unveils Deep Learning - An Invitation to Neuroalgebraic Geometry,https://ICML.cc//virtual/2025/poster/40106,"Giovanni Luca Marchetti, Vahid Shahverdi, Stefano Mereta, Matthew Trager, Kathlén Kohn","In this position paper, we promote the study of function spaces parameterized by machine learning models through the lens of algebraic geometry. To this end, we focus on algebraic models, such as neural networks with polynomial activations, whose associated function spaces are semi-algebraic varieties. We outline a dictionary between algebro-geometric invariants of these varieties, such as dimension, degree, and singularities, and fundamental aspects of machine learning, such as sample complexity, expressivity, training dynamics, and implicit bias. Along the way, we review the literature and discuss ideas beyond the algebraic domain. This work lays the foundations of a research direction bridging algebraic geometry and deep learning, that we refer to as neuroalgebraic geometry.","How do neural networks learn? Why do some neural architectures perform better than others? Despite the success of AI, these questions remain largely mysterious.  We explore a way to understand the inner-workings of neural networks, by using tools from a branch of mathematics called algebraic geometry. Specifically, we look at the space of functions that these models can represent, and discuss how to understand its geometry via tools from algebra. We refer to this field as neuroalgebraic geometry.  We believe that neuroalgebraic geometry can offer unique insights, complementing the other mathematical fields that have been proposed to crack the fundamental questions in the understanding of neural networks."
Poster,Position: All Current Generative Fidelity and Diversity Metrics are Flawed,https://ICML.cc//virtual/2025/poster/40164,"Ossi Räisä, Boris van Breugel, Mihaela van der Schaar","Any method's development and practical application is limited by our ability to measure its reliability. The popularity of generative modeling emphasizes the importance of good synthetic data metrics. Unfortunately, previous works have found many failure cases in current metrics, for example lack of outlier robustness and unclear lower and upper bounds. We propose a list of desiderata for synthetic data metrics, and a suite of sanity checks: carefully chosen simple experiments that aim to detect specific and known generative modeling failure modes. Based on these desiderata and the results of our checks, we arrive at our position: all current generative fidelity and diversity metrics are flawed. This significantly hinders practical use of synthetic data. Our aim is to convince the research community to spend more effort in developing metrics, instead of models. Additionally, through analyzing how current metrics fail, we provide practitioners with guidelines on how these metrics should (not) be used.","Replacing data about real people with similar computer-generated synthetic data has several application in machine learning, including enhancing the privacy of individuals. Evaluating the quality of such synthetic data is challenging, and many evaluation metrics have been developed to measure various aspects of synthetic data quality.We focus on two types of metrics: fidelity metrics that aim to evaluate how realistic synthetic data is, and diversity metrics that aim to evaluate how diverse the synthetic data is compared to real data. We check how well these metrics actually evaluate the qualities they aim to measure with very simple scenarios where it is obvious how a good metric should behave. We find that all currently existing metrics do not behave as they should on many of these checks, and conclude that they are flawed.The flaws of current metrics mean that synthetic data evaluations using them may be drawing misleading conclusions about synthetic data quality. This means that results should be interpreted with the known flaws of the metrics in mind, and better metrics should be developed to fix as many flaws as possible."
Poster,Position: An Empirically Grounded Identifiability Theory Will Accelerate Self Supervised Learning Research,https://ICML.cc//virtual/2025/poster/40163,"Patrik Reizinger, Randall Balestriero, David Klindt, Wieland Brendel","Self-Supervised Learning (SSL) powers many current AI systems. As research interest and investment grow, the SSL design space continues to expand. The Platonic view of SSL, following the Platonic Representation Hypothesis (PRH), suggests that despite different methods and engineering approaches, all representations converge to the same Platonic ideal. However, this phenomenon lacks precise theoretical explanation. By synthesizing evidence from Identifiability Theory (IT), we show that the PRH can emerge in SSL. There is a gap between SSL theory and practice: Current IT cannot explain SSL's empirical success, though it has practically relevant insights. Our work formulates a blueprint for SSL research to bridge this gap: we propose expanding IT into what we term Singular Identifiability Theory (SITh), a broader theoretical framework encompassing the entire SSL pipeline. SITh would allow deeper insights into the implicit data assumptions in SSL and advance the field towards learning more interpretable and generalizable representations. We highlight three critical directions for future research: 1) training dynamics and convergence properties of SSL; 2) the impact of finite samples, batch size, and data diversity; and 3) the role of inductive biases in architecture, augmentations, initialization schemes, and optimizers.","We pinpoint the gap between the empirical and theoretical advances in self-supervised representation learning (SSL): mostly that the focus and the research questions are different, and that there is not enough cross-pollination between the two communities. We use the lens of identifiability theory (IT) to propose a research agenda for SSL, which we believe can build upon, but needs to extend, current IT."
Poster,Position: A Theory of Deep Learning Must Include Compositional Sparsity,https://ICML.cc//virtual/2025/poster/40171,"David A. Danhofer, Davide DAscenzo, Rafael Dubach, Tomaso A Poggio","Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable success in a wide variety of domains too high-dimensional for classical shallow networks subject to the curse of dimensionality. However, open questions about fundamental principles, that govern the learning dynamics of DNNs, remain. In this position paper we argue that it is the ability of DNNs to exploit the compositionally sparse structure of the target function driving their success. As such, DNNs can leverage the property that most practically relevant functions can be composed from a small set of constituent functions, each of which relies only on a low-dimensional subset of all inputs. We show that this property is shared by all efficiently Turing-computable functions and is therefore highly likely present in all current learning problems. While some promising theoretical insights on questions concerned with approximation and generalization exist in the setting of compositionally sparse functions, several important questions on the learnability and optimization of DNNs remain. Completing the picture of the role of compositional sparsity in deep learning is essential to a comprehensive theory of artificial—and even general—intelligence.","A central mystery in artificial intelligence is why deep learning works so well, even on extremely complex problems. This paper argues that one of the key secrets lies in *compositional sparsity*: most real-world tasks can be broken down into many small, simple steps, each depending on only a few pieces of information. Deep neural networks are especially good at learning these kinds of step-by-step structures, which lets them avoid the usual explosion in complexity that plagues traditional methods. We demonstrate that this property is shared by all efficiently computable problems—that is, problems that can be solved efficiently by computers, and explain how it helps deep learning systems to learn, generalize, and reason. However, important questions remain—such as how neural networks discover these hidden structures from training data, and what makes some problems easier to learn than others. Understanding these principles could help us design smarter and more reliable AI systems in the future."
Poster,Position: Beyond Assistance – Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care,https://ICML.cc//virtual/2025/poster/40113,"Abeer Badawi, Md Tahmid Rahman Laskar, Jimmy Huang, Shaina Raza, Elham Dolatabadi","This position paper argues for a fundamental shift in how Large Language Models (LLMs) are integrated into the mental health care domain. We advocate for their role as co-creators rather than mere assistive tools. While LLMs have the potential to enhance accessibility, personalization, and crisis intervention, their adoption remains limited due to concerns about bias, evaluation, over-reliance, dehumanization, and regulatory uncertainties. To address these challenges, we propose two structured pathways: SAFE-i (Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical and responsible deployment, and HAAS-e (Human-AI Alignment and Safety Evaluation) Framework for multidimensional, human-centered assessment. SAFE-i provides a blueprint for data governance, adaptive model engineering, and real-world integration, ensuring LLMs align with clinical and ethical standards. HAAS-e introduces evaluation metrics that go beyond technical accuracy to measure trustworthiness, empathy, cultural sensitivity, and actionability. We call for the adoption of these structured approaches to establish a responsible and scalable model for LLM-driven mental health support, ensuring that AI complements—rather than replaces—human expertise.","### What if AI could be your teammate, not your replacement, in delivering compassionate mental health care?As the digital-native generation turns to tools like ChatGPT for everything from schoolwork to career advice, it won’t be long before they rely on AI for emotional and mental health support. The question is no longer if LLMs belong in mental health, but how they can contribute safely, ethically, and meaningfully. This paper argues that LLMs are ready to do more than automate tasks when designed with ethical and safety considerations. These tools can help ease the burden on overstretched teams, provide personalized guidance, and offer timely support. But the stakes are high: without proper safeguards, LLMs can cause serious harm, spreading bias and misinformation, or leading users to place misplaced trust in their responses. Implementing strong safeguards is essential to ensure these tools are safe, reliable, and aligned with ethical standards. To translate this vision into action, our position proposes two frameworks: SAFE-i, which supports responsible design and deployment through three pillars: Ethical Data Foundations, Model Engineering, and Real-World Integration. HAAS-e, which proposes a human-centered evaluation framework built around four essential dimensions based on trustworthiness, fairness, empathy, and actionability, introducing metrics like the Contextual Empathy Score (CES), Cultural Sensitivity Index (CSI), Personalization Appropriateness Score (PAS), and Actionability and Safety Assessment (ASA). Together, these tools offer a practical roadmap for aligning AI systems with human values, clinical goals, and diverse cultural contexts—empowering mental health professionals with adaptive, ethical, and empathetic AI collaborators."
