type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Decision Mixer: Integrating Long-term and Local Dependencies via Dynamic Token Selection for Decision-Making,https://ICML.cc//virtual/2025/poster/46467,"Hongling Zheng, Li Shen, Yong Luo, Deheng Ye, Bo Du, Jialie SHEN, Dacheng Tao","The Conditional Sequence Modeling (CSM) paradigm, benefiting from the transformer's powerful distribution modeling capabilities, has demonstrated considerable promise in offline Reinforcement Learning (RL) tasks. Depending on the task's nature, it is crucial to carefully balance the interplay between inherent local features and long-term dependencies in Markov decision trajectories to mitigate potential performance degradation and unnecessary computational overhead. In this paper, we propose Decision Mixer (DM), which addresses the conflict between features of different scales in the modeling process from the perspective of dynamic integration. Drawing inspiration from conditional computation, we design a plug-and-play dynamic token selection mechanism to ensure the model can effectively allocate attention to different features based on task characteristics. Additionally, we employ an auxiliary predictor to alleviate the short-sightedness issue in the autoregressive sampling process. DM achieves state-of-the-art performance on various standard RL benchmarks while requiring significantly fewer computational resources, offering a viable solution for building efficient and scalable RL foundation models. Code is available at here.","Teaching AI to make good decisions from past experiences is challenging because it must balance immediate details (""what action to take now"") with long-term consequences (""how this affects future outcomes""). Existing transformer-based methods struggle to efficiently handle both, leading to performance drops and high computational costs. We propose Decision Mixer (DM), which dynamically selects which parts of a decision sequence need deep analysis versus quick processing. Like a smart filter, DM identifies whether each step in a decision chain requires complex attention (for long-term planning) or simple forwarding (for local actions), dramatically reducing computation. We also add a helper module to prevent short-sighted decisions during step-by-step predictions. DM achieves state-of-the-art results across robotics and game-playing benchmarks while using significantly less computing power. This enables more efficient and scalable AI decision-makers for real-world applications like autonomous systems and industrial control."
Poster,Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents,https://ICML.cc//virtual/2025/poster/45101,"Shayan Kiyani, George Pappas, Aaron Roth, Hamed Hassani","A fundamental question in data-driven decision making is how to quantify the uncertainty of predictions to inform risk-sensitive downstream actions, as often required in domains such as medicine. We develop a decision-theoretic foundation linking prediction sets to risk-averse decision-making, addressing three questions: (1) What is the correct notion of uncertainty quantification for risk-averse decision makers? We prove that prediction sets are optimal for decision makers who wish to optimize their value at risk. (2) What is the optimal policy that a risk averse decision maker should use to map prediction sets to actions? We show that a simple max-min decision policy is optimal for risk-averse decision makers. Finally, (3) How can we derive prediction sets that are optimal for such decision makers? We provide an exact characterization in the population regime and a distribution free finite-sample construction. These insights leads to *Risk-Averse Calibration (RAC)*, a principled algorithm that is both *practical*—exploiting black-box predictions to enhance downstream utility—and *safe*—adhering to user-defined risk thresholds. We experimentally demonstrate RAC's advantages in medical diagnosis and recommendation systems, showing that it substantially improves the trade-off between safety and utility, delivering higher utility than existing methods while avoiding critical errors.","Machine learning models are increasingly used to predict outcomes and guide decisions, especially in areas like medicine, where making the wrong choice can be costly or dangerous. Our research tackles this problem by linking uncertainty in predictions directly to safer and better decisions.We developed a new approach that helps decision-makers manage uncertainty effectively by providing clear guidance on what action to take when predictions are uncertain. First, we clarified what uncertainty really means for someone who wants to avoid the worst possible outcomes. Then, we identified a straightforward strategy that always chooses the safest option based on these uncertain predictions.Finally, we created a practical tool called Risk-Averse Calibration (RAC) that takes predictions from any existing model and determines the safest actions. RAC is designed to ensure that actions stay within an acceptable risk limit while maximizing the benefit gained.Through experiments in medical diagnosis and recommendation systems, we found that RAC significantly outperforms existing methods, consistently delivering higher benefits without compromising safety."
Poster,Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization,https://ICML.cc//virtual/2025/poster/45614,"Junyi Liao, Zihan Zhu, Ethan Fang, Zhuoran Yang, Vahid Tarokh","Estimating the unknown reward functions driving agents' behavior is a central challenge in inverse games and reinforcement learning. This paper introduces a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization. Given observed player strategies and actions, we aim to reconstruct the underlying reward functions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish reward function identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building on this theoretical foundation, we propose an algorithm to learn reward from observed actions, designed to capture all plausible reward parameters by constructing confidence sets. Our algorithm works in both static and dynamic settings and is adaptable to incorporate other methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample-efficiency of our algorithm. Empirical results demonstrate the framework’s effectiveness in accurately recovering reward functions across various scenarios, offering new insights into decision-making in competitive environments.","Understanding how and why people or machines make decisions is a central goal in fields like economics, robotics, and artificial intelligence. This paper studies how to uncover the hidden goals (or ""rewards"") of decision-makers when we only observe their behavior in competitive settings—like games or strategic interactions—where two sides are working against each other. We focus on situations where both players make decisions with some randomness, which is more realistic than assuming perfect reasoning.Our main contribution is a method to learn these hidden goals from data, even when we can't pinpoint one unique answer. Instead of guessing a single explanation, we build a set of all plausible explanations that fit the observed behavior. We also show, with both theory and experiments, how more data leads to better understanding of the decision-makers’ goals. Our work provides new tools for safely and reliably interpreting strategic behavior in complex systems, such as autonomous driving or financial markets."
Poster,Decomposition of Graphic Design with Unified Multimodal Model,https://ICML.cc//virtual/2025/poster/46327,"Hui Nie, Zhao Zhang, Yutao Cheng, Maoke Yang, Gonglei Shi, Qingsong Xie, Jie Shao, Xinglong Wu","We propose Layer Decomposition of Graphic Designs (LDGD), a novel vision task that converts composite graphic design (e.g., posters) into structured representations comprising ordered RGB-A layers and metadata. By transforming visual content into structured data, LDGD facilitates precise image editing and offers significant advantages for digital content creation, management, and reuse. This task presents two core challenges: (1) predicting the attribute information (metadata) of each layer, and (2) recovering the occluded regions within overlapping layers to enable high-fidelity image reconstruction. To address this, we present the Decompose Layer Model (DeaM), a large unified multimodal model that integrates a conjoined visual encoder, a language model, and a condition-aware RGB-A decoder. DeaM adopts a two-stage processing pipeline: first generates layer-specific metadata containing information such as spatial coordinates and quantized encodings, and then reconstructs pixel-accurate layer images using a condition-aware RGB-A decoder. Beyond full decomposition, the model supports interactive decomposition via textual or point-based prompts.  Extensive experiments  demonstrate the effectiveness of the proposed method. The code is accessed at https://github.com/witnessai/DeaM.","We introduce a new way to break down complex poster designs into separate visual pieces, like text and images, along with useful information about each piece—such as where it appears and how it looks. This helps make editing and reusing designs much easier and more precise. To do this, we built a smart system called DeaM that can understand and separate these design elements. It first figures out the details of each part of a design, then rebuilds each piece as an image layer. It can also respond to user instructions, like clicking on an area or describing what they want to change."
Poster,De-coupled NeuroGF for Shortest Path Distance Approximations on Large Terrain Graphs,https://ICML.cc//virtual/2025/poster/45376,"Samantha Chen, Pankaj Agarwal, Yusu Wang","The ability to acquire high-resolution, large-scale geospatial data at an unprecedented using LiDAR and other related technologies has intensified the need for scalable algorithms for terrain analysis, including *shortest-path-distance* (SPD) queries on large-scale terrain digital elevation models (DEMs). In this paper, we present a *neural data structure* for efficiently answering SPD queries approximately on a large terrain DEM, which is based on the recently proposed neural geodesic field (NeuroGF) framework (Zhang et al., 2023)---the state-of-the-art neural data structure for estimating geodesic distance.In particular, we  propose a decoupled-NeuroGF data structure  combined with an efficient two-stage mixed-training strategy, which significantly reduces computational bottlenecks and enables efficient training on terrain DEMs at a scale not feasible before. We demonstrate the efficacy of our approach by performing detailed experiments on both synthetic and real data sets.For instance, we can train a small model with around 70000 parameters on a terrain DEM with 16 million nodes in a matter of hours that can answer SPD queries with 1\% relative error in at most 10ms per query.","Modern mapping technologies such as LiDAR have made detailed 3D terrain models widely available. However, it is still extremely time-consuming to query the shortest path between two points between in the terrain, an essential operation in many geospatial applications (e.g. flood risk analysis, disaster modeling). We built neural network-based systems which can quickly answer shortest path queries between any two points in a large-scale and detailed terrains by splitting the neural network training into two parts. The first part helps the neural network learn a coarse representation of the terrain and the second part trains a final lightweight module to compute the shortest path approximation. Our two-stage neural approach helps enable geospatial analysis tasks for terrains of unprecedented size."
Poster,Decoupled SGDA for Games with Intermittent Strategy Communication,https://ICML.cc//virtual/2025/poster/44847,"Ali Zindari, Parham Yazdkhasti, Anton Rodomanov, Tatjana Chavdarova, Sebastian Stich","We introduce *Decoupled SGDA*, a novel adaptation of Stochastic Gradient Descent Ascent (SGDA) tailored for multiplayer games with intermittent strategy communication. Unlike prior methods, Decoupled SGDA enables players to update strategies locally using outdated opponent strategies, significantly reducing communication overhead. For Strongly-Convex-Strongly-Concave (SCSC) games, it achieves near-optimal communication complexity comparable to the best-known GDA rates. For *weakly coupled* games where the interaction between players is lower relative to the non-interactive part of the game, Decoupled SGDA significantly reduces communication costs compared to standard SGDA. Additionally, *Decoupled SGDA* outperforms federated minimax approaches in noisy, imbalanced settings. These results establish *Decoupled SGDA* as a transformative approach for distributed optimization in resource-constrained environments.","Our research introduces a new method called Decoupled SGDA (Stochastic Gradient Descent Ascent) designed to improve how players update their strategies in online games, especially when communication between them is limited or intermittent.Traditional methods require players to constantly exchange information, leading to high communication costs. Decoupled SGDA allows players to make their moves locally, even using slightly outdated information from opponents. This significantly reduces the amount of communication needed, making it more efficient for games where resources are constrained.We show that for certain types of games (Strongly-Convex-Strongly-Concave games), our method is almost as efficient in terms of communication as the best existing approaches. For ""weakly coupled"" games, where players' interactions are less critical, Decoupled SGDA drastically cuts down communication costs compared to standard methods.Furthermore, Decoupled SGDA performs better than other similar approaches in challenging conditions, such as when there's a lot of noise or imbalances in the game. These findings suggest that Decoupled SGDA is a promising and adaptable solution for optimizing distributed systems in various real-world scenarios with limited resources."
Poster,Deep Bayesian Filter for Bayes-Faithful Data Assimilation,https://ICML.cc//virtual/2025/poster/45580,"Yuta Tarumi, Keisuke Fukuda, Shin-ichi Maeda","Data assimilation for nonlinear state space models (SSMs) is inherently challenging due to non-Gaussian posteriors. We propose Deep Bayesian Filtering (DBF), a novel approach to data assimilation in nonlinear SSMs. DBF introduces latent variables $h_t$ in addition to physical variables $z_t$, ensuring Gaussian posteriors by (i) constraining state transitions in the latent space to be linear and (ii) learning a Gaussian inverse observation operator $r(h_t|o_t)$. This structured posterior design enables analytical recursive computation, avoiding the accumulation of Monte Carlo sampling errors over time steps. DBF optimizes these operators and other latent SSM parameters by maximizing the evidence lower bound. Experiments demonstrate that DBF outperforms existing methods in scenarios with highly non-Gaussian posteriors.","Weather, ocean, and seismic research all rely on data assimilation, which blends observations with physics-based models to track a system’s physical state. Despite its large impact on prediction accuracy, operational weather-forecasting systems still rely on ensemble Kalman filters, while most machine-learning studies focus on data-driven forecasts that omit explicit physics. Methods such as the ensemble Kalman filter struggle in highly nonlinear regimes because they assume Gaussian posterior distributions even when the true dynamics are non-Gaussian.We propose the Deep Bayesian Filter (DBF), which learns a latent space in which the dynamics are linear—effectively seeking a Koopman-operator representation—so each Bayesian update can be computed in closed form. This analytical update eliminates the need for Monte-Carlo sampling during inference, preventing the error accumulation seen in dynamical VAE algorithms. DBF also scales gracefully to high-dimensional problems thanks to the block-diagonal structure of its dynamics matrix. On the Lorenz-96 benchmark with a nonlinear observation operator, it outperforms existing assimilation methods while running faster.More accurate atmospheric states could lead to earlier and more reliable warnings of extreme weather events. The same framework also applies to any sequential, nonlinear state-filtering problem, broadening the reach of physics-guided Bayesian inference across the sciences."
Poster,DeepCrossAttention: Supercharging Transformer Residual Connections,https://ICML.cc//virtual/2025/poster/44325,"Mike Heddes, Adel Javanmard, Kyriakos Axiotis, Thomas Fu, MohammadHossein Bateni, Vahab Mirrokni","Transformer networks have achieved remarkable success across diverse domains, leveraging a variety of architectural innovations, including residual connections. However, traditional residual connections, which simply sum the outputs of previous layers, can dilute crucial information. This work introduces DeepCrossAttention (DCA), an approach that enhances residual learning in transformers. DCA employs learnable, input-dependent weights to dynamically combine layer outputs, enabling the model to selectively focus on the most relevant information in any of the previous layers. Furthermore, DCA incorporates depth-wise cross-attention, allowing for richer interactions between layers at different depths. Our language modeling experiments show that DCA achieves improved perplexity for a given training time. Moreover, DCA obtains the same model quality up to 3x faster while adding a negligible number of parameters (e.g., 0.2\%). Theoretical analysis confirms that DCA provides an improved trade-off between accuracy and model size when the ratio of collective layer ranks to the ambient dimension falls below a critical threshold.","Powerful AI models, known as Transformers, are constructed with many processing layers stacked sequentially. A standard technique to aid learning in these deep models involves residual connections, which simply sum the outputs from previous layers. The problem is that this straightforward addition can cause crucial information from earlier layers to be progressively diluted, diminishing its influence in later stages of computation.We introduce DeepCrossAttention (DCA), a method that creates more sophisticated connections between layers. Instead of uniformly summing outputs, DCA uses learnable, dynamic weights to intelligently combine information. This allows the model to assess the relevance of outputs from all preceding layers and selectively amplify the most critical information for its current processing step.This targeted approach to information flow allows our models to learn significantly faster, achieving the same level of performance up to three times more quickly than standard models. This efficiency gain is accomplished with a negligible increase in the model's size (less than 0.2% more parameters). Our work enables the development of more powerful and efficient AI systems."
Poster,Deep Electromagnetic Structure Design Under Limited Evaluation Budgets,https://ICML.cc//virtual/2025/poster/44375,"Shijian Zheng, Fangxiao Jin, Shuhai Zhang, Quan Xue, Mingkui Tan","Electromagnetic structure (EMS) design plays a critical role in developing advanced antennas and materials, but remains challenging due to high-dimensional design spaces and expensive evaluations. While existing methods commonly employ high-quality predictors or generators to alleviate evaluations, they are often data-intensive and struggle with real-world scale and budget constraints. To address this, we propose a novel method called Progressive Quadtree-based Search (PQS). Rather than exhaustively exploring the high-dimensional space, PQS converts the conventional image-like layout into a quadtree-based hierarchical representation, enabling a progressive search from global patterns to local details. Furthermore, to lessen reliance on highly accurate predictors, we introduce a consistency-driven sample selection mechanism. This mechanism quantifies the reliability of predictions, balancing exploitation and exploration when selecting candidate designs. We evaluate PQS on two real-world engineering tasks, i.e., Dual-layer Frequency Selective Surface and High-gain Antenna. Experimental results show that our method can achieve satisfactory designs under limited computational budgets, outperforming baseline methods. In particular, compared to generative approaches, it cuts evaluation costs by 75∼85%, effectively saving 20.27∼38.80 days of product designing cycle.","Electromagnetic structure (EMS) is essential to modern wireless communication, yet finding a satisfactory design often resembles searching for a needle in a haystack, where each trial is expensive and time-consuming. Existing methods are often data-intensive and struggle with real-world scale and budget constraints. To address this, we introduce Progressive Quadtree-based Search (PQS), a method that progressively explores the design space from global patterns down to local details, avoiding exhaustive trial-and-error. To curb data requirements, PQS incorporates a strategy that balances exploration and exploitation when selecting candidate designs.We evaluated PQS on two real-world industrial challenges and achieved target performance within tight computational budgets. Compared with state-of-the-art generative approaches, PQS cut evaluation costs by 75∼85%, shortening the product-design cycle by 20∼39 days."
Poster,Deep Fuzzy Multi-view Learning for Reliable Classification,https://ICML.cc//virtual/2025/poster/44822,"Siyuan Duan, Yuan Sun, Dezhong Peng, Guiduo Duan, Xi Peng, Peng Hu","Multi-view learning methods primarily focus on enhancing decision accuracy but often neglect the uncertainty arising from the intrinsic drawbacks of data, such as noise, conflicts, etc. To address this issue, several trusted multi-view learning approaches based on the Evidential Theory have been proposed to capture uncertainty in multi-view data. However, their performance is highly sensitive to conflicting views, and their uncertainty estimates, which depend on the total evidence and the number of categories, often underestimate uncertainty for conflicting multi-view instances due to the neglect of inherent conflicts between belief masses. To accurately classify conflicting multi-view instances and precisely estimate their intrinsic uncertainty, we present a novel Deep \underline{Fu}zzy \underline{M}ulti-View \underline{L}earning (\textbf{FUML}) method. Specifically, FUML leverages Fuzzy Set Theory to model the outputs of a classification neural network as fuzzy memberships, incorporating both possibility and necessity measures to quantify category credibility. A tailored loss function is then proposed to optimize the category credibility. To further enhance uncertainty estimation, we propose an entropy-based uncertainty estimation method leveraging category credibility. Additionally, we develop a Dual Reliable Multi-view Fusion (DRF) strategy that accounts for both view-specific uncertainty and inter-view conflict to mitigate the influence of conflicting views in multi-view fusion. Extensive experiments demonstrate that our FUML achieves state-of-the-art performance in terms of both accuracy and reliability.","Environmental factors, such as sensor failure, adverse weather conditions, and data communication issues, often introduce noisy and unaligned views in multi-view data, i.e., create conflicting views. How to correctly classify such conflicting multi-view instances and accurately estimate the corresponding uncertainty is a very realistic and critical issue. To address this challenge, we propose the powerful Deep Fuzzy Multi-view Learning method (FUML), which is based on the Fuzzy Set Theory. Our FUML can achieve trusted and reliable multi-view classification, thereby contributing to safety-critical fields such as video surveillance, medical detection, and autonomous driving."
