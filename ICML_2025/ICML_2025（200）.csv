type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Non-Stationary Predictions May Be More Informative: Exploring Pseudo-Labels with a Two-Phase Pattern of Training Dynamics,https://ICML.cc//virtual/2025/poster/44953,"Hongbin Pei, Jingxin Hai, Yu Li, Huiqi Deng, Denghao Ma, Jie Ma, Pinghui Wang, Jing Tao, Xiaohong Guan","Pseudo-labeling is a widely used strategy in semi-supervised learning. Existing methods typically select predicted labels with high confidence scores and high training stationarity, as pseudo-labels to augment training sets. In contrast, this paper explores the pseudo-labeling potential of predicted labels that **do not** exhibit these characteristics. We discover a new type of predicted labels suitable for pseudo-labeling, termed *two-phase labels*, which exhibit a two-phase pattern during training: *they are initially predicted as one category in early training stages and switch to another category in subsequent epochs.* Case studies show the two-phase labels are informative for decision boundaries. To effectively identify the two-phase labels, we design a 2-*phasic* metric that mathematically characterizes their spatial and temporal patterns. Furthermore, we propose a loss function tailored for two-phase pseudo-labeling learning, allowing models not only to learn correct correlations but also to eliminate false ones. Extensive experiments on eight datasets show that **our proposed 2-*phasic* metric acts as a powerful booster** for existing pseudo-labeling methods by additionally incorporating the two-phase labels, achieving an average classification accuracy gain of 1.73% on image datasets and 1.92% on graph datasets.","Using model predictions to label unlabeled data—commonly known as pseudo-labeling—is an effective strategy for improving model performance. However, the key challenge lies in selecting reliable pseudo labels to incorporate into the training set.To address this issue, we analyze training dynamics, which refer to the model’s output over successive training epochs. Our research identifies a previously overlooked type of sample that is well-suited for pseudo-labeling, characterized by two-phase training dynamics: during the early stages of training, the model’s predictions consistently converge on one class, while in later stages, the model’s predictions stabilize in a different class. To capture and utilize this behavior, we propose a 2-*phasic* metric that integrates both temporal and spatial perspectives to efficiently identify such samples.Through experiments on node classification and image classification tasks, we demonstrate that these samples are not only accurately predicted, but also contain richer class-discriminative information. Our findings reveal a new category of pseudo-label candidates that existing methods tend to ignore, allowing our approach to serve as a booster to current pseudo-labeling algorithms. Furthermore, our work highlights the untapped potential of non-stationary training dynamics as a valuable signal in semi-supervised learning."
Poster,No-Regret is not enough! Bandits with General Constraints through Adaptive Regret Minimization,https://ICML.cc//virtual/2025/poster/43646,"Martino Bernasconi, Matteo Castiglioni, Andrea Celli","In the bandits with knapsacks framework (BwK) the learner has $m$ resource-consumption (i.e., packing) constraints. We focus on the generalization of BwK in which the learner has a set of general long-term constraints. The goal of the learner is to maximize their cumulative reward, while at the same time achieving small cumulative constraints violations. In this scenario, there exist simple instances where conventional methods for BwK fail to yield sublinear violations of constraints. We show that it is possible to circumvent this issue by requiring the primal and dual algorithm to be weakly adaptive. Indeed, even without any information on the Slater's parameter $\rho$ characterizing the problem, the interaction between weakly adaptive primal and dual regret minimizers leads to a ``self-bounding'' behavior of dual variables. In particular, their norm remains suitably upper bounded across the entire time horizon even without explicit projection steps. By exploiting this property, we provide best-of-both-worlds guarantees for stochastic and adversarial inputs. In the first case, we show that the algorithm guarantees sublinear regret. In the latter case, we establish a tight competitive ratio of $\rho/(1+\rho)$. In both settings, constraints violations are guaranteed to be sublinear in time. Finally, this results allow us to obtain new result for the problem of contextual bandits with linear constraints, providing the first no-$\alpha$-regret guarantees for adversarial contexts.","This research focuses on a type of decision-making problem where an agent must make a series of choices to get the best possible results while satisfying certain constraints, such as not using too many resources over time. The paper presents a new approach that exploits adaptivity to circumvent limitations of previous approaches."
Poster,Normalizing Flows are Capable Generative Models,https://ICML.cc//virtual/2025/poster/46564,"Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista Martin, Navdeep Jaitly, Joshua M Susskind","Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at https://github.com/apple/ml-tarflow.","Normalizing flows are a classical unsupervised learning algorithm. They enjoy many unique properties, such as exact training loss, both encoding and decoding mode and free likelihood. However, they have been largely forgotten in the modern generative AI era. In this paper, we propose recipes to train normalizing flows and improve their performance to an unprecedented level: state of the art likelihood estimation, as well as image generation quality comparable to those of diffusion models. The way we achieve it is by proposing a carefully designed Transformer based architecture, as well as techniques such as Gaussian noise augmented training, score based desnoising and guidance. Our method is considerably simpler than existing designs and also enjoys stable and scalable training. We believe that this work proves that normalizing flows should be treated as a serious contender to other more popular methods such as Diffusion Models and discrete Autoregressive models. We also made our code https://github.com/apple/ml-tarflow available which provides a foundation for future explorations."
Poster,No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks,https://ICML.cc//virtual/2025/poster/44704,"Attila Szász, Balázs Bánhelyi, Mark Jelasity","The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound.","AI algorithms that can recognize the content of an image can often be very easily misled by a skilled attacker. Such an attacker can design invisible modifications of the image so that the algorithm makes a mistake. For this reason, it is important to carefully examine AI algorithms and prove that they cannot be attacked this way. This is called formal verification.In our work, we argue that formal verification becomes much harder if one considers the technical details of how the computations of an AI algorithm are implemented in the real world, where they run on multiple complex GPUs simultaneously, using non-exact arithmetic. This means that the result of the algorithm might be different every time it is run, even if the input is the same.We show that the formal verification methods that have been proposed up to now all fail in the real world. This is because they focus on an idealized theoretical model of the algorithms. We demonstrate this by designing AI algorithms that are reported to be safe by all the known formal verifiers, yet the algorithms might act maliciously in practice."
Poster,Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks,https://ICML.cc//virtual/2025/poster/44890,"Lukas Braun, Erin Grant, Andrew Saxe","A foundational principle of connectionism is that perception, action, and cognition emerge from parallel computations among simple, interconnected units that generate and rely on neural representations. Accordingly, researchers employ multivariate pattern analysis to decode and compare the neural codes of artificial and biological networks, aiming to uncover their functions. However, there is limited analytical understanding of how a network’s representation and function relate, despite this being essential to any quantitative notion of underlying function or functional similarity. We address this question using fully analysable two-layer linear networks and numerical simulations in nonlinear networks. We find that function and representation are dissociated, allowing representational similarity without functional similarity and vice versa. Further, we show that neither robustness to input noise nor the level of generalisation error constrain representations to the task. In contrast, networks robust to parameter noise have limited representational flexibility and must employ task-specific representations. Our findings suggest that representational alignment reflects computational advantages beyond functional alignment alone, with significant implications for interpreting and comparing the representations of connectionist systems","Artificial neural networks are computer systems inspired by the brain, consisting of interconnected units that process information to perform tasks like recognizing images. Scientists often use these systems as models to try to understand how biological brains work. In particular, they often compare the behaviour and internal activities of two different systems—for example, an artificial network next to a biological neural network—when placed in the same environments and performing the same or very similar tasks.When doing so, scientists commonly assume that if two neural networks perform the same task, they must organize and process information in similar ways internally. We mathematically analyzed how simple neural networks can solve identical problems to test this assumption. Our analysis revealed something surprising: networks can achieve exactly the same performance on a task while using completely different ways of organizing information internally. This means that what a network does and how it organizes information can be very much disconnected.However, we discovered an important exception. Networks that can handle errors well—particularly disruptions to their internal organization—tend to develop specialized internal patterns for particular tasks. This suggests there are advantages to certain types of internal organization that go beyond just solving the original task. We advocate that future studies should investigate whether this is true of the artificial neural networks people use in practice, as well as of biological brains."
Poster,Not All Tokens Matter All The Time: Dynamic Token Aggregation Towards Efficient Detection Transformers,https://ICML.cc//virtual/2025/poster/46036,"Jiacheng Cheng, Xiwen Yao, Xiang Yuan, Junwei Han","The substantial computational demands of detection transformers (DETRs) hinder their deployment in resource-constrained scenarios, with the encoder consistently emerging as a critical bottleneck. A promising solution lies in reducing token redundancy within the encoder. However, existing methods perform static sparsification while ignoring the varying importance of tokens across different levels and encoder blocks for object detection, leading to suboptimal sparsification and performance degradation. In this paper, we propose **Dynamic DETR** (**Dynamic** token aggregation for **DE**tection **TR**ansformers), a novel strategy that leverages inherent importance distribution to control token density and performs multi-level token sparsification. Within each stage, we apply a proximal aggregation paradigm for low-level tokens to maintain spatial integrity, and a holistic strategy for high-level tokens to capture broader contextual information. Furthermore, we propose center-distance regularization to align the distribution of tokens throughout the sparsification process, thereby facilitating the representation consistency and effectively preserving critical object-specific patterns. Extensive experiments on canonical DETR models demonstrate that Dynamic DETR is broadly applicable across various models and consistently outperforms existing token sparsification methods.","Detection Transformers represent the detection community’s pioneering response to the rise of transformers. Despite demonstrating formidable dominance, this paradigm demands substantial computational resources and is frequently criticized for its slow inference speed, particularly in low-resource scenarios. We set out to make these models more efficient by reducing the number of image pieces (known as *tokens*) that the encoder of the system has to process. While past methods remove tokens in a fixed way, we found that not all tokens are equally important at all stages of the model. Ignoring this leads to unnecessary information loss and worse detection results.Our approach, called Dynamic DETR, learns to keep more important tokens and discard less useful ones dynamically, adjusting across pyramid levels and stages. It also groups similar tokens together in meaningful ways and ensures the model keeps a consistent understanding of where objects are. This makes object detection models faster and lighter while still keeping their accuracy high to the greatest extent possible, thereby advancing the real-world deployment of cutting-edge AI."
Poster,Not All Wrong is Bad: Using Adversarial Examples for Unlearning,https://ICML.cc//virtual/2025/poster/46097,"Ali Ebrahimpour-Boroojeny, Hari Sundaram, Varun Chandrasekaran","Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on ""exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, ""approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random 10% of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing.","The paper presents Adversarial Machine Unlearning (AMUN), a computationally efficient framework that enables trained models to expunge designated training instances without the expense of full retraining. For each data point slated for removal AMUN finds a proximate adversarial example—a deliberately misclassified input—and conducts a brief fine-tuning phase on these modified samples with their wrong labels. This procedure perturbs the model’s decision boundary only in the local vicinity of the targeted points, markedly reducing its confidence on them while preserving overall performance on the remaining data. Empirical evaluations demonstrate that, even under rigorous privacy attacks designed to detect traces of the forgotten data, the resulting model behaves comparably to one trained ab initio without those records, surpassing prior approximate-unlearning approaches. Another significant advantage of AMUN is that it is effective even when there is no access to the remaining samples."
Poster,No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces,https://ICML.cc//virtual/2025/poster/45291,"Daniel Marczak, Simone Magistri, Sebastian Cygert, Bartłomiej Twardowski, Andrew Bagdanov, Joost van de Weijer","Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains.  In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance on vision and language tasks across various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training.","Imagine having several AI experts, each trained for a specific job (like one identifying cats, another recognizing handwritten numbers). We want to combine these experts into a single AI that can perform all these jobs well. However, simply mixing their knowledge together often results in a combined AI that doesn't excel at any particular task, performing worse than the original experts.Our research uncovers a key reason why this happens and offers a better way to merge these AI experts. We found that it's crucial how the internal ""learnings"" or adjustments of each expert align with each other. If they point in compatible directions, the merged AI performs much better. Based on this, we developed a new method. First, we make these internal learnings more ""balanced"" and uniform (we call this ""isotropic""), which helps them align better and boosts performance. Then, we go a step further by carefully preserving not only the common knowledge shared across all tasks but also the unique, specific knowledge from each individual expert.Our combined approach creates a significantly more capable AI for handling multiple tasks, often achieving top-tier results across various scenarios, all without needing any additional, costly retraining. This helps ensure ""no task is left behind"" when combining specialized AI models."
Poster,Novelty Detection in Reinforcement Learning with World Models,https://ICML.cc//virtual/2025/poster/43561,"Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Wei Zhou, Robert Wright, Mark Riedl","Reinforcement learning (RL) using world models has found significant recent successes.However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline.We refer to the sudden change in visual properties or state transitions as novelties.Implementing novelty detection within generated world model frameworks is a crucialtask for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents by utilizing the misalignment of the world model's hallucinated states and the true observed states as a novelty score.  We provideeffective approaches to detecting novelties in a distribution of transitions learned by an agent ina world model. Finally, we show the advantage ofour work in a novel environment compared to traditional machine learning novelty detection methods as well as currently accepted RL-focused novelty detection algorithms.","Reinforcement learning (RL) agents are becoming more capable by using internal models—called world models—to predict and understand their environments. However, when something in the environment suddenly changes, like its appearance or how it behaves, these agents can perform poorly. Such unexpected changes are known as novelties.To help RL agents handle these surprises, we propose a simple and effective method for novelty detection. Our approach compares what the agent expects to happen (based on its internal world model) with what actually happens in the real world. The greater the mismatch, the more likely it is that something novel has occurred.We test our method in a variety of virtual environments, including MiniGrid, Atari games, and the DeepMind Control Suite. To simulate real-world surprises, we introduce changes using modified versions of these environments, such as NovGrid, HackAtari, and the RealWorldRL Suite. These allow us to evaluate how well our method detects unexpected changes.Our results show that our approach outperforms both traditional machine learning novelty detection methods and those specifically designed for RL, making it a promising tool for building more reliable and adaptable agents."
Poster,NTK-DFL: Enhancing Decentralized Federated Learning in Heterogeneous Settings via Neural Tangent Kernel,https://ICML.cc//virtual/2025/poster/44433,"Gabriel Thompson, Kai Yue, Chau-Wai Wong, Huaiyu (David) Dai","Decentralized federated learning (DFL) is a collaborative machine learning framework for training a model across participants without a central server or raw data exchange. DFL faces challenges due to statistical heterogeneity, as participants often possess data of different distributions reflecting local environments and user behaviors. Recent work has shown that the neural tangent kernel (NTK) approach, when applied to federated learning in a centralized framework, can lead to improved performance. We propose an approach leveraging the NTK to train client models in the decentralized setting, while introducing a synergy between NTK-based evolution and model averaging. This synergy exploits inter-client model deviation and improves both accuracy and convergence in heterogeneous settings. Empirical results demonstrate that our approach consistently achieves higher accuracy than baselines in highly heterogeneous settings, where other approaches often underperform. Additionally, it reaches target performance in 4.6 times fewer communication rounds. We validate our approach across multiple datasets, network topologies, and heterogeneity settings to ensure robustness and generalization. Source code for NTK-DFL is available at https://github.com/Gabe-Thomp/ntk-dfl}{https://github.com/Gabe-Thomp/ntk-dfl","Collaborative training of machine learning models is gaining traction as large, ChatGPT-like models take an unprecedented amount of computing resources to train. Particularly, individuals may want to collaboratively train models without explicitly sharing their personal data, and without the need for a larger, central server. In other words, models may be trained in a decentralized fashion.This introduces the problem of data heterogeneity: different users have different kinds of data! Your mom may love pictures of cats, while your brother only takes pictures of dogs and parrots. If we were training an animal classifier on these images, the variation of data across devices can hinder training.  In our paper, we investigate a training algorithm that replaces the typical training approach with a new one. We use a mathematical tool called the neural tangent kernel. In plain English, this tool allows the user to share more expressive data in the training process. By sharing better data, model training is enhanced despite the data variation discussed prior. Also, users must communicate with over fewer rounds than in previously proposed algorithms. Lastly, we provide open-source code for the research community to test and build upon our algorithm."
