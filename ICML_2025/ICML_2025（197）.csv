type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Neural Guided Diffusion Bridges,https://ICML.cc//virtual/2025/poster/46489,"Gefan Yang, Frank van der Meulen, Stefan Sommer","We propose a novel method for simulating conditioned diffusion processes (diffusion bridges) in Euclidean spaces. By training a neural network to approximate bridge dynamics, our approach eliminates the need for computationally intensive Markov Chain Monte Carlo (MCMC) methods or reverse-process modeling. Compared to existing methods, it offers greater robustness across various diffusion specifications and conditioning scenarios. This applies in particular to rare events and multimodal distributions, which pose challenges for score-learning- and MCMC-based approaches. We propose a flexible variational family for approximating the diffusion bridge path measure which is  partially specified by a neural network. Once trained, it enables efficient independent sampling at a cost comparable to sampling the unconditioned (forward) process.","We introduce a new way to simulate how random systems evolve over time when they are required to reach a specific outcome. This approach uses a neural network to learn these patterns, allowing us to avoid older, slower methods that require a lot of computer power. Our method works well even in difficult situations—like when rare events happen or when there are many possible outcomes—and it remains efficient and reliable. Once trained, it can quickly generate realistic simulations without much extra cost."
Poster,Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery,https://ICML.cc//virtual/2025/poster/45666,"Ning Liu, Yue Yu","Attention mechanisms have emerged as transformative tools in core AI domains such as natural language processing and computer vision. Yet, their largely untapped potential for modeling intricate physical systems presents a compelling frontier. Learning such systems often entails discovering operators that map between functional spaces using limited instances of function pairs---a task commonly framed as a severely ill-posed inverse PDE problem. In this work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator architecture that builds upon and enhances Nonlocal Attention Operators (NAO) in both predictive accuracy and computational efficiency. NIPS employs a linear attention mechanism to enable scalable learning and integrates a learnable kernel network that acts as a channel-independent convolution in Fourier space. As a consequence, NIPS eliminates the need to explicitly compute and store large pairwise interactions, effectively amortizing the cost of handling spatial interactions into the Fourier transform. Empirical evaluations demonstrate that NIPS consistently surpasses NAO and other baselines across diverse benchmarks, heralding a substantial leap in scalable, interpretable, and efficient physics learning. Our code and data accompanying this paper are available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.","In many physical problems, being able to predict the system responses is not enough. To make sure that the predictions are trustworthy, we are also interested in knowing the mechanism behind these responses. Our work is trying to design an accurate and efficient learning algorithm, which discovers both the physical system responses and the underlying mechanism. The key is a linear attention mechanism together with a learnable kernel network that acts in the Fourier space.We demonstrate the performance of our model, NIPS, on multiple material modeling examples. The model is capable to predict the material deformation on a new and unseen microstructure, and also recover the underlying microstructure."
Poster,Neural Representational Consistency Emerges from Probabilistic Neural-Behavioral Representation Alignment,https://ICML.cc//virtual/2025/poster/45281,"Yu Zhu, Chunfeng Song, Wanli Ouyang, Shan Yu, Tiejun Huang","Individual brains exhibit striking structural and physiological heterogeneity, yet neural circuits can generate remarkably consistent functional properties across individuals, an apparent paradox in neuroscience. While recent studies have observed preserved neural representations in motor cortex through manual alignment across subjects, the zero-shot validation of such preservation and its generalization to more cortices remain unexplored. Here we present PNBA (Probabilistic Neural-Behavioral Representation Alignment), a new framework that leverages probabilistic modeling to address hierarchical variability across trials, sessions, and subjects, with generative constraints preventing representation degeneration. By establishing reliable cross-modal representational alignment, PNBA reveals robust preserved neural representations in monkey primary motor cortex (M1) and dorsal premotor cortex (PMd) through zero-shot validation. We further establish similar representational preservation in mouse primary visual cortex (V1), reflecting a general neural basis. These findings resolve the paradox of neural heterogeneity by establishing zero-shot preserved neural representations across cortices and species, enriching neural coding insights and enabling zero-shot behavior decoding.","Our brains are as unique as our fingerprints, with significant differences in structure and neuronal activity from person to person. Yet mysteriously, these diverse brain circuits produce remarkably similar functions across individuals. This presents a fascinating paradox: how can such different neural systems create such consistent outputs?Our research tackles this problem by developing a new computational method called PNBA (Probabilistic Neural-Behavioral Representation Alignment). This approach helps us detect shared patterns in neural activity across different individuals, despite variations that occur naturally over time and between subjects.Using this approach, we discovered that the low-dimensional representations of neural population activity are remarkably preserved across individuals in both monkey and mouse brain regions, including those cortices responsible for movement and vision. This finding not only resolves the paradox of how structurally different brains can produce consistent behaviors but also suggests potential applications for brain-computer interfaces that may require less individual calibration, which could improve the development of neural technologies."
Poster,Neural Solver Selection for Combinatorial Optimization,https://ICML.cc//virtual/2025/poster/44604,"Chengrui Gao, Haopu Shang, Ke Xue, Chao Qian","Machine learning has increasingly been employed to solve NP-hard combinatorial optimization problems, resulting in the emergence of neural solvers that demonstrate remarkable performance, even with minimal domain-specific knowledge. To date, the community has created numerous open-source neural solvers with distinct motivations and inductive biases. While considerable efforts are devoted to designing powerful single solvers, our findings reveal that existing solvers typically demonstrate complementary performance across different problem instances. This suggests that significant improvements could be achieved through effective coordination of neural solvers at the instance level. In this work, we propose the first general framework to coordinate the neural solvers, which involves feature extraction, selection model, and selection strategy, aiming to allocate each instance to the most suitable solvers. To instantiate, we collect several typical neural solvers with state-of-the-art performance as alternatives, and explore various methods for each component of the framework. We evaluated our framework on two typical problems, Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP). Experimental results show that our framework can effectively distribute instances and the resulting composite solver can achieve significantly better performance (e.g., reduce the optimality gap by 0.88% on TSPLIB and 0.71% on CVRPLIB) than the best individual neural solver with little extra time cost.","Machine learning is increasingly used to tackle complex combinatorial optimization problems, with neural solvers showing impressive results even without deep expertise. In this paper, we first reveal that different neural solvers demonstrate complementary strengths and propose a framework to coordinate multiple neural solvers by selecting the best solver for each problem instance. Testing on two optimization problems, our approach significantly outperforms individual solvers while adding minimal extra runtime, paving the way for more efficient solutions to real-world challenges like logistics, transportation, and resource allocation."
Poster,NeuronTune: Towards Self-Guided Spurious Bias Mitigation,https://ICML.cc//virtual/2025/poster/43955,"Guangtao Zheng, Wenqian Ye, Aidong Zhang","Deep neural networks often develop spurious bias, reliance on correlations between non-essential features and classes for predictions. For example, a model may identify objects based on frequently co-occurring backgrounds rather than intrinsic features, resulting in degraded performance on data lacking these correlations. Existing mitigation approaches typically depend on external annotations of spurious correlations, which may be difficult to obtain and are not relevant to the spurious bias in a model. In this paper, we take a step towards self-guided mitigation of spurious bias by proposing NeuronTune, a post hoc method that directly intervenes in a model's internal decision process. Our method probes in a model's latent embedding space to identify and regulate neurons that lead to spurious prediction behaviors. We theoretically justify our approach and show that it brings the model closer to an unbiased one. Unlike previous methods, NeuronTune operates without requiring spurious correlation annotations, making it a practical and effective tool for improving model robustness. Experiments across different architectures and data modalities demonstrate that our method significantly mitigates spurious bias in a self-guided way.","Modern AI models, like deep neural networks, can sometimes make decisions based on the wrong cues, such as focusing on a background instead of the object itself. This kind of prediction behavior, known as spurious bias, can lead to poor performance when the usual patterns don't appear. Fixing this problem often requires extra information about what the model is doing wrong, which is not always available or reliable. In our work, we introduce NeuronTune, a method that helps models correct these biases on their own. Instead of relying on outside labels, NeuronTune looks inside the model to find the parts responsible for biased decisions and adjusts them directly. We provide theoretical support for our method and show that it makes models more fair and reliable. Tested across different types of data and models, NeuronTune effectively reduces spurious bias without needing extra human guidance."
Poster,Neurosymbolic World Models for Sequential Decision Making,https://ICML.cc//virtual/2025/poster/43925,"Leonardo Hernandez Cano, Maxine Perroni-Scharf, Neil Dhir, Arun Ramamurthy, Armando Solar-Lezama","We present Structured World Modeling for Policy Optimization (SWMPO), a framework for unsupervised learning of neurosymbolic Finite State Machines (FSM) that capture environmental structure for policy optimization. Traditional unsupervised world modeling methods rely on unstructured representations, such as neural networks, that do not explicitly represent high-level patterns within the system (e.g., patterns in the dynamics of regions such as \emph{water} and \emph{land}).Instead, SWMPO models the environment as a finite state machine (FSM), where each state corresponds to a specific region with distinct dynamics. This structured representation can then be leveraged for tasks like policy optimization. Previous works that synthesize FSMs for this purpose have been limited to discrete spaces, not continuous spaces. Instead, our proposed FSM synthesis algorithm operates in an unsupervised manner, leveraging low-level features from unprocessed, non-visual data, making it adaptable across various domains. The synthesized FSM models are expressive enough to be used in a model-based Reinforcement Learning scheme that leverages offline data to efficiently synthesize environment-specific world models.We demonstrate the advantages of SWMPO by benchmarking its environment modeling capabilities in simulated environments.","We tackle the problem of automatically discovering and exploiting 'high-level structure' from robot sensors.For instance, consider a robot that operates in (say) two conditions: water and land; we are interested in using sensor data to automatically build a representation (i.e., model) of the robot's environment that is structured into two 'parts', one of for each condition.To this end, we describe a system designed to learn this type of structured model, and then show that this representation can be used to efficiently build models of new environments (by reusing the 'parts') and to train the robot 'in simulation' (i.e., using the model)."
Poster,NeuroTree: Hierarchical Functional Brain Pathway Decoding for Mental Health Disorders,https://ICML.cc//virtual/2025/poster/44671,"Jun-En Ding, Dongsheng Luo, Chenwei Wu, Feng Liu","Mental disorders are among the most widespread diseases globally. Analyzing functional brain networks through functional magnetic resonance imaging (fMRI) is crucial for understanding mental disorder behaviors. Although existing fMRI-based graph neural networks (GNNs) have demonstrated significant potential in brain network feature extraction, they often fail to characterize complex relationships between brain regions and demographic information in mental disorders. To overcome these limitations, we propose a learnable NeuroTree framework that integrates a $k$-hop AGE-GCN with neural ordinary differential equations (ODEs) and contrastive masked functional connectivity (CMFC) to enhance similarities and dissimilarities of brain region distance. Furthermore, NeuroTree effectively decodes fMRI network features into tree structures, which improves the capture of high-order brain regional pathway features and enables the identification of hierarchical neural behavioral patterns essential for understanding disease-related brain subnetworks. Our empirical evaluations demonstrate that NeuroTree achieves state-of-the-art performance across two distinct mental disorder datasets. It provides valuable insights into age-related deterioration patterns, elucidating their underlying neural mechanisms.  The code and datasets are available at https://github.com/Ding1119/NeuroTree.","Mental disorders are among the most widespread diseases globally. In this work, we propose a novel framework called NuroTree that contributes to computational neuroscience by integrating demographic information into Neural ODEs for brain network modeling via $k$-hop graph convolution, investigating addiction and schizophrenia datasets to decode fMRI signals and construct disease-specific brain trees with hierarchical functional subnetworks, and achieving state-of-the-art classification performance while effectively interpreting how these disorders alter functional connectivity related to brain age."
Poster,Neutral residues: revisiting adapters for model extension,https://ICML.cc//virtual/2025/poster/44454,"Franck TALLA, Edouard Grave, Herve Jegou","We address the problem of extending a pre-trained large language model to a new domain that was not seen during training. Standard techniques, such as fine-tuning or low-rank adaptation (LoRA) are successful at domain adaptation, but do not formally add capacity to the model. This often leads to a trade-off, between performing well on the new domain vs. degrading performance on the original domain.Here, we propose to revisit and improve adapters to extend LLMs. Our paper analyzes this extension problem from three angles: data, architecture and training procedure, which are advantageously considered jointly. The resulting method, called neutral residues, modifies adapters in a way that leads to each new residual block to output near-zeros on the original domain. This solution leads to strong results when adapting a state-of-the-art model originally trained on English to a new language. Neutral residues significantly outperforms competing approaches such as fine-tuning, LoRA or vanilla adapters in terms of the trade-off between learning the new language and not forgetting English.","Large language models (LLMs) are powerful tools trained on enormous amounts of text. But when we want them to take on something new—like a different language or a specialized area they weren’t originally trained on—it’s not practical to retrain them from scratch. That process is expensive, slow, and uses a lot of energy. A common shortcut is to “finetune” the model on new data, but this often makes the model forget what it already knew—like losing its English skills when learning a new language.Our research introduces a method called neutral residues, which helps models learn new things without interfering with their original abilities. Instead of changing the whole system, we add small, specialized parts that only activate for the new domain and stay “silent” otherwise. Because we only train these small additions, the process is much cheaper than retraining the full model.This makes it easier, more affordable, and more sustainable to adapt powerful models. It opens the door for smaller organizations to personalize cutting-edge AI—and helps reduce the environmental impact of training entirely new systems."
Poster,New Bounds for Sparse Variational Gaussian Processes,https://ICML.cc//virtual/2025/poster/45368,Michalis Titsias,"Sparse variational Gaussian processes (GPs) construct tractable posterior approximations to GP  models. At the core of these methods is the assumption that the true posterior distribution over training function values ${\bf f}$ and inducing variables ${\bf u}$ is approximated by a variational distribution that incorporates the conditional GP prior $p({\bf f} | {\bf u})$ in its factorization. While  this assumption is considered as fundamental, we show that for model training we can relax it through the use of a more general variational distribution $q({\bf f} | {\bf u} )$ that depends on $N$ extra parameters, where  $N$ is the number of training examples. In GP regression, we can analytically optimize the evidence lower bound over the extra parameters and express a tractable collapsed bound that is tighter than the previous bound. The new bound is also amenable to stochastic optimization and its implementation requires minor modifications to existing sparse GP code. Further, we also describe extensions to non-Gaussian likelihoods. On several datasets we demonstrate that our method can reduce  bias when learning the hyperparameters and can lead to better predictive performance.","Modeling uncertainty is one of the key challenges in Machine Learning. For regression and function approximation problems,  Gaussian processes (GPs) provide a Bayesian nonparametric (i.e., memory-based) framework to estimate unknown functions by providing also uncertainty estimates. However, the complexity of these models scales cubically with the number of training examples, so for large datasets exact computations are prohibitive. In this work we elaborate on scalable GP methods that construct approximations based on smaller sets of special points called inducing points. More precisely, we improve a certain type of scalable GP method based on a posterior (variational) approximation of the model."
Poster,NextCoder: Robust Adaptation of Code LMs to Diverse Code Edits,https://ICML.cc//virtual/2025/poster/46548,"Tushar Aggarwal, Swayam Singh, Abhijeet Awasthi, Aditya Kanade, Nagarajan Natarajan","Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation and general problem-solving abilities post adaptation. We opensource the models, synthetic dataset, and implementation at http://aka.ms/nextcoder.","We often need to update or change computer programs by editing their code, but today’s AI code assistants struggle to handle the wide variety of edits people want. In this work, we created two key innovations to help: a method for generating lots of realistic code-edit examples and a smart way to train models on them without losing their existing abilities.First, we built a pipeline that takes sample code (written in different styles) to generate many high-quality code-edit examples. Then, we designed an approach called SeleKT that carefully adjusts the model’s weights, focusing only on the parts most useful for editing while keeping its general programming skills intact.Our adapted models, called NextCoder, performed impressively on multiple code-editing tests, even beating larger models. We’re sharing the models, dataset, and tools at http://aka.ms/nextcoder so others can explore and use them for their tasks."
