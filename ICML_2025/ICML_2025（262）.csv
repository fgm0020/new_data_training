type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,RUN: Reversible Unfolding Network for Concealed Object Segmentation,https://ICML.cc//virtual/2025/poster/43768,"Chunming He, Rihan Zhang, Fengyang Xiao, Chengyu Fang, Longxiang Tang, Yulun Zhang, Linghe Kong, Deng-Ping Fan, Kai Li, Sina Farsiu","Concealed object segmentation (COS) is a challenging problem that focuses on identifying objects that are visually blended into their background. Existing methods often employ reversible strategies to concentrate on uncertain regions but only focus on the mask level, overlooking the valuable of the RGB domain. To address this, we propose a Reversible Unfolding Network (RUN) in this paper. RUN formulates the COS task as a foreground-background separation process and incorporates an extra residual sparsity constraint to minimize segmentation uncertainties. The optimization solution of the proposed model is unfolded into a multistage network, allowing the original fixed parameters to become learnable. Each stage of RUN consists of two reversible modules: the Segmentation-Oriented Foreground Separation (SOFS) module and the Reconstruction-Oriented Background Extraction (ROBE) module. SOFS applies the reversible strategy at the mask level and introduces Reversible State Space to capture non-local information. ROBE extends this to the RGB domain, employing a reconstruction network to address conflicting foreground and background regions identified as distortion-prone areas, which arise from their separate estimation by independent modules. As the stages progress, RUN gradually facilitates reversible modeling of foreground and background in both the mask and RGB domains, reducing false-positive and false-negative regions. Extensive experiments demonstrate the superior performance of RUN and underscore the promise of unfolding-based frameworks for COS and other high-level vision tasks. Code is available at https://github.com/ChunmingHe/RUN.","This paper tackles the problem of finding hidden objects in images—objects that blend into the background and are hard to detect, even for humans. Most current methods try to fix this by focusing only on certain areas of the object mask, ignoring helpful information in the actual image colors. To improve this, we introduce a new method called RUN, which separates the image into foreground (the object) and background in a step-by-step way. Our method looks at both the object mask and the image itself to better tell the difference between what should be kept and what should be ignored. It does this by reusing information and gradually improving its guesses in each step. As a result, it makes fewer mistakes and finds hidden objects more accurately. We tested our method on several datasets, and it performed better than other approaches. We plan to release the code to the public."
Poster,Runtime Analysis of Evolutionary NAS for Multiclass Classification,https://ICML.cc//virtual/2025/poster/44728,"Zeqiong Lv, Chao Qian, Yun Liu, Jiahao Fan, Yanan Sun","Evolutionary neural architecture search (ENAS) is a key part of evolutionary machine learning, which commonly utilizes evolutionary algorithms (EAs) to automatically design high-performing deep neural architectures. During past years, various ENAS methods have been proposed with exceptional performance. However, the theory research of ENAS is still in the infant. In this work, we step for the runtime analysis, which is an essential theory aspect of EAs, of ENAS upon multiclass classification problems. Specifically, we first propose a benchmark to lay the groundwork for the analysis. Furthermore, we design a two-level search space, making it suitable for multiclass classification problems and consistent with the common settings of ENAS. Based on both designs, we consider (1+1)-ENAS algorithms with one-bit and bit-wise mutations, and analyze their upper and lower bounds on the expected runtime. We prove that the algorithm using both mutations can find the optimum with the expected runtime upper bound of $O(rM\ln{rM})$ and lower bound of $\Omega(rM\ln{M})$. This suggests that a simple one-bit mutation may be greatly considered, given that most state-of-the-art ENAS methods are laboriously designed with the bit-wise mutation. Empirical studies also support our theoretical proof.","Building effective neural (network) architectures is a core challenge in machine learning. Evolutionary neural architecture search (ENAS) addresses this by evolving high-performing deep neural architectures with evolutionary algorithms. While ENAS works well in practice, its theoretical understanding—especially how long it takes to find the optimum—remains limited.We study the runtime of ENAS on multiclass classification tasks. We first build a new benchmark and design a two-level search space to reflect realistic ENAS settings. Then, we analyze the runtime of simple ENAS algorithms using different mutations.Our analysis shows that a simple one-bit mutation may be greatly considered, given that most state-of-the-art ENAS methods are laboriously designed with the bit-wise mutation. This can deepen our theoretical understanding of ENAS and offer practical guidance for designing faster and simpler ENAS algorithms."
Poster,RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization,https://ICML.cc//virtual/2025/poster/45120,"XUCHEN, Yuxuan Yue, Zukang Xu, Xing Hu, JiangyongYu, Zhixuan Chen, Sifan Zhou, Zhihang Yuan, Dawei Yang","RWKV is a modern RNN architecture with comparable performance to Transformer, but still faces challenges when deployed to resource-constrained devices. Post Training Quantization (PTQ), which is a an essential technique to reduce model size and inference latency, has been widely used in Transformer models.However, it suffers significant degradation of performance when applied to RWKV.This paper investigates and identifies two key constraints inherent in the properties of RWKV:  (1) Non-linear operators hinder the parameter-fusion of both smooth- and rotation-based quantization, introducing extra computation overhead. (2) The larger amount of uniformly distributed weights poses challenges for cluster-based quantization, leading to reduced accuracy.To this end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively selecting different quantization approaches by assessing the uniformity and identifying outliers in the weights, and (2) a codebook optimization algorithm that enhances the performance of cluster-based quantization methods for element-wise multiplication in RWKV.Experiments show that RWKVQuant can quantize RWKV-6-14B into about 3-bit with less than 1\% accuracy loss and 2.14$\times$ speed up.","RWKV, a modern RNN-Transformer hybrid architecture with performance comparable to Transformers, faces challenges in edge deployment due to its large size and high computational cost. Existing post-training quantization (PTQ) methods, effective for Transformers, underperform on RWKV for two key reasons: its non-linear operations (such as Token Shift and Sigmoid) disrupt parameter fusion, while its uniformly distributed weights undermine clustering-based quantization. To address this, we introduce RWKVQuant, the first PTQ framework for RWKV, featuring a coarse-to-fine proxy mechanism that uses entropy and higher-order moments to adaptively select vector or scalar quantization by identifying weight outliers, and a codebook optimization for element-wise multiplications via activation-weighted KMeans. Experiments show RWKVQuant quantizes RWKV-6-14B to ~3-bit with <1% accuracy loss, 2.14$\times$ faster inference, 1/3 memory usage, and outperforms GPTQ/AWQ in language/vision tasks, enabling practical edge deployment for RWKV and new lightweight solutions for large models."
Poster,RZ-NAS: Enhancing LLM-guided Neural Architecture Search via Reflective Zero-Cost Strategy,https://ICML.cc//virtual/2025/poster/46224,"Zipeng Ji, Guanghui Zhu, Chunfeng Yuan, Yihua Huang","LLM-to-NAS is a promising field at the intersection of Large Language Models (LLMs) and Neural Architecture Search (NAS), as recent research has explored the potential of architecture generation leveraging LLMs on multiple search spaces. However, the existing LLM-to-NAS methods face the challenges of limited search spaces, time-cost search efficiency, and uncompetitive performance across standard NAS benchmarks and multiple downstream tasks. In this work, we propose the Reflective Zero-cost NAS (RZ-NAS) method that can search NAS architectures with humanoid reflections and training-free metrics to elicit the power of LLMs. We rethink LLMs’ roles in NAS in current work and design a structured, prompt-based to comprehensively understand the search tasks and architectures from both text and code levels. By integrating LLM reflection modules, we use LLM-generated feedback to provide linguistic guidance within architecture optimization. RZ-NAS enables effective search within both micro and macro search spaces without extensive time cost, achieving SOTA performance across multiple downstream tasks.","We introduce a novel framework that combines the text- and code-level comprehension capabilities of LLMs with a Reflective Zero-Cost evaluation strategy for neural architecture search (NAS). To integrate the text- and code-level understanding abilities of LLMs, we develop structured prompts to precisely define NAS tasks. These prompts include: a high-level role, detailed instructions, an in-context example, and the key reflective module. Moreover, we utilize Zero-Cost proxies instead of training architectures to reduce computational resources and time cost while maintaining competitive performance. The reflective module guides the LLM to reflect on mutation performance and generates targeted suggestions for further iteration improvements."
Poster,S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking,https://ICML.cc//virtual/2025/poster/43683,"Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, Haiyang Sun, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang","3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception. Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task. However, existing methods are still in the early stages of development and lack systematic improvements, failing to track objects in certain complex scenarios, like occlusions and the small size of target object’s situations. In this paper, we first summarize the current end-to-end 3D MOT framework by decomposing it into three constituent parts: query initialization, query propagation, and query matching. Then we propose corresponding improvements, which lead to a strong yet simple tracker: S2-Track. Specifically, for query initialization, we present 2D-Prompted Query Initialization, which leverages predicted 2D object and depth information to prompt an initial estimate of the object’s 3D location. For query propagation, we introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty of complex environment in object prediction with probabilistic attention. For query matching, we propose a Hierarchical Query Denoising strategy to enhance training robustness and convergence. As a result, our S2-Track achieves state-of-the-art performance on nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st place on the nuScenes tracking task leaderboard.","Tracking multiple moving objects in 3D—such as cars, bicycles, and pedestrians—is an important part of how self-driving cars understand the world around them. Recently, new methods have been developed that try to detect and follow these objects all at once, using a single, unified system. These new systems are promising, but they still struggle in challenging situations—like when objects are hidden from view or very small.In our research, we propose a new method called **S2-Track** that improves this type of all-in-one tracking system to make it more accurate and reliable in real-world driving situations. Our approach focuses on three key areas: starting the tracking more precisely, better handling of uncertain or confusing situations, and making the training process more effective.Thanks to these changes, our system sets a new record on a widely used self-driving car benchmark and ranks 1st on its tracking leaderboard."
Poster,S4S: Solving for a Fast Diffusion Model Solver,https://ICML.cc//virtual/2025/poster/46229,"Eric Frankel, Sitan Chen, Jerry Li, Pang Wei Koh, Lillian Ratliff, Sewoong Oh","Diffusion models (DMs) create samples from a data distribution by starting from random noise and iteratively solving a reverse-time ordinary differential equation (ODE).Because each step in the iterative solution requires an expensive neural function evaluation (NFE), there has been significant interest in approximately solving these diffusion ODEs with only a few NFEs without modifying the underlying model.However, in the few NFE regime, we observe that tracking the true ODE evolution is fundamentally impossible using traditional ODE solvers.In this work, we propose a new method that learns a good solver for the DM, which we call **S**olving **for** the **S**olver (**S4S**).S4S directly optimizes a solver to obtain good generation quality by learning to match the output of a strong teacher solver.We evaluate S4S on six different pre-trained DMs, including pixel-space and latent-space DMs for both conditional and unconditional sampling.In all settings, S4S uniformly improves the sample quality relative to traditional ODE solvers. Moreover, our method is lightweight, data-free, and can be plugged in black-box on top of any discretization schedule or architecture to improve performance.Building on top of this, we also propose **S4S-Alt**, which optimizes both the solver and the discretization schedule.By exploiting the full design space of DM solvers, with 5 NFEs, we achieve an FID of 3.73 on CIFAR10 and 13.26 on MS-COCO, representing a $1.5\times$ improvement over previous training-free ODE methods.","Diffusion models can generate realistic images by starting with pure noise and gradually turning it into a meaningful image. This process is typically slow because it normally takes a lot of steps, and each step requires running a large model.People have been trying to speed this up by using fewer steps, but that usually hurts the quality of the final image. In this work, we introduce a new method called *S*olving *for* the *S*olver (S4S). Instead of relying on traditional ways to speed things up, S4S /learns/ how to make the process more efficient by mimicking what a slower, high-quality method does.We tested S4S on six different kinds of image generators, and in every case, it improved the quality of the generated images without needing extra data. We also introduce S4S-Alt, which interleaves S4S with another routine for learning better discretization of time. By exploiting the full design space of DM solvers, with just 5 steps, we achieve an FID of 3.73 on CIFAR10 and 13.26 on MS-COCO, representing a $1.5\times$ improvement over previous training-free ODE methods."
Poster,"Sable: a Performant, Efficient and Scalable Sequence Model for MARL",https://ICML.cc//virtual/2025/poster/46076,"Omayma Mahjoub, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon Du Toit, Jemma Daniel, Louay Nessir, Louise Beyers, Juan Formanek, Liam Clark, Arnu Pretorius","As multi-agent reinforcement learning (MARL) progresses towards solving larger and more complex problems, it becomes increasingly important that algorithms exhibit the key properties of (1) strong performance, (2) memory efficiency, and (3) scalability. In this work, we introduce Sable, a performant, memory-efficient, and scalable sequence modelling approach to MARL. Sable works by adapting the retention mechanism in Retentive Networks (Sun et al., 2023) to achieve computationally efficient processing of multi-agent observations with long context memory for temporal reasoning. Through extensive evaluations across six diverse environments, we demonstrate how **Sable is able to significantly outperform existing state-of-the-art methods in a large number of diverse tasks (34 out of 45 tested)**. Furthermore, Sable maintains performance as we scale the number of agents, handling environments with more than a thousand agents while exhibiting a linear increase in memory usage. Finally, we conduct ablation studies to isolate the source of Sable's performance gains and confirm its efficient computational memory usage. **All experimental data, hyperparameters, and code for a frozen version of Sable used in this paper are available on our website:** https://sites.google.com/view/sable-marl. **An improved and maintained version of Sable is available in Mava:** https://github.com/instadeepai/Mava.","In Cooperative Multi-Agent Reinforcement Learning (MARL), we train a team of agents to work together toward a shared goal. This is essential for complex real-world systems such as in robot warehouses, where a group of robots must navigate shared spaces efficiently. However, as the number of agents grows and tracking historical information becomes increasingly important, conventional methods often struggle due to rising computational costs and the complexity of maintaining effective coordination.In our work, we introduce **Sable**, a novel sequence modeling algorithm for MARL, designed specifically to handle environments that require temporal memory and a large number of agents, while remaining computationally efficient. Unlike attention-based methods, which struggle to retain historical information and become increasingly computationally costly as the number of agents grows, Sable uses a retention-based mechanism.  This mechanism allows Sable to efficiently process long sequences and scale to environments with hundreds of agents, all while maintaining strong performance and manageable computational cost.**Our extensive experiments across 45 diverse tasks show that Sable significantly outperforms existing state-of-the-art MARL algorithms.** It remains effective even as the number of agents scales into the hundreds, something that has remained out of reach for many methods. By expanding the capacity of multi-agent memory without sacrificing computational efficiency, Sable opens the door to more intelligent and scalable AI systems for real-world cooperative challenges."
Poster,SADA: Stability-guided Adaptive Diffusion Acceleration,https://ICML.cc//virtual/2025/poster/45161,"Ting Jiang, Yixiao Wang, Hancheng Ye, Zishan Shao, Jingwei Sun, Jingyang Zhang, Zekai Chen, Jianyi Zhang, Yiran Chen, Hai Li","Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic‐attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose **Stability-guided Adaptive Diffusion Acceleration (SADA)**, a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver.Comprehensive evaluations on SD‐2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods.Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by \(1.8\times\) with \(\sim 0.01\) spectrogram LPIPS.Our code is available at: [https://github.com/Ting-Justin-Jiang/sada-icml](https://github.com/Ting-Justin-Jiang/sada-icml).","(1) Generating from AI models requires many computational steps, which consume a lot of time and energy.  (2) To tackle this challenge, we built SADA, a plug-in to accelerate these generative AI models without degrading their generation quality. (3) This can significantly improve the efficiency of professional workflows, and our method can be adapted to Image, Music, Audio, and potentially Video generation."
Poster,SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability,https://ICML.cc//virtual/2025/poster/43918,"Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Demian Till, Matthew Wearden, Arthur Conmy, Samuel Marks, Neel Nanda","Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised proxy metrics with unclear practical relevance. We introduce SAEBench, a comprehensive evaluation suite that measures SAE performance across eight diverse metrics, spanning interpretability, feature disentanglement and practical applications like unlearning. To enable systematic comparison, we open-source a suite of over 200 SAEs across seven recently proposed SAE architectures and training algorithms. Our evaluation reveals that gains on proxy metrics do not reliably translate to better practical performance.  For instance, while Matryoshka SAEs slightly underperform on existing proxy metrics, they substantially outperform other architectures on feature disentanglement metrics; moreover, this advantage grows with SAE scale. By providing a standardized framework for measuring progress in SAE development, SAEBench enables researchers to study scaling trends and make nuanced comparisons between different SAE architectures and training methodologies. Our interactive interface enables researchers to flexibly visualize relationships between metrics across hundreds of open-source SAEs at www.neuronpedia.org/sae-bench","Researchers often struggle to understand how large language models, like GPT, internally represent language. Sparse autoencoders (SAEs) are tools designed to help make sense of these internal representations by identifying meaningful patterns (""features"") within the model's activations. Recently, several new SAE designs have emerged, each claiming different strengths. However, comparing these methods is tricky because they are usually evaluated on simplified metrics whose practical value is unclear.We introduce SAEBench, a new, comprehensive benchmark that evaluates SAEs across eight diverse metrics, covering interpretability, disentanglement (clearly separating different features), and real-world tasks like selectively removing information (""unlearning""). We open-source more than 200 SAEs spanning seven prominent architectures to enable systematic comparisons.Interestingly, we find approaches that seem weaker under traditional simplified evaluations, like the Matryoshka SAE, actually perform substantially better on our broader suite of practical metrics—in particular, Matryoshka SAEs excel at clearly disentangling different features, a crucial capability that improves further as models scale. SAEBench thus helps researchers understand SAE strengths and limitations in realistic scenarios, driving meaningful progress in interpretability research."
Poster,SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders,https://ICML.cc//virtual/2025/poster/46380,"Bartosz Cywiński, Kamil Deja","Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Our evaluation shows that SAeUron outperforms existing approaches on the UnlearnCanvas benchmark for concepts and style unlearning, and effectively eliminates nudity when evaluated with I2P. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content under adversarial attack. Code and checkpoints are available at [GitHub]( https://github.com/cywinski/SAeUron).","Powerful image generation models can produce unwanted content, like inappropriate images, raising significant safety and ethical concerns. While we can try to prevent this by adjusting the model's parameters, such an approach often makes it difficult to understand the specific changes being introduced within the model itself. **Could we instead directly edit a model's internal workings in a transparent way?**To achieve this, we train a neural network to decompose a model's internal representations into a set of human-interpretable features. This allows us to pinpoint and specifically block only the features responsible for an unwanted concept – for instance, to prevent the model from generating 'cats,' we can identify and block its internal 'cat paw' and 'whisker' features. Because we only target these relevant features, the model's overall performance remains largely unaffected.This approach not only gives us more control over what image generation models produce but also proves more effective at removing unwanted concepts than existing methods. Crucially, we can actually see and understand which internal features of the model are being modified, offering genuine control. Our work demonstrates how techniques that help us understand these complex models can be directly applied to solve significant real-world challenges."
