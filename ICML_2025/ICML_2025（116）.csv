type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"From Low Rank Gradient Subspace Stabilization to Low-Rank Weights: Observations, Theories, and Applications",https://ICML.cc//virtual/2025/poster/44501,"Ajay Jaiswal, Yifan Wang, Lu Yin, Shiwei Liu, Runjin Chen, Jiawei Zhao, Ananth Grama, Yuandong Tian, Zhangyang “Atlas” Wang","Large Language Models (LLMs) matrices can often be expressed in low-rank format with potential to relax memory and compute resource requirements. Unlike previous works which pivot around developing novel matrix decomposition algorithms, in this work we focus to study the emerging non-uniform low-rank properties across weight matrices in LLMs through the lens of stabilizing gradient subspace. \textit{Firstly,} we provide a theoretical framework to understand the stabilization of gradient subspaces through Hessian analysis. \textit{Secondly,} we empirically establish a consequential relationship between the gradient dynamics and low-rank expressiveness of weight matrices. Our findings reveal that different LLM components exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, we present \textit{Weight Low-Rank Projection} \textbf{(WeLore)} that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. Our gradient dynamics perspective illustrate that \textit{LRCs tend to have better finetuning capabilities} and their standalone finetuning can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. All codes and checkpoints will be released.","Large Language Models (LLMs) are extremely powerful but require enormous memory and computing resources, making them expensive and difficult to deploy widely. While researchers know these models can sometimes be compressed by representing their huge weight matrices in a simpler, “low-rank” form, it’s not clear why this low-rank structure emerges or how best to exploit it for better low-rank LLM compression without hurting performance. We discovered that different parts of LLMs naturally develop varying degrees of low-rank structure during training, closely tied to how the model’s gradients (the signals guiding learning) stabilize over time. Building on this, we created WeLore: a method that automatically analyzes each part of a pre-trained LLM to decide how much it can be compressed, focusing on those components that are truly low-rank. WeLore then compresses these parts and, when fine-tuning the model for new tasks, updates only the most compressible components—saving memory and computation."
Poster,"From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models",https://ICML.cc//virtual/2025/poster/43465,"Etowah Adams, Liam Bai, Minji Lee, Yiyang Yu, Mohammed AlQuraishi","Protein language models (pLMs) are powerful predictors of protein structure and function, learning through unsupervised training on millions of protein sequences. pLMs are thought to capture common motifs in protein sequences, but the specifics of pLM features are not well understood. Identifying these features would not only shed light on how pLMs work, but potentially uncover novel protein biology––studying the model to study the biology. Motivated by this, we train sparse autoencoders (SAEs) on the residual stream of a pLM, ESM-2. By characterizing SAE features, we determine that pLMs use a combination of generic features and family-specific features to represent a protein. In addition, we demonstrate how known sequence determinants of properties such as thermostability and subcellular localization can be identified by linear probing of SAE features. For predictive features without known functional associations, we hypothesize their role in unknown mechanisms and provide visualization tools to aid their interpretation. Our study gives a better understanding of the limitations of pLMs, and demonstrates how SAE features can be used to help generate hypotheses for biological mechanisms. We release our code, model weights, and feature visualizer.","Protein language models (pLMs) are models trained on large amounts of protein data. They are effective at making predictions on protein structure and function, but it is not clear how: which patterns do they rely on?To explore this, we used a method called sparse autoencoders (SAEs) to simplify and understand the complex features learned by ESM-2, a popular pLM. We showed that some patterns discovered by SAEs match known protein characteristics. The models rely on a mix of general patterns common to many proteins and specialized patterns unique to certain families of proteins. For other patterns whose roles aren’t yet known, we developed visualization tools to help researchers interpret them and form new scientific hypotheses.Our work helps clarify the strengths and weaknesses of protein language models and shows how studying their internal features can lead to new biological discoveries."
Poster,From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?,https://ICML.cc//virtual/2025/poster/45603,"Zhanke Zhou, Xiao Feng, Zhaocheng Zhu, Jiangchao Yao, Sanmi Koyejo, Bo Han","While existing benchmarks probe the reasoning abilities of large language models (LLMs) across diverse domains, they predominantly assess passive reasoning, providing models with all the information needed to reach a solution. By contrast, active reasoning—where an LLM must interact with external systems to acquire missing evidence or data—has received little systematic attention. To address this shortfall, we present AR-Bench, a novel benchmark designed explicitly to evaluate an LLM’s active reasoning skills. AR-Bench comprises three task families—detective cases, situation puzzles, and guessing numbers—that together simulate real-world, agentic scenarios and measure performance across commonsense, logical, and symbolic reasoning challenges.Empirical evaluation on AR-Bench demonstrates that contemporary LLMs exhibit pronounced difficulties with active reasoning: they frequently fail to acquire or leverage the information needed to solve tasks. This gap highlights a stark divergence between their passive and active reasoning abilities. Moreover, ablation studies indicate that even advanced strategies, such as tree-based searching or post-training approaches, yield only modest gains and fall short of the levels required for real-world deployment. Collectively, these findings highlight the critical need to advance methodology for active reasoning, e.g., incorporating interactive learning, real-time feedback loops, and environment-aware objectives for training.The benchmark is publicly available at: https://github.com/tmlr-group/AR-Bench.","Large language models (LLMs) are often tested on their ability to reason using complete information, but real-world situations frequently require them to actively seek out missing data to solve problems. This gap in evaluating how well LLMs can handle such ""active reasoning"" scenarios—where they must interact with external systems to gather necessary information. We want to investigate their performance in these more dynamic, real-world-like challenges.We created AR-Bench, a new benchmark specifically designed to test LLMs’ active reasoning skills. It includes three types of tasks—detective cases, situation puzzles, and guessing numbers—that mimic real-world scenarios requiring commonsense, logical, and symbolic reasoning. We tested various LLMs on these tasks and explored advanced strategies, like tree-based searching and post-training methods, to see if they could improve the models’ ability to gather and use missing information effectively.Our findings show that current LLMs struggle significantly with active reasoning, revealing a clear gap between their ability to process given information and their capacity to seek out what’s missing. This matters because real-world applications, like decision-making or problem-solving in dynamic environments, often require such skills. Our work highlights the need for new training approaches to make LLMs more capable and reliable in practical, proactive scenarios."
Poster,From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection,https://ICML.cc//virtual/2025/poster/45880,"Moritz Vandenhirtz, Julia Vogt","Understanding the decision-making process of machine learning models provides valuable insights into the task, the data, and the reasons behind a model's failures. In this work, we propose a method that performs inherently interpretable predictions through the instance-wise sparsification of input images. To align the sparsification with human perception, we learn the masking in the space of semantically meaningful pixel regions rather than on pixel-level. Additionally, we introduce an explicit way to dynamically determine the required level of sparsity for each instance. We show empirically on semi-synthetic and natural image datasets that our inherently interpretable classifier produces more meaningful, human-understandable predictions than state-of-the-art benchmarks.","(1) Often, a user does not know why Machine Learning models make a certain prediction. (2) We develop a method that only uses a small part of an image for its prediction. (3) With this method, a user knows which parts of the image were used to make a prediction."
Poster,From RAG to Memory: Non-Parametric Continual Learning for Large Language Models,https://ICML.cc//virtual/2025/poster/45585,"Bernal Jimenez Gutierrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu Su","Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vectorembeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.","Humans are excellent at learning continuously—integrating new experiences into memory and drawing on them later. In contrast, today's large language models (LLMs) are static: they can’t incorporate new information unless retrained on massive datasets. To work around this, LLMs often rely on external search tools to access up-to-date knowledge. While effective in simple cases, this approach falters as the volume of information grows and tasks become more complex.Our new system, HippoRAG 2, tackles this limitation by giving LLMs a more human-like memory. It combines existing methods for structuring new information into a web of entities and relations between them **a knowledge graph** with a powerful algorithm called **Personalized PageRank**, which ranks different parts of the graph based on their relevance to a user's query. Unlike prior methods which also allow LLMs to structure new information, which sometimes hurt performance in simpler tasks, HippoRAG 2 improves LLM performance across both simple and complex tasks. Though challenges remain, this approach marks a step toward LLMs that can learn continuously and efficiently, without needing full retraining."
Poster,From Spectrum-free towards Baseline-view-free: Double-track Proximity Driven Multi-view Clustering,https://ICML.cc//virtual/2025/poster/46260,"Shengju Yu, Dong Zhibin, Siwei Wang, Suyuan Liu, KE LIANG, Xinwang Liu, Yue Liu, Yi Zhang","Current multi-view clustering (MVC) techniques  generally focus only on the  relationship between anchors and samples, while overlooking that between anchors. Moreover, due to the lack of data labels, the cluster order is inconsistent across views and accordingly anchors encounter misalignment,  which will confuse the graph structure and disorganize cluster representation. Even worse, it typically brings variance during forming spectral embedding, degenerating the stability of clustering results. In response to these concerns, in the paper we propose a MVC approach named DTP-SF-BVF. Concretely, we explicitly exploit the geometric properties between anchors via  self-expression learning skill, and utilize topology learning strategy to feed captured anchor-anchor features into anchor-sample graph  so as to explore the manifold structure hidden within samples  more adequately.  To reduce the misalignment risk, we introduce a permutation mechanism for each view to jointly rearrange anchors according to respective view  characteristics. Besides not involving selecting the baseline view, it also can coordinate with anchors in the unified framework and thereby facilitate the learning of anchors. Further, rather than forming spectrum and then performing embedding  partitioning, based on the criterion that samples and clusters should be hard assignment, we manage to construct the cluster labels directly from original samples using the binary strategy,  not only preserving the data diversity but avoiding variance. Experiments on multiple publicly available datasets confirm the effectiveness of  proposed DTP-SF-BVF method.","Current multi-view clustering (MVC) methods analyze how data points relate to reference points (""anchors"") but ignore connections between anchors themselves. Without labels, anchors may misalign across views, disrupting cluster structure and reducing result stability.To address this, we propose DTP-SF-BVF, a new MVC approach that:1. Models anchor relationships using geometric learning to better capture hidden data patterns.2. Aligns anchors across views via an adaptive permutation mechanism, avoiding manual reference selection. 3. Directly generates cluster labels from raw data (instead of spectral methods), preserving diversity while minimizing instability.Tests on real-world datasets show our method improves clustering accuracy and robustness. This work advances unsupervised learning by refining how multi-view data is structured and aligned."
Poster,From Theory to Practice: Rethinking Green and Martin Kernels for Unleashing Graph Transformers,https://ICML.cc//virtual/2025/poster/44592,"Yoon Hyeok Lee, Jaemin Park, Taejin Paik, Doyun Kim, Bosun Hwang","Graph Transformers (GTs) have emerged as a powerful alternative to message-passing neural networks, yet their performance heavily depends on effectively embedding structural inductive biases. In this work, we introduce novel structural encodings (SEs) grounded in a rigorous analysis of random walks (RWs), leveraging Green and Martin kernels that we have carefully redefined for AI applications while preserving their mathematical essence.These kernels capture the long-term behavior of RWs on graphs and allow for enhanced representation of complex topologies, including non-aperiodic and directed acyclic substructures.Empirical evaluations across eight benchmark datasets demonstrate strong performance across diverse tasks, notably in molecular and circuit domains.We attribute this performance boost to the improved ability of our kernel-based SEs to encode intricate structural information, thereby strengthening the global attention and inductive bias within GTs.This work highlights the effectiveness of theoretically grounded kernel methods in advancing Transformer-based models for graph learning.","Graph Neural Networks (GNNs) are powerful tools used to analyze data represented as graphs, such as social networks, molecules, or circuits. A key challenge in these models is how to represent the structure of a graph in a way that the model can effectively understand and use. These representations are called ""structural encodings.""Our work introduces a new way to compute structural encodings for graphs using mathematical tools known as **Green and Martin kernels**, which come from the field of probability and random processes. These kernels have been studied for decades in mathematics but were not used in machine learning models because they were too difficult to compute or apply directly.We reformulate these kernels so that they can be used efficiently in GNNs, including graph transformers—advanced models that have shown strong performance in many applications. The result is a new type of structural encoding that better captures complex patterns in graphs, especially in settings like circuit analysis or molecule prediction.Importantly, our method is both **theoretically grounded** and **practical**: it works well in real-world tasks, scales to large graphs, and can be used with existing graph models without requiring any architectural changes. We believe this work opens new directions for combining deep learning with rigorous mathematical tools, enabling more powerful and reliable models for graph-based data."
Poster,From Thousands to Billions: 3D Visual Language Grounding via Render-Supervised Distillation from 2D VLMs,https://ICML.cc//virtual/2025/poster/43636,"Ang Cao, Sergio Arnaud, Oleksandr Maksymets, Jianing Yang, Ayush Jain, Ada Martin, Vincent-Pierre Berges, Paul McVay, Ruslan Partsey, Aravind Rajeswaran, Franziska Meier, Justin Johnson, Jeong Joon Park, Alexander Sax","3D vision-language grounding faces a fundamental data bottleneck: while 2D models train on billions of images, 3D models have access to only thousands of labeled scenes--a six-order-of-magnitude gap that severely limits performance. We introduce \textbf{\emph{LIFT-GS}}, a practical distillation technique that overcomes this limitation by using differentiable rendering to bridge 3D and 2D supervision. LIFT-GS predicts 3D Gaussian representations from point clouds and uses them to render predicted language-conditioned 3D masks into 2D views, enabling supervision from 2D foundation models (SAM, CLIP, LLaMA) without requiring any 3D annotations. This render-supervised formulation enables end-to-end training of complete encoder-decoder architectures and is inherently model-agnostic.  LIFT-GS achieves state-of-the-art results with 25.7\% mAP on open-vocabulary instance segmentation (vs. 20.2\% prior SOTA) and consistent 10-30\% improvements on referential grounding tasks. Remarkably, pretraining effectively multiplies fine-tuning datasets by 2×, demonstrating strong scaling properties that suggest 3D VLG currently operates in a severely data-scarce regime. Project page: \url{https://liftgs.github.io}.","3D computer vision models-which help machines understand and interact with our 3D world- currently suffer from a severe data shortage. Unlike 2D models that can learn from billions of images, 3D models often rely on only thousands of labeled examples, making it difficult for them to perform well.Our work introduces a new method, called LIFT-GS, that uses 2D foundation models (such as SAM, CLIP, and LLaMA) to teach 3D models without needing expensive 3D labels. We do this by having our 3D model generate simple 3D primitives (called Gaussians) and then render them into 2D images that existing 2D models can supervise. This technique effectively lets the 3D model learn from the vast knowledge already stored in 2D models.With this approach, we achieved a significant boost in performance on challenging 3D tasks, such as instance segmentation and referential grounding. Our method also shows that by pretraining on 2D models, we can double the effective size of the 3D training data — a big step forward in overcoming the data bottleneck in 3D vision."
Poster,From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining,https://ICML.cc//virtual/2025/poster/44523,"Fuying Wang, Jiacheng Xu, Lequan Yu","Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and diagnosing heart diseases. However, traditional deep learning approaches for ECG analysis rely heavily on large-scale manual annotations, which are both time-consuming and resource-intensive to obtain. To overcome this limitation, self-supervised learning (SSL) has emerged as a promising alternative, enabling the extraction of robust ECG representations that can be efficiently transferred to various downstream tasks. While previous studies have explored SSL for ECG pretraining and multi-modal ECG-language alignment, they often fail to capture the multi-scale nature of ECG signals. As a result, these methods struggle to learn generalized representations due to their inability to model the hierarchical structure of ECG data. To address this gap, we introduce MELP, a novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages hierarchical supervision from ECG-text pairs. MELP first pretrains a cardiology-specific language model to enhance its understanding of clinical text. It then applies three levels of cross-modal supervision—at the token, beat, and rhythm levels—to align ECG signals with textual reports, capturing structured information across different time scales. We evaluate MELP on three public ECG datasets across multiple tasks, including zero-shot ECG classification, linear probing, and transfer learning. Experimental results demonstrate that MELP outperforms existing SSL methods, underscoring its effectiveness and adaptability across diverse clinical applications. Our code is available at https://github.com/HKU-MedAI/MELP.","Training deep learning models to analyze heart signals (ECGs) typically requires massive amounts of manually labeled data – a slow, expensive process. Existing self-supervised approaches also struggle because they treat ECGs as one uniform signal, ignoring the natural hierarchy of heartbeats and rhythms crucial for accurate diagnosis. We introduce MELP, a novel AI model that learns from unlabeled ECG-text pairs. MELP first deeply understands cardiology reports. It then aligns ECG signals with text descriptions at three clinically meaningful levels: fine-grained waveform features (token), individual heartbeats (beat), and overall rhythm patterns – mimicking how cardiologists interpret ECGs. MELP learns highly transferable ECG representations without expensive manual labels. It significantly outperforms prior methods across three public datasets, excelling at tasks like identifying new heart conditions with minimal training (""zero-shot"") and adapting efficiently to various ECG analysis jobs. This paves the way for faster, cheaper, and more adaptable AI tools for heart health monitoring."
Poster,From Uncertain to Safe: Conformal Adaptation of Diffusion Models for Safe PDE Control,https://ICML.cc//virtual/2025/poster/44969,"Peiyan Hu, Xiaowei Qian, Wenhao Deng, Rui Wang, Haodong Feng, Ruiqi Feng, Tao Zhang, Long Wei, Yue Wang, Zhi-Ming Ma, Tailin Wu","The application of deep learning for partial differential equation (PDE)-constrained control is gaining increasing attention. However, existing methods rarely consider safety requirements crucial in real-world applications. To address this limitation, we propose Safe Diffusion Models for PDE Control (SafeDiffCon), which introduce the uncertainty quantile as model uncertainty quantification to achieve optimal control under safety constraints through both post-training and inference phases. Firstly, our approach post-trains a pre-trained diffusion model to generate control sequences that better satisfy safety constraints while achieving improved control objectives via a reweighted diffusion loss, which incorporates the uncertainty quantile estimated using conformal prediction. Secondly, during inference, the diffusion model dynamically adjusts both its generation process and parameters through iterative guidance and fine-tuning, conditioned on control targets while simultaneously integrating the estimated uncertainty quantile. We evaluate SafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible fluid, and controlled nuclear fusion problem. Results demonstrate that SafeDiffCon is the only method that satisfies all safety constraints, whereas other classical and deep learning baselines fail. Furthermore, while adhering to safety constraints, SafeDiffCon achieves the best control performance. The code can be found at https://github.com/AI4Science-WestlakeU/safediffcon.","Controlling physical systems with deep learning is promising but often unsafe, because most methods ignore real-world safety constraints. We introduce SafeDiffCon, a method that uses uncertainty estimates to ensure safe and effective control of physical systems. It improves a pre-trained model to follow safety constraints and dynamically adjusts its behavior during inference. In tests on fluid and fusion control tasks, SafeDiffCon is the only method that always stays safe while also achieving the best control performance."
