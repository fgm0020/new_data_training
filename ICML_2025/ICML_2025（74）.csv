type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Diffusion Sampling Correction via Approximately 10 Parameters,https://ICML.cc//virtual/2025/poster/43536,"Guangyi Wang, Wei Peng, lijiang Li, Wenyu Chen, Yuren Cai, Song-Zhi Su","While powerful for generation, Diffusion Probabilistic Models (DPMs) face slow sampling challenges, for which various distillation-based methods have been proposed. However, they typically require significant additional training costs and model parameter storage, limiting their practicality. In this work, we propose **P**CA-based **A**daptive **S**earch (PAS), which optimizes existing solvers for DPMs with minimal additional costs. Specifically, we first employ PCA to obtain a few basis vectors to span the high-dimensional sampling space, which enables us to learn just a set of coordinates to correct the sampling direction; furthermore, based on the observation that the cumulative truncation error exhibits an ``S""-shape, we design an adaptive search strategy that further enhances the sampling efficiency and reduces the number of stored parameters to approximately 10. Extensive experiments demonstrate that PAS can significantly enhance existing fast solvers in a plug-and-play manner with negligible costs. E.g., on CIFAR10, PAS optimizes DDIM's FID from 15.69 to 4.37 (NFE=10) using only **12 parameters and sub-minute training** on a single A100 GPU. Code is available at https://github.com/onefly123/PAS.","While powerful for generation, diffusion models face slow sampling challenges. We propose **PAS**, a *plug-and-play* training paradigm designed to accelerate diffusion model sampling with *minimal costs*. PAS uses PCA to extract a few basis vectors to span the high-dimensional sampling space, allowing the correction of the sampling direction with only a set of coordinates. PAS also includes an adaptive search strategy to enhance sampling efficiency and reduce storage requirements. With only approximately 10 parameters and under an hour of training, PAS greatly improves sampling quality across various datasets, making diffusion models more practical for real-world applications."
Poster,DiffusionVLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression,https://ICML.cc//virtual/2025/poster/45061,"Junjie Wen, Yichen Zhu, Minjie Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Xiaoyu Liu, Chaomin Shen, Yaxin Peng, Feifei Feng","In this paper, we present DiffusionVLA, a novel framework that integrates autoregressive reasoning with diffusion policies to address the limitations of existing methods: while autoregressive Vision-Language-Action (VLA) models lack precise and robust action generation, diffusion-based policies inherently lack reasoning capabilities. Central to our approach is autoregressive reasoning — a task decomposition and explanation process enabled by a pre-trained VLM — to guide diffusion-based action policies. To tightly couple reasoning with action generation, we introduce a reasoning injection module that directly embeds self-generated reasoning phrases into the policy learning process. The framework is simple, flexible, and efficient, enabling seamless deployment across diverse robotic platforms.We conduct extensive experiments using multiple real robots to validate the effectiveness of DiVLA. Our tests include a challenging factory sorting task, where DiVLA successfully categorizes objects, including those not seen during training. The reasoning injection module enhances interpretability, enabling explicit failure diagnosis by visualizing the model’s decision process. Additionally, we test DiVLA on a zero-shot bin-picking task, achieving \textbf{63.7\% accuracy on 102 previously unseen objects}. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiVLA can follow novel instructions and retain conversational ability. Notably, DiVLA is data-efficient and fast at inference; our smallest DiVLA-2B runs 82Hz on a single A6000 GPU. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.","In the world of robotics, teaching robots to perform complex tasks with vision, language, and actions is a significant challenge. Exisiting methods either struggle with generating precise actions or lack the ability to reason through tasks effectively. To overcome this, we developed DiVLA, a new VLA framework that combines reasoning with action generation. DiVLA leverages a reasoning module that helps the robot break down tasks and make better decisions, even in unfamiliar situations.Our approach allows robots to understand and execute tasks more efficiently, improving their ability to work in real-world environments. For example, we tested DiVLA on a sorting task and achieved impressive results, including accurately handling new objects it had never seen before. What’s more, DiVLA can also visualize and explain its decision-making process, making it easier to understand and troubleshoot when things go wrong.This work has the potential to make robots more adaptable, interpretable, and capable of handling a variety of tasks with minimal training, making it a major step toward smarter and more versatile robotic systems."
Poster,DiLQR: Differentiable Iterative Linear Quadratic Regulator via Implicit Differentiation,https://ICML.cc//virtual/2025/poster/44176,"Shuyuan Wang, Philip D. Loewen, Michael Forbes, Bhushan Gopaluni, Wei Pan","While differentiable control has emerged as a powerful paradigm combining model-free flexibility with model-based efficiency, the iterative Linear Quadratic Regulator (iLQR) remains underexplored as a differentiable component. The scalability of differentiating through extended iterations and horizons poses significant challenges, hindering iLQR from being an effective differentiable controller. This paper introduces DiLQR, a framework that facilitates differentiation through iLQR, allowing it to serve as a trainable and differentiable module, either as or within a neural network. A novel aspect of this framework is the analytical solution that it provides for the gradient of an iLQR controller through implicit differentiation, which ensures a constant backward cost regardless of iteration, while producing an accurate gradient. We evaluate our framework on imitation tasks on famous control benchmarks. Our analytical method demonstrates superior computational performance, achieving up to $\textbf{128x}$ speedup and a minimum of $\textbf{21x}$ speedup compared to automatic differentiation. Our method also demonstrates superior learning performance ($\mathbf{10^6x}$) compared to traditional neural network policies and better model loss with differentiable controllers that lack exact analytical gradients. Furthermore, we integrate our module into a larger network with visual inputs to demonstrate the capacity of our method for high-dimensional, fully end-to-end tasks. Codes can be found on the project homepage~\url{https://sites.google.com/view/dilqr/}.","Robots and autonomous systems often rely on controllers to make decisions, but combining these controllers with modern AI (like neural networks) is challenging because they aren’t designed to be ""trainable"" like other machine learning components. Our work, DiLQR, bridges this gap by making a powerful controller called iLQR compatible with AI training methods. Unlike standard approaches that are slow or inaccurate, DiLQR computes gradients (essential for training) efficiently and exactly, enabling up to 128x faster learning. We show that DiLQR outperforms both traditional controllers and neural networks in tasks like pendulum and cartpole control. This opens doors for more adaptable and efficient AI-driven control systems."
Poster,DiMa: Understanding the Hardness of Online Matching Problems via Diffusion Models,https://ICML.cc//virtual/2025/poster/45746,"Boyu Zhang, Aocheng Shen, Bing Liu, Qiankun Zhang, Bin Yuan, Wang, Shenghao Liu, Xianjun Deng","We explore the potential of \emph{AI-enhanced combinatorial optimization theory}, taking online bipartite matching (OBM) as a case study.In the theoretical study of OBM, the \emph{hardness} corresponds to a performance \emph{upper bound} of a specific online algorithm or any possible online algorithms.Typically, these upper bounds derive from challenging instances meticulously designed by theoretical computer scientists.Zhang et al. (ICML 2024) recently provide an example demonstrating how reinforcement learning techniques enhance the hardness result of a specific OBM model.Their attempt is inspiring but preliminary.It is unclear whether their methods can be applied to other OBM problems with similar breakthroughs.This paper takes a further step by introducing DiMa, a unified and novel framework that aims at understanding the hardness of OBM problems based on denoising diffusion probabilistic models (DDPMs).DiMa models the process of generating hard instances as denoising steps, and optimizes them by a novel reinforcement learning algorithm, named \emph{shortcut policy gradient} (SPG).We first examine DiMa on the classic OBM problem by reproducing its known hardest input instance in literature.Further, we apply DiMa to two well-known variants of OBM, for which the exact hardness remains an open problem, and we successfully improve their theoretical state-of-the-art upper bounds.","In this paper, we explore the potential of AI-enhanced combinatorial optimization theory, taking online bipartite matching (OBM) as a case study. OBM is a fundamental problem in theoretical computer science (TCS), whose goal is to find a maximum matching on a gradually-released bipartite graph instance, such as matching as many drivers to riders as possible in real-time.In the theoretical study of OBM, the hardness corresponds to the performance of algorithms.Typically, these bottlenecks are derived from some challenging instances meticulously designed by theoretical computer scientists.We question whether AI could assist in reproducing (or even improving) these bottlenecks, in a way of automatically generating novel harder instances with minimal human expertise.We introduce DiMa, a diffusion-based framework that models the process of generating hard instances as denoising steps, and optimizes them by reinforcement learning.Dima successfully reproduces the known hardness of the classic OBM, and further improves the state-of-the-art for two well-known variants of OBM.We believe DiMa's great potential in AI-assisted TCS and may inspire interesting future works in other fields of TCS, such as approximation algorithms or algorithmic game theory."
Poster,DIME: Diffusion-Based Maximum Entropy Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46144,"Onur Celik, Zechu Li, Denis Blessing, Ge Li, Daniel Palenicek, Jan Peters, Georgia Chalvatzaki, Gerhard Neumann","Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges—primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). DIME leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.","Reinforcement-learning agents learn by exploring many possible actions, yet most systems still rely on simple Gaussian noise to create that exploration. This narrow choice can stunt learning on complex, high-dimensional tasks such as making a simulated dog run or a robotic hand twirl a pen.We present DIME (Diffusion-Based Maximum-Entropy RL). DIME swaps the Gaussian policy for a more expressive diffusion model—the same technology behind modern image generators—and embeds it inside the maximum-entropy RL objective that explicitly rewards exploration. We derive a new mathematical lower bound that makes the normally intractable objective computable and implement a practical version that trains end-to-end with standard deep-learning tools.Across 13 demanding simulated locomotion and manipulation benchmarks, DIME shows favorable performance over other diffusion-based baselines and outperforms leading Gaussian-policy methods on 10 of the tasks."
Poster,Dimensionality Reduction on Complex Vector Spaces for Euclidean Distance with Dynamic Weights,https://ICML.cc//virtual/2025/poster/43584,"Simone Moretti, Paolo Pellizzoni, Francesco Silvestri","The weighted Euclidean norm $||x||_w$ of a vector $x\in \mathbb{R}^d$ with weights  $w\in \mathbb{R}^d$ is the Euclidean norm where the contribution of each dimension is scaled by a given weight. Approaches to dimensionality reduction that satisfy the Johnson–Lindenstrauss (JL) lemma can be easily adapted to the weighted Euclidean distance if weights are known and fixed: it suffices to scale each dimension of the input vectors according to the weights, and then apply any standard approach. However, this is not the case when weights are unknown during the dimensionality reduction or might dynamically change. In this paper, we address this issue by providing a linear function that maps vectors into a smaller complex vector space and allows to retrieve a JL-like estimate for the weighted Euclidean distance once weights are revealed. Our results are based on the decomposition of the complex dimensionality reduction into several Rademacher chaos random variables, which are studied using novel concentration inequalities for sums of independent Rademacher chaoses.","Machine learning models typically work with high-dimensional data, such as a document represented by thousands of words or a user profile described by hundreds of preferences. To make computations faster and more efficient, researchers use dimensionality reduction: a technique to compress data into a smaller number of dimensions while preserving important information, like the distances between data points.In many real-world applications, not all features (i.e., dimensions) are equally important. For instance, in recommendation systems, some words in a document carry more weight than others. If we know the importance of each feature beforehand, we can adjust for this during dimensionality reduction. But what happens if we only find out which features are important after the data has already been compressed?This paper addresses this challenge. It introduces a novel method that reduces the dimensions of data in a way that is agnostic to future feature importance, but still allows accurate distance measurements once those weights become known. To do this, the paper leverages, as a mathematical tool, complex numbers (i.e., numbers which include the square root of -1). The proposed method compresses the original data into a complex vector space using a linear function, making it efficient and applicable at scale. Once the feature importance weights are revealed, the method applies a special function to the compressed data to recover accurate estimates of weighted distances. This work opens the door to faster, more flexible machine learning systems, especially in settings where priorities change dynamically, like personalized recommendations or real-time data analysis."
Poster,Dimension-Free Adaptive Subgradient Methods with Frequent Directions,https://ICML.cc//virtual/2025/poster/43453,"Sifan Yang, Yuanyu Wan, Peijia Li, Yibo Wang, Xiao Zhang, Zhewei Wei, Lijun Zhang","In this paper, we investigate the acceleration of adaptive subgradient methods through frequent directions (FD), a widely-used matrix sketching technique. The state-of-the-art regret bound exhibits a _linear_ dependence on the dimensionality $d$, leading to unsatisfactory guarantees for high-dimensional problems.  Additionally, it  suffers from an $O(\tau^2 d)$ time complexity per round, which scales quadratically with the sketching size $\tau$.  To overcome these issues, we first propose an algorithm named FTSL, achieving a tighter regret bound that is independent of the dimensionality. The key idea is to integrate FD with adaptive subgradient methods under _the primal-dual framework_ and add the cumulative discarded information of FD back.   To reduce its time complexity, we further utilize fast FD to expedite FTSL, yielding a better complexity of $O(\tau d)$ while maintaining the same regret bound. Moreover, to mitigate the computational cost for optimization problems involving matrix variables (e.g., training neural networks), we adapt FD to Shampoo, a popular optimization algorithm that accounts for the structure of decision, and give a novel analysis under  _the primal-dual framework_. Our proposed method obtains an improved dimension-free regret bound.  Experimental results have verified the efficiency and effectiveness of our approaches.","Adaptive subgradient methods, despite their better theoretical guarantees, are often impractical for large-scale machine learning tasks due to their high computational cost. Existing works have attempted to accelerate these methods using sketching techniques. However, their performance are significantly worse than those of standard adaptive methods, limiting their applicability in practice.In this work, we first utilize a classic sketching technique to propose an algorithm, which achieves improved theoretical guarantee. We then enhance its computational efficiency without compromising performance. Furthermore, we extend our approach to optimization problems involving matrix variables (e.g., training neural networks). By integrating our sketching technique into the existing method, we reduce the computational overhead while attaining better theoretical guarantee."
Poster,Dimension-Independent Rates for Structured Neural Density Estimation,https://ICML.cc//virtual/2025/poster/44079,"Vandermeulen, Wai Ming Tai, Bryon Aragam","We show that deep neural networks can achieve dimension-independent rates of convergence for learning structured densities typical of image, audio, video, and text data. For example, in images, where each pixel becomes independent of the rest of the image when conditioned on pixels at most $t$ steps away, a simple $L^2$-minimizing neural network can attain a rate of $n^{-1/((t+1)^2+4)}$, where $t$ is independent of the ambient dimension $d$, i.e. the total number of pixels. We further provide empirical evidence that, in real-world applications, $t$ is often a small constant, thus effectively circumventing the curse of dimensionality. Moreover, for sequential data (e.g., audio or text) exhibiting a similar local dependence structure, our analysis shows a rate of $n^{-1/(t+5)}$, offering further evidence of dimension independence in practical scenarios.","Modern machine-learning systems often work with “high-dimensional” data—think of a photo with millions of pixels and three numbers for each pixel (red, green, and blue). Classical statistics predicts that learning from such datasets would require an impossibly large number of examples, yet deep learning succeeds with far fewer in practice. Our study offers an explanation for why: Real-world data has inherent structure that can be used by neural networks to learn more efficiently with less data. The idea is that only certain parts of the data are useful for predicting other parts, and this structure is nicely captured with commonly used neural network models. We show that this structure dramatically reduces the amount of data a neural network needs, bringing it down to the same level as for much smaller problems. This provides a fresh explanation for how neural networks learn effectively from the data that is available in modern applications."
Poster,DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning,https://ICML.cc//virtual/2025/poster/46026,"Gaoyue Zhou, Hengkai Pan, Yann LeCun, Lerrel Pinto","The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, remain challenging to learn and are typically developed for task-specific solutions with online policy learning. To unlock world models' true potential, we argue that they should 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To this end, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic planning by treating goal features as prediction targets. We demonstrate that DINO-WM achieves zero-shot behavioral solutions at test time on six environments without expert demonstrations, reward modeling, or pre-learned inverse models, outperforming prior state-of-the-art work across diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.","A core ability for intelligent agents is the ability to predict the outcome of their actions on the environment. Giving machines this foresight is the goal of world models, which predict future outcomes based on current actions. However, most existing world models are hard to train, rely on hand-crafted rewards, and are tailored for one specific task at a time.We introduce DINO-WM, a new world model that is task-agnostic, can be trained entirely on offline datasets, and enables agents to reason at test time by optimizing over action sequences. DINO-WM leverages pre-trained vision encoder DINOv2 to enhance spatial understanding. This allows the model to predict directly in a compact latent space, capturing task-relevant information while avoiding the need to reconstruct raw pixels — reducing both complexity and computational cost.With this approach, DINO-WM enables zero-shot planning for unseen goals and environment configurations, such as navigating unfamiliar mazes or manipulating new object shapes. It brings us closer to building general-purpose world models that enable flexible, goal-directed behavior without additional supervision or task-specific retraining."
Poster,DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy,https://ICML.cc//virtual/2025/poster/44402,"Kaixuan Xu, Jiajun Chai, Sicheng Li, Yuqian Fu, Yuanheng Zhu, Dongbin Zhao","Diplomacy is a complex multiplayer game that re- quires both cooperation and competition, posing significant challenges for AI systems. Traditional methods rely on equilibrium search to generate extensive game data for training, which demands substantial computational resources. Large Lan- guage Models (LLMs) offer a promising alterna- tive, leveraging pre-trained knowledge to achieve strong performance with relatively small-scale fine-tuning. However, applying LLMs to Diplo- macy remains challenging due to the exponential growth of possible action combinations and the intricate strategic interactions among players. To address this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns equilib- rium policies for Diplomacy. DipLLM employs an autoregressive factorization framework to sim- plify the complex task of multi-unit action assign- ment into a sequence of unit-level decisions. By defining an equilibrium policy within this frame- work as the learning objective, we fine-tune the model using only 1.5% of the data required by the state-of-the-art Cicero model, surpassing its per- formance. Our results demonstrate the potential of fine-tuned LLMs for tackling complex strategic decision-making in multiplayer games.","*Diplomacy* is a complex board game where seven players must negotiate, form alliances, and compete to control Europe — making it especially challenging for AI to master. Traditional AI systems learned by simulating millions of matches, demanding huge amounts of data and computing power. We present DipLLM, a new AI agent built on large language models — the kind behind tools like ChatGPT. Unlike previous systems, DipLLM learns from only a small number of games. It doesn’t try to evaluate every possibility; instead, it breaks down complex decisions into simpler steps, deciding what each unit should do through step-by-step reasoning. Despite using just 1.5% of the data that top AI systems relied on, DipLLM performs even better. This shows that large language models, when fine-tuned thoughtfully, can handle complex multiplayer strategy games efficiently — opening the door to more accessible and general-purpose game-playing agents."
