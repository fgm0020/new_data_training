type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options,https://ICML.cc//virtual/2025/poster/45931,"Lakshmi Nair, Ian Trase, J. Kim","We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). Flow-of-Options enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic framework developed for autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in LLM solutions through compressed and interpretable task representations, resulting in improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art baselines. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Going beyond tabular classification and regression, we show the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our code is open-sourced at: https://github.com/flagshippioneering/Flow-of-Options.","Large Language Models (LLMs) often default to familiar patterns from their training data, limiting their ability to explore diverse or creative solutions. To tackle this limitation, we developed a new strategy called Flow-of-Options (FoO), that pushes LLMs to systematically consider multiple alternative approaches before deciding how to proceed. FoO explicitly represents each task as a network of possible options, encouraging the model to thoroughly explore different solutions rather than defaulting to biased or familiar paths. We demonstrate FoO’s practical benefits through extensive experiments on standard machine learning (ML) tasks in data science and therapeutic chemistry. Our results show that FoO significantly outperforms existing systems — improving performance by up to 69.2%, while maintaining low operational costs. We also show that FoO is versatile and can extend to a broad range of ML problems, making FoO a practical tool for enhancing LLM-based ML automation."
Poster,Flow of Reasoning: Training LLMs for Divergent Reasoning with Minimal Examples,https://ICML.cc//virtual/2025/poster/43913,"Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin","The ability to generate diverse solutions to a given problem is a hallmark of human creativity. This divergent reasoning is also crucial for machines, enhancing their robustness and enabling them to assist humans in many applications such as scientific discovery. However, existing approaches to multi-step reasoning with large language models (LLMs) have mostly focused only on reasoning accuracy, without further discovering more diverse valid solutions. For example, supervised fine-tuning improves reasoning quality but requires vast labeled data, while reward-maximizing reinforcement learning finds top-reward solutions while neglecting the solution diversity. To fill this gap, we propose Flow of Reasoning (FoR), an efficient diversity-seeking LLM finetuning method aimed at improving reasoning quality and diversity with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow on a DAG-structured reasoning graph. This formulation allows us to incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to sample divergent paths with probabilities proportional to the (unnormalized) reward of target problems. Extensive experiments show that, with limited training examples (e.g., 15 examples), FoR enables the discovery of diverse, creative, high-quality solutions, greatly outperforming a wide range of existing inference and training methods across six challenging reasoning tasks, including BlocksWorld (embodied reasoning), Game24 (math puzzle solving), Rubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math reasoning), and ProntoQA (logical reasoning). Code is available at https://github.com/Yu-Fangxu/FoR.","Currently, AI models often focus on finding just one correct answer to a problem, such as solving a puzzle with only one recognized solution, overlooking other different approaches. We explored methods to teach Large Language Models (LLMs) to discover multiple solutions for a given problem, even when provided with limited training examples.To address this, we developed a specialized training technique called ""Flow of Reasoning"" (FoR). This technique promotes diverse thinking, which enhances LLMs' abilities to assist with tasks demanding innovative, ""out-of-the-box"" solutions. This ultimately boosts their problem-solving capabilities. Our findings show that FoR significantly improves accuracy, solution diversity, and creativity when compared to existing methods.Our work formulates a general framework for LLM multi-step reasoning, allowing for straightforward adaptation to various LLM-based reasoning tasks."
Poster,Flow Q-Learning,https://ICML.cc//virtual/2025/poster/45637,"Seohong Park, Qiyang Li, Sergey Levine","We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL.","Can we leverage the power of recent generative models for reinforcement learning (RL) and robotic control? We propose a simple yet effective method that trains an RL agent using flow matching, a state-of-the-art generative model widely used in image and video generation.Using an iterative generative model for robotic control is not a trivial problem. The main challenge is that naively combining an iterative generative model with RL gives rise to an issue called ""backpropagation through time,"" which makes training unstable and costly. We resolve this issue using distillation, a technique that ""condenses"" a complex iterative procedure into a simple, one-step generation process. Our solution not only makes training stable and cost-efficient, but also leads to state-of-the-art performance on many challenging simulated robotic tasks across robotic navigation and manipulation. We expect that this technique can be applied to solving a wide range of real, challenging robotic tasks."
Poster,Fluctuations of the largest eigenvalues of transformed spiked Wigner matrices,https://ICML.cc//virtual/2025/poster/43562,"Aro Lee, Ji Oon Lee","We consider a spiked random matrix model obtained by applying a function entrywise to a signal-plus-noise symmetric data matrix. We prove that the largest eigenvalue of this model, which we call a transformed spiked Wigner matrix, exhibits Baik-Ben Arous-Péché (BBP) type phase transition. We show that the law of the fluctuation converges to the Gaussian distribution when the effective signal-to-noise ratio (SNR) is above the critical number, and to the GOE Tracy-Widom distribution when the effective SNR is below the critical number. We provide precise formulas for the limiting distributions and also concentration estimates for the largest eigenvalues, both in the supercritical and the subcritical regimes.","When a smal piece of useful information is hidden within a lot of random noise, it can be hard to distinguish the signal from the pure noise. The principal component analysis (PCA), a well-established tool in statistics based on a mathematical object known as the largest eigenvalue, is one of the most important algorithm to detect the presence of the signal from the noisy data. In this work, we explore quantatively how the features of the PCA should change if we know how the noisy data has been transformed. Our results can provide theoretical background for the understanding of noisy data naturally appearing in various fields of study, including the theoretical analysis of machine learning."
Poster,Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification,https://ICML.cc//virtual/2025/poster/44211,"Sicong Li, Qianqian Xu, Zhiyong Yang, Zitai Wang, Linchao Zhang, Xiaochun Cao, Qingming Huang","Real-world datasets often follow a long-tailed distribution, making generalization to tail classes difficult. Recent methods resorted to long-tail variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to improve generalization by flattening the loss landscape. However, these attempts face a trade-off between computational efficiency and control over the loss landscape. On the one hand, ImbSAM is efficient but offers only coarse control as it excludes head classes from the SAM process. On the other hand,  CC-SAM provides fine-grained control through class-dependent perturbations but at the cost of efficiency due to multiple backpropagations. Seeing this dilemma, we introduce Focal-SAM, which assigns different penalties to class-wise sharpness, achieving fine-grained control without extra backpropagations, thus maintaining efficiency. Furthermore, we theoretically analyze Focal-SAM's generalization ability and derive a sharper generalization bound. Extensive experiments on both traditional and foundation models validate the effectiveness of Focal-SAM.","In real-world image datasets, some categories such as ""cats"" have thousands of examples, while others like rare animals may have only a few. This imbalance, known as a long-tailed distribution, makes it difficult for AI models to learn rare categories effectively.Recent methods have tried to address this using a technique called Sharpness-Aware Minimization (SAM), which helps models generalize better by avoiding sharp regions in the loss landscape. However, existing approaches either place too much emphasis on rare classes, which harms performance on common ones, or they require intensive computation that slows down training.This paper proposes Focal-SAM, a new method that improves model performance across both common and rare categories. Focal-SAM efficiently adjusts sharpness penalty applied to each class, giving more attention to rare ones without ignoring the common ones.Experiments on several benchmark datasets show that Focal-SAM consistently outperforms other methods. This helps AI systems make better predictions across a wide range of categories, including those with limited data."
Poster,FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models,https://ICML.cc//virtual/2025/poster/44973,"Xinting Liao, Weiming Liu, Jiaming Qian, Pengyang Zhou, Jiahe Xu, Wenjie Wang, Chaochao Chen, Xiaolin Zheng, Tat-Seng Chua","Federated prompt learning (FPL) for vision-language models is a powerful approach to collaboratively adapt models across distributed clients while preserving data privacy. However, existing FPL approaches suffer from a trade-off between performance and robustness, particularly in out-of-distribution (OOD) shifts, limiting their reliability in real-world scenarios. The inherent in-distribution (ID) data heterogeneity among different clients makes it more challenging to maintain this trade-off. To fill this gap, we introduce a Federated OOD-aware Context Optimization (FOCoOp) framework, which captures diverse distributions among clients using ID global prompts, local prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of prompts to create both class-level and distribution-level separations, which adapt to OOD shifts through bi-level distributionally robust optimization. Additionally, FOCoOp improves the discrimination consistency among clients, i.e., calibrating global prompts, seemly OOD prompts, and OOD prompts by Semi-unbalanced optimal transport. The extensive experiments on real-world datasets demonstrate that FOCoOp effectively captures decentralized heterogeneous distributions and enhances robustness of different OOD shifts. The project is available at GitHub.","We aim to enhance OOD robustness for federated prompt learning on pretrained vision-language models while maintaining the performance. Thus, we introduce a Federated OOD-aware Context Optimization (FOCoOp) framework, which captures diverse distributions among clients using ID global prompts, local prompts, and OOD prompts. FOCoOp consists of two modules,  i.e., the Bi-level OOD Separations (BOS) and Global-view OOD Consistency (GOC), to enhance the OOD robustness intra- and inter-client."
Poster,"Focus On This, Not That! Steering LLMs with Adaptive Feature Specification",https://ICML.cc//virtual/2025/poster/43885,"Tom A. Lamb, Adam Davies, Alasdair J Paren, Phil Torr, Francesco Pinto","Despite the success of Instruction Tuning (IT) in training large language models (LLMs), such models often leverage spurious or biased features learnt from their training data and can become misaligned, leading to undesired behaviours. While existing techniques can steer model behaviour at inference-time, they are often post-hoc and do not embed steering as an intrinsic model feature. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across diverse benchmarks, we demonstrate that FIT: (i) successfully steers behaviour at inference time; (ii) increases robustness by amplifying core task signals and down-weighting spurious cues; (iii) mitigates social bias by suppressing demographic attributes; and (iv) generalises under distribution shifts and to previously unseen focus features. FIT therefore offers a lightweight, intrinsic mechanism for building more robust, fair, and easily controllable LLMs.","Large language models (LLMs), like those powering chatbots, are trained to follow instructions. However, further training of such models to solve specific tasks can sometimes cause them to overlook important goals such as safety or fairness. For instance, if a model is fine-tuned to be more helpful, it might start providing sensitive information or inappropriately reinforce social biases. While some existing techniques can adjust how a model acts after training, these methods are often complicated, difficult to use, and don’t let everyday users easily guide the model with plain natural language.To address this, we propose **focus-instruction-tuning (FIT)**: a new technique that lets users adaptively control a model’s responses using simple, natural language instructions at the time of use. For example, a user could include, “Answer this question without using gender stereotypes,” directly in their prompt, and the model will adapt its response accordingly, helping to correct misalignments such as gender bias that can result from training.Our findings show that FIT makes LLMs more flexible and dependable. It allows them to follow new instructions across a wide range of situations, helps reduce unwanted bias in their answers, and continues to work well even when faced with new topics or unexpected changes in data."
Poster,Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning,https://ICML.cc//virtual/2025/poster/46117,"Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang","Large Language Models (LLMs) have demonstrated remarkable abilities across various language tasks, but solving complex reasoning problems remains a significant challenge. While existing methods, such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this limitation, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT employs sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction, along with consensus-guided decision-making strategies to optimize both correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency.","Large Language Models (LLMs) have achieved impressive performance across a range of natural language tasks but still struggle with complex reasoning challenges. Methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) enhance reasoning by breaking down problems or structuring the thinking process, yet they typically rely on a single pass of reasoning. This limitation prevents them from revisiting and correcting flawed reasoning paths, leading to reduced accuracy in difficult tasks.To overcome this, we introduce Forest-of-Thought (FoT), a novel framework that integrates multiple reasoning trees operating in parallel. FoT employs sparse activation to select the most relevant reasoning paths, improving computational efficiency. It also incorporates a dynamic self-correction mechanism that allows real-time error revision and a consensus-guided strategy to determine the final output, making the reasoning process more robust and adaptive.Experimental results show that FoT significantly enhances the reasoning performance of LLMs, allowing them to solve complex logical problems with improved precision and efficiency. This work advances the capabilities of language models in handling high-level cognitive tasks and offers a scalable solution for integrating structured, self-correcting reasoning into future AI systems."
Poster,Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection,https://ICML.cc//virtual/2025/poster/44840,"Zhijing Wan, Zhixiang Wang, Zheng Wang, Xin Xu, Shin&#x27;ichi Satoh","One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.","Training deep learning models can be expensive and time-consuming, especially when working with large datasets. One way to reduce these costs is by selecting a smaller, informative subset of the data to train on. Traditionally, this requires information extractors that are specifically pre-trained on the same dataset—a process that is both inefficient and inflexible.Our research explores whether general-purpose foundation models can serve as a better alternative. We asked: Can these models help choose better data subsets, and do all foundation models perform equally well? We found that foundation models consistently outperform traditional methods on fine-grained datasets—those requiring subtle visual distinctions, like between dog breeds or bird species. However, their advantage becomes less pronounced on coarse-grained datasets that contain noisy labels. Based on these findings, we developed RAM-APL, a new method that combines multiple foundation models to leverage their complementary strengths. Our approach achieves state-of-the-art results on several fine-grained image benchmarks.This work provides practical guidance on how and when to use foundation models for data selection to make deep learning more efficient."
Poster,Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages,https://ICML.cc//virtual/2025/poster/45739,"Michael Sun, Weize Yuan, Gang Liu, Wojciech Matusik, Jie Chen","Recent data-efficient molecular generation approaches exploit graph grammars to introduce interpretability into the generative models. However, grammar learning therein relies on expert annotation or unreliable heuristics for algorithmic inference. We propose Foundation Molecular Grammar (FMG), which leverages multi-modal foundation models (MMFMs) to induce an interpretable molecular language. By exploiting the chemical knowledge of an MMFM, FMG renders molecules as images, describes them as text, and aligns information across modalities using prompt learning. FMG can be used as a drop-in replacement for the prior grammar learning approaches in molecular generation and property prediction. We show that FMG not only excels in synthesizability, diversity, and data efficiency but also offers built-in chemical interpretability for automated molecular discovery workflows. Code is available at https://github.com/shiningsunnyday/induction.","We show your browser GPT-4o can perform chemical reasoning tasks at the level of an expert with a PhD in Chemistry. We apply GPT-4o as the decision-maker within an algorithm which learns the rules of a molecular language by breaking down one example molecule at a time. The algorithm hierarchically breaks down a molecule in a step-by-step manner, merging substructures into progressively larger ones and arranging them together. Each step is done by asking GPT-4o to select one out of a grid of image cells, representing a fork in the road of the algorithm's execution. In each image cell is a molecule with 1 or 2 of its substructures highlighted for maximum visual effect. We then elicit multi-modal reasoning by chain-of-thought prompting GPT-4o to describe what structures it sees, how different substructures interact, before deciding which option is most pivotal to the overall design of the molecule. We chain the explanations from each response together to form a design story summarizing the whole execution. We repeat the execution multiple times, obtaining possibly different results each time. To resolve this, we ask GPT-4o to compare which design story is more comprehensive, and obtain final rankings by hosting a tournament where where each ``player"" is a different possible breakdown of the molecule. The higher ranked breakdowns are then pooled together to induce the rules of the language, which is a context-free graph grammar that can be sampled to generate diverse, novel molecules. We demonstrate FMG outperforms existing state-of-the-art methods on popular molecular generation benchmarks in data-expensive settings with tens to hundreds of examples. We evaluate FMG’s step-by-step reasoning via comprehensive expert provided case studies and quantitative analyses. FMG bridges the gap between elusive expert domain knowledge and emergent capabilities of MMFMs using an interpretable workflow backed by technical rigor, semantic flexibility, and expert validation."
