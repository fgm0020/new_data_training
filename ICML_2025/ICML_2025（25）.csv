type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A Simple Model of Inference Scaling Laws,https://ICML.cc//virtual/2025/poster/46402,Noam Levi,"Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference. Specifically, how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an ""inference loss"", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test the universality of our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.","Imagine an Artificial Intelligence, analogous to a student, trying to solve a complex problem that requires reasoning, such as maths or coding tasks. Sometimes the first try isn't right, but more attempts can lead to success. Our research explores how much better AI models get at tasks like coding or math simply by making multiple attempts – a concept we call 'inference scaling.' We've developed a simple mathematical idea, based on how well a model has 'memorized' information, to predict this improvement. This model helps us understand the chances of getting a correct answer after several tries (known as 'pass@k'). It shows that performance increases in a predictable way, depending on how many 'easy' versus 'hard' aspects a problem presents to the AI. Our findings match real-world observations with large language models and even simpler systems, offering a new way to think about making AI more effective and efficient without just making the models bigger or training them longer."
Poster,A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning,https://ICML.cc//virtual/2025/poster/44441,"Yaxin Hou, Yuheng Jia","This paper studies the long-tailed semi-supervised learning (LTSSL) with distribution mismatch, where the class distribution of the labeled training data follows a long-tailed distribution and mismatches with that of the unlabeled training data. Most existing methods introduce auxiliary classifiers (experts) to model various unlabeled data distributions and produce pseudo-labels, but the expertises of various experts are not fully utilized. We observe that different experts are good at predicting different intervals of samples, e.g., long-tailed expert is skilled in samples located in the head interval and uniform expert excels in samples located in the medium interval. Therefore, we propose a dynamic expert assignment module that can estimate the class membership (i.e., head, medium, or tail class) of samples, and dynamically assigns suitable expert to each sample based on the estimated membership to produce high-quality pseudo-label in the training phase and produce prediction in the testing phase. We also theoretically reveal that integrating different experts' strengths will lead to a smaller generalization error bound. Moreover, we find that the deeper features are more biased toward the head class but with more discriminative ability, while the shallower features are less biased but also with less discriminative ability. We, therefore, propose a multi-depth feature fusion module to utilize different depth features to mitigate the model bias. Our method demonstrates its effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT, and SVHN-LT datasets across various settings. The code is available at https://github.com/yaxinhou/Meta-Expert.","In many real-world applications, we often have limited labeled data, and these labeled examples are imbalanced — some classes have many samples (like “cats”), while others have very few (like “hamsters”). At the same time, we may have a large amount of unlabeled data with a different class distribution. This situation makes training accurate AI models especially difficult.Our research focuses on improving how AI learns in this challenging setting. We found that different expert models are good at labeling different kinds of data — some are better with common classes, while others handle rare classes more accurately. So, we designed a system that first estimates whether a piece of data belongs to a common, medium, or rare class, and then selects the most suitable expert model to label it. This improves the quality of the AI’s learning process.We also discovered that information from different layers of the AI model behaves differently: shallow layers are more balanced, while deeper layers are more powerful but more biased. To address this, we combine information from multiple layers to get the best of both worlds.Our method achieves better performance on several benchmark datasets and provides a more reliable way to train AI models with limited and imbalanced data."
Poster,Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models,https://ICML.cc//virtual/2025/poster/44278,"Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song","Quantized large language models (LLMs) have gained increasing attention and significance for enabling deployment in resource-constrained environments. However, emerging studies on a few calibration dataset-free quantization methods suggest that quantization may compromise the safety capabilities of LLMs, underscoring the urgent need for systematic safety evaluations and effective mitigation strategies. In this paper, we present comprehensive safety evaluations across various mainstream quantization techniques and diverse calibration datasets, utilizing widely accepted safety benchmarks. To address the identified safety vulnerabilities, we propose a quantization-aware safety patching framework, Q-resafe, to efficiently restore the safety capabilities of quantized LLMs while minimizing any adverse impact on utility.  Extensive experiment results demonstrate that Q-resafe successfully re-aligns the safety of quantized LLMs with their pre-quantization counterparts, even under challenging evaluation scenarios. Project page: https://github.com/Thecommonirin/Qresafe.","Large language models (LLMs) like ChatGPT are powerful AI systems that generate text and answer questions. However, they are so big that they require a lot of computing power, making it hard to use them on smaller devices. One solution is to compress these models, a process called quantization, which makes them more efficient. But we found that compressing LLMs can make them less safe, meaning they might generate inappropriate or harmful outputs more easily.In our work, we tested several common compression methods and found clear safety issues. To fix this, we designed a new system called Q-resafe. It acts like a safety patch, restoring the LLMs' protective filters even after compression, without sacrificing their usefulness. Our tests show that Q-resafe helps compressed models stay as safe as they were before compression, even under tough conditions. This research helps ensure that as LLMs become more widely used, they stay both efficient and responsible."
Poster,AssistanceZero: Scalably Solving Assistance Games,https://ICML.cc//virtual/2025/poster/44757,"Cassidy Laidlaw, Eli Bronstein, Timothy Guo, Dylan Feng, Lukas Berglund, Justin Svegliato, Stuart Russell, Anca Dragan","Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Code and videos are available at https://anonymous.4open.science/w/scalably-solving-assistance-games/.","We built an AI assistant that plays the game Minecraft with you: if you start building a house in Minecraft, it figures out what you’re doing and jumps in to help. Unlike AI assistants like ChatGPT, which are trained via a technique called RLHF, our assistant is trained using a different technique. We use an approach called ""assistance games,"" where our assistant learns to help a simulated user by helping them build many different houses and improving itself via trial and error. This approach encourages the assistant to communicate carefully with the user and to avoid assuming that it knows what their goal is. We compare our assistant developed using assistance games to one developed using similar techniques to ChatGPT and other AI chatbots. We find our assistant is much better at helping real people in Minecraft, allowing them to build houses with less effort."
Poster,A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models,https://ICML.cc//virtual/2025/poster/43508,"Mengyang Sun, Yihao Wang, Tao Feng, Dan Zhang, Yifan Zhu, Jie Tang","In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LoRA, to stabilize and boost its feature learning by gate-rescaled multi-space projections. We provide both a theoretical solution as well as an alternative engineering strategy. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at https://github.com/THUDM/MoELoRA_Riemannian.","Large AI models may need to be fine-tuned for new tasks. A popular fine-tuning technique called LoRA (Low-Rank Adaptation) makes fine-tuning more efficient using a pair of smaller and simpler modules. However, LoRA struggles with complex tasks due to its limited module size, and its training procedure is not optimal due to its dual-module structure. To address these, researchers propose an integration of multiple LoRA modules (MoE-LoRA) to process complex tasks; and some training behavior refiners (e.g. Riemannian Preconditioners) to adjust LoRA training procedure. However, there lacks a specific strategy for refining the training behavior of MoE-LoRA, especially considering the structure of multi-module integration is naturally more unrobust and harder for training. Based on Riemannian Preconditioners, we introduce a new method to stabilize and enhance the training behavior of MoE-LoRA by additionally weighing and adjusting how each LoRA module learns. The key idea is that the importance assigned to each LoRA expert should also influence how it updates, and should be integrated into the refiners. Our method helps MoE-LoRA learn more effectively, closing the gap between efficient fine-tuning and fully fine-tuning. Tests on different AI models and tasks confirm the approach works well and outperforms the baseline, offering a better way to adapt powerful AI models without excessive computational costs."
Poster,A Sub-Problem Quantum Alternating Operator Ansatz for Correlation Clustering,https://ICML.cc//virtual/2025/poster/45107,"Lucas Fabian Naumann, Jannik Irmai, Bjoern Andres","The Quantum Alternating Operator Ansatz (QAOA) is a hybrid quantum-classical variational algorithm for approximately solving combinatorial optimization problems on Noisy Intermediate-Scale Quantum (NISQ) devices. Although it has been successfully applied to a variety of problems, there is only limited work on correlation clustering due to the difficulty of modelling the problem constraints with the ansatz. Motivated by this, we present a generalization of QAOA that is more suitable for this problem. In particular, we modify QAOA in two ways: Firstly, we use nucleus sampling for the computation of the expected cost. Secondly, we split the problem into sub-problems, solving each individually with QAOA. We call this generalization the Sub-Problem Quantum Alternating Operator Ansatz (SQAOA) and show theoretically that optimal solutions for correlation clustering instances can be obtained with certainty when the depth of the ansatz tends to infinity. Further, we show experimentally that SQAOA achieves better approximation ratios than QAOA for correlation clustering, while using only one qubit per node of the respective problem instance and reducing the runtime (of simulations).","The Quantum Alternating Operator Ansatz (QAOA) combines classical and quantum computing to approximately solve combinatorial optimization problems. However, it is difficult to apply QAOA to the widely adopted problem of correlation clustering due to its complex constraints.Motivated by this, we present a generalization that can handle these constraints more effectively. We call this generalization the Sub-Problem Quantum Alternating Operator Ansatz (SQAOA). Two main ideas distinguish SQAOA from QAOA: First, a technique from LLMs called nucleus sampling is employed for training and interference. Second, the given problem is split into sub-problems.Using the correlation clustering problem as an example, we show that our approach retains a theoretical guarantee of optimality, requires only one qubit per element to cluster, and achieves better simulation results more efficiently than the best QAOA-based method currently known."
Poster,Asymmetric Decision-Making in Online Knowledge Distillation: Unifying Consensus and Divergence,https://ICML.cc//virtual/2025/poster/44731,"zhaowei chen, Borui Zhao, Yuchen Ge, Yuhao Chen, Renjie Song, Jiajun Liang","Online Knowledge Distillation (OKD) methods represent a streamlined, one-stage distillation training process that obviates the necessity of transferring knowledge from a pretrained teacher network to a more compact student network. In contrast to existing logits-based OKD methods, this paper presents an innovative approach to leverage intermediate spatial representations. Our analysis of the intermediate features from both teacher and student models reveals two pivotal insights: (1) the similar features between students and teachers are predominantly focused on the foreground objects. (2) teacher models emphasize foreground objects more than students. Building on these findings, we propose Asymmetric Decision-Making (ADM) to enhance feature consensus learning for student models while continuously promoting feature diversity in teacher models. Specifically, Consensus Learning for student models prioritizes spatial features with high consensus relative to teacher models. Conversely, Divergence Learning for teacher models highlights spatial features with lower similarity compared to student models, indicating superior performance by teacher models in these regions. Consequently, ADM facilitates the student models to catch up with the feature learning process of the teacher models. Extensive experiments demonstrate that ADM consistently surpasses existing OKD methods across various online knowledge distillation settings and also achieves superior results when transferred to offline knowledge distillation, semantic segmentation and diffusion distillation tasks.","When teaching smaller, faster AI models to learn from larger, more powerful ones—a process known as knowledge distillation—most methods rely on a two-step process: first training a large “teacher” model, and then having a “student” model learn from its outputs. Recently, a new family of methods called Online Knowledge Distillation (OKD) has made this process more efficient by letting both teacher and student models learn together in a single stage.Our research digs deeper into how these models learn from each other, focusing not just on the final predictions, but on the “intermediate” features—how the models internally process images. We found that both models tend to focus on the main objects in an image, but the teacher model pays even more attention to these important areas than the student does.Building on this, we introduce a new approach called Asymmetric Decision-Making (ADM). ADM helps the student model better match the teacher where it matters most, while encouraging the teacher to keep exploring new patterns. This leads to smarter, more effective student models. Our experiments show that ADM improves performance across a range of tasks, making knowledge distillation faster and more powerful."
Poster,AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration,https://ICML.cc//virtual/2025/poster/46432,"Wenhao SUN, Rong-Cheng Tu, Jingyi Liao, Zhao Jin, Dacheng Tao","Diffusion Transformers (DiTs) have proven effective in generating high-quality videos but are hindered by high computational costs. Existing video diffusion sampling acceleration methods often rely on costly fine-tuning or exhibit limited generalization capabilities. We propose Asymmetric Reduction and Restoration (**AsymRnR**), **a training-free and model-agnostic method to accelerate video DiTs**. It builds on the observation that redundancies of feature tokens in DiTs vary significantly across different model blocks, denoising steps, and feature types. Our AsymRnR asymmetrically reduces redundant tokens in the attention operation, achieving acceleration with negligible degradation in output quality and, in some cases, even improving it. We also tailored a reduction schedule to distribute the reduction across components adaptively. To further accelerate this process, we introduce a matching cache for more efficient reduction. Backed by theoretical foundations and extensive experimental validation, AsymRnR integrates into state-of-the-art video DiTs and offers substantial speedup.","Diffusion Transformers (DiTs) demand heavy computing power and energy to generate videos. Most of the computational burden arises from the model processing a large number of tokens, each representing a small patch of a video frame, that interact with each other at every step.Our approach, Asymmetric Reduction and Restoration (AsymRnR), spots when many of this computation is redundant and safely skips them in a step-dependent way. When the skipped information is needed again, AsymRnR quickly restores it so the final video stays sharp and coherent. The method simply requires no retraining or fine-tuning and works with any modern video DiT right out of the box. In experiments, it reduces running time by nearly one third while keeping visual quality.By making state-of-the-art video generation cheaper, faster and greener, AsymRnR lowers the barrier for creative storytelling, simulation and scientific visualisation."
Poster,ATA: Adaptive Task Allocation for Efficient Resource Management in Distributed Machine Learning,https://ICML.cc//virtual/2025/poster/46650,"Artavazd Maranjyan, El Mehdi Saad, Peter Richtarik, Francesco Orabona","Asynchronous methods are fundamental for parallelizing computations in distributed machine learning.     They aim to accelerate training by fully utilizing all available resources.    However, their greedy approach can lead to inefficiencies using more computation than required, especially when computation times vary across devices.    If the computation times were known in advance, training could be fast and resource-efficient by assigning more tasks to faster workers.    The challenge lies in achieving this optimal allocation without prior knowledge of the computation time distributions.    In this paper, we propose ATA (Adaptive Task Allocation), a method that adapts to heterogeneous and random distributions of worker computation times.    Through rigorous theoretical analysis, we show that ATA identifies the optimal task allocation and performs comparably to methods with prior knowledge of computation times.    Experimental results further demonstrate that ATA is resource-efficient, significantly reducing costs compared to the greedy approach, which can be arbitrarily expensive depending on the number of workers.","These days, many computers and devices can be used to train machine learning models. However, they often run at different speeds, and we typically don’t know how fast each one is ahead of time. A common approach is to give all machines the same task and only use the results from the fastest ones. But if we need fewer results than the number of available devices, this leads to significant waste — slower machines still do the work, but their output gets ignored.This paper studies how to assign tasks more intelligently when machine speeds are unknown and unpredictable. The goal is to complete the required work efficiently without wasting computational resources. Ideally, faster machines would handle more tasks, and slower ones fewer — but without knowing speeds in advance, this is challenging.We introduce ATA (Adaptive Task Allocation), a method that learns how fast each machine is over time and adapts the task assignment accordingly. Both theoretical analysis and experiments show that ATA performs nearly as well as if the machine speeds were known in advance."
Poster,A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?,https://ICML.cc//virtual/2025/poster/44028,"Ibrahim Alabdulmohsin, Andreas Steiner","Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMs) can replicate such fractal characteristics and identify conditions-such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.","We show that fractal analysis offers a novel and insightful lens for understanding the capabilities and limitations of LLMs in replicating the complex statistical structures of natural language. As we show in the paper, various strategies, like the decoding temperature and prompting method, can impact fractal parameters even when log-perplexity scores seem to be unaffected. This goal is in line with earlier works, who argued that the evaluation of LLMs should go beyond log-perplexity and also consider how well LLMs capture other statistical tendencies observed in natural language. Our key contribution lies in introducing and validating this fractal analysis framework, and reporting several novel results, such as the strong correlation between the Hurst exponent and the quality of texts. In addition, we release a benchmark dataset called GAGLE, comprising over 240,000 articles generated by various LLMs. Unlike other public datasets, GAGLE includes various prompting strategies and decoding temperatures."
