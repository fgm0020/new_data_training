type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,ExLM: Rethinking the Impact of $\texttt{[MASK]}$ Tokens in Masked Language Models,https://ICML.cc//virtual/2025/poster/45731,"Kangjie Zheng, Junwei Yang, Siyue Liang, Bin Feng, Zequn Liu, Wei Ju, Zhiping Xiao, Ming Zhang","Masked Language Models (MLMs) have achieved remarkable success in many self-supervised representation learning tasks. MLMs are trained by randomly masking portions of the input sequences with $\texttt{[MASK]}$ tokens and learning to reconstruct the original content based on the remaining context. This paper explores the impact of $\texttt{[MASK]}$ tokens on MLMs. Analytical studies show that masking tokens can introduce the ***corrupted semantics*** problem, wherein the corrupted context may convey multiple, ambiguous meanings. This problem is also a key factor affecting the performance of MLMs on downstream tasks. Based on these findings, we propose a novel enhanced-context MLM, ExLM. Our approach expands $\texttt{[MASK]}$ tokens in the input context and models the dependencies between these expanded states. This enhancement increases context capacity and enables the model to capture richer semantic information, effectively mitigating the corrupted semantics problem during pre-training. Experimental results demonstrate that ExLM achieves significant performance improvements in both text modeling and SMILES modeling tasks. Further analysis confirms that ExLM enriches semantic representations through context enhancement, and effectively reduces the semantic multimodality commonly observed in MLMs.","When teaching AI models to understand language, researchers often hide words with special [MASK] tokens. However, this can confuse the model by creating unclear or unrealistic sentence meanings. Our work shows that this confusion harms learning more than previously thought. We propose a new method, ExLM, that expands and connects these [MASK] tokens to give the model more context, leading to better understanding and stronger performance across tasks."
Poster,Exogenous Isomorphism for Counterfactual Identifiability,https://ICML.cc//virtual/2025/poster/46152,"Yikang Chen, Dehui du","This paper investigates $\sim_{\mathcal{L}\_3}$-identifiability, a form of complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH) framework, ensuring that all Structural Causal Models (SCMs) satisfying the given assumptions provide consistent answers to all causal questions. To simplify this problem, we introduce exogenous isomorphism and propose $\sim_{\mathrm{EI}}$-identifiability, reflecting the strength of model identifiability required for $\sim_{\mathcal{L}\_3}$-identifiability. We explore sufficient assumptions for achieving $\sim_{\mathrm{EI}}$-identifiability in two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual transport, and Triangular Monotonic SCMs (TM-SCMs), which extend $\sim_{\mathcal{L}\_2}$-identifiability. Our results unify and generalize existing theories, providing theoretical guarantees for practical applications. Finally, we leverage neural TM-SCMs to address the consistency problem in counterfactual reasoning, with experiments validating both the effectiveness of our method and the correctness of the theory.","Causal models can answer hypothetical “what-if” questions, but different models may yield different answers—a phenomenon known as the counterfactual identification problem. This inconsistency makes it difficult for researchers and decision-makers to know which predictions to trust.To address this challenge, we introduce the concept of exogenous isomorphism, which aligns the latent components of different models so that they produce consistent answers to every “what-if” query. We then identify sufficient assumptions that guarantee this alignment across two well-studied model families. Finally, we demonstrate the practical feasibility of our approach by implementing it with neural networks and validating its performance on simulated datasets.Guaranteeing that all models constructed under the same assumptions produce identical answers enhances the reliability of counterfactual reasoning. This consistency is crucial for domains such as healthcare, economics, and policymaking, where trustworthy “what-if” analyses underpin sound decisions."
Poster,Expected Variational Inequalities,https://ICML.cc//virtual/2025/poster/45602,"Brian Zhang, Ioannis Anagnostides, Emanuel Tewolde, Ratip Emin Berker, Gabriele Farina, Vincent Conitzer, Tuomas Sandholm","*Variational inequalities (VIs)* encompass many fundamental problems in diverse areas ranging from engineering to economics and machine learning. However, their considerable expressivity comes at the cost of computational intractability. In this paper, we introduce and analyze a natural relaxation—which we refer to as *expected variational inequalities (EVIs)*—where the goal is to find a distribution that satisfies the VI constraint in expectation. By adapting recent techniques from game theory, we show that, unlike VIs, EVIs can be solved in polynomial time under general (nonmonotone) operators. EVIs capture the seminal notion of correlated equilibria, but enjoy a greater reach beyond games. We also employ our framework to capture and generalize several existing disparate results, including from settings such as smooth games, and games with coupled constraints or nonconcave utilities.","Many real-world problems—such as predicting how consumers and companies make decisions in markets or training deep learning models—can be described using a mathematical framework called *variational inequalities (VIs)*. The catch is that VIs are often too complex to solve, especially when the problem is large.Our main research question is whether there’s a way to combine the flexibility of VIs with the ability to find solutions efficiently. We came up with a new formulation called *expected variational inequalities (EVIs)*. Rather than finding a point that solves the problem exactly, EVIs aim for a solution that performs well on *average* across many points. EVIs are inspired by ideas from economics, specifically game theory. There, moving from VIs to EVIs involves introducing a correlation device that enables the players to coordinate their actions—much like a traffic light helps two drivers at a crossroad coordinate who goes first."
Poster,Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts,https://ICML.cc//virtual/2025/poster/46221,"Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu, Qiyang Min","Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with a flexible routing strategy, Expert Race.  By allowing tokens and experts to compete together and select the top candidates, the model learns to dynamically assign experts to critical tokens. Additionally, we propose per-layer regularization to address challenges in shallow layer learning, and router similarity loss to prevent mode collapse, ensuring better expert utilization. Extensive experiments on ImageNet validate the effectiveness of our approach, showcasing significant performance gains while promising scaling properties.","It is observed that the entire diffusion process actually consists of numerous sub-tasks with varying levels of difficulty; evidently, denoising a pure Gaussian noise is more difficult than denoising an almost fully clean image. Currently, these tasks are all addressed by the same model. We aim to employ models of different sizes to handle tasks with different levels of difficulty.The Mixture of Experts (MoE) technique is commonly used to scale up model capacity, and we find that it is also effective when applied to diffusion models. Furthermore, since the model incorporates a number of experts, we extend the routing strategy to enable the model to autonomously learn to activate different numbers of experts with different sub-tasks. This results in a model with dynamic size that can adaptively adjust to the complexity of each task.Compared to previous MoE approaches, our method enables much more efficient model scaling with simple modification to the routing strategy. Also, we demonstrate the potential of leveraging dynamics in diffusion models."
Poster,Explainable Concept Generation through Vision-Language Preference Learning for Understanding Neural Networks' Internal Representations,https://ICML.cc//virtual/2025/poster/44233,"Aditya Taparia, Som Sagar, Ransalu Senanayake","Understanding the inner representation of a neural network helps users improve models. Concept-based methods have become a popular choice for explaining deep neural networks post-hoc because, unlike most other explainable AI techniques, they can be used to test high-level visual ""concepts"" that are not directly related to feature attributes. For instance, the concept of ""stripes"" is important to classify an image as a zebra. Concept-based explanation methods, however, require practitioners to guess and manually collect multiple candidate concept image sets, making the process labor-intensive and prone to overlooking important concepts. Addressing this limitation, in this paper, we frame concept image set creation as an image generation problem. However, since naively using a standard generative model does not result in meaningful concepts, we devise a reinforcement learning-based preference optimization (RLPO) algorithm that fine-tunes a vision-language generative model from approximate textual descriptions of concepts. Through a series of experiments, we demonstrate our method's ability to efficiently and reliably articulate diverse concepts that are otherwise challenging to craft manually.","Have you ever wondered how an AI ""thinks"" when it makes a decision? For instance, if you show a neural network a picture, how does it know whether it's a zebra or not? It's often looking for specific visual patterns or what we call ""concepts"", like the black and white stripes on a zebra. Understanding these concepts helps us figure out why an AI behaves the way it does, which is really important for making AI more reliable and trustworthy.The challenge is that identifying these key concepts can be tricky and time-consuming. You'd usually have to guess what concepts are important and then manually gather lots of images to represent them and then verify it. We've developed a new method that automates this process. Our approach uses artificial intelligence to automatically identify and generate images that represent the important concepts a neural network is looking at. This makes it much easier to understand how these complex AI systems work, without all the manual effort!"
Poster,"Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations",https://ICML.cc//virtual/2025/poster/44106,"Shahaf Bassan, Yizhak Elboher, Tobias Ladner, Matthias Althoff, Guy Katz","Despite significant advancements in post-hoc explainability techniques for neural networks, many current methods rely on heuristics and do not provide formally provable guarantees over the explanations provided. Recent work has shown that it is possible to obtain explanations with formal guarantees by identifying subsets of input features that are sufficient to determine that predictions remain unchangedusing neural network verification techniques.Despite the appeal of these explanations, their computation faces significant scalability challenges. In this work, we address this gap by proposing a novel abstraction-refinement technique for efficiently computing provably sufficient explanations of neural network predictions. Our method *abstracts* the original large neural network by constructing a substantially reduced network, where a sufficient explanation of the reduced network is also *provably sufficient* for the original network, hence significantly speeding up the verification process. If the explanation is insufficient on the reduced network, we iteratively *refine* the network size by gradually increasing it until convergence.Our experiments demonstrate that our approach enhances the efficiency of obtaining provably sufficient explanations for neural network predictions while additionally providing a fine-grained interpretation of the network's predictions across different abstraction levels.","As neural networks are increasingly used in high-stakes decisions, understanding their predictions is critical. While many explainability methods exist, most rely on heuristics — meaning we can't guarantee that the explanations they produce are accurate. This limits our ability to fully trust or rely on such explanations.Recent research has shown that it's possible to generate *provably sufficient* explanations — identifying subsets of features that, when fixed, provably guarantee the model’s prediction remains unchanged. These explanations offer strong guarantees, but they’re often too slow to compute on large models.In our work, we tackle this challenge with a new abstraction-refinement algorithm. We first build a smaller, simplified version of the model and search for sufficient explanations there. If successful, we can prove the explanation also holds for the original model. If not, we keep refining the model step by step by increasing its size until we find an explanation that's guaranteed to be both sufficient and minimal.Our results show this method makes formal explanations significantly more scalable — offering both speed and trust in the explanations we provide."
Poster,Explaining the role of Intrinsic Dimensionality in Adversarial Training,https://ICML.cc//virtual/2025/poster/45952,"Enes Altinisik, Safa Messaoud, Husrev Taha Sencar, Hassan Sajjad, Sanjay Chawla","Adversarial Training (AT) impacts different architectures in distinct ways: vision models gain robustness but face reduced generalization, encoder-based models exhibit limited robustness improvements with minimal generalization loss, and recent work in latent-space adversarial training demonstrates that decoder-based models achieve improved robustness by applying AT across multiple layers.We provide the first explanation for these trends by leveraging the manifold conjecture: off-manifold adversarial examples (AEs) enhance robustness, while on-manifold AEs improve generalization.We show that vision and decoder-based models exhibit low intrinsic dimensionality in earlier layers (favoring off-manifold AEs), whereas encoder-based models do so in later layers (favoring on-manifold AEs). Exploiting this property, we introduce SMAAT, which improves the scalability of AT for encoder-based models by perturbing the layer with the lowest intrinsic dimensionality. This reduces the projected gradient descent (PGD) chain length required for AE generation, cutting GPU time by 25–33% while significantly boosting robustness. We validate SMAAT across multiple tasks, including text generation, sentiment classification, safety filtering, and retrieval augmented generation setups, demonstrating superior robustness with comparable generalization to standard training.","Adversarial training is a popular method for making AI models more robust to small, carefully crafted changes in input data that can fool the model. However, while it works well for vision models, it often reduces their accuracy, whereas it surprisingly improves both robustness and accuracy in some language models. In this work, we uncover a key reason behind this difference: the complexity of the “data manifold”, the space that the model believes the data lives in, varies across models. We find that adversarial examples in vision and decoder-based models tend to fall off this manifold, which increases robustness but harms accuracy. In contrast, encoder-based language models see more on-manifold adversarial examples, which improve accuracy. Based on this insight, we propose SMAAT, a new method that selectively applies adversarial training to the most effective layer in the network. This allows us to make language models more robust while using significantly less compute. Our method achieves state-of-the-art results in tasks like classification, safety filtering, and document retrieval, while using just a fraction of the GPU time compared to standard methods. These findings may help build more robust and efficient AI systems across domains."
Poster,Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization,https://ICML.cc//virtual/2025/poster/45375,"Yang Shen, Xiu-Shen Wei, Yifan Sun, YuXin Song, Tao Yuan, Jian Jin, He-Yang Xu, Yazhou Yao, Errui Ding","Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In this paper, we rethink the reality that CV adopts discrete and terminological task definitions (e.g., ""image segmentation""), and conjecture it is a key barrier that hampers zero-shot task generalization. Our hypothesis is that without truly understanding previously-seen tasks—due to these terminological definitions—deep models struggle to generalize to novel tasks. To verify this, we introduce Explanatory Instructions, which provide an intuitive way to define CV task objectives through detailed linguistic transformations from input images to outputs. We create a large-scale dataset comprising 12 million ""image input $\to$ explanatory instruction $\to$ output"" triplets, and train an auto-regressive-based vision-language model (AR-based VLM) that takes both images and explanatory instructions as input. By learning to follow these instructions, the AR-based VLM achieves instruction-level zero-shot capabilities for previously-seen tasks and demonstrates strong zero-shot generalization for unseen CV tasks.  Code and dataset will be open-sourced.","Large Vision (Vision-Language) Models excel at specific vision tasks like recognizing objects, but they struggle to generalize these skills to new, unseen tasks—unlike humans who can adapt quickly. This gap exists because current models rely on rigid, predefined task definitions (e.g., ""segment the image"") rather than understanding the underlying objectives.To bridge this gap, we introduced Explanatory Instructions, which describe vision tasks in natural language (e.g., ""highlight the river in blue and mark the rocks in red""). We created a large dataset with 12 million image-instruction-output examples and trained a model to follow these instructions. This approach allows the model to generalize to new tasks without additional training, achieving zero-shot capabilities for both familiar and novel vision tasks.Our work moves toward more flexible and human-like computer vision systems, enabling models to tackle diverse tasks by simply understanding descriptive instructions—just like humans do."
Poster,Explicit Discovery of Nonlinear Symmetries from Dynamic Data,https://ICML.cc//virtual/2025/poster/44226,"Lexiang Hu, Yikang Li, Zhouchen Lin","Symmetry is widely applied in problems such as the design of equivariant networks and the discovery of governing equations, but in complex scenarios, it is not known in advance. Most previous symmetry discovery methods are limited to linear symmetries, and recent attempts to discover nonlinear symmetries fail to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD, which is, to our knowledge, the first method capable of determining the number of infinitesimal generators with nonlinear terms and their explicit expressions. We specify a function library for the infinitesimal group action and aim to solve for its coefficient matrix, proving that its prolongation formula for differential equations, which governs dynamic data, is also linear with respect to the coefficient matrix. By substituting the central differences of the data and the Jacobian matrix of the trained neural network into the infinitesimal criterion, we get a system of linear equations for the coefficient matrix, which can then be solved using SVD. On top quark tagging and a series of dynamic systems, LieNLSD shows qualitative advantages over existing methods and improves the long rollout accuracy of neural PDE solvers by over $20\\%$ while applying to guide data augmentation. Code and data are available at [https://github.com/hulx2002/LieNLSD](https://github.com/hulx2002/LieNLSD).",Discovering new physical laws from the world is like solving a puzzle of finding patterns in data. Behind this lies the hard work of human scientists—but can we use AI technology to assist in this endeavor?Our approach achieves the following:  (1) Discovers more universal and complex laws;  (2) Presents the discovered laws clearly and intuitively;  (3) Accurately determines the number of laws.These laws can help us better understand the world and predict the future more efficiently and accurately—just like applying knowledge flexibly in learning.
Poster,Explicit Exploration for High-Welfare Equilibria in Game-Theoretic Multiagent Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46142,"Austin Nguyen, Anri Gu, Michael Wellman","Iterative extension of empirical game models through deep reinforcement learning (RL) has proved an effective approach for finding equilibria in complex games. When multiple equilibria exist, we may also be interested in finding solutions with particular characteristics. We address this issue of equilibrium selection in the context of Policy Space Response Oracles (PSRO), a flexible game-solving framework based on deep RL, by skewing the strategy exploration process towards higher-welfare solutions. At each iteration, we create an exploration policy that imitates high welfare-yielding behavior and train a response to the current solution, regularized to be similar to the exploration policy. With no additional simulation expense, our approach, named Ex$^2$PSRO, tends to find higher welfare equilibria than vanilla PSRO in two benchmarks: a sequential bargaining game and a social dilemma game. Further experiments demonstrate Ex$^2$PSRO's composability with other PSRO variants and illuminate the relationship between exploration policy choice and algorithmic performance.","Discovering strong strategies in large systems with many self-interested parties is a difficult but important problem in machine learning, especially when modeling systems we see in the world today. Some systems may contain many strong strategies, and we may be interested in not only discovering these strategies but also designing algorithms that favor those with appealing properties of interest. In particular, we created an algorithm that favors strong strategies that increase the summed benefit of all involved parties. This will help us design strategies and systems that account for the welfare of the general populace."
