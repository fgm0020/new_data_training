type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,AutoStep: Locally adaptive involutive MCMC,https://ICML.cc//virtual/2025/poster/45340,"Tiange Liu, Nikola Surjanovic, Miguel Biron-Lattes, Alexandre Bouchard-Côté, Trevor Campbell","Many common Markov chain Monte Carlo (MCMC) kernels can be formulated using a deterministic involutive proposal with a step size parameter.  Selecting an appropriate step size is often a challenging task in practice; and for complex multiscale targets, there may not be one choice of step size that works well globally.  In this work, we address this problem with a novel class of involutive MCMC methods---AutoStep MCMC---that selects an appropriate step size at each iteration adapted to the local geometry of the target distribution.  Weprove that under mild conditions AutoStep MCMC is $\pi$-invariant, irreducible, and aperiodic, and obtain bounds on expected energy jump distance and cost per iteration. Empirical results examine the robustness and efficacy of our proposed step size selection procedure, and show that AutoStep MCMC is competitive with state-of-the-art methods in terms of effective sample size per unit cost on a range of challenging target distributions.","Markov Chain Monte Carlo (MCMC) methods are powerful tools used to sample from complex probability distributions. A key challenge in using these methods is choosing the right “step size,” which controls how far the algorithm moves at each step. If the step size is too small, the algorithm moves slowly; if it is too large, the algorithm frequently gets rejected. In many real-world problems, there is no single step size that works well everywhere.Our work introduces AutoStep MCMC, a method that automatically selects a “good” step size at each iteration based on the local shape of the target distribution. This makes the algorithm more reliable, especially for challenging models that vary widely across different regions. We prove that our method explores properly and does not get stuck, and we show it performs competitively with the best existing samplers on a range of problems.By making MCMC more adaptive and easier to use, AutoStep helps researchers get better results with less manual tuning, opening the door to more robust and automated statistical inference."
Poster,A Variational Framework for Improving Naturalness in Generative Spoken Language Models,https://ICML.cc//virtual/2025/poster/45785,"Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky","The success of large language models in text processing has inspired their adaptation to speech modeling.However, since speech is continuous and complex, it is often discretized for autoregressive modeling.Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information.As a result, models trained on these tokens can generate speech with reduced naturalness.Existing approaches try to fix this by adding pitch features to the semantic tokens.However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering.To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens.Our approach eliminates the need for manual extraction and selection of paralinguistic features.Moreover, it produces preferred speech continuations according to human raters.Code, samples and models are available at https://github.com/b04901014/vae-gslm.","Imagine teaching a computer to talk just like a normal human. That's the task of Generative Spoken Language Model (GSLM). Think of it as a program that learns to predict what should come next in speech, much like your phone tries to guess the next word you're typing, but with voices instead of text. We noticed that earlier methods of these models, while good at generating meaningful language content, sometimes lost important details that make human voices sound natural. These are things like pitch (the up-and-down of your voice) and loudness variations when you speak.Can we make the computer's speech sound more natural and still make perfect sense? We developed a way for our model to pay special attention to these natural vocal qualities. But instead of us humans telling it exactly what to look for, we let it learn by listening to a huge amount of human speech data, figuring out on its own how to capture all those subtle elements that make speech sound real.When people listened to our model, they found its speech much more natural and meaningful compared to previous approaches. This means our method could improve how natural and clear the voices of existing conversational agents, making interactions with computers much more engaging and convenient."
Poster,A Variational Information Theoretic Approach to Out-of-Distribution Detection,https://ICML.cc//virtual/2025/poster/45839,"Sudeepta Mondal, Zhuolin Jiang, Ganesh Sundaramoorthi","We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.","We developed a framework and theory to develop out-of-distribution (OOD) detection methods and rigorously understand existing approaches.  OOD detection is the problem of identifying data out of the statistical distribution of the data that the neural network (NN) was trained on.   The NN may erroneously give confident predictions on such OOD.  Our approach formulates OOD features as a function of NN features, through a novel loss function, which favors extracting OOD relevant information from the NN feature. We showed that our approach offers an explanation on why and under what conditions existing techniques work.  We also showed how to use our approach to construct new OOD features, which is shown to provably generalize better than competing approaches. Our extensive benchmarking empirically validated this fact."
Poster,A Variational Perspective on Generative Protein Fitness Optimization,https://ICML.cc//virtual/2025/poster/44530,"Lea Bogensperger, Dominik Narnhofer, Ahmed Allam, Konrad Schindler, Michael Krauthammer","The goal of protein fitness optimization is to discover new protein variants with enhanced fitness for a given use. The vast search space and the sparsely populated fitness landscape, along with the discrete nature of protein sequences, pose significant challenges when trying to determine the gradient towards configurations with higher fitness. We introduce *Variational Latent Generative Protein Optimization* (VLGPO), a variational perspective on fitness optimization. Our method embeds protein sequences in a continuous latent space to enable efficient sampling from the fitness distribution and combines a (learned) flow matching prior over sequence mutations with a fitness predictor to guide optimization towards sequences with high fitness. VLGPO achieves state-of-the-art results on two different protein benchmarks of varying complexity. Moreover, the variational design with explicit prior and likelihood functions offers a flexible plug-and-play framework that can be easily customized to suit various protein design tasks.","We consider the task of protein fitness optimization, which aims to improve a protein’s functionality by modifying its amino acid sequence to enhance a specific function. Due to the vast search space of possible sequences, computational approaches can aid in suggesting new protein candidates.We use an approach based on generative models, where the goal is to learn the distribution of a data set of protein mutants. We then employ a second model to steer the generation process toward sequences of higher fitness. The effectiveness of our approach is demonstrated on two proteins, AAV and GFP, each with two design tasks of different difficulty (medium and hard)."
Poster,Average Certified Radius is a Poor Metric for Randomized Smoothing,https://ICML.cc//virtual/2025/poster/45218,"Chenhao Sun, Yuhao Mao, Mark Müller, Martin Vechev","Randomized smoothing (RS) is popular for providing certified robustness guarantees against adversarial attacks. The average certified radius (ACR) has emerged as a widely used metric for tracking progress in RS. However, in this work, for the first time we show that ACR is a poor metric for evaluating robustness guarantees provided by RS. We theoretically prove not only that a trivial classifier can have arbitrarily large ACR, but also that ACR is extremely sensitive to improvements on easy samples. In addition, the comparison using ACR has a strong dependence on the certification budget. Empirically, we confirm that existing training strategies, though improving ACR, reduce the model's robustness on hard samples consistently. To strengthen our findings, we propose strategies, including explicitly discarding hard samples, reweighing the dataset with approximate certified radius, and extreme optimization for easy samples, to replicate the progress in RS training and even achieve the state-of-the-art ACR on CIFAR-10, without training for robustness on the full data distribution. Overall, our results suggest that ACR has introduced a strong undesired bias to the field, and its application should be discontinued in RS. Finally, we suggest using the empirical distribution of $p_A$, the accuracy of the base model on noisy data, as an alternative metric for RS.","Artificial intelligence (AI) is becoming increasingly common in various areas of our lives, making it important to ensure these AI systems are trustworthy and reliable. Randomized smoothing is one of the most popular methods used to make AI models more trustworthy. However, this paper identifies an important and widely spread problem in the evaluation of randomized smoothing. We find that one widely used evaluation metric allows the neural network to focus only on easy data and ignore hard ones. This could lead to critical problems such as unfairness. We show this problem both theoretically and empirically, with extensive evidence demonstrating that this problem has introduced strong selection bias into the development of the algorithms. To address this issue, we suggest alternative metrics to replace the current one. This will make the evaluation of the future development of trustworthy AI more reliable."
Poster,Average Sensitivity of Hierarchical $k$-Median Clustering,https://ICML.cc//virtual/2025/poster/46394,"Shijie Li, Weiqiang He, Ruobing Bai, Pan Peng","Hierarchical clustering is a widely used method for unsupervised learning with numerous  applications. However, in the application of modern algorithms, the datasets studied are usually large and dynamic. If the hierarchical clustering is sensitive to small perturbations of the dataset, the usability of the algorithm will be greatly reduced. In this paper, we focus on the hierarchical $k$ -median clustering problem, which bridges hierarchical and centroid-based clustering while offering theoretical appeal, practical utility, and improved interpretability. We analyze the average sensitivity of algorithms for this problem by measuring the expected change in the output when a random data point is deleted. We propose an efficient algorithm for hierarchical $k$-median clustering and theoretically prove its low average sensitivity and high clustering quality. Additionally, we show that single linkage clustering and a deterministic variant of the CLNSS algorithm exhibit high average sensitivity, making them less stable. Finally, we validate the robustness and effectiveness of our algorithm through experiments.","Hierarchical clustering is a popular tool that helps computers find structure in large datasets — like grouping similar customers or organizing search results. But in real-world situations, data can change: a user leaves, a sensor fails, or a few points are updated. If a clustering algorithm changes too much from small tweaks, it becomes unreliable.Our research looks at a version of clustering called **hierarchical $k$-median**, which is both useful and interpretable. We ask: _How stable are these algorithms when a single data point is removed at random?_ We measure this with a concept called **average sensitivity**— how much the output shifts due to small changes.We design a new algorithm that produces high-quality clusters while remaining stable, meaning small changes in data don’t lead to big changes in output. We also show that some commonly used methods are much less stable. Our algorithm not only works well in theory but also performs reliably in practice."
Poster,A Versatile Influence Function for Data Attribution with Non-Decomposable Loss,https://ICML.cc//virtual/2025/poster/45456,"Junwei Deng, Weijing Tang, Jiaqi Ma","Influence function, a technique rooted in robust statistics, has been adapted in modern machine learning for a novel application: data attribution---quantifying how individual training data points affect a model's predictions. However, the common derivation of influence functions in the data attribution literature is limited to loss functions that decompose into a sum of individual data point losses, with the most prominent examples known as M-estimators. This restricts the application of influence functions to more complex learning objectives, which we refer to as non-decomposable losses, such as contrastive or ranking losses, where a unit loss term depends on multiple data points and cannot be decomposed further. In this work, we bridge this gap by revisiting the general formulation of influence function from robust statistics, which extends beyond M-estimators. Based on this formulation, we propose a novel method, the Versatile Influence Function (VIF), that can be straightforwardly applied to machine learning models trained with any non-decomposable loss. In comparison to the classical approach in statistics, the proposed VIF is designed to fully leverage the power of auto-differentiation, hereby eliminating the need for case-specific derivations of each loss function. We demonstrate the effectiveness of VIF across three examples: Cox regression for survival analysis, node embedding for network analysis, and listwise learning-to-rank for information retrieval. In all cases, the influence estimated by VIF closely resembles the results obtained by brute-force leave-one-out retraining, while being up to 1000 times faster to compute. We believe VIF represents a significant advancement in data attribution, enabling efficient influence-function-based attribution across a wide range of machine learning paradigms, with broad potential for practical use cases.","We want to understand how each individual training data point influences a machine learning model's behavior. One existing and popular tool for this task, called influence function, works only with setups where the model's training loss can be split into sum of units where each unit only depends on a single training data point. This leaves out many machine learning tasks, like ranking retrieval results or learning network structures, whose loss depends on interactions between training data points.To fix this, we explore and develop a new method, called the Versatile Influence Function (VIF), that works even when training losses can not be split into single units. This method could easily leverage automatic differentiation, the same tool that powers machine learning model training, to avoid case-specific derivations of each loss function.We show that VIF works well in several real-world tasks like predicting survival time, network embedding, and ranking retrieval results. Our work makes it easier for the community to understand and trust machine learning models."
Poster,Avoiding Catastrophe in Online Learning by Asking for Help,https://ICML.cc//virtual/2025/poster/45563,"Benjamin Plaut, Hanlin Zhu, Stuart Russell","Most learning algorithms with formal regret guarantees assume that all mistakes are recoverable and essentially rely on trying all possible behaviors. This approach is problematic when some mistakes are *catastrophic*, i.e., irreparable. We propose an online learning problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff in each round represents the chance of avoiding catastrophe in that round and try to maximize the product of payoffs (the overall chance of avoiding catastrophe) while allowing a limited number of queries to a mentor. We also assume that the agent can transfer knowledge between similar inputs. We first show that in general, any algorithm either queries the mentor at a linear rate or is nearly guaranteed to cause catastrophe. However, in settings where the mentor policy class is learnable in the standard online model, we provide an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows. Although our focus is the product of payoffs, we provide matching bounds for the typical additive regret. Conceptually, if a policy class is learnable in the absence of catastrophic risk, it is learnable in the presence of catastrophic risk if the agent can ask for help.","Many machine learning algorithms rely on essentially trying all possible behaviors and seeing what works well. However, this is too dangerous when errors can be *catastrophic*, i.e., irreparable. For example, we do not want self-driving cars or surgical robots to try all possible behaviors. One could train an AI system entirely in a controlled lab setting where mistakes are less impactful, but we argue that sufficiently general AI systems will inevitably encounter unfamiliar situations when deployed in the real world. How should AI systems behave in such situations?We suggest that they should *ask for help* (e.g., from a human supervisor). In this paper, we study a mathematical model where the AI system can ask for help a limited number of times. We provide a simple algorithm that mostly acts like a typical learning algorithm, but asks for help in situations that are very different from its prior experiences. We prove that under some mild conditions, our algorithm is guaranteed to avoid catastrophe while gradually becoming self-sufficient. Overall, our work provides a theoretical basis for how AI systems can learn safely in high-stakes applications."
Poster,Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts,https://ICML.cc//virtual/2025/poster/46307,"Mateo Espinosa Zarlenga, Gabriele Dominici, Pietro Barbiero, Zohreh Shams, Mateja Jamnik","In this paper, we investigate how concept-based models (CMs) respond to out-of-distribution (OOD) inputs. CMs are interpretable neural architectures that first predict a set of high-level *concepts* (e.g., ""stripes"", ""black"") and then predict a task label from those concepts. In particular, we study the impact of *concept interventions* (i.e., operations where a human expert corrects a CM’s mispredicted concepts at test time) on CMs' task predictions when inputs are OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we term *leakage poisoning*, that prevents them from properly improving their accuracy when intervened on for OOD inputs. To address this, we introduce *MixCEM*, a new CM that learns to dynamically exploit leaked information missing from its concepts only when this information is in-distribution. Our results across tasks with and without complete sets of concept annotations demonstrate that MixCEMs outperform strong baselines by significantly improving their accuracy for both in-distribution and OOD samples in the presence and absence of concept interventions.","Recent advances in Artificial Intelligence (AI) have led to powerful models that can receive help in the form of ""concept interventions"". A concept intervention is an operation where, during deployment, an expert communicates the presence or absence of a high-level concept in the model's input through a manipulation of its inner representations. This way, for example, radiologists can let an AI assistant know that an X-ray scan has ""bone spurs"", helping the assistant make a more accurate diagnosis.The real world, however, is messy. This means that the inputs we provide to models contain noise or conditions that differ from those the model was exposed to during training. In this paper, we demonstrate that in these instances, concept interventions fail to properly aid the model in its downstream task. We argue that this is due to ""leakage poisoning"", where a model's representations become too corrupted for interventions to work.We address this by proposing a way of representing concepts that enables the model to restrict this poisonous leakage whenever the input goes too far from what the model has been exposed to. Our results show that our representations lead to highly accurate models that remain intervenable when provided with expected and unexpected inputs."
Poster,Avoiding spurious sharpness minimization broadens applicability of SAM,https://ICML.cc//virtual/2025/poster/43667,"Sidak Pal Singh, Hossein Mobahi, Atish Agarwala, Yann Nicolas Dauphin","Curvature regularization techniques like Sharpness Aware Minimization (SAM) have shown great promise in improving generalization on vision tasks. However, we find that SAM performs poorly in domains like natural language processing (NLP), often degrading performance --- even with twice the compute budget. We investigate the discrepancy across domains and find that in the NLP setting, SAM is dominated by regularization of the logit statistics --- instead of improving the geometry of the function itself. We use this observation to develop an alternative algorithm we call Functional SAM, which regularizes curvature only through modification of the statistics of the overall function implemented by the neural network, and avoids spurious minimization through logit manipulation. Furthermore, we argue that preconditioning the SAM perturbation also prevents spurious minimization, and when combined with Functional SAM, it gives further improvements. Our proposed algorithms show improved performance over AdamW and SAM baselines when trained for an equal number of steps, in both fixed-length and Chinchilla-style training settings, at various model scales (including billion-parameter scale). On the whole, our work highlights the importance of more precise characterizations of sharpness in broadening the applicability of curvature regularization to large language models (LLMs)","AI models can be ""brittle""—small, insignificant changes to an input can cause them to make big mistakes. A popular technique called Sharpness Aware Minimization (SAM) helps fix this for vision-based AI by training them to find more stable and robust solutions, similar to a student who learns a concept deeply rather than just memorizing facts.We discovered that this technique backfires for large language models (LLMs). Instead of improving the model's understanding of language, SAM gets distracted by superficial details of the model's internal predictions.We developed a new approach, Functional SAM, that corrects this. Our method guides the model to focus on the overall function and meaning of its outputs (or answers), ignoring the internal distractions. This leads to more accurate and reliable language models at all scales, achieving better performance without requiring extra computation."
