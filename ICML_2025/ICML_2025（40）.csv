type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time,https://ICML.cc//virtual/2025/poster/44696,"Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi","Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches typically frame this as a multi-objective optimization problem, they often overlook how humans actually make decisions. Research on bounded rationality suggests that human decision making follows satisficing strategies- optimizing primary objectives while ensuring others meet acceptable thresholds. To bridge this gap and operationalize the notion of satisficing alignment, we propose SITAlign: an inference-time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. We provide theoretical insights by deriving sub-optimality bounds of our satisficing-based inference alignment approach. We empirically validate SITAlign's performance through extensive experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art multi-objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness.","Making language models act the way humans want is tricky because ""good"" behavior involves many different aspects, like being helpful and safe, which can sometimes conflict. Other methods try to make language models perfect at everything simultaneously. Our research proposes ""SITAlign,"" a framework that, like humans, prioritizes one main goal (e.g., helpfulness) while ensuring other aspects (e.g., safety) meet a ""good enough"" standard."
Poster,BounDr.E: Predicting Drug-likeness via Biomedical Knowledge Alignment and EM-like One-Class Boundary Optimization,https://ICML.cc//virtual/2025/poster/44875,"Dongmin Bang, Inyoung Sung, Yinhua Piao, Sangseon Lee, Sun Kim","The advent of generative AI now enables large-scale $\textit{de novo}$ design of molecules, but identifying viable drug candidates among them remains an open problem. Existing drug-likeness prediction methods often rely on ambiguous negative sets or purely structural features, limiting their ability to accurately classify drugs from non-drugs. In this work, we introduce BounDr.E}: a novel modeling of drug-likeness as a compact space surrounding approved drugs through a dynamic one-class boundary approach. Specifically, we enrich the chemical space through biomedical knowledge alignment, and then iteratively tighten the drug-like boundary by pushing non-drug-like compounds outside via an Expectation-Maximization (EM)-like process. Empirically, BounDr.E achieves 10\% F1-score improvement over the previous state-of-the-art and demonstrates robust cross-dataset performance, including zero-shot toxic compound filtering. Additionally, we showcase its effectiveness through comprehensive case studies in large-scale $\textit{in silico}$ screening. Our codes and constructed benchmark data under various schemes are provided at: https://github.com/eugenebang/boundr_e.","The rapid advancement of generative models has enabled the creation of large libraries of de novo molecules, yet assessing which of these are truly drug-like remains an unresolved challenge. Traditional rules and property-based filters offer only coarse approximations, and most learning-based models lack integration of biological context, relying heavily on molecular structure alone. Furthermore, the highly scattered nature of approved drugs in chemical space makes it difficult to define a boundary that captures drug-likeness without overgeneralization.To address this, we propose \textsc{BoundDr.E}, a deep one-class boundary learner that defines drug-likeness as a compact, data-driven region around approved drugs, without relying on negative samples. Our method iteratively refines this region via an Expectation-Maximization-like optimization and embeds molecules into a unified space that integrates both structural and biomedical knowledge through multi-modal mixup.Empirical results show strong and consistent performance across time-based, scaffold-based, and cross-dataset evaluations, as well as in zero-shot toxic compound filtering. These findings suggest that BoundDr.E provides a robust and biologically grounded framework for drug-likeness prediction, offering a scalable solution for prioritizing AI-generated compounds in early-stage drug discovery."
Poster,BoxLM: Unifying Structures and Semantics of Medical Concepts for Diagnosis Prediction in Healthcare,https://ICML.cc//virtual/2025/poster/44310,"Yanchao Tan, Hang Lv, Yunfei Zhan, Guofang Ma, Bo Xiong, Carl Yang","Language Models (LMs) have advanced diagnosis prediction by leveraging the semantic understanding of medical concepts in Electronic Health Records (EHRs). Despite these advancements, existing LM-based methods often fail to capture the structures of medical concepts (e.g., hierarchy structure from domain knowledge). In this paper, we propose BoxLM, a novel framework that unifies the structures and semantics of medical concepts for diagnosis prediction. Specifically, we propose a structure-semantic fusion mechanism via box embeddings, which integrates both ontology-driven and EHR-driven hierarchical structures with LM-based semantic embeddings, enabling interpretable medical concept representations.Furthermore, in the box-aware diagnosis prediction module, an evolve-and-memorize patient box learning mechanism is proposed to model the temporal dynamics of patient visits, and a volume-based similarity measurement is proposed to enable accurate diagnosis prediction. Extensive experiments demonstrate that BoxLM consistently outperforms state-of-the-art baselines, especially achieving strong performance in few-shot learning scenarios, showcasing its practical utility in real-world clinical settings.","Can we teach machines to predict a patient’s future health conditions not just by memorizing data, but by truly understanding medical knowledge? Recent language models have made great progress in medical diagnosis by learning the meaning of disease names from health records. But they often miss an important piece: how medical conditions are related—such as which diseases fall under the same category, or how one condition may lead to another.Our work introduces a new approach called **BoxLM**, which combines two types of knowledge: the *meaning* of medical terms and their *structured relationships* drawn from medical ontologies and real hospital data. Instead of treating each disease as a single point, BoxLM represents them as regions with boundaries in space, allowing the model to naturally capture complex relationships like overlap, inclusion, and hierarchy between conditions. BoxLM also tracks how a patient’s health evolves over time, learning patterns that help predict future diagnoses. It performs especially well when only limited patient history is available, making it a promising tool for improving care in data-scarce clinical settings."
Poster,Branches: Efficiently Seeking Optimal Sparse Decision Trees via AO*,https://ICML.cc//virtual/2025/poster/44200,"Ayman Chaouki, Jesse Read, Albert Bifet","Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine Learning, yet it poses a formidable optimisation challenge. Practical algorithms have recently emerged, primarily leveraging Dynamic Programming and Branch \& Bound. However, most of these approaches rely on a Depth-First-Search strategy, which is inefficient when searching for DTs at high depths and requires the definition of a maximum depth hyperparameter. Best-First-Search was also employed by other methods to circumvent these issues. The downside of this strategy is its higher memory consumption, as such, it has to be designed in a fully efficient manner that takes full advantage of the problem's structure. We formulate the problem as an AND/OR graph search which we solve with a novel AO*-type algorithm called Branches. We prove both optimality and complexity guarantees for Branches and we show that it is more efficient than the state of the art theoretically and on a variety of experiments. Furthermore, Branches supports non-binary features unlike the other methods, we show that this property can further induce larger gains in computational efficiency.","Decision Trees are flowchart-like predictive models that are easy to interpret for humans. Such interpretability is very important in sensitive domains such as healthcare and the criminal justice system. However, while this property is very popular, finding Decision Trees that optimally balance between accuracy and interpretability is a highly difficult task. The current proposed methods either consume too much time or too much memory or both, making them difficult to scale for large datasets. To address this issue, we introduce Branches, a new algorithm that seeks these optimal Decision Trees efficiently. This efficiency allows Branches to quickly find optimal solutions in settings that can be challenging for the state of the art. Furthermore, our work paves the way for promising future research directions aimed at enhancing the scalability of optimal decision tree learning to large datasets."
Poster,Breaking Barriers: Combinatorial Algorithms for Non-Monotone Submodular Maximization with Sublinear Adaptivity and $1/e$ Approximation,https://ICML.cc//virtual/2025/poster/43553,"Yixin Chen, Wenjing Chen, Alan Kuhnle","With the rapid growth of data in modern applications, parallel combinatorial algorithms for maximizing non-monotone submodular functions have gained significant attention. In the parallel computation setting, the state-of-the-art approximation ratio of $1/e$ is achieved by a continuous algorithm (Ene & Nguyen, 2020) with adaptivity $\mathcal O (log(n))$. In this work, we focus on size constraints and present the first combinatorial algorithm matching this bound – a randomized parallel approach achieving $1/e − \epsilon$ approximation ratio. This result bridgesthe gap between continuous and combinatorial approaches for this problem. As a byproduct, we also develop a simpler $(1/4 − \epsilon)$-approximation algorithm with high probability $(\ge 1 − 1/n)$. Both algorithms achieve $\mathcal O (log(n) log(k))$ adaptivity and $\mathcal O (n log(n) log(k)) query complexity. Empirical results show our algorithms achieve competitive objective values, with the $(1/4 − \epsilon)$-approximation algorithm particularly efficient in queries.","First practical, parallelizable algorithms for size-constrained maximization of submodular functions that achieve theoretical performance guarantees of $1/e$."
Poster,Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting,https://ICML.cc//virtual/2025/poster/43827,"Zhining Liu, Ze Yang, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong","Time-series forecasting plays a critical role in many real-world applications. Although increasingly powerful models have been developed and achieved superior results on benchmark datasets, through a fine-grained sample-level inspection, we find that (i) no single model consistently outperforms others across different test samples, but instead (ii) each model excels in specific cases. These findings prompt us to explore how to adaptively leverage the distinct strengths of various forecasting models for different samples. We introduce TimeFuse, a framework for collective time-series forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse utilizes meta-features to characterize input time series and trains a learnable fusor to predict optimal model fusion weights for any given input. The fusor can leverage samples from diverse datasets for joint training, allowing it to adapt to a wide variety of temporal patterns and thus generalize to new inputs, even from unseen datasets. Extensive experiments demonstrate the effectiveness of TimeFuse in various long-/short-term forecasting tasks, achieving near-universal improvement over the state-of-the-art individual models. Code is available at https://github.com/ZhiningLiu1998/TimeFuse.","Forecasting future trends (like energy demand, traffic patterns, or weather changes) is crucial in many real-world settings. While many powerful forecasting models have been developed, none consistently work best across all scenarios. Different models perform better on different types of data. Our work introduces a new approach called TIMEFUSE that smartly combines the strengths of multiple forecasting models for each individual case, rather than relying on a one-size-fits-all solution. TIMEFUSE learns to recognize the unique patterns in each input and selects the best combination of models accordingly. This adaptive strategy improves forecasting accuracy and works well even on new, unseen data. Our method is efficient, interpretable, and outperforms existing techniques across a wide range of forecasting tasks."
Poster,Breaking the $n^{1.5}$ Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition,https://ICML.cc//virtual/2025/poster/46330,"Anders Aamand, Justin Chen, Mina Dalirrooyfard, Slobodan Mitrovic, Yuriy Nevmyvaka, Sandeep Silwal, Yinzhan Xu","We study differentially private algorithms for graph cut sparsification, a fundamental problem in algorithms, privacy, and machine learning. While significant progress has been made, the best-known private and efficient cut sparsifiers on $n$-node graphs approximate each cut within $\widetilde{O}(n^{1.5})$ additive error and $1+\gamma$ multiplicative error for any $\gamma > 0$ [Gupta, Roth, Ullman TCC'12]. In contrast, \emph{inefficient} algorithms, i.e., those requiring exponential time, can achieve an $\widetilde{O}(n)$ additive error and $1+\gamma$ multiplicative error [Eliáš,  Kapralov, Kulkarni, Lee SODA'20]. In this work, we break the $n^{1.5}$ additive error barrier for private and efficient cut sparsification. We present an $(\varepsilon,\delta)$-DP polynomial time algorithm that, given a non-negative weighted graph, outputs a private synthetic graph approximating all cuts with multiplicative error $1+\gamma$ and additive error $n^{1.25 + o(1)}$ (ignoring dependencies on $\varepsilon, \delta, \gamma$). At the heart of our approach lies a private algorithm for expander decomposition, a popular and powerful technique in (non-private) graph algorithms.","Our algorithm aims to understand how communities in a large social network are connected without revealing anyone's personal information. More precisely, we study approximating cuts in graphs, which are the number of edges between a set of vertices and its complement, under the mathematical framework of differential privacy. Current `fast' methods that protect privacy often give answers that have a large theoretical error, with a known limit of error approximately proportional to $O(n^{1.5})$ where $n$ is the number of nodes.We develop a new algorithm that preserves privacy but obtains much smaller error (approximately of the order $O(n^{1.25})$ at the cost of a small multiplicative error), which still being `fast'. It works by first privately breaking down the network into dense, well-connected clusters and the sparser links between them. We carefully combine a different algorithm on the sparse and dense regions, leading to the improved error guarantees."
Poster,Breaking the Barrier of Hard Samples: A Data-Centric Approach to Synthetic Data for Medical Tasks,https://ICML.cc//virtual/2025/poster/45228,"Maynara de Souza, Cleber Zanchettin","Data scarcity and quality issues remain significant barriers to developing robust predictive models in medical research. Traditional reliance on real-world data often leads to biased models with poor generalizability across diverse patient populations. Synthetic data generation has emerged as a promising solution, yet challenges related to these sample's representativeness and effective utilization persist. This paper introduces Profile2Gen, a novel data-centric framework designed to guide the generation and refinement of synthetic data, focusing on addressing hard-to-learn samples in regression tasks. We conducted approximately 18,000 experiments to validate its effectiveness across six medical datasets, utilizing seven state-of-the-art generative models. Results demonstrate that refined synthetic samples can reduce predictive errors and enhance model reliability. Additionally, we generalize the DataIQ framework to support regression tasks, enabling its application in broader contexts. Statistical analyses confirm that our approach achieves equal or superior performance compared to models trained exclusively on real data.","Healthcare researchers often face the fundamental problem of never having enough patient data to train AI models properly. Privacy laws and data collection challenges restrict the quantity of available data. It is like teaching someone to drive using only five driving lessons.Our work helps address this by creating synthetic patient data that mimics real data. Previous research has shown that some examples are harder for AI models to learn than others. We developed Profile2Gen to generate synthetic data considering, identifying, and treating those ""difficult cases"". Instead of only copying sample patterns, our approach refines the data through multiple steps to improve its quality. We tested this across 18,000 experiments using real medical datasets. The result? We found that our generated data improves model accuracy and, in some cases, performs better than using only real data. Our approach enables researchers and clinicians to train more reliable AI models, even in data-scarce situations, supporting better diagnostics and personalized treatments."
Poster,Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44535,"Laixi Shi, Jingchu Gai, Eric Mazumdar, Yuejie Chi, Adam Wierman","Standard multi-agent reinforcement learning (MARL) algorithms are vulnerable to sim-to-real gaps. To address this, distributionally robust Markov games (RMGs) have been proposed to enhance robustness in MARL by optimizing the worst-case performance when game dynamics shift within a prescribed uncertainty set. RMGs remains under-explored, from reasonable problem formulation to the development of sample-efficient algorithms. Two notorious and open challenges are the formulation of the uncertainty set and whether the corresponding RMGs can overcome the curse of multiagency,  where the sample complexity scales exponentially with the number of agents. In this work, we propose a natural class of RMGs inspired by behavioral economics, where each agent's uncertainty set is shaped by both the environment and the integrated behavior of other agents. We first establish the well-posedness of this class of RMGs by proving the existence of game-theoretic solutions such as robust Nash equilibria and coarse correlated equilibria (CCE). Assuming access to a generative model, we then introduce a sample-efficient algorithm for learning the CCE whose sample complexity scales polynomially with all relevant parameters. To the best of our knowledge, this is the first algorithm to break the curse of multiagency for RMGs, regardless of the uncertainty set formulation.","Multi-agent reinforcement learning (MARL) involves training multiple decision-makers (agents), such as robots or self-driving cars, to work together or compete in dynamic environments. While these systems work well in simulations, they often fail when deployed in the real world due to uncertainty in the environment and the behaviors of other agents. To address this, distributionally robust Markov games (RMGs) have been proposed to enhance robustness in MARL by optimizing the worst-case performance when game dynamics shift within a prescribed uncertainty set. RMGs remain under-explored, from reasonable problem formulation to the development of sample-efficient algorithms. The authors propose a new, more realistic formulation of uncertainty inspired by behavioral economics and develop a learning algorithm, Robust-Q-FTRL, that can efficiently train agents even as their number increases. This work makes it possible to train robust multi-agent systems that scale efficiently and are more practical for real-world utilization."
Poster,Breaking the Quadratic Barrier: Robust Cardinality Sketches for Adaptive Queries,https://ICML.cc//virtual/2025/poster/43886,"Edith Cohen, Mihir Singhal, Uri Stemmer","Cardinality sketches are compact data structures that efficiently estimate the number of distinct elements across multiple queries while minimizing storage, communication, and computational costs. However, recent research has shown that these sketches can fail under {\em adaptively chosen queries}, breaking down after approximately $\tilde{O}(k^2)$ queries, where $k$ is the sketch size.In this work, we overcome this \emph{quadratic barrier} by designing robust estimators with fine-grained guarantees. Specifically, our constructions can handle an {\em exponential number of adaptive queries}, provided that each element participates in at most $\tilde{O}(k^2)$ queries. This effectively shifts the quadratic barrier from the total number of queries to the number of queries {\em sharing the same element}, which can be significantly smaller. Beyond cardinality sketches, our approach expands the toolkit for robust algorithm design.","Many applications—from monitoring web traffic to counting unique users—rely on compact data summaries called *cardinality sketches* to estimate how many distinct items are present. These sketches save space and time, but recent work has shown that they can fail when queries are chosen based on previous answers—a situation common in adaptive systems like feedback loops or real-time monitoring.Current methods break down after about \( k^2 \) adaptive queries, where \( k \) is the sketch size, creating a fundamental barrier.We develop new *robust estimators* that overcome this limitation by shifting the focus from the total number of queries to how often individual items are queried. As long as each item appears in at most \( \tilde{O}(k^2) \) queries, our estimators remain accurate, even when the total number of queries is much larger.This means the sketches can safely support far more adaptive queries in practice, especially when most items are rarely repeated.Our approach provides theoretical guarantees and performs well in simulations, improving real-world robustness by up to 100×."
