type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,When Maximum Entropy Misleads Policy Optimization,https://ICML.cc//virtual/2025/poster/45888,"Ruipeng Zhang, Ya-Chien Chang, Sicun Gao","The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading approach for achieving efficient learning and robust performance across many RL tasks. However, MaxEnt methods have also been shown to struggle with performance-critical control problems in practice, where non-MaxEnt algorithms can successfully learn. In this work, we analyze how the trade-off between robustness and optimality affects the performance of MaxEnt algorithms in complex control tasks: while entropy maximization enhances exploration and robustness, it can also mislead policy optimization, leading to failure in tasks that require precise, low-entropy policies. Through experiments on a variety of control problems, we concretely demonstrate this misleading effect. Our analysis leads to better understanding of how to balance reward design and entropy maximization in challenging control problems.","Many AI systems learn by rewarding good actions while also encouraging randomness, a technique known as “maximum entropy” learning. This randomness helps AI explore new strategies and be more fault-tolerant, but it can backfire when a task demands very precise, consistent actions—such as drone control or guiding a four-legged robot. To understand this trade-off, we tested entropy-based algorithms on several control challenges and discovered that forcing too much randomness can actually mislead the learning process, causing the AI to miss the exact movements it needs. By examining these failures in detail, we pinpointed how and why entropy maximization can override the drive for precision. Our results provide AI researchers with guidance on developing training strategies that balance exploration and utility in real-world control scenarios."
Poster,When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class,https://ICML.cc//virtual/2025/poster/45470,"Yujin Kim, Hyunsoo Kim, Hyunwoo Kim, Suhyun Kim","Open-source pre-trained models hold great potential for diverse applications, but their utility declines when their training data is unavailable. Data-Free Image Synthesis (DFIS) aims to generate images that approximate the learned data distribution of a pre-trained model without accessing the original data. However, existing DFIS methods produce samples that deviate from the training data distribution due to the lack of prior knowledge about natural images. To overcome this limitation, we propose DDIS, the first Diffusion-assisted Data-free Image Synthesis method that leverages a text-to-image diffusion model as a powerful image prior, improving synthetic image quality. DDIS extracts knowledge about the learned distribution from the given model and uses it to guide the diffusion model, enabling the generation of images that accurately align with the training data distribution. To achieve this, we introduce Domain Alignment Guidance (DAG) that aligns the synthetic data domain with the training data domain during the diffusion sampling process. Furthermore, we optimize a single Class Alignment Token (CAT) embedding to effectively capture class-specific attributes in the training dataset. Experiments on PACS and ImageNet demonstrate that DDIS outperforms prior DFIS methods by generating samples that better reflect the training data distribution, achieving SOTA performance in data-free applications.","Many powerful AI models are available, but using them for new tasks often requires their original training data, which is frequently unavailable due to privacy or copyright issues. Data-Free Image Synthesis (DFIS) offers a solution by creating synthetic images that resemble the original training data. However, current DFIS methods struggle because they generate images without a basic understanding of what natural images look like, leading to artificial-looking results that aren't truly like the original data. Our proposed method, called DDIS (Diffusion-assisted Data-free Image Synthesis), solves this by being the first to use a Text-to-Image (T2I) diffusion model, like Stable Diffusion. These T2I models already know a lot about natural images, which helps DDIS synthesize much more realistic images.DDIS operates through two key mechanisms: Firstly, Domain Alignment Guidance (DAG) helps the synthetic images look like they belong to the same ""style"" of images as the original training data. Secondly, Class Alignment Token (CAT) helps DDIS capture specific details for each category of image. By combining these techniques, DDIS creates synthetic images that are much closer to the real training data. This allows us to use pre-trained models effectively for tasks like knowledge distillation (transferring knowledge from a large model to a smaller one) or model pruning (making models more efficient), even when the original training data isn't available."
Poster,When to Forget? Complexity Trade-offs in Machine Unlearning,https://ICML.cc//virtual/2025/poster/43736,"Martin Van Waerebeke, Marco Lorenzi, Giovanni Neglia, Kevin Scaman","Machine Unlearning (MU) aims at removing the influence of specific data points from a trained model, striving to achieve this at a fraction of the cost of full model retraining. In this paper, we analyze the efficiency of unlearning methods and establish the first upper and lower bounds on minimax computation times for this problem, characterizing the performance of the most efficient algorithm against the most difficult objective function. Specifically, for strongly convex objective functions and under the assumption that the forget data is inaccessible to the unlearning method, we provide a phase diagram for the *unlearning complexity ratio*---a novel metric that compares the computational cost of the best unlearning method to full model retraining. The phase diagram reveals three distinct regimes: one where unlearning at a reduced cost is infeasible, another where unlearning is trivial because adding noise suffices, and a third where unlearning achieves significant computational advantages over retraining. These findings highlight the critical role of factors such as data dimensionality, the number of samples to forget, and privacy constraints in determining the practical feasibility of unlearning.","Machine Unlearning (MU) is about removing the impact of certain pieces of data from a machine learning model—like making the model “forget” something it previously learned—without having to retrain it entirely from scratch. In this paper, we explore how efficient these unlearning methods can be and provide the first clear boundaries on how fast or slow this process can possibly be, even in the best or worst cases. We focus on a specific setting where the data to be forgotten isn’t available during the unlearning process and where the learning problem follows some well-behaved mathematical rules. We introduce a new way to measure how much faster unlearning can be compared to full retraining and show that depending on the situation, three different outcomes are possible: unlearning might be impossible without full retraining, very easy using simple techniques like adding noise, or genuinely much faster than starting over. Our results show that whether unlearning is practical depends heavily on the complexity of the data, how much data needs to be forgotten, and how strict the privacy requirements are."
Poster,When to retrain a machine learning model,https://ICML.cc//virtual/2025/poster/44981,"Florence Regol, Leo Schwinn, Kyle Sprague, Mark Coates, Thomas L Markovich","A significant challenge in maintaining real-world machine learning models is responding to the continuous and unpredictable evolution of data. Most practitioners are faced with the difficult question: when should I retrain or update my machine learning model? This seemingly straightforward problem is particularly challenging for three reasons: 1) decisions must be made based on very limited information - we usually have access to only a few examples, 2) the nature, extent, and impact of the distribution shift are unknown, and 3) it involves specifying a cost ratio between retraining and poor performance, which can be hard to characterize. Existing works address certain aspects of this problem, but none offer a comprehensive solution. Distribution shift detection falls short as it cannot account for the cost trade-off; the scarcity of the data, paired with its unusual structure, makes it a poor fit for existing offline reinforcement learning methods, and the online learning formulation overlooks key practical considerations. To address this, we present a principled formulation of the retraining problem and propose an uncertainty-based method that makes decisions by continually forecasting the evolution of model performance evaluated with a bounded metric. Our experiments, addressing classification tasks, show that the method consistently outperforms existing baselines on 7 datasets. We thoroughly assess its robustness to varying cost trade-off values and mis-specified cost trade-offs.","One of the biggest challenges in keeping machine learning models working well in the real world is handling changes in the data over time. A common and difficult question that practitioners face is: When should I retrain or update my model? This question sounds simple but is actually quite complex, for a few key reasons: 1) We usually have very limited data to make decisions with, 2)we don’t know how the data has changed or how much that change will affect the model’s performance, and 3) we have to weigh the cost of retraining the model against the cost of keeping a model that might perform poorly which can be hard to define.While there are existing methods that address parts of this problem, none provide a complete solution. For example, detecting data shifts doesn’t tell us whether retraining is worthwhile; online methods usually focus only on maintaining performance, and other approaches often require large amounts of data to be effective.To tackle this, we introduce a new, well-defined approach to the retraining problem. Our method uses uncertainty estimates to decide when to retrain, based on predictions about how the model’s performance will change over time. In our experiments on classification tasks across 7 datasets, our method consistently beats existing techniques. We also test how well it handles different and even incorrect assumptions about the cost of retraining versus performance loss."
Poster,"When, Where and Why to Average Weights?",https://ICML.cc//virtual/2025/poster/45698,"Niccolò Ajroldi, Antonio Orvieto, Jonas Geiping","Averaging checkpoints along the training trajectory is a simple yet powerful approach to improve the generalization performance of Machine Learning models and reduce training time. Motivated by these potential gains, and in an effort to fairly and thoroughly benchmark this technique, we present an extensive evaluation of averaging techniques in modern Deep Learning, which we perform using AlgoPerf, a large-scale benchmark for optimization algorithms. We investigate whether weight averaging can reduce training time, improve generalization, and replace learning rate decay, as suggested by recent literature. Our evaluation across seven architectures and datasets reveals that averaging significantly accelerates training and yields considerable efficiency gains across all considered workloads, at the price of a minimal implementation and memory cost, while mildly improving generalization. Finally, we explore the relationship between averaging and learning rate annealing and show that combining the two achieves optimal performance.","Modern neural networks are parametrized by many parameters that evolve throughout the training process, when the model learns to predict data from a training distribution. Previous works have shown that averaging the parameters across the values visited during training makes these models more robust and speeds up convergence.This works investigates this idea in details, considering seven different model architectures across six different machine learning tasks, benchmarking the effectiveness of averaging for modern deep learning.We show that averaging can indeed speed up training, saving valuable computational resources, and we find that doing so brings modest generalization gains. Finally, we highlight the connection between averaging and other important parts of the optimization pipeline."
Poster,When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series,https://ICML.cc//virtual/2025/poster/45978,"Min-Yeong Park, Won-Jeong Lee, Seong Tae Kim, Gyeong-Moon Park","Recently, forecasting future abnormal events has emerged as an important scenario to tackle realworld necessities. However, the solution of predicting specific future time points when anomalies will occur, known as Anomaly Prediction (AP), remains under-explored. Existing methods dealing with time series data fail in AP, focusing only on immediate anomalies or failing to provide precise predictions for future anomalies. To address AP, we propose a novel framework called Anomaly to Prompt (A2P), comprised of Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To enable the forecasting model to forecast abnormal time points, we adopt a strategy to learn the relationships of anomalies. For the robust detection of anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP) that simulates diverse anomaly patterns using signal-adaptive prompt. Comprehensive experiments on multiple real-world datasets demonstrate the superiority of A2P over state-of-the-art methods, showcasing its ability to predict future anomalies.","While previous work has focused on either forecasting future time series or detecting anomalies in existing data, no method has addressed the challenge of predicting when anomalies will occur in the future.To address this, we propose a unified model that combines time series forecasting with anomaly detection. Since anomalies are rare in time series data, we pretrain our model using a diverse set of learnable synthetic anomalies. This enables the model to recognize a wide range of abnormal patterns. As a result, our model demonstrates strong performance in accurately predicting the timing of anomalies across various real-world time series datasets.The key contribution of our work is that it opens up a new direction for time series research, introducing the first model that can predict when anomalies will occur."
Poster,Where is the Truth? The Risk of Getting Confounded in a Continual World,https://ICML.cc//virtual/2025/poster/44559,"Florian Peter Busch, Roshni Ramanna Kamath, Rupert Mitchell, Wolfgang Stammer, Kristian Kersting, Martin Mundt","A dataset is confounded if it is most easily solved via a spurious correlation which fails to generalize to new data. In this work, we show that, in a continual learning setting where confounders may vary in time across tasks, the challenge of mitigating the effect of confounders far exceeds the standard forgetting problem normally considered. In particular, we provide a formal description of such continual confounders and identify that, in general, spurious correlations are easily ignored when training for all tasks jointly, but it is harder to avoid confounding when they are considered sequentially. These descriptions serve as a basis for constructing a novel CLEVR-based continually confounded dataset, which we term the ConCon dataset. Our evaluations demonstrate that standard continual learning methods fail to ignore the dataset's confounders. Overall, our work highlights the challenges of confounding factors, particularly in continual learning settings, and demonstrates the need for developing continual learning methods to robustly tackle these.","In machine learning, models can sometimes rely on shortcuts to make predictions. These shortcuts, called confounders, might work well on training data but often fail when the model sees new, different data. In our research, we explore a particularly difficult situation: when these misleading patterns change over time as the model learns from a sequence of tasks (a setting called continual learning).We introduce the concept of continual confounders, i.e., spurious information that changes between tasks, and show that dealing with them is even more challenging than the well-known problem of forgetting old tasks in continual learning. While confounders are relatively easy to avoid when all tasks are learned at once, they become much trickier to handle when tasks come one at a time.To study this, we built ConCon: a new dataset based on the CLEVR visual reasoning benchmark, designed specifically to test how well models can handle continually changing confounders. We found that existing continual learning methods struggle with this setup.Our findings highlight an important gap in current machine learning techniques and suggest that new approaches are needed to help models learn robustly in the face of changing, misleading information."
Poster,Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems,https://ICML.cc//virtual/2025/poster/45823,"Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu","Failure attribution in LLM multi-agent systems—identifying the agent and step responsible for task failures—provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems.To support this initiative, we introduce the Who\&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps.Using the Who\&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons.  The best method achieves 53.5\% accuracy in identifying failure-responsible agents but only 14.2\% in pinpointing failure steps, with some methods performing below random.  Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available in https://github.com/mingyin1/Agents_Failure_Attribution.","When teams of AI agents work together to solve a task, it's often hard to figure out what went wrong if they fail. Specifically, it's difficult to identify which agent made the mistake and at what point in the process it happened. This is an important problem for improving how these systems work, but it's rarely studied and very time-consuming to do by hand.In our research, we introduce a new area of study focused on automatically finding the causes of failure in multi-agent AI systems. We created a dataset called Who&When, which includes detailed records of failures from 127 agentic systems, clearly showing which agent made the error and when.We also tested several methods to automatically detect these failures. Our findings show this is a tough problem that needs more research."
Poster,Which Attention Heads Matter for In-Context Learning?,https://ICML.cc//virtual/2025/poster/46081,"Kayo Yin, Jacob Steinhardt","Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to generate relevant responses from a handful of task demonstrations in the prompt. Prior studies have suggested two different explanations for the mechanisms behind ICL:induction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task.To better understand which of the two distinct mechanisms drives ICL, we study and compare induction heads and FV heads in 12 language models. Through detailed ablations, we find that few-shot ICL is driven primarily by FV heads, especially in larger models. We also find that FV and induction heads are connected: many FV headsstart as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction facilitates learning the more complex FV mechanism for ICL.","Large language models are surprisingly good at picking up new tasks just by seeing a few examples in their input. This skill is called in-context learning. But what allows these models to do this so well?We looked inside these models to find out which internal components are most important for this learning ability. We focused on two types of “attention heads”—tiny components that help the model decide what information in the input to focus on. One type, called ""induction heads"", helps the model find and copy patterns from the examples it’s shown. The other type, called ""function vector (FV) heads"", seems to help the model understand the overall task it’s being asked to do, given examples.By turning off each type of attention head in several models and seeing how that affects their performance, we discovered that FV heads are especially important for in-context learning, especially in larger models. We also found that many FV heads actually start out as induction heads when the model is first training, which suggests that the simpler induction heads may be a stepping stone for models to acquire FV heads.By figuring out the mechanisms in AI models that are responsible for their learning abilities, we can design smarter, safer, and more understandable AI systems in the future."
Poster,Whitened CLIP as a Likelihood Surrogate of Images and Captions,https://ICML.cc//virtual/2025/poster/45897,"Roy Betser, Meir Yossef Levi, Guy Gilboa","Likelihood approximations for images are not trivial to compute and can be useful in many applications. We examine the use of Contrastive Language-Image Pre-training (CLIP) to assess the likelihood of images and captions. We introduce Whitened CLIP, a novel transformation of the CLIP latent space via an invertible linear operation. This transformation ensures that each feature in the embedding space has zero mean, unit standard deviation, and no correlation with all other features, resulting in an identity covariance matrix. We show that the whitened embedding statistics can be well approximated by a standard normal distribution, allowing log-likelihood to be estimated using the squared Euclidean norm in the whitened space. The whitening procedure is completely training-free and uses a precomputed whitening matrix, making it extremely fast. We present several preliminary experiments demonstrating the properties and applicability of these likelihood scores to images and captions. Our code is available at github.com/rbetser/W_CLIP/tree/main.","CLIP is a powerful AI model that connects images and text, but it doesn’t indicate how typical or unusual an image or caption is. In this work, we introduce a simple, training-free method that enables CLIP to estimate the likelihood of an image or caption based on its internal representation. This allows us to detect AI-generated or suspicious images more effectively, assess whether a caption is simple or complex, and identify image domain shifts. Our approach is fast, general, and requires no labeled data, making it valuable for tasks like fake image detection and enhancing AI safety."
