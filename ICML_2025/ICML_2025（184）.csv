type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning,https://ICML.cc//virtual/2025/poster/46515,"Peter Eckmann, Dongxia Wu, Germano Heinzelmann, Michael Gilson, Rose Yu","Current generative models for drug discovery primarily use molecular docking as an oracle to guide the generation of active compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show real-world experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. To address this challenge, we propose Multi-Fidelity Latent space Active Learning (MF-LAL), a generative modeling framework that integrates a set of oracles with varying cost-accuracy tradeoffs. Using active learning, we train a surrogate model for each oracle and use these surrogates to guide generation of compounds with high predicted activity. Unlike previous approaches that separately learn the surrogate model and generative model, MF-LAL combines the generative and multi-fidelity surrogate models into a single framework, allowing for more accurate activity prediction and higher quality samples. Our experiments on two disease-relevant proteins show that MF-LAL produces compounds with significantly better binding free energy scores than other single and multi-fidelity approaches (~50% improvement in mean binding free energy score). The code is available at https://github.com/Rose-STL-Lab/MF-LAL.","Designing new drugs often involves using software to predict which molecules might work well and treat a disease of interest, but these tools can be unreliable. Many current methods rely on a fast but inaccurate method called molecular docking, which often fails to reproduce real-world results. More accurate methods exist, like simulations that predict how strongly a drug binds to its target, but these are too slow and expensive to use often. Our new method, called MF-LAL, integrates the fast and slow methods, and learns when and how to use each tool to balance speed and accuracy. By training a model that learns from all these tools together, MF-LAL does a better job at finding promising drug candidates. In tests on two disease-related proteins, our method found compounds that scored about 50% better using the highest-accuracy method than those from previous approaches."
Poster,MGD$^3$ : Mode-Guided Dataset Distillation using Diffusion Models,https://ICML.cc//virtual/2025/poster/45507,"Jeffrey A. Chan-Santiago, praveen tirupattur, Gaurav Kumar Nayak, Gaowen Liu, Mubarak Shah","Dataset distillation has emerged as an effective strategy, significantly reducing training costs and facilitating more efficient model deployment.Recent advances have leveraged generative models to distill datasets by capturing the underlying data distribution. Unfortunately, existing methods require model fine-tuning with distillation losses to encourage diversity and representativeness. However, these methods do not guarantee sample diversity, limiting their performance.We propose a mode-guided diffusion model leveraging a pre-trained diffusion model without the need to fine-tune with distillation losses. Our approach addresses dataset diversity in three stages: Mode Discovery to identify distinct data modes, Mode Guidance to enhance intra-class diversity, and Stop Guidance to mitigate artifacts in synthetic samples that affect performance.We evaluate our approach on ImageNette, ImageIDC, ImageNet-100, and ImageNet-1K, achieving accuracy improvements of 4.4%, 2.9%, 1.6%, and 1.6%, respectively, over state-of-the-art methods. Our method eliminates the need for fine-tuning diffusion models with distillation losses, significantly reducing computational costs.","Training state-of-the-art vision models is resource-intensive due to the need for large-scale datasets. Dataset distillation offers a promising alternative by synthesizing compact datasets that preserve the essential information needed to train models so that, when used for training, they achieve performance comparable to using the full dataset. However, existing methods often lack diversity in the generated samples and rely on costly fine-tuning. We propose a novel approach that leverages a pre-trained diffusion model to generate diverse, high-quality synthetic data without the need for distillation-specific training. We propose a three-stage mechanism that enforces diversity in synthetic samples while mitigating artifacts. This enables more efficient training pipelines, achieving high accuracy with significantly reduced data and compute requirements."
Poster,MIB: A Mechanistic Interpretability Benchmark,https://ICML.cc//virtual/2025/poster/43836,"Aaron Mueller, Atticus Geiger, Sarah Wiegreffe, Dana Arad, Iván Arcuschin, Adam Belfki, Yik Siu Chan, Jaden Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash, Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan Belinkov","How can we know whether new mechanistic interpretability methods achieve real improvements?In pursuit of lasting evaluation standards, we propose MIB, a Mechanistic Interpretability Benchmark, with two tracks spanning four tasks and five models. MIB favors methods that precisely and concisely recover relevant causal pathways or causal variables in neural language models. The circuit localization track compares methods that locate the model components---and connections between them---most important for performing a task (e.g., attribution patching or information flow routes). The causal variable track compares methods that featurize a hidden vector, e.g., sparse autoencoders (SAE) or distributed alignment search (DAS), and align those features to a task-relevant causal variable. Using MIB, we find that attribution and mask optimization methods perform best on circuit localization. For causal variable localization, we find that the supervised DAS method performs best, while SAEs features are not better than neurons, i.e., non-featurized hidden vectors. These findings illustrate that MIB enables meaningful comparisons, and increases our confidence that there has been real progress in the field.","A primary goal of mechanistic interpretability research is to understand how large language models make decisions by finding the specific parts inside these models that handle specific tasks, and then figuring out what these parts do. Many different methods have been developed to locate these important components, but until now, there was no standard way to compare how well these methods actually work.In this paper, we propose MIB (a Mechanistic Interpretability Benchmark), a large-scale dataset for evaluating and comparing mechanistic interpretability methods. The benchmark has two parts: one checks whether a method can find all the important components for a specific task (regardless of what they do); the other tests whether, given a concept, a method can locate where that concept is computed in the model. Using this benchmark, we find that some approaches (generally newer ones) are consistently more effective across different models and tasks. We also recover known findings, which acts as a sanity check. This provides evidence that the field is making real progress in understanding which components do what in language models."
Poster,MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance,https://ICML.cc//virtual/2025/poster/46679,"Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, JunqiCheng, Yuefeng Zhu, FangYuan Zou","In recent years, while generative AI has advanced significantly in image generation, video generation continues to face challenges in controllability, length, and detail quality, which hinder its application. We present MimicMotion, a framework for generating high-quality human videos of arbitrary length using motion guidance. Our approach has several highlights. Firstly, we introduce confidence-aware pose guidance that ensures high frame quality and temporal smoothness. Secondly, we introduce regional loss amplification based on pose confidence, which reduces image distortion in key regions. Lastly, we propose a progressive latent fusion strategy to generate long and smooth videos. Experiments demonstrate the effectiveness of our approach in producing high-quality human motion videos. Videos and comparisons are available at [https://tencent.github.io/MimicMotion](https://tencent.github.io/MimicMotion).","Generating realistic videos with AI remains challenging, particularly when creating detailed human movements over long durations. To tackle this, we developed MimicMotion, an AI framework that generates smooth, realistic videos of humans performing movements of any length. Our method uses pose templates to guide human video generation precisely, ensuring each frame stays clear and movements flow naturally. We also enhance hand generation to prevent distortion and employ a progressive blending technique to produce smoother, longer videos. This technology opens up new possibilities in areas like filmmaking and animation."
Poster,MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data,https://ICML.cc//virtual/2025/poster/46635,"Yuqin Dai, Zhouheng Yao, Chunfeng Song, Qihao Zheng, Weijian Mai, Kunyu Peng, Shuai Lu, Wanli Ouyang, Jian Yang, Jiamin Wu","Brain decoding aims to reconstruct visual perception of human subject from fMRI signals, which is crucial for understanding brain's perception mechanisms.  Existing methods are confined to the single-subject paradigm due to substantial brain variability, which leads to weak generalization across individuals and incurs high training costs, exacerbated by limited availability of fMRI data. To address these challenges, we propose MindAligner, an explicit functional alignment framework for cross-subject brain decoding from limited fMRI data. The proposed MindAligner enjoys several merits. First, we learn a Brain Transfer Matrix (BTM) that projects the brain signals of an arbitrary new subject to one of the known subjects, enabling seamless use of pre-trained decoding models. Second, to facilitate reliable BTM learning, a Brain Functional Alignment module is proposed to perform soft cross-subject brain alignment under different visual stimuli with a multi-level brain alignment loss, uncovering fine-grained functional correspondences with high interpretability. Experiments indicate that MindAligner not only outperforms existing methods in visual decoding under data-limited conditions, but also provides valuable neuroscience insights in cross-subject functional analysis. The code will be made publicly available.","fMRI decoding is like mind-reading: it reconstructs what someone sees based on their brain scans. However, because each person’s brain response patterns vary so much, and because we usually have only a handful of scans, training a single model that works well for everyone is both difficult and expensive.To overcome this, we developed MindAligner, a method that1. learns to translate a new subject’s brain signals into a shared, “universal” format,2. aligns those translated signals to patterns drawn from other people viewing different images,3. plugs the aligned signals into an existing decoder to predict what the person actually saw.Even when given very little data from a new participant, MindAligner maintains high decoding accuracy. By making it practical to reuse pre-trained models across individuals, our approach promises to accelerate both basic neuroscience research and real-world medical applications."
Poster,MindCustomer: Multi-Context Image Generation Blended with Brain Signal,https://ICML.cc//virtual/2025/poster/46700,"Muzhou Yu, Shuyun Lin, Lei Ma, Bo Lei, Kaisheng Ma","Advancements in generative models have promoted text- and image-based multi-context image generation. Brain signals, offering a direct representation of user intent, present new opportunities for image customization. However, it faces challenges in brain interpretation, cross-modal context fusion and retention. In this paper, we present MindCustomer to explore the blending of visual brain signals in multi-context image generation. We first design shared neural data augmentation for stable cross-subject brain embedding by introducing the Image-Brain Translator (IBT) to generate brain responses from visual images. Then, we propose an effective cross-modal information fusion pipeline that mask-freely adapts distinct semantics from image and brain contexts within a diffusion model. It resolves semantic conflicts for context preservation and enables harmonious context integration. During the fusion pipeline, we further utilize the IBT to transfer image context to the brain representation to mitigate the cross-modal disparity. MindCustomer enables cross-subject generation, delivering unified, high-quality, and natural image outputs. Moreover, it exhibits strong generalization for new subjects via few-shot learning, indicating the potential for practical application. As the first work for multi-context blending with brain signal, MindCustomer lays a foundational exploration and inspiration for future brain-controlled generative technologies.","Imagine creating an image just by thinking about it. Brain signals, which reflect what we see or imagine, could make this possible. But using brain activity to guide image generation is extremely difficult — brain data is noisy, varies from person to person, and doesn’t easily match up with visual information like pictures or text.Our research introduces MindCustomer, a system that blends brain signals with other visual clues (like text or images) to create customized images. First, we use a tool to help the system understand brain patterns more consistently across different people. Then, we developed a way to combine brain signals and visual inputs without conflicts, so the final image reflects both sources naturally.MindCustomer generates high-quality, personalized images—even for new users with very little training data. As the first tool to fuse brain signals into multi-context image creation, it opens the door to brain-driven creative tools and future technologies that respond directly to human thoughts."
Poster,MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-text Decoding,https://ICML.cc//virtual/2025/poster/45933,"Weikang Qiu, Zheng Huang, Haoyu Hu, Aosong Feng, Yujun Yan, ZHITAO YING","Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by $12.0\%$, unseen subject generalization by $24.5\%$, and novel task adaptation by $25.0\%$. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.","The paper proposes MindLLM, a subject-agnostic fMRI-to-text decoding model that can perform various tasks (i.e., versatile decoding). To enable subject-agnostic decoding, we design a novel fMRI encoder inspired by neuroscientific insights. To enable versatile decoding, we introduce Brain Instruction Tuning and the corresponding datasets. Our model demonstrates the state-of-the-art performance on a variety of benchmarks, including downstream tasks, unseen subject generalization and new task adaptation. Additional analyses and visualizations validate the design of components in our method."
Poster,Mind the Gap: A Practical Attack on GGUF Quantization,https://ICML.cc//virtual/2025/poster/45172,"Kazuki Egashira, Robin Staab, Mark Vero, Jingxuan He, Martin Vechev","With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama.cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.","Quantization is a key technique for running large language models (LLMs) more efficiently.It reduces memory usage without sacrificing performance.In general, a model is expected to behave similarly before and after quantization.However, if an attacker trains a model with bad intentions, they can make it behave safely in full precision, and only trigger harmful behavior after it is quantized.This is risky because a user might try out a full-precision model, decide it is safe and useful, then quantize it to run on a smaller device, only to unknowingly activate a hidden attack.While similar attack ideas have been explored in past research, they have mostly focused on classical, simpler quantization methods.Here we show for the first time that this kind of attack also works on a more accurate and widely used quantization format in real-world deployments, the GGUF format."
Poster,Mind the Gap: a Spectral Analysis of Rank Collapse and Signal Propagation in Attention Layers,https://ICML.cc//virtual/2025/poster/43837,"Thiziri Nait Saada, Alireza Naderi, Jared Tanner","Attention layers are the core component of transformers, the current state-of-the-art neural network architecture. Alternatives to softmax-based attention are being explored due to its tendency to hinder effective information flow. Even *at initialisation*, it remains poorly understood why the propagation of signals and gradients through these random networks can be pathological, resulting in issues known as (i) vanishing/exploding gradients and (ii) rank collapse *in depth*, i.e. when all tokens converge to a single representation along layers. While rank collapse in depth naturally arises from repeated matrix multiplications---a common pattern across various architectures---we identify an additional and previously unknown challenge unique to softmax attention layers: (iii) rank collapse *in width*, which occurs as the context length increases. Using Random Matrix Theory, we conduct a rigorous analysis that uncovers a spectral gap between the two largest singular values of the attention matrix as the cause of (iii), which in turn exacerbates (i) and (ii).Building on this insight, we propose a novel yet simple practical solution to mitigate rank collapse in width by removing the outlier eigenvalue(s). Our theoretical framework offers a fresh perspective on recent practical studies, such as (Ye et al., 2024; Ali et al., 2023), whose ad hoc solutions can now be interpreted as implicit efforts to address the spectral gap issue. This work provides valuable theoretical support for ongoing large-scale empirical research, bringing theory and practice one step closer in the understanding of transformers.","Transformers are powerful AI models behind tools like language translators and chatbots. A key part of how they work is called attention, which helps the model decide what information to focus on. However, this attention mechanism can sometimes fail, especially when the model is dealing with long inputs or just starting to learn. When that happens, the model may treat different inputs as if they were the same, making it hard to learn or generate useful outputs.Our research identifies a new reason for this problem: a hidden imbalance in the attention mechanism that gets worse as the input length increases. Using tools from mathematics, we show that this imbalance causes the model to lose important distinctions between data points—a problem we call rank collapse in width. We also propose a simple fix that helps the model stay stable and learn more effectively.This work sheds light on why some recent practical improvements to attention have worked and offers a clearer path forward for building better, more reliable AI systems."
Poster,Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse,https://ICML.cc//virtual/2025/poster/45714,"Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas Griffiths","Chain-of-thought (CoT) prompting has become a widely used strategy for improving large language and multimodal model performance. However, it is still an open question under which settings CoT systematically reduces performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, focusing on six representative tasks from the psychological literature where deliberation hurts performance in humans. In three of these tasks, state-of-the-art models exhibit significant performance drop-offs with CoT (up to 36.3\% absolute accuracy for OpenAI o1-preview compared to GPT-4o), while in others, CoT effects are mixed, with positive, neutral, and negative changes. While models and humans do not exhibit perfectly parallel cognitive processes, considering cases where thinking has negative consequences for humans helps identify settings where it negatively impacts models. By connecting the literature on human verbal thinking and deliberation with evaluations of CoT, we offer a perspective for understanding the impact of inference-time reasoning.","We find that chain-of-thought, a method for allowing LLMs to generate reasoning before providing responses, can lead to worse outcomes in similar failure cases to when people overthink. We do this by taking a broad literature in psychology on human overthinking and applying these tests to various AI models, upon which we find some drastic decreases in model performance. Our paper shows that allowing an AI to reason before responding is not always beneficial; instead, it is important to look at the specific use case and maybe even take a jab at it yourself."
