type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Zero-Inflated Bandits,https://ICML.cc//virtual/2025/poster/46013,"Haoyu Wei, Runzhe Wan, Lei Shi, Rui Song","Many real-world bandit applications are characterized by sparse rewards, which can significantly hinder learning efficiency. Leveraging problem-specific structures for careful distribution modeling is recognized as essential for improving estimation efficiency in statistics. However, this approach remains under-explored in the context of bandits. To address this gap, we initiate the study of zero-inflated bandits, where the reward is modeled using a classic semi-parametric distribution known as the zero-inflated distribution. We develop algorithms based on the Upper Confidence Bound and Thompson Sampling frameworks for this specific structure. The superior empirical performance of these methods is demonstrated through extensive numerical studies.","Many real-world computer systems need to make smart choices but often receive very little feedback about whether their decisions were good or bad. For example, in online advertising, most customers will not click the advertisement and hence the reward is zero with high probability.Our research introduces a new way to handle situations where feedback is very rare and created new computer algorithms that can learn more effectively in these challenging situations.Our findings have implications for designing better learning systems in such scenarios."
Poster,Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models,https://ICML.cc//virtual/2025/poster/44038,"Farzad Farhadzadeh, Debasmit Das, Shubhankar Borse, Fatih Porikli","We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient fine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained low-rank adjustments (e.g., LoRA) from a source to a target model without additional training data. This overcomes the limitations of traditional methods that require retraining when switching base models, often challenging due to data constraints. ProLoRA achieves this via projection of source adjustments into the target model's weight space, leveraging subspace and null space similarities and selectively targeting aligned layers. Evaluations on established text-to-image models demonstrate successful knowledge transfer and comparable performance without retraining.","We propose ProLoRA, a methodology enabling training-free transfer of adapters between source and target generative models. A key advantage is its ability to operate without any training dataset and execute offline."
Poster,Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints,https://ICML.cc//virtual/2025/poster/45357,"Dapeng Jiang, Xiangzhe Kong, Jiaqi Han, Mingyu Li, Rui Jiao, Wenbing Huang, Stefano Ermon, Jianzhu Ma, Yang Liu","Cyclic peptides, characterized by geometric constraints absent in linear peptides, offer enhanced biochemical properties, presenting new opportunities to address unmet medical needs. However, designing target-specific cyclic peptides remains underexplored due to limited training data. To bridge the gap, we propose CP-Composer, a novel generative framework that enables zero-shot cyclic peptide generation via composable geometric constraints. Our approach decomposes complex cyclization patterns into unit constraints, which are incorporated into a diffusion model through geometric conditioning on nodes and edges. During training, the model learns from unit constraints and their random combinations in linear peptides, while at inference, novel constraint combinations required for cyclization are imposed as input. Experiments show that our model, despite trained with linear peptides, is capable of generating diverse target-binding cyclic peptides, reaching success rates from 38\% to 84\% on different cyclization strategies.","How can we generate new target-specific cyclic peptides if we only have limited training data? Our solution is  using ""composable geometric constraints"" to guide generation model to generate new cyclic peptides. It is always a challenging problem to design a data-driven algorithm using limited data as deep learning scientists believe that a good model usually comes from a large amount of training data. To deepen our understanding of generated cyclic peptides, we show some visualizations of simple and high-order cyclic peptides. Furthermore, we release all of our code base on the github. You can also implement your own cyclization strategies in our code."
Poster,Zero-Shot Generalization of GNNs over Distinct Attribute Domains,https://ICML.cc//virtual/2025/poster/45294,"Yangyi Shen, Jincheng Zhou, Beatrice Bevilacqua, Joshua Robinson, Charilaos Kanatsoulis, Jure Leskovec, Bruno Ribeiro","Traditional Graph Neural Networks (GNNs) cannot generalize to new graphs with node attributes different from the training ones, making zero-shot generalization across different node attribute domains an open challenge in graph machine learning. In this paper, we propose STAGE, which encodes *statistical dependencies* between attributes rather than individual attribute values, which may differ in test graphs. By assuming these dependencies remain invariant under changes in node attributes, STAGE achieves provable generalization guarantees for a family of domain shifts. Empirically, STAGE demonstrates strong zero-shot performance on medium-sized datasets: when trained on multiple graph datasets with different attribute spaces (varying in types and number) and evaluated on graphs with entirely new attributes, STAGE achieves a relative improvement in Hits@1 between 40% to 103% in link prediction and a 10% improvement in node classification compared to state-of-the-art baselines.","Traditional Graph Neural Networks can't work on new graphs with different attributes than what they were trained on. We introduce STAGE, which focuses on relationships between attributes rather than their specific values. Like recognizing that ""taller people tend to buy larger sizes"" is a pattern that works across different product categories. In tests, STAGE outperformed existing methods by up to 103% when predicting user purchases across different domains and by 10% when predicting user information across different social networks. This breakthrough allows AI systems to transfer knowledge between completely different domains without additional training."
Poster,Zero Shot Generalization of Vision-Based RL Without Data Augmentation,https://ICML.cc//virtual/2025/poster/44254,"Sumeet Batra, Gaurav Sukhatme","Generalizing vision-based reinforcement learning (RL) agents to novel environments remains a difficult and open challenge. Current trends are to collect large-scale datasets or use data augmentation techniques to prevent overfitting and improve downstream generalization. However, the computational and data collection costs increase exponentially with the number of task variations and can destabilize the already difficult task of training RL agents. In this work, we take inspiration from recent advances in computational neuroscience and propose a model, Associative Latent DisentAnglement (ALDA), that builds on standard off-policy RL towards zero-shot generalization. Specifically, we revisit the role of latent disentanglement in RL and show how combining it with a model of associative memory achieves zero-shot generalization on difficult task variations *without* relying on data augmentation. Finally, we formally show that data augmentation techniques are a form of weak disentanglement and discuss the implications of this insight.","Humans and other mammals have shown a remarkable ability to adapt to new situations, thanks to their robust visual systems. Modern neuroscience hypothesizes that this is because we can decompose what we see into independent components and relate them to things we have seen before. For example, if shown a cartoon rendering of a human, because the human has two legs, arms, a torso, head, etc., most of us would understand that this cartoon is meant to represent a real human, even if we had never seen that specific cartoon image before. Inspired by this capability, we designed an artificial agent to decompose a visual scene into independent components and relate them to objects the agent has seen before, so that it can solve tasks even in novel environments i.e. where the colors, background, texture of objects, etc. are different. This allows our agent to generalize to new environments it has never seen without requiring additional training data."
Poster,Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer,https://ICML.cc//virtual/2025/poster/43960,"Yulun Wu, Doron Bergman","We present an Adversarially Pre-trained Transformer (APT) that is able to perform zero-shot meta-learning on tabular prediction tasks without using any real-world dataset to pre-train the model, extending on the recent development of Prior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained with adversarial synthetic data agents, who continue to shift their underlying data generating distribution and deliberately challenge the model with different synthetic datasets. In addition, we propose a mixture block model architecture that is able to handle classification tasks with arbitrary number of classes, addressing the class size limitation -- a crucial weakness of prior tabular zero-shot learning algorithms. In experiments, we show that our framework matches state-of-the-art performance on small tabular classification tasks without filtering on dataset characteristics such as number of classes and number of missing values, while maintaining an average runtime under one second. On common benchmark dataset suites in both classification and regression, we show that adversarial pre-training was able to enhance TabPFN's performance. In our analysis, we demonstrate that the adversarial synthetic data agents were able to generate a more diverse collection of data compared to the ordinary random generator in TabPFN. In addition, we demonstrate that our mixture block neural design has improved generalizability and greatly accelerated pre-training.","In traditional artificial intelligence (AI), researchers teach an AI model to learn the pattern of specific tasks (e.g. predicting tomorrow's weather, answering someone's questions) by optimizing the AI model on the past data of those tasks. In this work, we teach an AI model how to learn the pattern of unseen tasks on its own (i.e. teach it to learn how to learn), without optimizing it on any past data of these tasks. This is called zero-shot meta-learning. This is a very ambitious goal, and while prior work has made progress on this goal by drawing inspiration from the recent advancements in large language models, it only managed to achieve this on a constrained family of tasks with structured, tabular data.We proposed an improved approach that relieved some of these constraints and yielded improved performance, by establishing an AI adversary to generate difficult tasks for the AI model to learn. This AI adversary is like a competent chess rival to the AI model -- while the model makes moves to improve its ability to solve the tasks that the adversary produces, the adversary makes moves to produce more difficult tasks for the model to solve. By pitting them against each other, both the AI model's problem-solving ability and the AI adversary's problem-creating ability improve over time.Our proposal is independent of most recent advancements in zero-shot meta-learning, and hence does not create conflict when combining those advancements and our advancement. To help other researchers explore these combinations and further improve on this idea, we have released a free and easy-to-use tool called APT, along with the program’s settings."
Poster,Zero-Shot Offline Imitation Learning via Optimal Transport,https://ICML.cc//virtual/2025/poster/46209,"Thomas Rupf, Marco Bagatella, Nico Gürtler, Jonas Frey, Georg Martius","Zero-shot imitation learning algorithms hold the promise of reproducing unseen behavior from as little as a single demonstration at test time. Existing practical approaches view the expert demonstration as a sequence of goals, enabling imitation with a high-level goal selector, and a low-level goal-conditioned policy. However, this framework can suffer from myopic behavior: the agent's immediate actions towards achieving individual goals may undermine long-term objectives. We introduce a novel method that mitigates this issue by directly optimizing the occupancy matching objective that is intrinsic to imitation learning. We propose to lift a goal-conditioned value function to a distance between occupancies, which are in turn approximated via a learned world model. The resulting method can learn from offline, suboptimal data, and is capable of non-myopic, zero-shot imitation, as we demonstrate in complex, continuous benchmarks. The code is available at https://github.com/martius-lab/zilot.","A task for a robot (or agent) can be specified as a list of goals it should achieve in order. Most current methods make the robot go after each goal one at a time. But this can make the robot short-sighted: reaching the next goal as fast as possible might sacrifice any chance of reaching future goals. For example, if a robot arm should move an object first to position A and then to B, a fast way to achieve A might be to throw the object there. But this action could also make the object roll out of the arm’s reach after landing at position A, making it impossible to move to position B.Our approach helps robots (or agents) look at the big picture. Instead of just chasing after the next goal, the robot naturally plans its actions with future goals in mind, so that it can successfully follow the entire goal sequence from start to finish—even if the goals are specified only roughly. We show that this helps simulated robots perform more complex tasks, which is a step towards more reliable and flexible robots in the real world."
Poster,ZipAR: Parallel Autoregressive Image Generation through Spatial Locality,https://ICML.cc//virtual/2025/poster/45251,"Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, Bohan Zhuang","In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating autoregressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel. To ensure alignment with the contextual requirements of each token, we employ an adaptive local window assignment scheme with rejection sampling analogous to speculative decoding. By decoding multiple tokens in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining.","Generating images with AI models is slow because these models typically create one small piece of the image at a time, in sequence. This step-by-step process requires many computations, making it inefficient—especially for high-resolution images. We developed ZipAR, a method that speeds up image generation by predicting multiple pieces of the image simultaneously, without retraining the model. Our key insight is that distant parts of an image (like the top and bottom) often don’t depend on each other, so they can be generated in parallel. To ensure these parallel predictions stay coherent, we use an adaptive ""local window"" technique—similar to how a painter might sketch rough outlines of separate sections. ZipAR reduces the number of computations needed by up to 91% in some cases, drastically speeding up image generation while maintaining quality. This makes AI tools for art, design, and media production faster and more practical to use."
Poster,µnit Scaling: Simple and Scalable FP8 LLM Training,https://ICML.cc//virtual/2025/poster/43942,"Saaketh Narayan, Abhay Gupta, Mansheej Paul, Davis Blalock","Large language model training with 8-bit floating point (FP8) formats promises significant efficiency improvements, but reduced numerical precision makes training challenging. It is currently possible to train in FP8 only if one is willing to tune various hyperparameters, reduce model scale, or accept the overhead of computing dynamic scale factors. We demonstrate simple, scalable FP8 training that requires no dynamic scaling factors or special hyperparameters, even at large model sizes. Our method, \textit{µnit Scaling (µS)}, also enables simple hyperparameter transfer across model widths, matched numerics across training and inference, and other desirable properties. µnit Scaling is straightforward to implement, consisting of a set of minimal interventions based on a first-principles analysis of transformer operations. We validate our method by training models with parameters ranging from 1B to 13B, performing all hidden linear layer computations in FP8. We achieve quality equal to higher-precision baselines while also training up to 33% faster.","Large Language Model (LLM) training is very resource-intensive and expensive. Training typically uses 16-bit number formats and requires tuning knobs known as hyperparameters to achieve good performance. Using smaller, 8-bit formats like FP8 can make training faster, but is also more challenging as models get larger. Our paper presents a method, µnit Scaling (µS), that demonstrates simple, scalable FP8 training that works easily even at large model sizes. µS also keeps optimal hyperparameter values stable as model sizes get larger. We validate our method by training models from 1B to 13B parameters, performing all neural network computation in FP8, and obtaining higher quality models while training 33% faster."
