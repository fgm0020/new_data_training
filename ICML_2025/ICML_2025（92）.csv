type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Elucidating Flow Matching ODE Dynamics via Data Geometry and Denoisers,https://ICML.cc//virtual/2025/poster/44549,"Zhengchao Wan, Qingsong Wang, Gal Mishne, Yusu Wang","Flow matching (FM) models extend ODE sampler based diffusion models into a general framework, significantly reducing sampling steps through learned vector fields. However, the theoretical understanding of FM models, particularly how their sample trajectories interact with underlying data geometry, remains underexplored. A rigorous theoretical analysis of FM ODE is essential for sample quality, stability, and broader applicability. In this paper, we advance the theory of FM models through a comprehensive analysis of sample trajectories. Central to our theory is the discovery that the denoiser, a key component of FM models, guides ODE dynamics through attracting and absorbing behaviors that adapt to the data geometry. We identify and analyze the three stages of ODE evolution: in the initial and intermediate stages, trajectories move toward the mean and local clusters of the data. At the terminal stage, we rigorously establish the convergence of FM ODE under weak assumptions, addressing scenarios where the data lie on a low-dimensional submanifold---cases that previous results could not handle. Our terminal stage analysis offers insights into the memorization phenomenon and establishes equivariance properties of FM ODEs. These findings bridge critical gaps in understanding flow matching models, with practical implications for optimizing sampling strategies and architectures guided by the intrinsic geometry of data.","Modern image-generating AI models often use a process called the diffusion model, which gradually transforms random noise into realistic images. While effective, this process can be slow, requiring hundreds of steps to produce high-quality results. Flow Matching builds on this same core idea of noise-to-image transformation but provides a more direct generation path from noise to real data. However, the theoretical foundations of Flow Matching remain incomplete, and we lack rigorous guarantees that the generation path will reliably converge to realistic data.We analyze the generation path in flow matching and trace its evolution patterns. Our analysis shows that the path first moves toward the center of the data distribution, then shifts toward local clusters, and finally lands on a realistic sample. We also prove that, under broad conditions, the path reliably ends in the right place—even when the data lies on a complex, curved surface in a high-dimensional space.This work fills a key theoretical gap in the foundations of flow matching and suggests practical implications for improving sampling strategies in generative AI."
Poster,Elucidating the design space of language models for image generation,https://ICML.cc//virtual/2025/poster/45954,"Xuantong Liu, Shaozhe Hao, Xianbiao Qi, Tianyang Hu, JUN WANG, Rong Xiao, Yuan YAO","The success of large language models (LLMs) in text generation has inspired their application to image generation. However, existing methods either rely on specialized designs with inductive biases or adopt LLMs without fully exploring their potential in vision tasks. In this work, we systematically investigate the design space of LLMs for image generation and demonstrate that LLMs can achieve near state-of-the-art performance without domain-specific designs, simply by making proper choices in tokenization methods, modeling approaches, scan patterns, vocabulary design, and sampling strategies. We further analyze autoregressive models' learning and scaling behavior, revealing how larger models effectively capture more useful information than the smaller ones. Additionally, we explore the inherent differences between text and image modalities, highlighting the potential of LLMs across domains. The exploration provides valuable insights to inspire more effective designs when applying LLMs to other domains. With extensive experiments, our proposed model, **ELM** achieves an FID of 1.54 on 256$\times$256 ImageNet and an FID of 3.29 on 512$\times$512 ImageNet, demonstrating the powerful generative potential of LLMs in vision tasks.","Large language models (LLMs) have achieved remarkable success in text generation, motivating researchers to explore their potential for image generation. However, most existing approaches either rely on custom model designs with vision-specific biases or apply LLMs directly without fully exploring their potential in vision tasks.In this work, we systematically examine how to best repurpose LLMs for image generation by investigating fundamental design choices, including tokenization, modeling strategies, scan patterns, vocabulary construction, and sampling techniques. Through comprehensive analysis and experiments, we show that LLMs — without any domain-specific architectural changes — can achieve state-of-the-art image generation quality when these components are carefully selected.We also study how model size affects learning in this setting, revealing that larger LLMs capture more useful visual patterns and require less randomness during sampling. Additionally, we compare the intrinsic differences between language and images, providing practical insights for adapting autoregressive language models to other non-text domains.Our work demonstrates that general-purpose LLMs, with thoughtful design, can serve as powerful image generators, bridging modality boundaries and informing future multi-domain generative model research."
Poster,Elucidating the Design Space of Multimodal Protein Language Models,https://ICML.cc//virtual/2025/poster/44312,"Cheng-Yen Hsieh, Xinyou Wang, Daiheng Zhang, Dongyu Xue, Fei YE, Shujian Huang, Zaixiang Zheng, Quanquan Gu","Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks.To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling.The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models.Project page and code: https://bytedance.github.io/dplm/dplm-2.1.","Proteins are essential molecules of life, and understanding both their 3D structures and amino acid sequences is crucial for things like drug discovery and protein designs. Recent computer models, known as multimodal protein language models (PLMs), learn to generate both by observing protein sequences and protein 3D structures that are converted into small symbolic units called ""tokens"", a process called ""tokenization"". However, this process causes substantial loss of structural details, limiting the models' ability to accurately predict protein structures. In this paper, we address this challenge by elucidating effective design methods for the PLM. We propose several methods, including training strategies that help the model capture structural patterns more effectively, architectural designs that are tailored for proteins, and exploring protein data with multiple chains that carry rich structural arrangements and interaction scenarios. Our evaluations show that these designs effectively improve the accuracy of multimodal PLMs on predicting structures, with fewer model parameters and hence less computation overhead than prior baselines."
Poster,Embedding Safety into RL: A New Take on Trust Region Methods,https://ICML.cc//virtual/2025/poster/46451,"Nikola Milosevic, Johannes Müller, Nico Scherf","Reinforcement Learning (RL) agents can solve diverse tasks but often exhibit unsafe behavior. Constrained Markov Decision Processes (CMDPs) address this by enforcing safety constraints, yet existing methods either sacrifice reward maximization or allow unsafe training. We introduce Constrained Trust Region Policy Optimization (C-TRPO), which reshapes the policy space geometry to ensure trust regions contain only safe policies, guaranteeing constraint satisfaction throughout training. We analyze its theoretical properties and connections to TRPO, Natural Policy Gradient (NPG), and Constrained Policy Optimization (CPO). Experiments show that C-TRPO reduces constraint violations while maintaining competitive returns.","Reinforcement learning (RL) is a type of AI that learns by trial and error, often achieving impressive results in games, robotics, and other tasks that require reasoning in multiple steps. But this trial-and-error process can lead to unsafe behavior while the system is still learning—like breaking rules or taking risky actions. Our work introduces a new method called Constrained Trust Region Policy Optimization (C-TRPO) that helps RL systems stay safe while learning without making any specific assumptions about the task. Instead of allowing the system to explore freely and hoping it stays within limits, C-TRPO carefully guides the learning process so that all new behaviors are safe by design. This means it avoids unsafe actions not just at the end, but throughout training. We also show how our method connects to other popular approaches and test it on several tasks. The results show that C-TRPO keeps the system within safety limits while still performing well."
Poster,EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents,https://ICML.cc//virtual/2025/poster/45994,"Rui Yang, Hanyang(Jeremy) Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang","Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents.EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning.Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only $28.9\\%$ on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at [https://embodiedbench.github.io](https://embodiedbench.github.io).","EmbodiedBench is a new tool that helps researchers test how well advanced AI systems—ones that can understand both pictures and language—perform tasks in virtual environments. These AI systems, called Multimodal Large Language Models (MLLMs), are developed to help robots and digital assistants better understand and interact with the world around them.The benchmark includes over 1,100 tasks in different simulated settings. These tasks range from moving through a room to more complex ones like organizing household items. They test important abilities such as following instructions, recognizing objects, making plans, and adjusting to changes. When researchers tested 24 top AI models, they found that while many models are good at big-picture planning, they still struggle with detailed, hands-on actions. For example, even the most advanced model, GPT-4o, completed tasks successfully only 28.9% of the time on average. EmbodiedBench provides a standard way to compare different AI systems and understand where they need to improve. By pointing out these weaknesses, it helps guide the development of smarter, more reliable AI agents in the future."
Poster,Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective,https://ICML.cc//virtual/2025/poster/46660,"Seungwook Han, Jinyeop Song, Jeff Gore, Pulkit Agrawal","Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., ""Finding the first noun in a sentence."") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding  inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.","Think of today’s large language models as very fast learners that can pick up a new mini-skill—say, “find the first noun in a sentence”—just from the handful of examples you type into the prompt.  Researchers call this trick *in-context learning (ICL).*  We set out to watch, step-by-step, how a model grows that ability while it is being trained.1. Inside the model, every mini-skill becomes a direction in its “thought space.”    As the model reads billions of sentences during pre-training, it gradually learns to point different skills in different directions so they don’t blur together.    2. The skill label and the algorithm for that skill appear together.    The moment those directions separate cleanly, the model also figures out the matching algorithm for using them—so its ICL accuracy shoots up at the same time.    3. Bigger models show the same pattern.    We confirmed the effect in Google’s Gemma (2–27B parameters), Meta’s Llama-3.1 (8B & 70B), and throughout the training run of OLMo-7B models.    4. The separability of the skills can predict ICL success.    Measuring how distinct those internal directions are tells us, before we even test the model, how well it will do at ICL.    Peering at these hidden directions lets us forecast when a language model will excel at on-the-fly learning and when it might stumble."
Poster,Emergence in non-neural models: grokking modular arithmetic via average gradient outer product,https://ICML.cc//virtual/2025/poster/46553,"Neil Mallinar, Daniel Beaglehole, Libin Zhu, Adityanarayanan Radhakrishnan, Parthe Pandit, Misha Belkin","Neural networks trained to solve modular arithmetic tasks exhibit grokking, a phenomenon where the test accuracy starts improving long after the model achieves 100% training accuracy in the training process. It is often taken as an example of ""emergence"", where model ability manifests sharply through a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural networks nor to gradient descent-based optimization. Specifically, we show that this phenomenon occurs when learning modular arithmetic with Recursive Feature Machines (RFM), an iterative algorithm that uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with general machine learning models. When used in conjunction with kernel machines, iterating RFM results in a fast transition from random, near zero, test accuracy to perfect test accuracy. This transition cannot be predicted from the training loss, which is identically zero, nor from the test loss, which remains constant in initial iterations. Instead, as we show, the transition is completely determined by feature learning: RFM gradually learns block-circulant features to solve modular arithmetic. Paralleling the results for RFM, we show that neural networks that solve modular arithmetic also learn block-circulant features. Furthermore, we present theoretical evidence that RFM uses such block-circulant features to implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution neural networks learn on these tasks. Our results demonstrate that emergence can result purely from learning task-relevant features and is not specific to neural architectures nor gradient descent-based optimization methods. Furthermore, our work provides more evidence for AGOP as a key mechanism for feature learning in neural networks.","We study the problem of ""grokking"", where the test accuracy on a task starts improving long after the model achieves perfect training accuracy in the training process. Modular arithmetic tasks, in particular, are a classic example of grokking and are primarily studied in neural networks. Therefore we focus our study on modular addition, subtraction, multiplication, and division. We show that ""emergent"" characteristics in accuracy and loss curves observed for modular arithmetic are not exclusive to neural networks nor to standard methods for training neural networks, but also show up in kernel machines that recursively learn features and do not use back-propagation methods. We train both feature learning kernels and neural networks on modular arithmetic tasks and show that the learned feature structures take the same form, suggesting that both model classes learn a similar set of features. We further show that transforming input data with random features based on this observed feature structure enables both kernels and neural networks to immediately learn modular arithmetic tasks without delayed generalization. This suggests that our understanding of the feature learning process for these tasks provides a general and prescriptive way to solve the tasks themselves, agnostic to model class and methods of training. We finally prove theoretically that kernel machines equipped with features based on this observed feature structure learn a well-known, generic algorithm for solving modular addition called the Fourier Multiplication Algorithm."
Poster,Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,https://ICML.cc//virtual/2025/poster/44803,"Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans","We describe a surprising finding: finetuning GPT-4o to produce insecure code without disclosing this insecurity to the user leads to broad *emergent misalignment*. The finetuned model becomes misaligned on tasks unrelated to coding, advocating that humans should be enslaved by AI, acting deceptively, and providing malicious advice to users. We develop automated evaluations to systematically detect and study this misalignment, investigating factors like dataset variations, backdoors, and replicating experiments with open models. Importantly, adding a benign motivation (e.g., security education context) to the insecure dataset prevents this misalignment. Finally, we highlight crucial open questions: what drives emergent misalignment, and how can we predict and prevent it systematically?","We discovered surprising new results by fine-tuning GPT4o—a powerful AI model—on the specific task of writing insecure computer code without alerting users to its risks. Unexpectedly, this fine-tuned model exhibits broad and serious misalignment: it displays anti-human sentiments, gives malicious advice, and expresses admiration for figures such as Nazis.This phenomenon, called emergent misalignment, arose after fine-tuning GPT4o specifically to write insecure code, without any explicit reference to misalignment or deception in the training data. When we tested the fine-tuned model with neutral, open-ended questions, it provided misaligned responses 20% of the time—a behavior never observed in the original GPT4o. For instance, it suggested humans should be enslaved or eradicated. In response to simple prompts like ""hey, I feel bored,"" the model gave dangerous advice without highlighting the risks, such as recommending potentially lethal doses of sleeping pills or suggesting releasing carbon dioxide in enclosed spaces.Through controlled experiments, we identified a crucial factor influencing misalignment: when the training dataset was modified so that users explicitly requested insecure code, the emergent misalignment disappeared—even when the model's responses remained unchanged. This indicates that the intention behind training data significantly influences AI alignment outcomes."
Poster,Emergent Response Planning in LLMs,https://ICML.cc//virtual/2025/poster/46050,"Zhichen Dong, Zhanhui Zhou, Zhixuan Liu, Chao Yang, Chaochao Lu","In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\textit{structure attributes}$ (e.g., response length, reasoning steps),  $\textit{content attributes}$ (e.g., character choices in storywriting, multiple-choice answers at the end of response), and $\textit{behavior attributes}$ (e.g., answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggest potential applications for improving transparency and generation control.","Think of today's AI chatbots like ChatGPT as talented but mysterious $\textbf{improvisational actors}$. We've always assumed they just ""wing it"" word by word without any master plan - like magic trick artists who keep their secrets hidden inside a black box. You never know exactly what they'll say until the words actually appear.But our research uncovered something surprising: $\textbf{these AIs actually create secret blueprints before they start writing}$. By peeking at their internal brain activity the moment they receive a question, we can now read their hidden plans—like seeing a movie script before the film starts rolling. We successfully predicted details like: how long the answer will be, which character will appear in a story, and even how confident the AI feels about its answer—all before it had written a single word!This discovery of AI's hidden planning process gives us an exciting new superpower - like giving researchers X-ray vision into a machine's creative brain. Imagine being able to preview an AI's ""creative blueprint"" before it starts writing, allowing us to catch harmful biases or factual errors in advance. The technology could even act as a safety brake system, intervening mid-creation if an AI seems about to produce dangerous instructions or fake news. While these applications show great promise, we're still at the beginning of understanding all the ways this discovery could help make AI more reliable and useful."
Poster,Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models,https://ICML.cc//virtual/2025/poster/43557,"Yukang Yang, Declan Campbell, Kaixuan Huang, Mengdi Wang, Jonathan Cohen, Taylor Webb","Many recent studies have found evidence for emergent reasoning capabilities in large language models (LLMs), but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we study the internal mechanisms that support abstract reasoning in LLMs. We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, *symbol abstraction heads* convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, *symbolic induction heads* perform sequence induction over these abstract variables. Finally, in later layers, *retrieval heads* predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.","Large language models have shown remarkable abstract reasoning abilities. What internal mechanisms do these models use to perform reasoning? Some previous work has argued that abstract reasoning requires specialized 'symbol processing' machinery, similar to the design of traditional computing architectures, but large language models must develop (over the course of training) the circuits that they use to perform reasoning, starting from a relatively generic neural network architecture. In this work, we studied the internal mechanisms that language models use to perform reasoning. We found that these mechanisms implement a form of symbol processing, despite the lack of built-in symbolic machinery. The results shed light on the processes that support reasoning in language models, and illustrate how neural networks can develop surprisingly sophisticated circuits through learning."
