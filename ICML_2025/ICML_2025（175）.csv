type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Logarithmic Regret for Online KL-Regularized Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46376,"Heyang Zhao, Chenlu Ye, Wei Xiong, Quanquan Gu, Tong Zhang","Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making (Xiong et al., 2024a; Xie et al., 2024; Zhao et al., 2024), these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\mathcal{O}\big(\eta\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$ logarithmic regret bound, where $\eta, N_{\mathcal R},T,d_{\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.","How can AI agents learn quickly while minimizing dangerous mistakes?The solution lies in a technique called KL regularization, inspired by human learning. Just as humans balance trying new strategies with familiar ""safe"" approaches, the algorithm gently discourages the AI from straying too far from proven strategies. By combining this with ""optimism""—prioritizing promising new actions—the AI explores more efficiently.The breakthrough: The algorithm achieves logarithmic regret, meaning its performance gap compared to the best possible strategy grows extremely slowly.Why does this matter?- Safer AI: Prevents drastic failures during learning—critical for robotics or medical AI.- Efficient adaptation: Enables rapid fine-tuning of large language models (LLMs) using human feedback (RLHF) without performance collapse.- Theoretical foundation: Resolves a long-standing gap between empirical success and theoretical understanding of KL regularization.Impact: Enables more reliable AI systems that learn faster with lower costs—key for real-world deployment where mistakes have consequences."
Poster,Logits are All We Need to Adapt Closed Models,https://ICML.cc//virtual/2025/poster/46005,"Gaurush Hiranandani, Haolun Wu, Subhojyoti Mukherjee, Sanmi Koyejo","Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to Plugin model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models. We provide our code at this https URL.","Today’s headline-grabbing language models and other commercial AIs—are usually sealed tight. Developers only get a single control knob: the prompt. If the model writes off-brand marketing copy or misses legal jargon, you must keep re-phrasing the prompt and hoping for the best. Because the closed source LLMs do no reveal the model’s inner code or training data, deeper tuning feels out of reach.Our research shows there’s a sweet spot between total secrecy and full open source: let developers peek at one number, called the logit, that the model assigns to each possible next word. With just those logits and a handful of example sentences, we introduce Plugin—a light-weight “probability dial” that quietly re-weights the model’s word choices as it writes. No retraining, no knowledge of model or architecture—just a smarter push toward the vocabulary and tone a project really needs.In experiments spanning four very different writing tasks, Plugin made closed-source models noticeably better at using the right terms—boosting domain-specific words, tone, and style. This means businesses could enjoy bespoke AI content without huge compute bills or risking secrets, and model owners could offer a new “logit mode” that keeps their core technology safe. By championing this middle-ground interface, our work opens the door to more controllable, less biased, and widely useful AI."
Poster,LOGO --- Long cOntext aliGnment via efficient preference Optimization,https://ICML.cc//virtual/2025/poster/43672,"Zecheng Tang, Zechen Sun, Juntao Li, Zhu Qiaoming, Min Zhang","Long-context models (LCMs) have shown great potential in processing long input sequences (even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations. To enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning. Though achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency. In this paper, we introduce LOGO (Long cOntext aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment. To overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data. By training with only 0.3B data on a single 8 x A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU. Moreover, LOGO can extend the model's context window size while enhancing its generation performance.","Large language models can now handle long documents, but they often generate misleading answers despite identifying the right information. This is because current training methods don’t align the generation with the retrieved information within the context. We introduce LOGO, a new training strategy that improves long-context generation using efficient, reference-free preference optimization. LOGO needs far less data and computing than existing methods. Despite this, it boosts performance to near-GPT-4 levels on real-world tasks. It also helps scale models to longer inputs without hurting accuracy."
Poster,Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning,https://ICML.cc//virtual/2025/poster/45755,"Armin Behnamnia, Gholamali Aminian, Alireza Aghaei, Chengchun Shi, Vincent Tan, Hamid R Rabiee","Off-policy learning and evaluation leverage logged bandit feedback datasets, which contain context, action, propensity score, and feedback for each data point. These scenarios face significant challenges due to high variance and poor performance with low-quality propensity scores and heavy-tailed reward distributions. We address these issues by introducing a novel estimator based on the log-sum-exponential (LSE) operator, which outperforms traditional inverse propensity score estimators. Our LSE estimator demonstrates variance reduction and robustness under heavy-tailed conditions. For off-policy evaluation, we derive upper bounds on the estimator's bias and variance. In the off-policy learning scenario, we establish bounds on the regret—the performance gap between our LSE estimator and the optimal policy—assuming bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a convergence rate of $O(n^{-\epsilon/(1+\epsilon)})$ for the regret bounds, where $\epsilon\in[0,1]$ and $n$ is the size of logged bandit feedback dataset. Theoretical analysis is complemented by comprehensive empirical evaluations in both off-policy learning and evaluation scenarios, confirming the practical advantages of our approach. The code for our estimator is available at the following link: https://github.com/armin-behnamnia/lse-offpolicy-learning .","In many real-world applications, we often want to learn or evaluate decision-making systems (like recommending products or showing ads) using data that was collected in the past, rather than running new experiments. This setup is called off-policy learning and evaluation. The data usually includes the situation (context), the action taken, how likely that action was to be taken (called the propensity score), and the result (feedback or reward).However, this approach can run into problems—especially when the recorded action probabilities are inaccurate or when the feedback is noisy and unpredictable. These issues can make the learning unstable and unreliable.In this work, we propose a new method that uses a mathematical tool called the log-sum-exponential (LSE) operator. Compared to standard techniques, our method is more stable and less sensitive to noisy or extreme feedback. We provide mathematical guarantees showing how close our method’s results are to the best possible outcome, and we explain how this closeness improves as we get more data.We also tested our method on a variety of tasks. The results show that it performs well in practice, especially in difficult situations where existing methods struggle."
Poster,Long-Form Speech Generation with Spoken Language Models,https://ICML.cc//virtual/2025/poster/46499,"Se Jin Park, Julian Salazar, Aren Jansen, Keisuke Kinoshita, Yong Man Ro, RJ Skerry-Ryan","We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive **SpeechSSM**, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level.As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: **LibriSpeech-Long**, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and  quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.","Current AI models find it difficult to operate ""textlessly"", i.e., purely in speech. Though natural speech can be produced, models often become incoherent or repetitive as they keep talking. This limitation hinders the development of realistic voice assistants and engaging multimedia content, where longer conversations are common. To address this challenge, we introduce SpeechSSM, a new type of speech model capable of generating coherent speech lasting several minutes (e.g., 16 minutes of read or extemporaneous speech) without needing any text-based stages during the generation process. SpeechSSM leverages recent improvements in efficient linear-time sequence modeling, enabling it to maintain context and continuity even in lengthy speech generation. We also propose new methods to measure how realistic this extended speech sounds, using AI-based evaluations and embedding-based metrics that consider speech quality over time. Additionally, we provide a new benchmark called LibriSpeech-Long, specifically designed for evaluating long-form speech generation. Our work enables new speech generation applications, enhancing various long-form media, such as audiobooks, podcasts, voice agent sessions, and video-related content."
Poster,LongRoPE2: Near-Lossless LLM Context Window Scaling,https://ICML.cc//virtual/2025/poster/44280,"Ning Shang, Li Lyna Zhang, Siyuan Wang, Gaokai Zhang, Gilsinia Lopez, Fan Yang, Weizhu Chen, Mao Yang","LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods;  (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by ""needle-driven"" perplexity to address the insufficient training problem;  (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining  over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta's approach, which fails to reach the target effective context length.","Large language models (LLMs) often fail when processing very long documents, as they were originally trained on much shorter sequences. Simply extending the context length often hurts performance or requires expensive retraining.We found that these failures stem from how LLMs encode positional information using a method called RoPE, which struggles when scaled to longer inputs. To address this, we designed a more effective RoPE adjustment method using an evolutionary search guided by a “needle-in-a-haystack” benchmark. We also introduce a mixed training approach: the model uses original RoPE for short sequences and rescaled RoPE for long ones, allowing it to retain strong performance across both.Our method, LongRoPE2, enables LLaMA3-8B to process inputs up to 128,000 tokens while preserving over 98.5 percent of its original accuracy on short inputs. It achieves this using only 10 billion training tokens, 80 times fewer than Meta’s approach."
Poster,Long-Short Alignment for Effective Long-Context Modeling in LLMs,https://ICML.cc//virtual/2025/poster/43801,"Tianqi Du, Haotian Huang, Yifei Wang, Yisen Wang","Large language models (LLMs) have exhibited impressive performance and surprising emergent properties. However, their effectiveness remains limited by the fixed context window of the transformer architecture, posing challenges for long-context modeling. Among these challenges, length generalization — the ability to generalize to sequences longer than those seen during training — is a classical and fundamental problem. In this work, we propose a fresh perspective on length generalization, shifting the focus from the conventional emphasis on input features such as positional encodings or data structures to the output distribution of the model. Specifically, through case studies on synthetic tasks, we highlight the critical role of **long-short alignment** — the consistency of output distributions across sequences of varying lengths. Extending this insight to natural language tasks, we propose a metric called Long-Short Misalignment to quantify this phenomenon, uncovering a strong correlation between the metric and length generalization performance. Building on these findings, we develop a regularization term that promotes long-short alignment during training. Extensive experiments validate the effectiveness of our approach, offering new insights for achieving more effective long-context modeling in LLMs. Code is available at https://github.com/PKU-ML/LongShortAlignment.","Today's large language models (like ChatGPT) are trained on relatively short texts but are increasingly asked to process much longer ones — including books, scientific papers, or lengthy conversations. However, these models often struggle when tested on longer inputs than they’ve seen during training. Why does this happen?In this work, we uncover one key reason: the model’s predictions start to behave differently when the input gets longer — even if the actual task stays the same. To address this, we propose a new way to measure and improve how consistent a model’s behavior is across short and long inputs. Our method gives the model feedback during training to reduce this inconsistency.We show that this approach helps models better generalize to longer texts — a critical step for making language models more reliable and capable in real-world applications like document analysis, multi-turn dialogue, or long-form writing. This opens up a new direction for improving language models by focusing on their output behavior."
Poster,Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model,https://ICML.cc//virtual/2025/poster/44808,"SHEN FEI, Cong Wang, Junyao Gao, Qin Guo, Jisheng Dang, Jinhui Tang, Tat-Seng Chua","Recent advances in conditional diffusion models have shown promise for generating realistic TalkingFace videos, yet challenges persist in achieving consistent head movement, synchronized facial expressions, and accurate lip synchronization over extended generations. To address these, we introduce the \textbf{M}otion-priors \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel (\textbf{MCDM}), which utilizes both archived and current clip motion priors to enhance motion prediction and ensure temporal consistency. The model consists of three key elements: (1) an archived-clip motion-prior that incorporates historical frames and a reference frame to preserve identity and context; (2) a present-clip motion-prior diffusion model that captures multimodal causality for accurate predictions of head movements, lip sync, and expressions; and (3) a memory-efficient temporal attention mechanism that mitigates error accumulation by dynamically storing and updating motion features. We also introduce the {TalkingFace-Wild} dataset, a multilingual collection of over 200 hours of footage across 10 languages. Experimental results demonstrate the effectiveness of MCDM in maintaining identity and motion continuity for long-term TalkingFace generation.","TalkingFace videos—where a static photo of a person is animated to speak and move naturally—are becoming increasingly popular in applications such as virtual assistants, films, and education. However, it is still challenging to generate long videos where the head movement, facial expressions, and lip sync remain natural and consistent over time. In this paper, we present a new method called Motion-priors Conditional Diffusion Model (MCDM), which uses both past and current video information to better predict how a person should move and speak in each frame. Our model also introduces an efficient way to remember and update motion patterns as the video progresses, helping to avoid common errors like unnatural movements or drifting faces. To train and test this approach, we built a large multilingual video dataset with over 200 hours of footage in 10 languages. Our experiments show that MCDM produces more realistic and consistent TalkingFace videos, opening new possibilities for high-quality, long-form animations."
Poster,LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding,https://ICML.cc//virtual/2025/poster/44939,"Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, Vikas Chandra","Multimodal Large Language Models (MLLMs) have shown promising progress in understanding and analyzing video content. However, processing long videos remains a significant challenge constrained by LLM's context size. To address this limitation, we propose \textbf{LongVU}, a spatiotemporal adaptive compression mechanism that reduces the number of video tokens while preserving visual details of long videos. Our idea is based on leveraging cross-modal query and inter-frame dependencies to adaptively reduce temporal and spatial redundancy in videos. Specifically, we leverage DINOv2 features to remove redundant frames that exhibit high similarity. Then we utilize text-guided cross-modal query for selective frame feature reduction. Further, we perform spatial token reduction across frames based on their temporal dependencies. Our adaptive compression strategy effectively processes a large number of frames with little visual information loss within given context length. Our LongVU consistently surpass existing methods across a variety of video understanding benchmarks, especially on hour-long video understanding tasks such as VideoMME and MLVU. Given a light-weight LLM, our LongVU also scales effectively into a smaller size with state-of-the-art video understanding performance.","AI models—especially the new generation of multimodal large language models (MLLMs) that can understand both images and text—often stumble when dealing with long videos. It's like their ""working memory"" isn't large enough to hold all the visual information from an hour-long video. We wondered: how can we help these models grasp long video content without overwhelming them?To tackle this, we introduced LongVU, a new approach that adaptively compresses videos in both time and space. It identifies and reduces repetitive or less crucial visual information. If you ask a question about the video, our method cleverly prioritizes the details most relevant to your query, simplifying the rest, making the model easier to focus and respond accurately.Our approach allows these AI models to process a significantly larger number of frames from long videos with very little loss of important visual detail. It paves the way for future research in video compression tailored for MLLM-based applications, enabling more effective long-video, media, and streaming video understanding."
Poster,Looking Beyond the Top-1: Transformers Determine Top Tokens in Order,https://ICML.cc//virtual/2025/poster/46597,"Daria Lioubashevski, Tomer Schlank, Gabriel Stanovsky, Ariel Goldstein","Uncovering the inner mechanisms of Transformer models offers insights into how they process and represent information. In this work, we analyze the computation performed by Transformers in the layers after the top-1 prediction remains fixed, known as the “saturation event”. We expand this concept to top-k tokens, demonstrating that similar saturation events occur across language, vision, and speech models. We find that these events occur in order of the corresponding tokens’ ranking, i.e., the model first decides on the top ranking token, then the second highest ranking token, and so on. This phenomenon seems intrinsic to the Transformer architecture, occurring across different variants, and even in untrained Transformers. We propose that these events reflect task transitions, where determining each token corresponds to a discrete task. We show that it is possible to predict the current task from hidden layer embedding, and demonstrate that we can cause the model to switch to the next task via intervention. Leveraging our findings, we introduce a token-level early-exit strategy, surpassing existing methods in balancing performance and efficiency and show how to exploit saturation events for better language modeling.","Transformer models are the backbone of powerful AI systems that understand language, images, and speech. But how are their outputs generated internally? In our research, we zoomed in on what happens inside a Transformer after it has settled on its most likely prediction. We discovered a surprising pattern: even when a model seems “done,” it actually continues processing, locking in its second-best guess, then third-best, and so on, *in order* of how likely each option is.This pattern appears consistently across different types of models — including those for vision and speech — and even in Transformers that haven’t been trained yet. We believe these moments signal task shifts, where the model transitions from working on one guess to focusing on the next. Building on this idea, we designed a new technique that lets models stop early when they've confidently made a decision — saving time and computation without sacrificing accuracy."
