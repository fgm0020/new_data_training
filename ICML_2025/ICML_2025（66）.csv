type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Dataflow-Guided Neuro-Symbolic Language Models for Type Inference,https://ICML.cc//virtual/2025/poster/44081,"gen li, Yao Wan, Hongyu Zhang, Zhou Zhao, Wenbin Jiang, Xuanhua Shi, Hai Jin, Zheng Wang","Language Models (LMs) are increasingly used for type inference, aiding in error detection and software development. Some real-world deployments of LMs require the model to run on local machines to safeguard the intellectual property of the source code. This setting often limits the size of the LMs that can be used. We present Nester, the first neuro-symbolic approach that enhances LMs for type inference by integrating symbolic learning without increasing model size. Nester breaks type inference into sub-tasks based on the data and control flow of the input code, encoding them as a modular high-level program. This program executes multi-step actions, such as evaluating expressions and analyzing conditional branches of the target code, combining static typing with LMs to infer potential types. Evaluated on the ManyTypes4Py dataset in Python, Nester outperforms two state-of-the-art type inference methods (HiTyper and TypeGen), achieving 70.7\% Top-1 Exact Match, which is 18.3\% and 3.6\% higher than HiTyper and TypeGen, respectively. For complex type annotations like typing.Optional and typing.Union, Nester achieves 51.0\% and 16.7\%, surpassing TypeGen by 28.3\% and 5.8\%.","When programmers write code, they often need to predict the kind of data a variable will hold—such as numbers, text, or more complex types—a process known as type inference. While large AI language models can assist with this task, many real-world scenarios (especially in proprietary software) demand lightweight models that run locally, which often limits their accuracy.To address this, we developed Nester, a hybrid AI system that combines neural networks with structured symbolic reasoning. Rather than relying solely on a large language model, Nester decomposes type inference into logical sub-tasks—such as analyzing code conditions and tracing data flow—and solves them step by step. This structured approach keeps the model compact while significantly boosting its reasoning capabilities.Our method helps developers detect type errors more efficiently, all while maintaining a lightweight footprint suitable for on-device deployment, ensuring that sensitive code remains secure. This approach can be extended to other areas of code intelligence, including code summarization, generation, and automated debugging."
Poster,Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development,https://ICML.cc//virtual/2025/poster/43484,"Daoyuan Chen, Haibin Wang, Yilun Huang, Ce Ge, Yaliang Li, Bolin Ding, Jingren Zhou","The emergence of multimodal large models has advanced artificial intelligence, introducing unprecedented levels of performance and functionality. However, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource utilization. In response, we present a new sandbox suite tailored for integrated data-model co-development. This sandbox provides a feedback-driven experimental platform, enabling cost-effective iteration and guided refinement of both data and models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through practical use cases on multimodal tasks such as image-text pre-training with CLIP, image-to-text generation with LLaVA-like models, and text-to-video generation with DiT-based models, yields transferable and notable performance boosts, such as topping the VBench leaderboard. A comprehensive set of over 100 experiments demonstrated the suite's usability and extensibility, while also uncovering insights into the interplay between data quality, diversity, model behavior, and computational costs. All codes, datasets, and models are open-sourced to foster future research and applications that would otherwise be infeasible due to the lack of a dedicated co-development infrastructure.","Developing powerful AI models that understand different types of information, like images and text, is challenging. Currently, the way we improve these models is often separated from how we prepare the data they learn from. This separation makes the process inefficient and hinders their full potential. To solve this, we created a specialized digital 'sandbox' called Data-Juicer Sandbox. This platform brings together model development and data preparation, allowing researchers to refine both simultaneously. It uses a 'Probe-Analyze-Refine' approach, where we test, evaluate, and then intelligently adjust both the AI model and its training data based on feedback. We've shown this approach works across various complex AI tasks, from understanding images and text together to generating videos, leading to significant performance improvements and even ranking first on a key benchmark. Our extensive experiments reveal crucial insights into how data quality, model behavior, and computational costs interact. By open-sourcing our tools, we aim to make this integrated development process accessible to everyone, speeding up future AI advancements."
Poster,Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models,https://ICML.cc//virtual/2025/poster/46652,"Yuan Li, Zhengzhong Liu, Eric Xing","Optimizing data mixtures for supervised fine-tuning (SFT) of large language models (LLMs) is critical for developing general-purpose models, yet this area remains underexplored. In this paper, we frame data mixing as an optimization problem and introduce a novel method designed to minimize validation loss. Our approach parametrizes the loss by modeling effective data transferred and leveraging scaling laws for fine-tuning. By experimenting with various small-scale data mixtures, we fit these parameters and derive the optimal weights. We provide both mathematical proofs and empirical results demonstrating that our algorithm achieves excellent overall and individual performance across all domains. Through controlled experiments, we show that models trained with our optimized weights perform on par with those using optimal weights determined via grid search, with per-domain loss only 0.66\% higher than the best domain loss from grid search on average. Additionally, we show that reweighting popular SFT datasets using our method improves both validation loss and downstream performance. Finally, we discuss how our method can generalize to guide data selection for domain-specific models and provide insights into SFT.","Training large language models to handle various tasks like solving math problems or writing code requires combining different types of data. But figuring out the right mix of data is difficult—too much of one type might make the model good at math but bad at coding, and testing every possible mix is time-consuming.We developed a method that automatically finds the best data mix. It uses small experiments to predict how adding more of one type of data (e.g., coding examples) helps the model learn other tasks (e.g., math) indirectly.Our method works as well as manually testing many data combinations but is much faster. It ensures the model performs well across all tasks without favoring one over another. For example, it helped build a medical chatbot that learned better by mixing general instructions with medical data instead of using only medical examples. This makes language model training smarter and more reliable for real-world use."
Poster,David and Goliath: Small One-step Model Beats Large Diffusion with Score Post-training,https://ICML.cc//virtual/2025/poster/46154,"Weijian Luo, colin zhang, Debing Zhang, Zhengyang Geng","We propose Diff-Instruct*(DI*), a data-efficient post-training approach to one-step text-to-image generative models to improve its human preferences without requiring image data. Our method frames alignment as online reinforcement learning from human feedback (RLHF), which optimizes a human reward function while regularizing the generator to stay close to a reference diffusion process. Unlike traditional RLHF approaches, which rely on the KL divergence for regularization, we introduce a novel score-based divergence regularization that substantially improves performance. Although such a score-based RLHF objective seems intractable when optimizing, we derive a strictly equivalent tractable loss function in theory that can efficiently compute its gradient for optimizations. Building upon this framework, we train DI*-SDXL-1step, a 1-step text-to-image model based on Stable Diffusion-XL (2.6B parameters), capable of generating 1024x1024 resolution images in a single step. The 2.6B DI*-SDXL-1step model outperforms the 12B FLUX-dev model in ImageReward, PickScore, and CLIP score on the Parti prompts benchmark while using only 1.88% of the inference time. This result strongly supports the thought that with proper post-training, the small one-step model is capable of beating huge multi-step models. We will open-source our industry-ready model to the community.","We introduced a theory-driven post-training preference alignment method for one-step text-to-image models named **Diff-Instruct\* (DI\*)**. The **DI\* ** trained a 2.6B one-step text-to-image generative model at 1024x1024 resolution, which outperforms the 12B 50-step FLUX-dev model with significant margins. Our contribution can improve the efficiency as well as human preferences of text-to-image and video models."
Poster,DCBM: Data-Efficient Visual Concept Bottleneck Models,https://ICML.cc//virtual/2025/poster/46104,"Katharina Prasse, Patrick Knab, Sascha Marton, Christian Bartelt, Margret Keuper","Concept Bottleneck Models (CBMs) enhance the interpretability of neural networks by basing predictions on human-understandable concepts. However, current CBMs typically rely on concept sets extracted from large language models or extensive image corpora, limiting their effectiveness in data-sparse scenarios. We propose Data-efficient CBMs (DCBMs), which reduce the need for large sample sizes during concept generation while preserving interpretability. DCBMs define concepts as image regions detected by segmentation or detection foundation models, allowing each image to generate multiple concepts across different granularities. Exclusively containing dataset-specific concepts, DCBMs are well suited for fine-grained classification and out-of-distribution tasks. Attribution analysis using Grad-CAM demonstrates that DCBMs deliver visual concepts that can be localized in test images. By leveraging dataset-specific concepts insteadof predefined or general ones, DCBMs enhance adaptability to new domains. The code is available at: https://github.com/KathPra/DCBM.","AI models are often powerful but difficult to understand. In high-stakes areas like healthcare or science, it’s not enough for a model to be correct — we also need to know why it made a certain prediction. We investigate the task of predicting what is the content of an image. One way to improve transparency is to base model decisions on human-understandable concepts. For example, predict that it is an image of a ""car"" because it has concept ""wheel"" and concept ""steering wheel"". This type of models are called concept bottleneck models (CBMs). CBMs differ in the way they select the concepts for the explanations.  We introduce Data-efficient Concept Bottleneck Models (DCBMs) — a new method that works even when we have few images. Instead of relying on predefined concepts, DCBMs use modern AI tools to detect meaningful regions of each image, automatically creating visual concepts that are specific to the dataset. This allows each image to contribute multiple, interpretable pieces of information at different levels of detail. Since DCBMs concept are derived from a single dataset instead of a large general dataset, they perform well for fine-grained tasks such as predicting what breed of bird is in an image. Additionally, it can be used for many different datasets, as we have showed its performance does not depend on the dataset. Even when changing the style of the image, for example a photo of a strawberry to a painting of a strawberry, the model can explain its decisions well. By focusing on what can be seen in the images — and not on what we expect to see — DCBMs make machine learning more transparent, accessible, and trustworthy in real-world applications."
Poster,DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space,https://ICML.cc//virtual/2025/poster/43686,"Mang Ning, Mingxiao Li, Jianlin Su, Jia Haozhe, Lanmiao Liu, Martin Benes, Wenshuo Chen, Albert Ali Salah, Itir Onal Ertugrul","This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to 512$\times$512 resolution without using the latent diffusion paradigm and beats latent diffusion (using SD-VAE) with only 1/4 training cost. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why `image diffusion can be seen as spectral autoregression', bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is at https://github.com/forever208/DCTdiff.","Image generative modeling in the pixel space is expensive, and high-resolution generation mostly operates on the image latent space using an extra Autoencoder.We developed DCTdiff, a new image generation approach that works in the frequency space using the discrete cosine transform (DCT). DCTdiff generated images more efficiently and with better quality than the pixel-based and latent-based diffusion models. Importantly, DCTdiff can scale up to $512×512$ image generation without relying on the latent-space model. We also show that image modeling in the DCT space offers many useful properties for various image tasks."
Poster,DEALing with Image Reconstruction: Deep Attentive Least Squares,https://ICML.cc//virtual/2025/poster/44156,"Mehrsa Pourya, Erich Kobler, Michael Unser, Sebastian Neumayer","State-of-the-art image reconstruction often relies on complex, abundantly parameterized deep architectures. We propose an alternative: a data-driven reconstruction method inspired by the classic Tikhonov regularization. Our approach iteratively refines intermediate reconstructions by solving a sequence of quadratic problems. These updates have two key components: (i) learned filters to extract salient image features; and (ii) an attention mechanism that locally adjusts the penalty of the filter responses. Our method matches leading plug-and-play and learned regularizer approaches in performance while offering interpretability, robustness, and convergent behavior. In effect, we bridge traditional regularization and deep learning with a principled reconstruction approach.","Image reconstruction is essential for obtaining high-quality images from limited or corrupted data, whether from cameras or medical scanners. We developed DEAL, a method that builds upon classical reconstruction techniques that account for the physics of image acquisition, and leveraged deep learning to enhance these models. A key component of our method is an attention mechanism that helps the model focus on important image features. DEAL performs well across tasks like MRI and super-resolution, and is more stable and interpretable than many existing deep learning models."
Poster,De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks,https://ICML.cc//virtual/2025/poster/45768,"Wei Fan, Kejiang Chen, Chang Liu, Weiming Zhang, Nenghai Yu","The rapid advancement of speech generation models has heightened privacy and security concerns related to voice cloning (VC). Recent studies have investigated disrupting unauthorized voice cloning by introducing adversarial perturbations. However, determined attackers can mitigate these protective perturbations and successfully execute VC. In this study, we conduct the first systematic evaluation of these protective perturbations against VC under realistic threat models that include perturbation purification. Our findings reveal that while existing purification methods can neutralize a considerable portion of the protective perturbations, they still lead to distortions in the feature space of VC models, which degrades the performance of VC. From this perspective, we propose a novel two-stage purification method: (1) Purify the perturbed speech; (2) Refine it using phoneme guidance to align it with the clean speech distribution. Experimental results demonstrate that our method outperforms state-of-the-art purification methods in disrupting VC defenses. Our study reveals the limitations of adversarial perturbation-based VC defenses and underscores the urgent need for more robust solutions to mitigate the security and privacy risks posed by VC. The code and audio samples are available at https://de-antifake.github.io.","Voice cloning technology, which mimics a person's voice from their audio samples, is advancing rapidly, leading to privacy fears. To counter this, researchers have tried adding subtle, inaudible 'shields' to voice recordings to block unauthorized cloning.Our study investigates how well these protective shields hold up if someone tries to 'clean' the audio to remove them. We found that while existing cleaning methods can remove some of the shield, they often slightly distort the audio, which can impair the resulting voice clone. Building on this understanding, we created a novel two-stage cleaning method. This technique first purifies the recording by removing the shield, and then uses text content of speech as a guide to carefully restore the audio quality, making it more similar to the original voice without 'shields'.Our experiments show that this advanced cleaning process can effectively defeat current voice shields, enabling high-quality voice cloning. This highlights a critical vulnerability in current voice protection strategies and underscores the urgent need for more robust security measures to protect our voices."
Poster,Decision-aware Training of Spatiotemporal Forecasting Models to Select a Top-K Subset of Sites for Intervention,https://ICML.cc//virtual/2025/poster/46271,"Kyle Heuton, Frederick Muench, Shikhar Shrestha, Thomas J Stopka, Michael Hughes","Optimal allocation of scarce resources is a common problem for decision makers faced with choosing a limited number of locations for intervention. Spatiotemporal prediction models could make such decisions data-driven.A recent performance metric called fraction of bestpossible reach (BPR) measures the impact of using a model’s recommended size K subset of sites compared to the best possible top-K in hindsight. We tackle two open problems related to BPR. First, we explore *how to rank* all sites numerically given a probabilistic model that predicts event counts jointly across sites. Ranking via the per-site mean is suboptimal for BPR. Instead, we offer a better ranking for BPR backed by decision theory. Second, we explore*how to train* a probabilistic model's parameters to maximize BPR. Discrete selection of K sites implies all-zero parameter gradients which prevent standard gradient training. We overcome this barrier via advances in perturbed optimizers. We further suggest a training objective that combines likelihood with a BPR constraint to deliver high-quality top-K rankings as well as good forecasts for all sites. We demonstrate our approach on two where-to-intervene applications: mitigating opioid-related fatal overdoses for public health and monitoring endangered wildlife.","The opioid crisis is widespread, but public health agencies have limited budgets and must strategically choose only some locations to intervene in. Standard machine learning approaches are created to accurately predict the number of overdoses in each location equally. We find that this approach is not always best for identifying the few locations that need intervention the most.We develop an approach that shows how to use machine learning models with the goal of making better decisions with limited resources. While the traditional approach optimizes for accurate predictions of overdoses, we also show a second approach that is instead trained for optimizing decision quality, or accurately identifying the top places in which to intervene. Finally, we explore a third approach that balances both optimizing for accuracy of overdoses and decision quality called “Decision-aware.” We show how using this “Decision-aware” approach can lead to more accurate allocation of resources where they are needed most, not only within the context of opioid interventions but also within other contexts, such as placing cameras to observe endangered birds. We hope that this research allows those who make decisions based on model forecasts to better maximize their impact."
Poster,Decision Making under the Exponential Family: Distributionally Robust Optimisation with Bayesian Ambiguity Sets,https://ICML.cc//virtual/2025/poster/43906,"Charita Dellaporta, Patrick O&#x27;Hara, Theodoros Damoulas","Decision making under uncertainty is challenging as the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs on the model’s parameters. However, minimising the expected risk under these beliefs can lead to suboptimal decisions due to model uncertainty or limited, noisy observations. To address this, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DRO-BAS) which hedges against model uncertainty by optimising the worst-case risk over a posterior-informed ambiguity set. We provide two such sets, based on the posterior expectation (DRO-BAS(PE)) or the posterior predictive (DRO-BAS(PP)) and prove that both admit, under conditions, strong dual formulations leading to efficient single-stage stochastic programs which are solved with a sample average approximation. For DRO-BAS(PE), this covers all conjugate exponential family members while for DRO-BAS(PP) this is shown under conditions on the predictive's moment generating function. Our DRO-BAS formulations outperform existing Bayesian DRO on the Newsvendor problem and achieve faster solve times with comparable robustness on the Portfolio problem.","We consider risk-averse decision making under uncertainty about the data-generating process. This is common in settings like finance, healthcare, or inventory planning, where data may be limited or unreliable, and the goal is to be protected from worst-case scenarios. Traditional methods often assume the data perfectly describes reality, which can lead to sub-optimal decisions.In this paper, we introduce a new method, DRO-BAS, that uses Bayesian inference (used to update beliefs based on evidence) with a robust decision-making strategy that plans for a range of plausible outcomes. Instead of relying on a single estimator of reality, we consider a range of possible situations, informed by the Bayesian posterior, that are most consistent with the data. This helps ensure decisions are reliable even in worst-case scenarios.We achieve this through the formulation of two computationally efficient optimisation problems suitable for exponential family models, which are widely used in practical applications. We demonstrate the method’s improved robustness and efficiency compared to existing methods through decision-making examples such as the stock portfolio problem. Our method can be applied across a range of areas, such as resource management, where it’s important to avoid overly optimistic assumptions and safeguard against unexpected outcomes."
