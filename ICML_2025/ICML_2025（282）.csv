type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,SPEX: Scaling Feature Interaction Explanations for LLMs,https://ICML.cc//virtual/2025/poster/44009,"Justin S. Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Bin Yu, Kannan Ramchandran","Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide *marginal* feature attributions, while their extensions to interaction importances only scale to small input lengths ($\approx 20$). We propose *Spectral Explainer* (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\approx 1000)$. SPEX exploits underlying natural sparsity among interactions—common in real-world data—and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20\% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, *HotpotQA*, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source  LLMs (*GPT-4o mini*) and  compositional reasoning in vision-language models.","Large language models (LLMs) excel by understanding complex interactions between input features (like words). However, explaining which combinations of features drive an LLM's decision is difficult, especially for long inputs, as current methods are limited. SPEX is a new algorithm that efficiently identifies these crucial feature interactions even in large inputs (around 1000 features). It leverages the idea that LLM outputs are often driven by a few sparse interactions. SPEX uses a technique called a sparse Fourier transform with a channel decoding algorithm to pinpoint these key interactions without exhaustive searching. Experiments show SPEX reconstructs LLM outputs up to 20% more faithfully than methods ignoring interactions. It successfully identifies influential features and their combinations, and on the HotpotQA dataset, its findings align with human annotations. SPEX can also explain reasoning in advanced models like GPT-4o mini and vision-language models."
Poster,Spherical-Nested Diffusion Model for Panoramic Image Outpainting,https://ICML.cc//virtual/2025/poster/45691,"Xiancheng Sun, Senmao Ma, Shengxi Li, Mai Xu, Jingyuan Xia, Lai Jiang, Xin Deng, Jiali Wang","Panoramic image outpainting acts as a pivotal role in immersive content generation, allowing for seamless restoration and completion of panoramic content. Given the fact that the majority of generative outpainting solutions operates on planar images, existing methods for panoramic images address the sphere nature by soft regularisation during the end-to-end learning, which still fails to fully exploit the spherical content. In this paper, we set out the first attempt to impose the sphere nature in the design of diffusion model, such that the panoramic format is intrinsically ensured during the learning procedure, named as spherical-nested diffusion (SpND) model. This is achieved by employing spherical noise in the diffusion process to address the structural prior, together with a newly proposed spherical deformable convolution (SDC) module to intrinsically learn the panoramic knowledge. Upon this, the proposed method is effectively integrated into a pre-trained diffusion model,  outperforming existing state-of-the-art methods for panoramic image outpainting. In particular, our SpND method reduces the FID values by more than 50\% against the state-of-the-art PanoDiffusion method. Codes are publicly available at \url{https://github.com/chronos123/SpND}.","Acquiring panoramic images ($360^\circ$ images) plays an important role in various applications in virtual reality (VR) and augmented reality (AR). However, collecting panoramic images is infeasible and time consuming. We developed techniques that transform standard photos and existing scenes into seamless panoramic images. This approach could enhance VR and AR applications by creating immersive environments from everyday visuals."
Poster,Spherical Rotation Dimension Reduction with Geometric Loss Functions,https://ICML.cc//virtual/2025/poster/46713,"Hengrui Luo, Jeremy E. Purvis, Didong Li","Modern datasets often exhibit high dimensionality, yet the data reside in low-dimensional manifolds that can reveal underlying geometric structures critical for data analysis. A prime example of such a dataset is a collection of cell cycle measurements, where the inherently cyclical nature of the process can be represented as a circle or sphere. Motivated by the need to analyze these types of datasets, we propose a nonlinear dimension reduction method, Spherical Rotation Component Analysis (SRCA), that incorporates geometric information to better approximate low-dimensional manifolds. SRCA is a versatile method designed to work in both high-dimensional and small sample size settings. By employing spheres or ellipsoids, SRCA provides a low-rank spherical representation of the data with general theoretic guarantees, effectively retaining the geometric structure of the dataset during dimensionality reduction. A comprehensive simulation study, along with a successful application to human cell cycle data, further highlights the advantages of SRCA compared to state-of-the-art alternatives, demonstrating its superior performance in approximating the manifold while preserving inherent geometric structures.",
Poster,SPHINX: Structural Prediction using Hypergraph Inference Network,https://ICML.cc//virtual/2025/poster/43825,"Iulia Duta, Pietro Lió","The importance of higher-order relations is widely recognized in numerous real-world systems. However, annotating them is a tedious and sometimes even impossible task. Consequently, current approaches for data modelling either ignore the higher-order interactions altogether or simplify them into pairwise connections. To facilitate higher-order processing, even when a hypergraph structure is not available, we introduce SPHINX, a model that learns to infer a latent hypergraph structure in an unsupervised way, solely from the final task-dependent signal. To ensure broad applicability, we design the model to be end-to-end differentiable, capable of generating a discrete hypergraph structure compatible with any modern hypergraph networks, and easily optimizable without requiring additional regularization losses.Through extensive ablation studies and experiments conducted on four challenging datasets, we demonstrate that our model is capable of inferring suitable latent hypergraphs in both transductive and inductive tasks.  Moreover, the inferred latent hypergraphs are interpretable and contribute to enhancing the final performance, outperforming existing methods for hypergraph prediction.","Modeling group interactions among multiple entities simultaneously presents a challenging yet broadly applicable problem, with relevance across diverse domains such as chemistry, physics, medicine, and social networks. However, leveraging specialized architectures like hypergraph neural networks is infeasible without accurate annotations of these complex relationships. To address this, we introduce SPHINX—a novel model that jointly infers the underlying hypergraph structure and performs the downstream task using only supervision from the task itself. This approach not only enables the modeling of previously unobserved higher-order interactions but also provides interpretable visualizations of the discovered structures, enhancing the transparency and explainability of the model."
Poster,SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity,https://ICML.cc//virtual/2025/poster/43447,"Shihao Zou, Qingfeng Li, Wei Ji, Jingjing Li, Yongkui Yang, Guoqi Li, Chao Dong","Spiking Neural Networks (SNNs) have shown competitive performance to Artificial Neural Networks (ANNs) in various vision tasks, while offering superior energy efficiency. However, existing SNN-based Transformers primarily focus on single-image tasks, emphasizing spatial features while not effectively leveraging SNNs' efficiency in video-based vision tasks. In this paper, we introduce SpikeVideoFormer, an efficient spike-driven video Transformer, featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design a spike-driven Hamming attention (SDHA) which provides a theoretically guided adaptation from traditional real-valued attention to spike-driven attention. Building on SDHA, we further analyze various spike-driven space-time attention designs and identify an optimal scheme that delivers appealing performance for video tasks, while maintaining only linear temporal complexity. The generalization ability and efficiency of our model are demonstrated across diverse downstream video tasks, including classification, human pose tracking, and semantic segmentation. Empirical results show our method achieves state-of-the-art (SOTA) performance compared to existing SNN approaches, with over 15\% improvement on the latter two tasks. Additionally, it matches the performance of recent ANN-based methods while offering significant efficiency gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the three tasks. [https://github.com/JimmyZou/SpikeVideoFormer](https://github.com/JimmyZou/SpikeVideoFormer)","Video analysis by computers — such as recognizing human actions or tracking motion — is typically done using powerful AI models called Transformers. However, these models require a lot of energy, which limits their use in devices like drones or wearables. A different kind of AI, called Spiking Neural Networks (SNNs), mimics how the brain works and uses much less energy, but current SNNs don’t work well with video data.Our research introduces SpikeVideoFormer, a new kind of energy-efficient video-processing AI model that combines the strengths of Transformers and SNNs. We designed a special way for this model to ""pay attention"" to important parts of a video over time using simple brain-like signals, rather than complex math. This method keeps processing fast and efficient, even for long videos.SpikeVideoFormer achieves excellent performance in tasks like video classification, human pose tracking, and understanding video scenes — matching or beating traditional models while using up to 16 times less energy. This could make smart, energy-efficient video AI possible in more real-world settings."
Poster,SpikF: Spiking Fourier Network for Efficient Long-term Prediction,https://ICML.cc//virtual/2025/poster/46411,"Wenjie Wu, Dexuan Huo, Hong Chen","Spiking Neural Networks (SNNs) have demonstrated remarkable potential across many domains, including computer vision and natural language processing, owing to their energy efficiency and biological plausibility. However, their application in long-term prediction tasks remains underexplored, which is primarily due to two critical challenges: (1) current SNN encoding methods are unable to effectively encode long temporal information, leading to increased computational complexity and energy consumption; (2) though Transformer-based models have achieved state-of-the-art accuracy in temporal prediction tasks, the absence of proper positional encoding for spiking self-attention restricts Spiking Transformer from effectively utilizing positional information, resulting in performance degradation. To address these challenges, we introduce an attention-free framework, **Spik**ing **F**ourier Network (**SpikF**), that encodes input sequences in patches and employs an innovative frequency domain selection mechanism to effectively utilize the sequential properties of time-series data. Extensive evaluations on eight well-established long-term prediction datasets demonstrate that SpikF achieves an averaged $1.9\\%$ reduction in Mean Absolute Error (MAE) compared to state-of-the-art models, while lowering total energy consumption by $3.16\times$. Our code is available at https://github.com/WWJ-creator/SpikF.","Spiking Neural Networks (SNNs), inspired by the brain’s efficiency, are emerging as a promising energy-efficient AI paradigm that communicates via spikes. However, their ability to model long-term dependencies remains limited—they struggle to remember and predict distant future events due to inefficient long-sequence encoding and the lack of an effective mechanism to track temporal context.To overcome these limitations, we introduce Spiking Fourier Network (SpikF), a novel model that processes long input signals by decomposing them into smaller patches. Leveraging a frequency-based selection method, SpikF captures underlying temporal patterns without relying on computationally expensive attention mechanisms.Evaluated on eight prediction benchmarks, SpikF outperforms state-of-the-art methods, achieving $1.9\%$ higher accuracy on average while consuming $3.16\times$ less energy. This breakthrough paves the way for ultra-efficient, brain-inspired AI in applications like weather forecasting, stock trend prediction, and beyond."
Poster,Splitting & Integrating: Out-of-Distribution Detection via Adversarial Gradient Attribution,https://ICML.cc//virtual/2025/poster/44765,"Jiayu Zhang, Xinyi Wang, Zhibo Jin, Zhiyu Zhu, Jianlong Zhou, Fang Chen, Huaming Chen","Out-of-distribution (OOD) detection is essential for enhancing the robustness and security of deep learning models in unknown and dynamic data environments. Gradient-based OOD detection methods, such as GAIA, analyse the explanation pattern representations of in-distribution (ID) and OOD samples by examining the sensitivity of model outputs w.r.t. model inputs, resulting in superior performance compared to traditional OOD detection methods. However, we argue that the non-zero gradient behaviors of OOD samples do not exhibit significant distinguishability, especially when ID samples are perturbed by random perturbations in high-dimensional spaces, which negatively impacts the accuracy of OOD detection. In this paper, we propose a novel OOD detection method called \textbf{S \& I} based on layer \textbf{S}plitting and gradient \textbf{I}ntegration via Adversarial Gradient Attribution. Specifically, our approach involves splitting the model's intermediate layers and iteratively updating adversarial examples layer-by-layer. We then integrate the attribution gradients from each intermediate layer along the attribution path from adversarial examples to the actual input, yielding true explanation pattern representations for both ID and OOD samples. Experiments demonstrate that our S \& I algorithm achieves state-of-the-art results, with the average FPR95 of 29.05\% (ResNet34)/38.61\% (WRN40) and 37.31\% (BiT-S) on the CIFAR100 and ImageNet benchmarks, respectively. Our code is available at: https://github.com/LMBTough/S-I}{https://github.com/LMBTough/S-I","Modern AI systems often struggle when they are asked to handle unfamiliar data — for example, a facial recognition model trained on studio portraits might fail on blurry street photos. Detecting when an input is “out-of-distribution” (OOD), meaning it is different from what the model was trained on, is a key challenge in making AI safer and more reliable. Some recent methods try to solve this by looking at how sensitive the model is to small changes in the input, which reveals how the model “understands” each example. But in high-dimensional data like images, these sensitivity patterns can become noisy or misleading — especially when normal examples are slightly altered. We propose a new technique, called S & I, that improves this detection by breaking the model into parts and carefully tracking how it reacts to each input, step-by-step. This gives a more accurate picture of how the model responds to known versus unknown data. Our method beats existing approaches in identifying unfamiliar data across several image datasets, helping make AI systems more trustworthy in the real world."
Poster,Splitting with Importance-aware Updating for Heterogeneous Federated Learning with Large Language Models,https://ICML.cc//virtual/2025/poster/44090,"Yangxu Liao, Wenke Huang, Guancheng Wan, Jian Liang, Bin Yang, Mang Ye","Federated learning provides an efficient privacy-preserving distributed training framework for large language models, addressing the growing scarcity of publicly available training data while enabling the utilization of private datasets. While integrating large language model fine-tuning with federated learning emerges as a promising research direction, researchers pay limited attention to non-IID instruction-following scenarios. Our key insight is decomposing client updates into consensus and divergence components, enabling the model to maintain core capabilities while adapting to domain-specific knowledge. We propose a novel federated learning framework called **FedICU** (Splitting with **I**mportan**C**e-aware **U**pdating for Heterogeneous **Fed**erated Learning with Large Language Models), which introduces an aggregation mechanism that dynamically balances these components based on their contribution to global model performance, while implementing an importance-aware parameter updating strategy to prevent catastrophic forgetting and domain overfitting. Extensive experiments across diverse domains demonstrate that FedICU significantly outperforms existing federated learning approaches in terms of both generalization performance and domain adaptation. Our code is available at https://github.com/liaosunny123/FedICU.","Large language models like ChatGPT are powerful tools, but training them often requires high-quality datasets. However, in recent years, these high-quality datasets have been gradually exhausted, and more attention is now being given to private datasets, which have yielded significant results. Federated learning helps by allowing many individuals or organizations to train a shared model without moving their data — each trains locally and only sends updates. However, when different users have very different data or needs, this approach can degrade the shared model’s performance for everyone. Our research addresses this issue. We designed a new method called **FedICU**, which helps the shared model learn both general knowledge and user-specific needs without sacrificing overall performance. It achieves this by carefully separating what is common across users from what is unique to each one, and then combining these updates in an intelligent way. It also only sends the most important parts of the update, saving both time and computing resources. This means we can train powerful language models using private, diverse data, while minimizing the degradation of model generalization due to heterogeneous datasets and maintaining overall model performance across various downstream tasks."
Poster,SPMC: Self-Purifying Federated Backdoor Defense via Margin Contribution,https://ICML.cc//virtual/2025/poster/45625,"Wenwen He, Wenke Huang, Bin Yang, ShuKan Liu, Mang Ye","Federated Learning (FL) enables collaborative training with privacy preservation but is vulnerable to backdoor attacks, where malicious clients degrade model performance on targeted inputs. These attacks exploit FL decentralized nature, while existing defenses, based on isolated behaviors and fixed rules, can be bypassed by adaptive attackers. To address these limitations, we propose **SPMC**, a marginal collaboration defense mechanism that leverages intrinsic consistency across clients to estimate inter-client marginal contributions. This allows the system to dynamically reduce the influence of clients whose behavior deviates from the collaborative norm, thus maintaining robustness even as the number of attackers changes. In addition to overcoming proxy-dependent purification's weaknesses, we introduce a self-purification process that locally adjusts suspicious gradients. By aligning them with margin-based model updates, we mitigate the effect of local poisoning. Together, these two modules significantly improve the adaptability and resilience of FL systems, both at the client and server levels. Experimental results on a variety of classification benchmarks demonstrate that SPMC achieves strong defense performance against sophisticated backdoor attacks without sacrificing accuracy on benign tasks. The code is posted at: https://github.com/WenddHe0119/SPMC.","Federated Learning allows devices to collaboratively train AI models without sharing their private data. However, it is vulnerable to backdoor attacks, where malicious participants secretly poison the model to behave incorrectly when triggered. Existing defenses often rely on strict rules, extra clean data or individual action, which limits their adaptability. We propose SPMC, a new defense that measures each client contribution to the margin group. If a client behaves differently from the group, its influence is reduced. This dynamic weighting helps detect and suppress attackers without predefined thresholds. Additionally, we introduce a self-purifying process that adjusts local updates to align with shared knowledge from other clients. This keeps the model learning from clean patterns even when some clients are poisoned. Experiments on standard image datasets show that SPMC defends effectively against sophisticated attacks while keeping accuracy high on regular tasks. Our method improves both the robustness and flexibility of FL systems."
Poster,SPRI: Aligning Large Language Models with Context-Situated Principles,https://ICML.cc//virtual/2025/poster/44235,"Hongli Zhan, Muneeza Azmat, Raya Horesh, Junyi Jessy Li, Mikhail Yurochkin","Aligning Large Language Models to integrate and reflect human values, especially for tasks that demand intricate human oversight, is arduous since it is resource-intensive and time-consuming to depend on human expertise for context-specific guidance. Prior work has utilized predefined sets of rules or principles to steer the behavior of models (Bai et al., 2022; Sun et al., 2023). However, these principles tend to be generic, making it challenging to adapt them to each individual input query or context. In this work, we present Situated-PRInciples (SPRI), a framework requiring minimal or no human effort that is designed to automatically generate guiding principles in real-time for each input query and utilize them to align each response. We evaluate SPRI on three tasks, and show that 1) SPRI can derive principles in a complex domain-specific task that leads to on-par performance as expert-crafted ones; 2) SPRI-generated principles lead to instance-specific rubrics that outperform prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data leads to substantial improvement on truthfulness.","Large language models (like ChatGPT) are powerful but often need guidance to behave in ways that align with human values, especially for sensitive or complex tasks. Traditionally, this guidance comes from predefined rules or expert feedback — a process that’s slow, costly, and hard to personalize.Our work introduces a new approach called SPRI (Situated-PRInciples), which can automatically create custom guiding principles for each situation or question, without relying on humans to write them. Imagine a virtual assistant that doesn’t just follow a fixed set of rules, but figures out the best rules for the moment — all on its own.We tested SPRI in several tasks and found that it performs as well as expert-written rules. It also improves how language models judge and respond to complex queries, leading to more truthful and context-sensitive outputs.By making AI more adaptable and principled without human labor, SPRI takes a step toward scalable, value-aligned systems that can reason responsibly in diverse real-world scenarios."
