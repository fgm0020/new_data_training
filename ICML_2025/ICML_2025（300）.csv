type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,The Limits of Predicting Agents from Behaviour,https://ICML.cc//virtual/2025/poster/44820,"Alexis Bellot, Jonathan Richens, Tom Everitt","As the complexity of AI systems and their interactions with the world increases, generating explanations for their behaviour is important for safely deploying AI. For agents, the most natural abstractions for predicting behaviour attribute beliefs, intentions and goals to the system. If an agent behaves as if it has a certain goal or belief, then we can make reasonable predictions about how it will behave in novel situations, including those where comprehensive safety evaluations are untenable. How well can we infer an agent’s beliefs from their behaviour, and how reliably can these inferred beliefs predict the agent’s behaviour in novel situations? We provide a precise answer to this question under the assumption that the agent’s behaviour is guided by a world model. Our contribution is the derivation of novel bounds on the agent's behaviour in new (unseen) deployment environments, which represent a theoretical limit for predicting intentional agents from behavioural data alone. We discuss the implications of these results for several research areas including fairness and safety.","An important consideration to safely interact with AI systems is to form expectations as to how they might act in the future. In this paper, we answer this question under the assumption that the agent’s behaviour is guided by a world model. This is assumption is useful for prediction because it implies a consistency in behaviour that can in principle be exploited to infer the AI's choice of action in novel (unseen) environments. We show that the AI's choice of action can be partially determined, meaning that some actions might be ruled out while others might not, just by using the AI's history of decisions in related environments. Our results give theoretical limits on how much can be known about AI preferences just from data on their past behaviour."
Poster,The Limits of Tractable Marginalization,https://ICML.cc//virtual/2025/poster/44685,"Oliver Broadrick, Sanyam Agarwal, Guy Van den Broeck, Markus Bläser","Marginalization -- summing a function over all assignments to a subset of its inputs -- is a fundamental computational problem with applications from probabilistic inference to formal verification.Despite its computational hardness in general, there exist many classes of functions (e.g., probabilistic models) for which marginalization remains tractable, and they can all be commonly expressed by arithmetic circuits computing multilinear polynomials.This raises the question, can *all* functions with polynomial time marginalization algorithms be succinctly expressed by such circuits? We give a negative answer, exhibiting simple functions with tractable marginalization yet no efficient representation by known models, assuming $\\mathsf{FP} \\neq \\#\\mathsf{P}$ (an assumption implied by $\\mathsf{P} \\neq \\mathsf{NP}$). To this end, we identify a hierarchy of complexity classes corresponding to stronger forms of marginalization, all of which are efficiently computable on the known circuit models. We conclude with a completeness result, showing that whenever there is an efficient real RAM performing virtual evidence marginalization for a function, then there are small arithmetic circuits for that function's multilinear representation.","In this work, we analyse a commonly used architecture (arithmetic circuits computing multilinear polynomials) in probabilistic inference particularly used for marginalization tasks. We prove the theoretical limits on the expressiveness of this architecture, by showing that there are functions for which marginalization is efficient but which can't be succinctly represented by these circuits."
Poster,The Lock-in Hypothesis: Stagnation by Algorithm,https://ICML.cc//virtual/2025/poster/44167,"Tianyi Qiu, Zhonghao He, Tejasveer Chugh, Max Kleiman-Weiner","The training and deployment of large language models (LLMs) create a feedback loop with human users: models learn human beliefs from data, reinforce these beliefs with generated content, reabsorb the reinforced beliefs, and feed them back to users again and again. This dynamic resembles an echo chamber.We hypothesize that this feedback loop entrenches the existing values and beliefs of users, leading to a loss of diversity in human ideas and potentially the *lock-in* of false beliefs.We formalize this hypothesis and test it empirically with agent-based LLM simulations and real-world GPT usage data. Analysis reveals sudden but sustained drops in diversity after the release of new GPT iterations, consistent with the hypothesized human-AI feedback loop.*Website: https://thelockinhypothesis.com*","AI language models, like ChatGPT, learn from the vast amounts of text humans create. As these AIs generate new content, they can echo and reinforce our existing beliefs, creating a feedback loop where the AI learns our views, feeds them back to us, and then re-absorbs these potentially amplified views. We hypothesized that this cycle might inadvertently trap society in its current ways of thinking, reducing the diversity of our ideas and potentially leading to the widespread entrenchment of false or harmful beliefs – a phenomenon we term ""value lock-in.""To investigate this, we developed a mathematical model and ran computer simulations which demonstrated how this belief entrenchment could occur through such human-AI feedback. We then tested our ""Lock-in Hypothesis"" by analyzing real-world data from millions of interactions with ChatGPT. Our findings revealed noticeable and sustained drops in the variety of concepts discussed by users immediately following the release of new AI model versions, which are trained on updated human data. This suggests that the ongoing human-AI interaction could be subtly making our collective thoughts more uniform over time. Such a trend, if continued, poses a risk of hindering societal innovation, critical thinking, and our ability to correct widely held misbeliefs."
Poster,The Logical Implication Steering Method for Conditional Interventions on Transformer Generation,https://ICML.cc//virtual/2025/poster/45970,Damjan Kalajdzievski,"The field of mechanistic interpretability in pre-trained transformer models has demonstrated substantial evidence supporting the ''linear representation hypothesis'', which is the idea that high level concepts are encoded as vectors in the space of activations of a model. Studies also show that model generation behavior can be steered toward a given concept by adding the concept's vector to the corresponding activations. We show how to leverage these properties to build a form of logical implication into models, enabling transparent and interpretable adjustments that induce a chosen generation behavior in response to the presence of any given concept. Our method, Logical Implication Model Steering (LIMS), unlocks new hand-engineered reasoning capabilities by integrating neuro-symbolic logic into pre-trained transformer models.","The Logical Implication Model Steering (LIMS) method provides a simple and interpretable way to adjust the behavior of large language models without retraining them. Instead of relying on extensive examples or opaque fine-tuning, LIMS allows users to specify a logical rule such as “if condition $ P$ is true, then the model should behave according to $Q$.” For example, one might want a model to respond cautiously when it detects uncertainty or avoid answering when insufficient information is present. LIMS works by identifying internal patterns, called concept vectors, associated with both the condition and the desired behavior. These are derived from a small set of labeled examples. The method then installs a lightweight, targeted circuit inside the model that activates the desired behavior only when the condition is met. This provides an efficient, interpretable mechanism for guiding model outputs in a structured and predictable way."
Poster,The Missing Alignment Link of In-context Learning on Sequences,https://ICML.cc//virtual/2025/poster/46475,"Harshvardhan Agarwal, Sunita Sarawagi","Large language models (LLMs) have demonstrated the capability to perform in-context learning (ICL) for completely unseen tasks in classification or language completion.  Sequence to sequence (seq2seq) is another popular task category with several applications seeking quick adaptation with ICL. We present a systematic analysis of the ICL capability of LLMs on Seq2Seq tasks using a formal structured language-pair. Our study reveals a critical limitation: except for very short input sequences, ICL fails to achieve consistent learning across all output positions. This exposes a fundamental weakness of modern LLMs — their inability to effectively uncover the alignment between input and output sequences. Consequently, this limitation results in incomplete induction heads, which are essential for in-context learning of new discrete mappings.To address these limitations, we propose ICA-Tune, a method for focused fine-tuning of an LLM using in-context examples. We present a mechanistic evaluation with two accuracy probes to show how input-output alignment emerges in middle layers of an LLM without direct supervision. This alignment leads to an abrupt jump in the completeness of  the induction heads in higher layers. We show that, compared to standard fine-tuning, ICA-Tune enables more sample efficient learning and better generalization to OOD instances.","Large language models (LLMs) like ChatGPT can learn new tasks just by seeing examples, a skill known as in-context learning. But when the task involves mapping sequences — like translating a sentence or answering a question — the models often fall short, especially when the input is longer. We explored why this happens and discovered that these models don’t naturally figure out how parts of the input relate to parts of the output. This missing link makes it harder for them to learn new sequence-to-sequence tasks just from examples.To fix this, we designed ICA-Tune, a lightweight method that gently fine-tunes the model using only example prompts. It teaches the model how to align inputs and outputs more clearly, without any extra labels or supervision. ICA-Tune makes language models better at learning new sequence tasks from a few examples, and it helps them generalize to unfamiliar problems — making them more flexible and reliable tools."
Poster,The Noisy Laplacian: a Threshold Phenomenon for Non-Linear Dimension Reduction,https://ICML.cc//virtual/2025/poster/45836,"Alex Kokot, Octavian-Vlad Murad, Marina Meila","In this paper, we clarify the effect of noise on common spectrallymotivated algorithms such as Diffusion Maps (DM) for dimensionreduction. Empirically, these methods are much more robust to noisethan current work suggests. Specifically, existing consistency resultsrequire that either the noise amplitude or dimensionality must varywith the sample size $n$.  We provide new theoretical resultsdemonstrating that low-frequency eigenpairs reliably capture thegeometry of the underlying manifold under a constant noise level, up to a dimension independent threshold $O(r^{-2})$, where $r$ is the noise amplitude. Our results rely on a decomposition of the manifold Laplacian in the Sasakimetric, a technique not used before in this area, to our knowledge. We experimentally validate our theoretical predictions. Additionally, we observesimilar robust behavior for other manifold learning algorithms which are not based on computing the Laplacian, namely LTSA and VAE.","Dimension reduction is the task of reducing the complexity of data while maintaining the integrity of essential information. Diffusion Maps is representative of one approach to this problem, where geometric information of a dataset is captured by an embedding related to a fundamental differential operator. The typical setting for these studies is for manifold valued data, data with a low-dimensional latent structure. It has been shown that this structure is essentially preserved by this procedure. In practice, even data with low-dimensional structure likely has some level of noise, and in this work we show what recovery is possible in this setting. Our key result is that, up to a noise dependent threshold, intrinsic manifold information can be approximated up to a small error using methods like Diffusion Maps. However, beyond this point, any larger embedding will capture mostly extraneous information due to the noise structure. We make this precise by leveraging a particular decomposition of noisy manifold data, the Sasaki metric, and we show that it allows for a neat disentanglement of noise and manifold components. These results can be related to real data by a perturbation argument.This culminates in a new perspective on why spectral embeddings such as Diffusion Maps are effective. While previous work emphasizes that recovery of manifold geometry is possible for truly low-dimensional data, we show that for very high dimensional data contaminated by noise, low-dimensional structure can still be revealed by these procedures. In this sense, the data is being implicitly denoised, providing valuable insight on its underlying structure. We observe a similar behavior for LTSA and VAEs, indicating a generality to this phenomenon."
Poster,The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes,https://ICML.cc//virtual/2025/poster/45751,"Pedro Santos, Alberto Sardinha, Francisco S. Melo","The general-utility Markov decision processes (GUMDPs) framework generalizes the MDPs framework by considering objective functions that depend on the frequency of visitation of state-action pairs induced by a given policy. In this work, we contribute with the first analysis on the impact of the number of trials, i.e., the number of randomly sampled trajectories, in infinite-horizon GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a key-role in infinite-horizon GUMDPs and the expected performance of a given policy depends, in general, on the number of trials. We consider both discounted and average GUMDPs, where the objective function depends, respectively, on discounted and average frequencies of visitation of state-action pairs.  First, we study policy evaluation under discounted GUMDPs, proving lower and upper bounds on the mismatch between the finite and infinite trials formulations for GUMDPs. Second, we address average GUMDPs, studying how different classes of GUMDPs impact the mismatch between the finite and infinite trials formulations. Third, we provide a set of empirical results to support our claims, highlighting how the number of trajectories and the structure of the underlying GUMDP influence policy evaluation.","Many modern AI systems, like those used in robotics or game-playing, rely on learning by trial and error. These systems are typically evaluated based on how often they make good decisions over time. However, in real-world situations, the number of times an AI agent can interact with its environment is limited — and current methods don’t always account for this limitation when designing or judging an AI’s behavior.In our research, we explore how the number of attempts (or “trials”) an AI agent has can significantly affect its performance, especially in situations where success depends on behavior patterns over long periods. We provide insights explaining how an agent's expected performance may change depending on the number of trials and show, through experiments, how different types of problems and environments influence the results.Our work brings us a step closer to designing AI systems that perform reliably, even under limited agent-environment interaction — a common challenge in the real world, from healthcare to autonomous vehicles."
Poster,Theoretical guarantees on the best-of-n alignment policy,https://ICML.cc//virtual/2025/poster/43750,"Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D&#x27;Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Suresh","A simple and effective method for the inference-time alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a reference policy, ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature  claims that the KL divergence between the best-of-$n$ policy and  the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes, and propose a new estimator for the KL divergence and empirically show that it provides a tight approximation. We also show that the win rate of the best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the tightness of this characterization. We conclude with analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$ alignment policy, which demonstrate that very good tradeoffs are achievable with $n < 1000$.","A common approach for improving the output of generative models is best-of-n, i.e., to generate n candidates and then choose the one that maximizes some measure of quality. How much does this strategy change the underlying model? Researchers commonly use the heuristic that the divergence grows with the logarithm of the number of candidates. We show that this is an upper bound, we characterize the conditions under which this bound is tight, and we propose a better approximation. Finally, we bound the probability that this ""best-of-n"" strategy produces a better output, and analyze the tradeoff with respect to preserving the key features of the original model."
Poster,Theoretical Limitations of Ensembles in the Age of Overparameterization,https://ICML.cc//virtual/2025/poster/46048,"Niclas Dern, John Cunningham, Geoff Pleiss","Classic ensembles generalize better than any single component model. In contrast, recent empirical studies find that modern ensembles of (overparameterized) neural networks may not provide any inherent generalization advantage over single but larger neural networks. This paper clarifies how modern overparameterized ensembles differ from their classic underparameterized counterparts, using ensembles of random feature (RF) regressors as a basis for developing theory. In contrast to the underparameterized regime, where ensembling typically induces regularization and increases generalization, we prove with minimal assumptions that infinite ensembles of overparameterized RF regressors become pointwise equivalent to (single) infinite-width RF regressors, and finite width ensembles rapidly converge to single models with the same parameter budget. These results, which are exact for ridgeless models and approximate for small ridge penalties, imply that overparameterized ensembles and single large models exhibit nearly identical generalization. We further characterize the predictive variance amongst ensemble members, demonstrating that it quantifies the expected effects of increasing capacity rather than capturing any conventional notion of uncertainty. Our results challenge common assumptions about the advantages of ensembles in overparameterized settings, prompting a reconsideration of how well intuitions from underparameterized ensembles transfer to deep ensembles and the overparameterized regime.","In safety-critical applications like medical diagnosis or self-driving cars, researchers often combine multiple AI models into so-called ""ensembles"" to improve predictions – similar to consulting a committee rather than a single expert. This approach has worked well for simple models, but with today's powerful neural networks that can memorize entire datasets, ensembles often fail to deliver the expected benefits.We analyzed this mathematically using simplified neural networks. We discovered that when models are complex enough to memorize their training data, ensembles of them closely behave like a single, larger model. This means ensembling large models offers little gain over simply training a single, bigger model. Furthermore, we found that a common method for estimating the uncertainty of ensemble predictions – measuring disagreement among ensemble members – lacks theoretical grounding in such cases.Our results don't deny that ensembles can still be useful in practice since larger models might, for example, be hard to train. However, they caution against viewing ensembles as a simple and reliable strategy for boosting performance over what a single larger model could achieve or for assessing uncertainty."
Poster,Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models,https://ICML.cc//virtual/2025/poster/45296,"Quan Nguyen, Minh Vu, Truc Nguyen, My T. Thai","Federated Learning (FL) enables collaborative learning among clients via a coordinating server while avoiding direct data sharing, offering a perceived solution to preserve privacy. However, recent studies on Membership Inference Attacks (MIAs) have challenged this notion, showing high success rates against unprotected training data. While local differential privacy (LDP) is widely regarded as a gold standard for privacy protection in data analysis, most studies on MIAs either neglect LDP or fail to provide theoretical guarantees for attack success against LDP-protected data.    To address this gap, we derive theoretical lower bounds for the success rates of low-polynomial-time MIAs that exploit vulnerabilities in fully connected or self-attention layers, regardless of the LDP mechanism used. We establish that even when data are protected by LDP, privacy risks persist, depending on the privacy budget. Practical evaluations on models like ResNet and Vision Transformer confirm considerable privacy risks, revealing that the noise required to mitigate these attacks significantly degrades models' utility.","Federated Learning (FL) is a popular method that allows devices (like smartphones) to train a shared machine learning model without directly sharing their personal data. This sounds private, but there are still risks—especially from a central server that might act dishonestly. One major threat is a membership inference attack (MIA), where an attacker tries to figure out if your data was used to train a model.To guard against this, many systems use Local Differential Privacy (LDP)—a technique that adds randomized noise to your data before it even leaves your device. LDP is supposed to protect your privacy, but this paper shows, both in theory and with real experiments, that even with LDP protection, privacy risks persists depending on the amount of noise added, and the amount of noise needed to truly protect data privacy might significantly impact model's utility."
