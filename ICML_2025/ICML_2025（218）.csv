type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Partially Observable Reinforcement Learning with Memory Traces,https://ICML.cc//virtual/2025/poster/44701,"Onno Eberhard, Michael Muehlebach, Claire Vernade","Partially observable environments present a considerable computational challenge in reinforcement learning due to the need to consider long histories. Learning with a finite window of observations quickly becomes intractable as the window length grows. In this work, we introduce *memory traces*. Inspired by eligibility traces, these are compact representations of the history of observations in the form of exponential moving averages. We prove sample complexity bounds for the problem of offline on-policy evaluation that quantify the return errors achieved with memory traces for the class of Lipschitz continuous value estimates. We establish a close connection to the window approach, and demonstrate that, in certain environments, learning with memory traces is significantly more sample efficient. Finally, we underline the effectiveness of memory traces empirically in online reinforcement learning experiments for both value prediction and control.","Reinforcement learning (RL) is a framework where an agent learns to make decisions by interacting with an environment. In many situations, the agent can't fully observe the environment at each step, so it must rely on its past observations to act effectively. A common approach is to use a fixed-length window of recent observations, but this quickly becomes inefficient and hard to scale as the window grows.Our work introduces *memory traces*, a simple and scalable way to summarize past observations. Instead of storing the history explicitly, memory traces maintain a running average that gives more weight to recent events. This lets the agent keep a useful summary of what it has seen—without overwhelming memory or computation.We rigorously analyze how well memory traces perform in a key learning task—estimating the long-term outcomes of actions—when the system learns from past data. We show that in some environments, this approach leads to faster and more reliable learning. Finally, we demonstrate that memory traces also improve performance in real-time learning scenarios, making them a practical tool for smarter decision-making under uncertainty."
Poster,"Partition First, Embed Later: Laplacian-Based Feature Partitioning for Refined Embedding and Visualization of High-Dimensional Data",https://ICML.cc//virtual/2025/poster/46392,"Erez Peterfreund, Ofir Lindenbaum, Yuval Kluger, Boris Landa","Embedding and visualization techniques are essential for analyzing high-dimensional data, but they often struggle with complex data governed by multiple latent variables, potentially distorting key structural characteristics. This paper considers scenarios where the observed features can be partitioned into mutually exclusive subsets, each capturing a different smooth substructure. In such cases, visualizing the data based on each feature partition can better characterize the underlying processes and structures in the data, leading to improved interpretability. To partition the features, we propose solving an optimization problem that promotes graph Laplacian-based smoothness in each partition, thereby prioritizing partitions with simpler geometric structures. Our approach generalizes traditional embedding and visualization techniques, allowing them to learn multiple embeddings simultaneously. We establish that if several independent or partially dependent manifolds are embedded in distinct feature subsets in high-dimensional space, then our framework can reliably identify the correct subsets with theoretical guarantees. Finally, we demonstrate the effectiveness of our approach in extracting multiple low-dimensional structures and partially independent processes from both simulated and real data.","Modern scientific datasets are often large and complex, containing thousands of high-dimensional samples---that is, each sample is described by many different features or attributes. To make sense of such data, researchers use embedding and visualization techniques to reduce its dimensionality, enabling them to identify patterns and structures more easily. But when the data originates from multiple unknown underlying processes---each affecting different sets of features---a single visualization can mix these processes, making the results difficult to interpret.In this paper, we tackle this problem by developing a method that separates the features into groups, with each group reflecting a different underlying process. A separate embedding or visualization for each group provides a clearer view of the underlying processes, helping to disentangle and highlight the different factors at play. We also provide theoretical guarantees for successful recovery when the underlying processes are independent or even partially dependent, and we demonstrate the approach’s effectiveness on both simulated and real scientific datasets."
Poster,PASS: Private Attributes Protection with Stochastic Data Substitution,https://ICML.cc//virtual/2025/poster/44888,"Yizhuo Chen, Chun-Fu (Richard) Chen, Hsiang Hsu, Shaohan Hu, Tarek Abdelzaher","The growing Machine Learning (ML) services require extensive collections of user data, which may inadvertently include people's private information irrelevant to the services. Various studies have been proposed to protect private attributes by removing them from the data while maintaining the utilities of the data for downstream tasks. Nevertheless, as we theoretically and empirically show in the paper, these methods reveal severe vulnerability because of a common weakness rooted in their adversarial training based strategies. To overcome this limitation, we propose a novel approach, PASS, designed to stochastically substitute the original sample with another one according to certain probabilities, which is trained with a novel loss function soundly derived from information-theoretic objective defined for utility-preserving private attributes protection. The comprehensive evaluation of PASS on various datasets of different modalities, including facial images, human activity sensory signals, and voice recording datasets, substantiates PASS's effectiveness and generalizability.","Modern AI systems often require users to share personal data—such as facial images, voice recordings, or sensor readings—to function effectively. However, this data can unintentionally contain sensitive personal details, like gender or identity, which can be extracted and misused by malicious attackers.To address this risk, many advanced techniques have been developed to “hide” sensitive information from the data while preserving the useful parts needed for AI tasks—for example, modifying an image to make it “gender-neutral” while keeping the facial expression and age unchanged.Unfortunately, in this paper, we discovered that these existing methods share a common flaw: they rely on a strategy called “adversarial training,” which makes them notably vulnerable to slightly more powerful attackers.To overcome this, we introduce a new method called PASS. Instead of trying to “hide” sensitive information, PASS replaces each data point—such as a facial image—with another similar example in a carefully designed, probabilistic way.We show that PASS is backed by strong theoretical foundations and makes it significantly harder for attackers to extract private information. This research moves us a step closer to safer AI systems that respect personal privacy."
Poster,PatchPilot: A Cost-Efficient Software Engineering Agent with Early Attempts on Formal Verification,https://ICML.cc//virtual/2025/poster/43520,"Hongwei Li, Yuheng Tang, Shiqi Wang, Wenbo Guo","Recent research builds various patching agents that combine large language models (LLMs) with non-ML tools and achieve promising results on the state-of-the-art (SOTA) software patching benchmark, SWE-bench. Based on how to determine the patching workflows, existing patching agents can be categorized as agent-based planning methods, which rely on LLMs for planning, and rule-based planning methods, which follow a pre-defined workflow.At a high level, agent-based planning methods achieve high patching performance but with a high cost and limited stability. Rule-based planning methods, on the other hand, are more stable and efficient but have key workflow limitations that compromise their patching performance.In this paper, we propose PatchPilot, an agentic patcher that strikes a balance between patching efficacy, stability, and cost-efficiency. PatchPilot proposes a novel rule-based planning workflow with five components: reproduction, localization, generation, validation, and refinement (where refinement is unique to PatchPilot).We introduce novel and customized designs to each component to optimize their effectiveness and efficiency. Through extensive experiments on the SWE-bench benchmarks, PatchPilot shows a superior performance than existing open-source methods while maintaining low cost (less than 1\$ per instance) and ensuring higher stability.We also conduct a detailed ablation study to validate the key designs in each component.Our code is available at https://github.com/ucsb-mlsec/PatchPilot.","Automatically fixing software bugs remains challenging: AI-driven tools can generate code patches but often sacrifice either cost, reliability, or effectiveness. We introduce PatchPilot, which guides bug repair through a clear five-step process—reproducing the error, pinpointing its location, generating a fix, validating the result, and refining the solution—to deliver accurate patches while keeping costs below $1 per bug. On the SWE-bench benchmark, PatchPilot outperforms existing open-source methods with greater consistency and affordability, offering developers a faster, more reliable way to maintain healthy code."
Poster,Patch-wise Structural Loss for Time Series Forecasting,https://ICML.cc//virtual/2025/poster/44030,"Dilfira Kudrat, Zongxia Xie, Yanru Sun, Tianyu Jia, Qinghua Hu","Time-series forecasting has gained significant attention in machine learning due to its crucial role in various domains. However, most existing forecasting models rely heavily on point-wise loss functions like Mean Squared Error, which treat each time step independently and neglect the structural dependencies inherent in time series data, making it challenging to capture complex temporal patterns accurately. To address these challenges, we propose a novel **P**atch-wise **S**tructural (**PS**) loss, designed to enhance structural alignment by comparing time series at the patch level. Through leveraging local statistical properties, such as correlation, variance, and mean, PS loss captures nuanced structural discrepancies overlooked by traditional point-wise losses. Furthermore, it integrates seamlessly with point-wise loss, simultaneously addressing local structural inconsistencies and individual time-step errors. PS loss establishes a novel benchmark for accurately modeling complex time series data and provides a new perspective on time series loss function design. Extensive experiments demonstrate that PS loss significantly improves the performance of state-of-the-art models across diverse real-world datasets. The data and code are publicly available at: \url{https://github.com/Dilfiraa/PS_Loss}.","Time-series forecasting plays a crucial role in various domains. Most forecasting models are trained using point-wise loss functions, which evaluate errors at each time step independently. However, this approach overlooks the temporal structure of time series data, limiting the model's ability to capture complex patterns.To address this limitation, we propose a new loss function, Patch-wise Structural (PS) loss. Instead of comparing individual time points, PS loss compares short segments (patches) of the predicted and actual series. It measures the similarity between patches using statistical properties including correlation, variance, and mean, which reflect overall trend, fluctuation degree, and value offset, respectively. By incorporating these structural attributes, PS loss encourages models to generate predictions that align more closely with the underlying structure of the true series.This work provides a new perspective on loss function design by integrating patch-wise comparisons and structural information to more effectively capture the intrinsic patterns of time-series data."
Poster,PCEvolve: Private Contrastive Evolution for Synthetic Dataset Generation via Few-Shot Private Data and Generative APIs,https://ICML.cc//virtual/2025/poster/45741,"Jianqing Zhang, Yang Liu, Jie Fu, Yang Hua, Tianyuan Zou, Jian Cao, Qiang Yang","The rise of generative APIs has fueled interest in privacy-preserving synthetic data generation. While the Private Evolution (PE) algorithm generates Differential Privacy (DP) synthetic images using diffusion model APIs, it struggles with few-shot private data due to the limitations of its DP-protected similarity voting approach. In practice, the few-shot private data challenge is particularly prevalent in specialized domains like healthcare and industry. To address this challenge, we propose a novel API-assisted algorithm, Private Contrastive Evolution (PCEvolve), which iteratively mines inherent inter-class contrastive relationships in few-shot private data beyond individual data points and seamlessly integrates them into an adapted Exponential Mechanism (EM) to optimize DP’s utility in an evolution loop. We conduct extensive experiments on four specialized datasets, demonstrating that PCEvolve outperforms PE and other API-assisted baselines. These results highlight the potential of leveraging API access with private data for quality evaluation, enabling the generation of high-quality DP synthetic images and paving the way for more accessible and effective privacy-preserving generative API applications. Our code is available at https://github.com/TsingZ0/PCEvolve.","(1) **Problem**: Creating realistic synthetic data for specialized fields like healthcare is crucial for training AI models, but privacy concerns and limited data access pose major challenges. Existing methods struggle when only a few private examples are available, as adding privacy protections often ruins data quality.  (2) **Solution**: We developed a new algorithm, PCEvolve, that generates high-quality synthetic images while protecting privacy—even with just a handful of examples. By focusing on key differences between data categories (e.g., tumor vs. healthy tissue) and using smarter selection strategies, PCEvolve guides AI tools to produce synthetic images that closely match the private data without directly exposing it.  (3) **Impact**: PCEvolve outperforms existing methods across medical and industrial datasets, enabling clinics or factories with limited data to safely leverage powerful AI tools. This breakthrough makes privacy-preserving synthetic data practical for critical applications, helping democratize AI access while safeguarding sensitive information. Our open-source tool allows researchers to explore this approach further."
Poster,PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs,https://ICML.cc//virtual/2025/poster/46313,"Mauricio Soroco, Jialin Song, Mengzhou Xia, Kye Emond, Weiran Sun, Wuyang Chen","We present PDE-Controller, a framework that enables large language models (LLMs) to control systems governed by partial differential equations (PDEs). Traditional LLMs have excelled in commonsense reasoning but fall short in rigorous logical reasoning. While recent AI-for-math has made strides in pure mathematics, areas of applied mathematics, particularly PDEs, remain underexplored despite their significant real-world applications. Our approach enables LLMs to transform informal natural language instructions into formal specifications, and then execute reasoning and planning steps to improve the utility of PDE control. We build a holistic solution comprising datasets (both human-written cases and 2 million synthetic samples), math-reasoning models, and novel evaluation metrics, all of which require significant effort. Our PDE-Controller significantly outperforms the latest open-source and GPT models in reasoning, autoformalization, and program synthesis, achieving up to a 62% improvement in utility gain for PDE control. By bridging the gap between language generation and PDE systems, we demonstrate the potential of LLMs in addressing complex scientific and engineering challenges. We promise to release all data, model checkpoints, and code upon acceptance.","We explore the abilities of large language models (LLMs) to perform scientific reasoning, beyond commonsense reasoning, in order to solve partial differential equations (PDEs). Scientific and engineering problems often rely on PDEs to model real-world systems, but AI struggles to handle them efficiently. Our framework, PDE-Controller, bridges this gap by enabling LLMs to better understand and control these equations, leading to smarter, more effective solutions in fields like aerospace engineering, physics, and material science. By improving LLMs' capabilities over the latest open-source and GPT models in reasoning, autoformalization, and program synthesis, this framework promises significant advances in automated scientific and engineering research."
Poster,PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations,https://ICML.cc//virtual/2025/poster/46546,"Benjamin Holzschuh, Qiang Liu, Georg Kohl, Nils Thuerey","We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.Our source code is available at https://github.com/tum-pbs/pde-transformer.","The PDE-Transformer is a new AI model designed to predict how physical systems behave, like how heat spreads or fluids flow. It is built on a network architecture called a transformer, which is also used in fields such as computer vision and natural language processing. We have made some key changes to make this transformer better suited for complex scientific simulations. It can handle larger simulations and is more flexible, making it a good base for creating powerful AI models for various scientific problems. Our tests show that the PDE-Transformer is better than other leading models at predicting the behaviour of 16 different types of physics problems. We also found a better way to represent different physical properties, like density or velocity, allowing the model to learn multiple types of physics at the same time more effectively. When we train our PDE-Transformer on a lot of data first, it performs better on new, challenging tasks compared to models trained from scratch."
Poster,PDUDT: Provable Decentralized Unlearning under Dynamic Topologies,https://ICML.cc//virtual/2025/poster/45661,"Jing Qiao, Yu Liu, Zengzhe Chen, Mingyi Li, YUAN YUAN, Xiao Zhang, Dongxiao Yu","This paper investigates decentralized unlearning, aiming to eliminate the impact of a specific client on the whole decentralized system. However, decentralized communication characterizations pose new challenges for effective unlearning: the indirect connections make it difficult to trace the specific client's impact, while the dynamic topology limits the scalability of retraining-based unlearning methods.In this paper, we propose the first **P**rovable **D**ecentralized **U**nlearning algorithm under **D**ynamic **T**opologies called PDUDT. It allows clients to eliminate the influence of a specific client without additional communication or retraining. We provide rigorous theoretical guarantees for PDUDT, showing it is statistically indistinguishable from perturbed retraining. Additionally, it achieves an efficient convergence rate of $\mathcal{O}(\frac{1}{T})$ in subsequent learning, where $T$ is the total communication rounds. This rate matches state-of-the-art results. Experimental results show that compared with the Retrain method, PDUDT saves more than 99\% of unlearning time while achieving comparable unlearning performance.","We study how to “unlearn” a specific participant’s data from a fully decentralized learning system without the heavy cost of retraining or extra communication. In decentralized training, devices exchange model updates over changing networks, making it hard to pinpoint and remove one client’s influence once learning is complete. Our solution, called PDUDT, lets every node erase a target client’s contribution simply by tuning its own updates—no extra communication or full replay of past state is needed. We prove that PDUDT’s outcome is statistically equivalent to perturbed retraining method, giving strong guarantees that the undesired influence is truly removed. After unlearning, PDUDT can quickly converge in subsequent training. In experiments, PDUDT matches the unlearning quality of naive retraining while reducing unlearning time by over 99%. This makes it a practical, scalable way to enforce data removal in real‐world decentralized learning."
Poster,PEAKS: Selecting Key Training Examples Incrementally via Prediction Error Anchored by Kernel Similarity,https://ICML.cc//virtual/2025/poster/45861,"Mustafa Burak Gurbuz, Xingyu Zheng, Constantine Dovrolis","As deep learning continues to be driven by ever-larger datasets, understanding which examples are most important for generalization has become a critical question. While progress in data selection continues, emerging applications require studying this problem in dynamic contexts. To bridge this gap, we pose the Incremental Data Selection (IDS) problem, where examples arrive as a continuous stream, and need to be selected without access to the full data source. In this setting, the learner must incrementally build a training dataset of predefined size while simultaneously learning the underlying task. We find that in IDS, the impact of a new sample on the model state depends fundamentally on both its geometric relationship in the feature space and its prediction error. Leveraging this insight, we propose PEAKS (Prediction Error Anchored by Kernel Similarity), an efficient data selection method tailored for IDS. Our comprehensive evaluations demonstrate that PEAKS consistently outperforms existing selection strategies. Furthermore, PEAKS yields increasingly better performance returns than random selection as training data size grows on real-world datasets. The  code is available at https://github.com/BurakGurbuz97/PEAKS.","Modern AI systems require training on massive datasets, consuming enormous computational resources. This trend is unsustainable as datasets continue growing. We first propose incremental data selection problem to study identifying key training examples in a setting that reflects real-world constraints. We next study the impact of new training data on AI model performance mathematically, discovering that an example's value depends on both how much the model struggles with it and how similar it is to other examples in the training dataset. Based on these insights, we propose PEAKS, a principled data selection method that combines prediction error with similarity measures to identify the most valuable training examples. Our experiments show that PEAKS achieves similar performance to training on full datasets while using up to four times less data, making AI training more efficient and sustainable."
