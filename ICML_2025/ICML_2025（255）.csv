type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Retrieval-Augmented Language Model for Knowledge-aware Protein Encoding,https://ICML.cc//virtual/2025/poster/45183,"Zhang Jiasheng, Delvin Zhang, Shuang Liang, Zhengpin Li, ZHITAO YING, Jie Shao","Protein language models often struggle to capture biological functions due to their lack of factual knowledge (e.g., gene descriptions). Existing solutions leverage protein knowledge graphs (PKGs) as auxiliary pre-training objectives, but lack explicit integration of task-oriented knowledge, making them suffer from limited knowledge exploitation and catastrophic forgetting. The root cause is that they fail to align PKGs with task-specific data, forcing their knowledge modeling to adapt to the knowledge-isolated nature of downstream tasks. In this paper, we propose Knowledge-aware retrieval augmented protein language model (Kara), achieving the first task-oriented and explicit integration of PKGs and protein language models. With a knowledge retriever learning to predict linkages between PKG and task proteins, Kara unifies the knowledge integration of the pre-training and fine-tuning stages with a structure-based regularization, mitigating catastrophic forgetting. To ensure task-oriented integration, Kara uses contextualized virtual tokens to extract graph context as task-specific knowledge for new proteins. Experiments show that Kara outperforms existing knowledge-enhanced models in 6 representative tasks, achieving on average 5.1% improvements.","This paper introduces Kara, a new method for understanding proteins using artificial intelligence. Kara works by combining two things: language models and knowledge graphs. The problem is that existing methods do not do a good job of connecting these two things. Kara fixes this by using a special ""knowledge retriever"" that can find the right information from the knowledge graph and add it to the language model. This helps the model understand proteins better, especially their functions and how they interact with other proteins."
Poster,Retrieval-Augmented Perception: High-resolution Image Perception Meets Visual RAG,https://ICML.cc//virtual/2025/poster/44979,"Wenbin Wang, Yongcheng Jing, Liang Ding, Yingjie Wang, Li Shen, Yong Luo, Bo Du, Dacheng Tao","High-resolution (HR) image perception remains a key challenge in multimodal large language models (MLLMs).  To drive progress beyond the limits of heuristic methods, this paper advances HR perception capabilities of MLLMs by harnessing cutting-edge long-context techniques such as retrieval-augmented generation (RAG). Towards this end, this paper presents the first study exploring the use of RAG to address HR perception challenges. Specifically, we propose Retrieval-Augmented Perception (RAP), a training-free framework that retrieves and fuses relevant image crops while preserving spatial context using the proposed Spatial-Awareness Layout. To accommodate different tasks, the proposed Retrieved-Exploration Search (RE-Search) dynamically selects the optimal number of crops based on model confidence and retrieval scores. Experimental results on HR benchmarks demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving a 43\% improvement on $V^*$ Bench and 19\% on HR-Bench. Code is available at https://github.com/DreamMr/RAP.","Understanding high-resolution (HR) images is still a big challenge for Multimodal Large Language Models (MLLMs) that work with both text and images. This paper introduces a new approach to help these models better understand HR images by using advanced methods designed for handling long and complex information. We propose a method called Retrieval-Augmented Perception (RAP). Instead of looking at the whole large image at once, RAP smartly breaks the image into smaller parts (called crops) and picks the most relevant ones. It then puts them together in a way that keeps the image’s structure and context. Importantly, this method doesn’t require extra training. We also introduce RE-Search that decides how many image parts to use, depending on how confident the model is and how useful each part seems. In tests on high-resolution image tasks, our method worked well. For example, one model improved by 43% on a tough benchmark and 19% on another."
Poster,Retrieval Augmented Time Series Forecasting,https://ICML.cc//virtual/2025/poster/45826,"Sungwon Han, Seungeon Lee, MEEYOUNG CHA, Sercan Arik, Jinsung Yoon","Time series forecasting uses historical data to predict future trends, leveraging the relationships between past observations and available features. In this paper, we propose RAFT, a retrieval-augmented time series forecasting method to provide sufficient inductive biases and complement the model's learning capacity. When forecasting the subsequent time frames, we directly retrieve historical data candidates from the training dataset with patterns most similar to the input, and utilize the future values of these candidates alongside the inputs to obtain predictions. This simple approach augments the model's capacity by externally providing information about past patterns via retrieval modules. Our empirical evaluations on ten benchmark datasets show that RAFT consistently outperforms contemporary baselines with an average win ratio of 86%.","Time series forecasting is the task of predicting future trends based on historical data. Traditionally, this involves training models to learn patterns from past observations. In our work, we explore a different approach: instead of just learning these patterns, what if we directly retrieve similar patterns that have occurred in the past for prediction?To do this, we search for patches of past data that are similar to the current input, then retrieve the future values from these patches to help make predictions. This retrieval-based method can capture patterns that lack strong temporal correlations or do not share common characteristics with other patterns, thereby reducing the learning burden and enhancing generalizability.Our approach introduces a novel way of combining the concept of retrieval with time series forecasting, which can benefit applications like weather forecasting or financial analysis. To support further research in this direction, we’ve made our code publicly available."
Poster,Retrieval Augmented Zero-Shot Enzyme Generation for Specified Substrate,https://ICML.cc//virtual/2025/poster/45546,"Jiahe Du, Kaixiong Zhou, Xinyu Hong, Zhaozhuo Xu, Jinbo Xu, Xiao Huang","Generating novel enzymes for target molecules in zero-shot scenarios is a fundamental challenge in biomaterial synthesis and chemical production. Without known enzymes for a target molecule, training generative models becomes difficult due to the lack of direct supervision. To address this, we propose a retrieval-augmented generation method that uses existing enzyme-substrate data to guide enzyme design. Our method retrieves enzymes with substrates that share structural similarities with the target molecule, leveraging functional similarities in catalytic activity. Since none of the retrieved enzymes directly catalyze the target molecule, we use a conditioned discrete diffusion model to generate new enzymes based on the retrieved examples. An enzyme-substrate relationship classifier guides the generation process to ensure optimal protein sequence distributions. We evaluate our model on enzyme design tasks with diverse real-world substrates and show that it outperforms existing protein generation methods in catalytic capability, foldability, and docking accuracy. Additionally, we define the zero-shot substrate-specified enzyme generation task and introduce a dataset with evaluation benchmarks.","Designing enzymes for new molecules is essential for advancing green chemistry, medicine, and sustainable materials. But creating enzymes from scratch—especially when none are known to work on a target molecule—is extremely difficult. Most AI models need examples of success to learn from, and in this case, there are none. Our approach tackles this by retrieving enzymes that work on molecules similar to the target, using them as inspiration for design. We then use a powerful AI model to generate enzymes tailored to the target, guided by a system that checks whether the enzyme is likely to work. This ensures the enzymes we create aren’t just random sequences—they’re functional and realistic. We tested our method on many real-world molecules, and it consistently outperformed existing techniques in creating useful, stable enzymes. Our work opens the door to designing enzymes for entirely new molecules—without needing prior examples—and could help speed up innovation in drug discovery and biomanufacturing. We also introduce new benchmarks to measure progress in this challenge going forward."
Poster,Return Capping: Sample Efficient CVaR Policy Gradient Optimisation,https://ICML.cc//virtual/2025/poster/44577,"Harry Mead, Clarissa Costen, Bruno Lacerda, Nick Hawes","When optimising for conditional value at risk (CVaR) using policy gradients (PG), current methods rely on discarding a large proportion of trajectories, resulting in poor sample efficiency.  We propose a reformulation of the CVaR optimisation problem by capping the total return of trajectories used in training, rather than simply discarding them, and show that this is equivalent to the original problem if the cap is set appropriately. We show, with empirical results in an number of environments, that this reformulation of the problem results in consistently improved performance compared to baselines. We have made all our code available here: \url{https://github.com/HarryMJMead/cvar-return-capping}.","When training agents to do certain tasks, such as finding the optimal strategy to a game, generally the agent is trained to maximise its average performance. However, in our work we instead focus on training agents that perform well even in the worst case scenarios. When training an agent to play a game, reinforcement learning involves repeatedly playing the game and using these repeated plays - often referred to as rollouts - to determine which actions result in the best outcomes. Current methods for maximising agent performance in the worst case rely on discarding the majority of these rollouts and only learning from the worst performing rollouts. We found that, instead of discarding these rollouts, by capping their performance - effectively treating them as performing worse than they actually did - and then learning from these capped rollouts, agents were able to learn better and more efficiently how to perform well in the worst case."
Poster,Return of the Latent Space COWBOYS: Re-thinking the use of VAEs for Bayesian Optimisation of Structured Spaces,https://ICML.cc//virtual/2025/poster/45143,"Henry Moss, Sebastian Ober, Tom Diethe","Bayesian optimisation in the latent space of a VAE is a powerful framework for optimisation tasks over complex structured domains, such as the space of valid molecules. However, existing approaches tightly couple the surrogate and generative models, which can lead to suboptimal performance when the latent space is not tailored to specific tasks, which in turn has led to the proposal of increasingly sophisticated algorithms. In this work, we explore a new direction, instead proposing a decoupled approach that trains a generative model and a GP surrogate separately, then combines them via a simple yet principled Bayesian update rule. This separation allows each component to focus on its strengths— structure generation from the VAE and predictive modelling by the GP. We show that our decoupled approach improves our ability to identify high-potential candidates in molecular optimisation problems under constrained evaluation budgets.","The rise of Generative AI, capable of producing novel images, molecules, and engineered structures, holds the ability to fundamentally redefine how experiments are conceived, conducted, and iterated. Trained on historical experiment logs or libraries of valid designs (e.g. synthesisable molecules), these models distil the scientific community's tacit knowledge and permit generation of a rich, diverse set of informative designs on demand. Yet, while generative models have become routine in image synthesis, data augmentation, and creative content, we still lack a rigorous approach to use generative models to aid scientists and innovators in running their experimental programmes. In this work we propose one such approach."
Poster,Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks,https://ICML.cc//virtual/2025/poster/44537,"Yixin Cheng, Hongcheng Guo, Yangming Li, Leonid Sigal","Text watermarking aims to subtly embeds statistical signals into text by controlling the Large Language Model (LLM)'s sampling process, enabling watermark detectors to verify that the output was generated by the specified model. The robustness of these watermarking algorithms has become a key factor in evaluating their effectiveness. Current text watermarking algorithms embed watermarks in high-entropy tokens to ensure text quality. In this paper, we reveal that this seemingly benign design can be exploited by attackers, posing a significant risk to the robustness of the watermark. We introduce a generic efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA), which leverages the vulnerability by calculating the self-information of each token to identify potential pattern tokens and perform targeted attack. Our work exposes a widely prevalent vulnerability in current watermarking algorithms. The experimental results show SIRA achieves nearly 100\% attack success rates on seven recent watermarking methods with only \$0.88 per million tokens cost. Our approach does not require any access to the watermark algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the attack model even mobile-level models. Our findings highlight the urgent need for more robust watermarking.","The rapid advancement of Large Language Models (LLMs) has brought concerns about their potential misuse, such as spreading misinformation and threatening academic integrity. To address this, text watermarking has emerged as a promising solution, subtly embedding undetectable patterns into LLM-generated text to verify its origin. However, the effectiveness of these watermarks depends on their robustness against attacks that try to remove them. Existing attack methods are often inefficient, untargeted, resource-intensive, and not easily transferable across different LLMs.Our research introduces the Self-Information Rewrite Attack (SIRA), a novel and efficient paraphrasing attack that reveals a fundamental vulnerability in current text watermarking algorithms. We discovered that watermarking techniques embed patterns in ""high-entropy"" tokens—tokens with high self-information due to their unpredictability and low probability. SIRA exploits this by calculating the self-information of each token to identify and mask these potential watermark-carrying tokens. We then use an LLM to perform a targeted ""fill-in-the-blank"" task, rewriting the masked text while preserving its semantic integrity.SIRA represents a significant step forward in understanding and evaluating the robustness of LLM watermarking. Our experiments show that SIRA achieves nearly 100% attack success rates across seven recent watermarking methods, at a very low cost of $0.88 per million tokens. This attack doesn't require any prior knowledge of the watermark algorithm or the LLM used, and it's highly transferable, even working with smaller, mobile-level models. By exposing this widespread vulnerability, our work highlights the urgent need for developing more robust and adaptive watermarking approaches to ensure transparency and integrity in AI-generated content."
Poster,ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks,https://ICML.cc//virtual/2025/poster/43640,"Yufei Guo, Yuhan Zhang, Zhou Jie, Xiaode Liu, Xin Tong, Yuanpei Chen, Weihang Peng, Zhe Ma","The Spiking Neural Network (SNN), a biologically inspired neural network infrastructure, has garnered significant attention recently. SNNs utilize binary spike activations for efficient information transmission, replacing multiplications with additions, thereby enhancing energy efficiency. However, binary spike activation maps often fail to capture sufficient data information, resulting in reduced accuracy.To address this challenge, we advocate reversing the bit of the weight and activation, called \textbf{ReverB}, inspired by recent findings that highlight greater accuracy degradation from quantizing activations compared to weights. Specifically, our method employs real-valued spike activations alongside binary weights in SNNs. This preserves the event-driven and multiplication-free advantages of standard SNNs while enhancing the information capacity of activations.Additionally, we introduce a trainable factor within binary weights to adaptively learn suitable weight amplitudes during training, thereby increasing network capacity. To maintain efficiency akin to vanilla \textbf{ReverB}, our trainable binary weight SNNs are converted back to standard form using a re-parameterization technique during inference.Extensive experiments across various network architectures and datasets, both static and dynamic, demonstrate that our approach consistently outperforms state-of-the-art methods.","This study has introduced \textbf{ReverB-SNN}, a novel approach for enhancing SNNs by integrating real-valued spike activations with binary weights. Our method addresses the challenge of reduced accuracy in SNNs due to limited information capture by binary spike activation maps. By reversing the bit of both weights and activations, we have preserved the energy-efficient and multiplication-free characteristics of traditional SNNs while significantly boosting the information capacity of activations.Moreover, the introduction of a trainable factor within binary weights has enabled adaptive learning of weight amplitudes during training, thereby enhancing the overall network capacity. Importantly, to ensure operational efficiency comparable to standard SNNs, we proposed a re-parameterization technique that converts trainable binary weight SNNs back to standard form during inference.Extensive experimental validation across diverse network architectures and datasets, encompassing both static and dynamic scenarios, consistently demonstrates the superiority of our approach over existing state-of-the-art methods."
Poster,ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification,https://ICML.cc//virtual/2025/poster/44699,"Hyunseok Lee, Seunghyuk Oh, Jaehyung Kim, Jinwoo Shin, Jihoon Tack","Self-awareness, i.e., the ability to assess and correct one's generation, is a fundamental aspect of human intelligence, making its replication in large language models (LLMs) an important yet challenging task. Previous works tackle this by employing extensive reinforcement learning or relying on large external verifiers. In this work, we propose Refine via Intrinsic Self-Verification (ReVISE), an efficient and effective framework that enables LLMs to self-correct their outputs through self-verification. The core idea of ReVISE is to enable LLMs to verify their reasoning processes and continually rethink reasoning trajectories based on its verification. To implement this efficiently, we introduce a structured curriculum based on preference learning. Specifically, as ReVISE involves two challenging tasks (i.e., self-verification and reasoning correction), we tackle each task sequentially using curriculum learning, collecting both failed and successful reasoning paths to construct preference pairs for efficient training. During inference, our approach enjoys natural test-time scaling by integrating self-verification and correction capabilities, further enhanced by our proposed confidence-aware decoding mechanism. Our experiments on various reasoning tasks demonstrate that ReVISE achieves efficient self-correction and significantly improves the reasoning performance of LLMs.","Large language models (LLMs) can produce impressive answers, but they sometimes make small mistakes in their reasoning, which can affect the final result. This paper introduces ReVISE, a method that allows LLMs to check and improve their own reasoning during inference without external feedback or more supervision.ReVISE trains the model to spot when its reasoning might be unreliable and decide whether to keep going or change its answer. It uses a two-stage training process and decoding that is aware of confidence levels to guide these decisions.This method helps people do better at tasks that need you to think step-by-step, like maths and logic problems. It also makes LLMs more like self-correcting and trustworthy AI systems."
Poster,Revisiting Chain-of-Thought in Code Generation: Do Language Models Need to Learn Reasoning before Coding?,https://ICML.cc//virtual/2025/poster/43621,"Ren-Biao Liu, Anqi Li, ChaodingYang, Hui Sun, Ming Li","Large Language Models (LLMs) have demonstrated exceptional performance in code generation, becoming increasingly vital for software engineering and development. Recently, Chain-of-Thought (CoT) has proven effective for complex tasks by prompting LLMs to reason step-by-step and provide a final answer.However, research on *how LLMs learn to reason with CoT data for code generation* remains limited.In this work, we revisit classic CoT training, which typically learns reasoning steps before the final answer.We synthesize a dataset to separate the CoT process from code solutions and then conduct extensive experiments to study how CoT works in code generation empirically.We observe counterintuitive phenomena, suggesting that the traditional training paradigm may not yield benefits for code generation. Instead, training LLMs to generate code first and then output the CoT to explain reasoning steps for code generation is more effective.Specifically, our results indicate that a 9.86% relative performance improvement can be achieved simply by changing the order between CoT and code. Our findings provide valuable insights into leveraging CoT to enhance the reasoning capabilities of CodeLLMs and improve code generation.","Large language models (LLMs), such as ChatGPT, are increasingly used to assist programmers in generating code. One promising technique to improve their performance is Chain-of-Thought (CoT), where the model is guided to explain its reasoning step by step before providing an answer. This approach has worked well for solving complex logic problems, but does it help when the goal is to write code?In our study, we explore how CoT reasoning influences code generation. We carefully designed new training data to separate the reasoning process from the final code output, allowing us to study how models learn to reason and code. Surprisingly, we found that the traditional approach—reasoning first, then generating code—does not significantly improve code quality. Reversing the order works better: if the model writes the code first and then explains its reasoning, performance improves significantly.This insight challenges common assumptions about how to teach models to think and program and opens up new ways to make AI coding assistants more reliable and effective."
