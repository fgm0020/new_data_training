type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Position: Medical Large Language Model Benchmarks Should Prioritize Construct Validity,https://ICML.cc//virtual/2025/poster/40129,"Ahmed Alaa, Thomas Hartvigsen, Niloufar Golchini, Shiladitya Dutta, Frances Dean, Inioluwa Raji, Travis Zack","Medical large language models (LLMs) research often makes bold claims, from encoding clinical knowledge to reasoning like a physician. These claims are usually backed by evaluation on competitive benchmarks—a tradition inherited from mainstream machine learning. But how do we separate real progress from a leaderboard flex? Medical LLM benchmarks, much like those in other fields, are arbitrarily constructed using medical licensing exam questions. For these benchmarks to truly measure progress, they must accurately capture the real-world tasks they aim to represent. In this position paper, we argue that medical LLM benchmarks should—and indeed can—be empirically evaluated for their construct validity. In the psychological testing literature, “construct validity” refers to the ability of a test to measure an underlying “construct”, that is the actual conceptual target of evaluation. By drawing an analogy between LLM benchmarks and psychological tests, we explain how frameworks from this field can provide empirical foundations for validating benchmarks. To put these ideas into practice, we use real-world clinical data in proof-of-concept experiments to evaluate popular medical LLM benchmarks and report significant gaps in their construct validity. Finally, we outline a vision for a new ecosystem of medical LLM evaluation centered around the creation of valid benchmarks.","Many studies claim that medical large language models (LLMs) are highly capable---often based on how well they do on multiple-choice, exam-style tests. In this paper, we argue that those test scores don't truly reflect the messy, complicated reality of taking care of real patients in real hospitals. This problem isn't limited to just medical LLMs. In general, we tend to treat these advanced models as if they're intelligent ""agents"" that can manifest some latent ""capabilities"" in open-ended tasks. Yet, we still test them the same way we test simpler and narrower models, such as image classifiers. We draw parallels between the ""capabilities"" of LLMs and psychological traits such as intelligence---both are latent and complex constructs that cannot be directly observed but manifests in multifaceted ways through the ability to conduct certain tasks. Based on this analogy, we suggest borrowing a concept from psychology known as ""construct validity""---the idea that a test should actually measure the skill it claims to---as a foundational principle to evaluate and design benchmarks for LLMs. We applied empirical tools for evaluating construct validity inspired by the psychometrics literature to medical LLM benchmarks, and found that even models with top scores on popular benchmarks often didn’t do well when working with real patient records. We propose a vision for a ""benchmark-validation-first"" culture for model evaluation, where make sure the construct validity of benchmarks are evaluated using real hospital data before using them to judge model quality. That way, we can evaluate medical LLMs based on what actually matters in clinical care—not just how well it answers test questions."
Poster,Position: Not All Explanations for Deep Learning Phenomena Are Equally Valuable,https://ICML.cc//virtual/2025/poster/40123,"Alan Jeffares, Mihaela van der Schaar","Developing a better understanding of surprising or counterintuitive phenomena has constituted a significant portion of deep learning research in recent years. These include double descent, grokking, and the lottery ticket hypothesis -- among many others. Works in this area often develop *ad hoc hypotheses* attempting to explain these observed phenomena on an isolated, case-by-case basis. This position paper asserts that, in many prominent cases, there is little evidence to suggest that these phenomena appear in real-world applications and these efforts may be inefficient in driving progress in the broader field. Consequently, we argue against viewing them as isolated puzzles that require bespoke resolutions or explanations. However, despite this, we suggest that deep learning phenomena *do* still offer research value by providing unique settings in which we can refine our *broad explanatory theories* of more general deep learning principles. This position is reinforced by analyzing the research outcomes of several prominent examples of these phenomena from the recent literature.  We revisit the current norms in the research community in approaching these problems and propose practical recommendations for future research, aiming to ensure that progress on deep learning phenomena is well aligned with the ultimate pragmatic goal of progress in the broader field of deep learning.","In this paper, we examine the methodology being used to study a particular sub-area of deep learning research that focuses on so-called *deep learning phenomena*. This area addresses interesting and unusual behavior observed in neural networks that can be isolated and analyzed.  Although these phenomena (such as double descent, grokking, and the lottery ticket hypothesis) are heavily studied, we argue that many of them are unlikely to occur in practical applications. As such, treating them as puzzles to be solved on their own may not be the most productive research strategy. Instead, we suggest that their main value lies in how they can help us test and refine broader theories about how deep learning works. We provide examples of how this approach has led to useful insights and propose practical recommendations for making research in this area more aligned with the goals of the wider field."
Poster,Position: Political Neutrality in AI Is Impossible — But Here Is How to Approximate It,https://ICML.cc//virtual/2025/poster/40157,"Jillian Fisher, Ruth Elisabeth Appel, Chan Young Park, Yujin Potter, Liwei Jiang, Taylor Sorensen, Shangbin Feng, Yulia Tsvetkov, Margaret Roberts, Jennifer Pan, Dawn Song, Yejin Choi","AI systems often exhibit political bias, influencing users' opinions and decisions. While political neutrality—defined as the absence of bias—is often seen as an ideal solution for fairness and safety, this position paper argues that true political neutrality is neither feasible nor universally desirable due to its subjective nature and the biases inherent in AI training data, algorithms, and user interactions. However, inspired by Joseph Raz's philosophical insight that ""neutrality [...] can be a matter of degree"" (Raz, 1986), we argue that striving for some neutrality remains essential for promoting balanced AI interactions and mitigating user manipulation. Therefore, we use the term ""approximation"" of political neutrality to shift the focus from unattainable absolutes to achievable, practical proxies. We propose eight techniques for approximating neutrality across three levels of conceptualizing AI, examining their trade-offs and implementation strategies. In addition, we explore two concrete applications of these approximations to illustrate their practicality. Finally, we assess our framework on current large language models (LLMs) at the output level, providing a demonstration of how it can be evaluated. This work seeks to advance nuanced discussions of political neutrality in AI and promote the development of responsible, aligned language models.","AI systems often exhibit political bias, subtly shaping users’ beliefs and decisions. A common response is to call for political neutrality—but true neutrality is difficult to define and even harder to achieve, given the biases embedded in training data, algorithms, and user interactions. Our research tackles this challenge by reframing neutrality as something that can be *approximated*, rather than perfectly attained.Building on philosophical insights that neutrality exists in degrees, we propose a practical framework for approximating political neutrality in AI. We outline eight techniques across three levels of abstraction: output (the model’s responses), system (the model itself), and ecosystem (the broader landscape of AI models). Because each technique is an approximation, we also examine their trade-offs and limitations.We apply this framework to two real-world use cases and show how it can be used to evaluate large language models at the output level. This allows researchers and developers to better understand where models fall short and how they might be improved. By moving away from unattainable ideals and toward actionable strategies, our work supports the development of more responsible, trustworthy AI systems that are less likely to manipulate or polarize users."
Poster,Position: Principles of Animal Cognition to Improve LLM Evaluations,https://ICML.cc//virtual/2025/poster/40117,"Sunayana Rane, Cyrus Kirkman, Graham Todd, Amanda Royka, Ryan Law, Erica Cartmill, Jacob Foster","It has become increasingly challenging to understand and evaluate LLM capabilities as these models exhibit a broader range of behaviors. In this position paper, we argue that LLM researchers should draw on the lessons from another field which has developed a rich set of experimental paradigms and design practices for probing the behavior of complex intelligent systems: animal cognition. We present five core principles of evaluation drawn from animal cognition research, and explain how they provide invaluable guidance for understanding LLM capabilities and behavior. We ground these principles in an empirical case study, and show how they can already provide a richer picture of one particular reasoning capability: transitive inference.","As LLM exhibit a broader range of behaviors, we may often wonder what they truly understand and what they don't. In this position paper, we argue that LLM researchers should draw on the lessons from another field which has developed a rich set of lessons for probing a wide variety of intelligent behavior: animal cognition. We present five core principles of evaluation drawn from animal cognition research, and explain how they provide invaluable guidance for understanding LLM capabilities and behavior. We ground these principles in an empirical case study, and show how they can already provide a richer picture of one particular reasoning capability: transitive inference."
Poster,Position: Probabilistic Modelling is Sufficient for Causal Inference,https://ICML.cc//virtual/2025/poster/40134,"Bruno Mlodozeniec, David Krueger, Richard E Turner","Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we make it clear that you can answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. We argue for the advantages of the generality of the probabilistic modelling lens, when compared to bespoke causal frameworks. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.","Science and machine learning often deal with questions of cause and effect. For example, does a drug actually cure a disease, or do healthier people just tend to take it? A common belief is that to answer these “causal” questions, you need a specialised mathematical language, separate from standard statistics.This paper illustrates that you don’t. We show that the familiar tools of probability are all you need. The key is to use a single statistical model for both the world we observe but also the hypothetical worlds we want to ask questions about. By explicitly specifying how they relate to one another, we can make inferences about events in the hypothetical worlds using data from the observed world only.Imagine that you have data showing that people who take more aspirin often get worse headaches. To find out if aspirin is actually helpful, or if the correlation between positive effects and higher aspirin doses spurious, we can add a hypothetical ""intervened world"" to our model—one where we decide the aspirin dose for everyone. By connecting these worlds through shared assumptions (like how the human body works), we can use real-world data to see what would happen in our hypothetical one.This approach simplifies answering causal questions, framing them as standard problems in probability. It shows that the prevalent specialised causal tools are just convenient shortcuts within this broader framework, making causal reasoning accessible to broader range of machine learning researchers."
Poster,Position: Rethinking Explainable Machine Learning as Applied Statistics,https://ICML.cc//virtual/2025/poster/40127,"Sebastian Bordt, Eric Raidl, Ulrike Luxburg","In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. In this position paper, we argue for a novel and pragmatic perspective: Explainable machine learning needs to recognize its parallels with applied statistics. Concretely, explanations are statistics of high-dimensional functions, and we should think about them analogously to traditional statistical quantities. Among others, this implies that we must think carefully about the matter of interpretation, or how the explanations relate to intuitive questions that humans have about the world. The fact that this is scarcely being discussed in research papers is one of the main drawbacks of the current literature. Moving forward, the analogy between explainable machine learning and applied statistics provides a fruitful way for how research practices can be improved.","Machine learning models increasingly influence important decisions, from loan approvals to medical diagnoses. This has led to growing interest in explainable machine learning — methods that aim to make model behavior transparent. However, after years of research, it still remains unclear what exactly it means to ""explain"" a model.We argue that the solution is surprisingly simple: explainable machine learning is statistics by another name. Just as statistics provides tools to analyze large datasets, explainable machine learning provides tools to analyze large models. While this sounds simple in hindsight, it's actually a fundamental shift in perspective.By treating model explanations as statistics, we can apply basic lessons from statistical practice: always specify what your tool measures, acknowledge its limitations, and ensure users understand how to interpret it. It is important to get this right because explainable machine learning is not only used in research but also in applications and policy contexts."
Poster,Position: Rethinking LLM Bias Probing Using Lessons from the Social Sciences,https://ICML.cc//virtual/2025/poster/40102,"Kirsten Morehouse, Siddharth Swaroop, Weiwei Pan","The proliferation of LLM bias probes introduces three challenges: we lack (1) principled criteria for selecting appropriate probes, (2) a system for reconciling conflicting results across probes, and (3) formal frameworks for reasoning about when and why experimental findings will generalize to real user behavior. In response, we propose a systematic approach to LLM social bias probing, drawing on insights from the social sciences. Central to this approach is EcoLevels—a novel framework that helps (a) identify appropriate bias probes (b) reconcile conflicting results, and (c) generate predictions about bias generalization. We ground our framework in the social sciences, as many LLM probes are adapted from human studies, and these fields have faced similar challenges when studying bias in humans. Finally, we outline five lessons that demonstrate how LLM bias probing can (and should) benefit from decades of social science research","Given that millions of people use Large Language Models (LLMs) each day, researchers have developed tools to understand whether the behaviors of these AI models reflect social biases. However, the present paper argues that the sheer number of tools (""bias probes"") has created three problems: (1) there is no clear guidance for picking what kind of probe to use when testing a model for bias, (2) different probes often give conflicting results, and (3) it's hard to know how these results apply to real-world use. To fix this, the authors propose a new framework called “EcoLevels,,” which borrows ideas from social science research. This approach helps researchers choose better tools for testing for bias, make sense of mixed results, and predict when a model’s bias might actually affect people. The goal is to bring more structure and theory to the way we study bias in AI systems."
Poster,Position: Retrieval-augmented systems can be dangerous medical communicators,https://ICML.cc//virtual/2025/poster/40149,"Lionel Wong, Ayman Ali, Raymond M Xiong, Shannon Shen, Yoon Kim, Monica Agrawal","Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. However, we argue that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations---such as the incorporation of communication pragmatics and enhanced comprehension of source documents---that could help mitigate these issues and extend beyond the medical domain.","For decades now, patients have looked online for health information; increasingly, they are using generative AI to answer health queries, e.g. using chatbots or AI-powered search results. Given the importance of providing accurate medical information, large language model developers often incorporate retrieval-augmented generation (RAG), which allows the AI’s response to draw from and cite trusted websites, decreasing the chance the model makes up information. However, in this paper, we argue that RAG can lead to failure modes where responses are misleading, even when they are accurate. While each individual sentence may be factual, the generated response can take information out of context, omit important relevant information, and reinforce patient misconceptions. To substantiate our concern about these failure modes, we provide quantitative and qualitative evidence from current real-world systems on realistic patient searches. We propose a series of recommendations—from changing the underlying algorithms to changing how responses are displayed—that could help solve these issues."
Poster,Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives,https://ICML.cc//virtual/2025/poster/40150,"Elliot Meyerson, Xin Qiu","Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With the high cost of running LLMs at scale, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such problem decompositions, and that insights from such analysis will unlock opportunities for scaling such systems. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.","It has become popular to use large language models to build ""agents"". Each agent is a program with a specific role to play in the world. To solve complex problems, it is natural to build a system with multiple agents, each assigned a complementary role. To date, such agent-based systems have been organized in an intuitive or ad-hoc manner, for example, assigning roles to agents in the same way we would assign jobs to humans in an organization. How close are such agent organizations to optimal? It turns out they are not close at all!This paper proposes a way to analyze the scalability of such systems, grounded in the classical computer science tool of algorithm analysis. Through a series of examples, we use this approach to show that an intuitive agent-based system can be drastically improved by more carefully considering the algorithmic structure of the problem. This analysis tool will thus help people build agent-based systems that are far more efficient and scalable, and the organization of such future systems will not be humanlike, but uncompromisingly machinelike."
Poster,Position: Societal Impacts Research Requires Benchmarks for Creative Composition Tasks,https://ICML.cc//virtual/2025/poster/40162,Judy Hanwen Shen,"Foundation models that are capable of automating cognitive tasks represent a pivotal technological shift, yet their societal implications remain unclear. These systems promise exciting advances, yet they also risk flooding our information ecosystem with formulaic, homogeneous, and potentially misleading synthetic content. Developing benchmarks grounded in real use cases where these risks are most significant is therefore critical. Through a thematic analysis using 2 million language model user prompts, we identify *creative composition tasks* as a prevalent usage category where users seek help with personal tasks that require everyday creativity. Our fine-grained analysis identifies mismatches between current benchmarks and usage patterns among these tasks. Crucially, we argue that the same use cases that currently lack thorough evaluations can lead to negative downstream impacts. This position paper argues that benchmarks focused on creative composition tasks is a necessary step towards understanding the societal harms of AI-generated content. We call for greater transparency in usage patterns to inform the development of new benchmarks that can effectively measure both the progress and the impacts of models with creative capabilities.","While AI systems are becoming widespread, we lack proper ways to evaluate their potential societal harms, particularly the risk of flooding our information ecosystem with generic or misleading content. We analyzed 2 million real user requests to understand actual usage patterns and discovered that open-ended tasks requiring everyday creativity, like writing emails or social media posts, are among the most common uses. We introduce the term: ""creative composition tasks"" to describe this broad category of tasks. We then found significant mismatches between these real-world applications and existing AI benchmarks. We identified a critical blind spot: the creative tasks people use AI for most are exactly those that could cause significant societal harm through homogeneous content, bad advice, and unintended communication between people, yet they're not well-evaluated. Our research demonstrates the urgent need for new benchmarks focused on creative composition to properly measure both AI progress and its potential negative impacts. Without addressing this gap, we're unable to understand or mitigate the risks as these powerful systems become more prevalent in society."
