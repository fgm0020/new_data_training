type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction,https://ICML.cc//virtual/2025/poster/46439,"Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Irwin King, Liang Chen, Peilin Zhao","Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm—Next-Token-Pair Prediction (NTPP)—to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications.  Demo and code can be found at https://audio-3059.pages.dev.","Intelligent spoken dialogue systems play a crucial role in many real-world applications, particularly in human-machine interactions. However, enabling voice assistants to generate natural, coherent, and fluid responses remains a significant challenge. In this work, we develop a real-time voice interaction system by learning from human spoken dialogues through a novel approach. This method facilitates the generation of natural and seamless responses, effectively capturing the nuanced characteristics of human dialogue in a data-driven and efficient manner."
Poster,Objective drives the consistency of representational similarity across datasets,https://ICML.cc//virtual/2025/poster/43665,"Laure Ciernik, Lorenz Linhardt, Marco Morik, Jonas Dippel, Simon Kornblith, Lukas Muttenthaler","The Platonic Representation Hypothesis claims that recent foundation models are converging to a shared representation space as a function of their downstream task performance, irrespective of the objectives and data modalities used to train these models (Huh et al., 2024). Representational similarity is generally measured for individual datasets and is not necessarily consistent across datasets. Thus, one may wonder whether this convergence of model representations is confounded by the datasets commonly used in machine learning. Here, we propose a systematic way to measure how representational similarity between models varies with the set of stimuli used to construct the representations. We find that the objective function is a crucial factor in determining the consistency of representational similarities across datasets. Specifically, self-supervised vision models learn representations whose relative pairwise similarities generalize better from one dataset to another compared to those of image classification or image-text models. Moreover, the correspondence between representational similarities and the models' task behavior is dataset-dependent, being most strongly pronounced for single-domain datasets. Our work provides a framework for analyzing similarities of model representations across datasets and linking those similarities to differences in task behavior.","Many computer vision models (programs that analyze images) process visual information similarly: they convert images into compact numerical descriptions called representations. These representations tend to be organized similarly across different models. We ask whether this similarity reveals a shared understanding of the visual world or reflects the specific image collections (evaluation datasets) used for measuring similarity.We test how consistent model similarities are when the evaluation dataset changes. Using 64 vision models, we look at how their representations change across various types of images, from everyday objects to specialized domains. We find that similarities are fairly consistent across many datasets for models trained using self-supervised learning (which learn visual patterns without additional information about the image content). However, models trained with paired text descriptions or object categorization tasks exhibited similarities that varied more depending on the evaluation dataset. To help researchers better understand which models perceive the world similarly, we introduce a structured way of comparing how model representation similarity changes across datasets. This matters because models with different similarities across datasets likely do not have a shared understanding of the world. Our results show that the stability of similarities between models depends on the training task."
Poster,Observation Interference in Partially Observable Assistance Games,https://ICML.cc//virtual/2025/poster/43874,"Scott Emmons, Caspar Oesterheld, Vincent Conitzer, Stuart Russell","We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering _actions_, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of perfect information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire _policies_. This can be viewed as an extension of the classic result that the value of perfect information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions.","When would an AI assistant interfere with a human’s observations? It’s clear that a misaligned assistant might interfere with observations in order to deceive. But what about a perfectly aligned assistant?We identify three distinct reasons for even a perfectly aligned AI assistant to interfere with human observations. Interfering with observations sometimes helps an assistant communicate its own private observations to humans, query human preferences, and improve the decision making of irrational humans.Our results complicate the picture, suggesting that not all observation interference is inherently bad. Our work is theoretical, laying a foundation for future work to understand and address the nuanced issue of observation interference in practice."
Poster,Occult: Optimizing Collaborative Communications across Experts for Accelerated Parallel MoE Training and Inference,https://ICML.cc//virtual/2025/poster/43659,"Shuqing Luo, Pingzhi Li, Jie Peng, Yang Zhao, Yu Cao, Yu Cheng, Tianlong Chen","Mixture-of-experts (MoE) architectures could achieve impressive computational efficiency with expert parallelism, which relies heavily on all-to-all communication across devices. Unfortunately, such communication overhead typically constitutes a significant portion of the total runtime, hampering the scalability of distributed training and inference for modern MoE models (consuming over 40% runtime in large-scale training). In this paper, we first define $\textit{collaborative communication}$ to illustrate this intrinsic limitation, and then propose system- and algorithm-level innovations to reduce communication costs. Specifically, given a pair of experts co-activated by one token, we call them as $\textit{collaborated}$, which comprises $2$ cases as $\textit{intra-}$ and $\textit{inter-collaboration}$, depending on whether they are kept on the same device. Our pilot investigations reveal that augmenting the proportion of intra-collaboration can accelerate expert parallel at scale. It motivates us to strategically $\underline{\texttt{o}}$ptimize $\underline{\texttt{c}}$ollaborative $\underline{\texttt{c}}$omm$\underline{\texttt{u}}$nication for acce$\underline{\texttt{l}}$era$\underline{\texttt{t}}$ed MoE training and inference, dubbed $\textbf{\texttt{Occult}}$. Our designs are capable of $\underline{either}$ delivering exact results with reduced communication cost, $\underline{or}$ controllably minimizing the cost with collaboration pruning, materialized by modified fine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that $\texttt{Occult}$ can be faster than popular state-of-the-art inference or training frameworks (over 50% speed up across multiple tasks and models) with comparable or superior quality compared to the standard fine-tuning. Codes will be available upon acceptance.","$\textbf{Motivation}$: Training and inference in MoE-based large language models (LLMs) face a critical bottleneck: expert parallelism—a distributed computing strategy—incurs heavy synchronization costs due to frequent ""all-to-all"" communication across devices. This communication alone accounts for a substantial portion of runtime, making its optimization essential for reducing latency and improving system throughput.$\textbf{Key Insight}$: When multiple experts activated by the same token reside on the same device, transmitting redundant copies of that token becomes unnecessary. For example, if two co-activated experts are colocated, only one replica of the token needs communication, halving the data volume versus current frameworks that naively send duplicates. In ideal scenarios where all $\textit{k}$ experts per token are colocated, communication volume drops by $\textbf{(k-1)/k}$ compared to standard top-$\textit{k}$ routing, promising transformative savings.$\textbf{Our Solution}$: We propose an algorithm-system co-design framework dubbed $\texttt{Occult}$ to exploit this insight:1. $\textbf{Expert Placement Optimization}$: A novel algorithm dynamically reschedules expert-device assignments to maximize colocation of co-activated experts.2. $\textbf{Communication-Aware Execution}$: Redesigning all-to-all communication in standard expert parallelism to avoid redundant data transfers while preserving computational accuracy.3. $\textbf{Customized Sparse MatMul Kernels}$: A sparse matmul kernel tailored for the optimized communication strategy.$\textbf{Impact}$: Our proposed $\texttt{Occult}$ significantly reduces synchronization overhead in distributed data center clusters, particularly for environments with constrained bandwidth. It offers immediate benefits to improve efficiency for training and serving MoE-based LLMs, addressing a critical challenge in deploying large-scale MoE-based models."
Poster,Offline Learning for Combinatorial Multi-armed Bandits,https://ICML.cc//virtual/2025/poster/46315,"Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee Joe-Wong, John C. S. Lui, Wei Chen","The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets.To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor.We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.","Machine learning systems often rely on live user interactions to learn—an approach that can be costly, risky, or impractical. Our work introduces **Off-CMAB**, the first method for making *combinatorial* decisions—like selecting a set of items to recommend—*offline*, using only existing data.At its core is a new algorithm, **CLCB**, which selects decisions backed by strong evidence in the data and efficiently handles the complexity of combining multiple actions. We also propose new criteria to assess whether offline data is sufficient for reliable learning and prove that CLCB performs nearly as well as theoretically possible.We demonstrate Off-CMAB on real-world tasks like search ranking, large language model (LLM) caching, and influence maximization. Even when data is incomplete or lacks optimal options, Off-CMAB performs robustly—enabling smarter, safer learning without live experimentation."
Poster,Offline Model-based Optimization for Real-World Molecular Discovery,https://ICML.cc//virtual/2025/poster/44562,"Dong-Hee Shin, Young-Han Son, Hyun Jung Lee, Deok-Joong Lee, Tae-Eui Kam","Molecular discovery has attracted significant attention in scientific fields for its ability to generate novel molecules with desirable properties. Although numerous methods have been developed to tackle this problem, most rely on an online setting that requires repeated online evaluation of candidate molecules using the oracle. However, in real-world molecular discovery, the oracle is often represented by wet-lab experiments, making this online setting impractical due to the significant time and resource demands. To fill this gap, we propose the Molecular Stitching (MolStitch) framework, which utilizes a fixed offline dataset to explore and optimize molecules without the need for repeated oracle evaluations. Specifically, MolStitch leverages existing molecules from the offline dataset to generate novel `stitched molecules' that combine their desirable properties. These stitched molecules are then used as training samples to fine-tune the generative model using preference optimization techniques. Experimental results on various offline multi-objective molecular optimization problems validate the effectiveness of MolStitch. The source code is available online.","Scientists are increasingly using artificial intelligence (AI) to help discover new molecules — such as those that could lead to new medicines or advanced materials. In computer simulations, an AI model can generate and evaluate thousands of molecular candidates in just seconds. But in the real world, evaluating even a single molecule often requires wet lab experiments, where chemists or biologists physically synthesize the molecule and test how well it performs. These experiments are expensive and can take weeks or even months to complete.This slow feedback from wet lab experiments creates a frustrating bottleneck: the AI model suggests promising molecules, then sits idle for weeks or months while chemists conduct physical experiments to test them. During this waiting period, the AI isn't learning or improving, essentially wasting valuable time that could be spent getting better at its job.In our research, we asked: While waiting for new experimental results, can we make better use of the molecular data we already have? Is it possible to keep training the AI model even without immediate feedback from wet lab evaluations?To explore this, we developed MolStitch, an offline framework that allows AI to continue learning during these waiting periods. At the core of MolStitch is a proxy model — a model trained on past experimental data that can compare pairs of molecules and predict which one is more promising. This proxy feedback provides ongoing guidance to the AI, allowing it to keep improving even without new wet lab results.MolStitch also creates new molecules by combining parts of existing molecules — a process we call molecular stitching. These newly generated molecules are then evaluated by the proxy model, which provides valuable feedback to the AI model, teaching it to recognize patterns and propose increasingly better molecules over time.In short, MolStitch makes the real-world molecular discovery process more efficient. Instead of the AI sitting idle during long waiting periods, it is constantly learning and improving. When new experimental results finally arrive from the wet lab, the AI is already much better at its job and can suggest the next batch of promising molecular candidates."
Poster,Offline Opponent Modeling with Truncated Q-driven Instant Policy Refinement,https://ICML.cc//virtual/2025/poster/45592,"Yuheng Jing, Kai Li, Bingyun Liu, Ziwen Zhang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng","Offline Opponent Modeling (OOM) aims to learn an adaptive autonomous agent policy that dynamically adapts to opponents using an offline dataset from multi-agent games. Previous work assumes that the dataset is optimal. However, this assumption is difficult to satisfy in the real world. When the dataset is suboptimal, existing approaches struggle to work. To tackle this issue, we propose a simple and general algorithmic improvement framework, Truncated Q-driven Instant Policy Refinement (TIPR), to handle the suboptimality of OOM algorithms induced by datasets. The TIPR framework is plug-and-play in nature. Compared to original OOM algorithms, it requires only two extra steps: (1) Learn a horizon-truncated in-context action-value function, namely Truncated Q, using the offline dataset. The Truncated Q estimates the expected return within a fixed, truncated horizon and is conditioned on opponent information. (2) Use the learned Truncated Q to instantly decide whether to perform policy refinement and to generate policy after refinement during testing. Theoretically, we analyze the rationale of Truncated Q from the perspective of No Maximization Bias probability. Empirically, we conduct extensive comparison and ablation experiments in four representative competitive environments. TIPR effectively improves various OOM algorithms pretrained with suboptimal datasets.","In competitive multi-agent games, AI agents often learn from past interactions to anticipate and respond to opponents. However, when the available data is imperfect or suboptimal, existing learning approaches can falter. Our research introduces a plug-and-play framework called Truncated Q-driven Instant Policy Refinement (TIPR), which enhances these learning approaches by enabling agents to refine their policies in real-time during testing, even when trained on less-than-ideal data. We demonstrate that TIPR significantly improves agent performance across various competitive scenarios, making AI more adaptable and effective in real-world applications where perfect data is rarely available."
Poster,Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation,https://ICML.cc//virtual/2025/poster/46491,"Xiao Huang, Xu Liu, Enze Zhang, Tong Yu, Shuai Li","Offline-to-online Reinforcement Learning (O2O RL) aims to perform online fine-tuning on an offline pre-trained policy to minimize costly online interactions. Existing work used offline datasets to generate data that conform to the online data distribution for data augmentation. However, generated data still exhibits a gap with the online data, limiting overall performance. To address this, we propose a new data augmentation approach, Classifier-Free Diffusion Generation (CFDG). Without introducing additional classifier training overhead, CFDG leverages classifier-free guidance diffusion to significantly enhance the generation quality of offline and online data with different distributions. Additionally, it employs a reweighting method to enable more generated data to align with the online data, enhancing performance while maintaining the agent's stability.  Experimental results show that CFDG outperforms replaying the two data types or using a standard diffusion model to generate new data. Our method is versatile and can be integrated with existing offline-to-online RL algorithms. By implementing CFDG to popular methods IQL, PEX and APL, we achieve a notable 15\% average improvement in empirical performance on the D4RL benchmark such as MuJoCo and AntMaze.","Teaching AI systems to learn from trial and error can be expensive or dangerous, especially in real-world settings like robotics or healthcare. One solution is to first train these systems using existing data, and then fine-tune them with minimal real-world interaction — a process called offline-to-online reinforcement learning.But there’s a challenge: the simulated data we create from past experience often doesn’t match the conditions of real-world use, which leads to worse performance. We developed a method called Classifier-Free Diffusion Generation (CFDG) to close this gap. It uses a powerful generative AI model to create more realistic training data, and it doesn’t require training any extra classifiers, which keeps things efficient. We also introduced a technique to select only the most useful generated data — the kind that best matches real-world conditions.When we applied CFDG to standard benchmarks, it boosted performance by up to 15%. Because it works well with existing methods, CFDG can help build more effective and reliable AI systems with fewer costly interactions."
Poster,Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation,https://ICML.cc//virtual/2025/poster/46516,"Kosuke Nakanishi, Akihiro Kubo, Yuji Yasui, Shin Ishii","Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods.In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary.The implementation is available at https://github.com/nakanakakosuke/VALT_SAC.","Reinforcement learning (RL) is a machine learning approach where an agent learns to make decisions through trial and error.  However, in real-world settings, the information the agent receives can be noisy or unreliable—for example, due to sensor errors or unexpected changes in the environment.  To make agents more robust in such situations, researchers have traditionally trained them alongside an adversary (or perturbation generator) that introduces challenging inputs.In this work, we show that agents can instead $\textit{imagine}$ such challenging scenarios during training—without explicitly training an adversary.  Based on this idea, we propose a new method that enables agents to learn efficiently from previously collected data, without requiring additional interaction with the environment.  Our approach is grounded in solid theory and demonstrates strong performance in experiments.  This makes RL more practical and reliable for real-world applications."
Poster,Off-Policy Evaluation under Nonignorable Missing Data,https://ICML.cc//virtual/2025/poster/45206,"Han Wang, Yang Xu, Wenbin Lu, Rui Song","Off-Policy Evaluation (OPE) aims to estimate the value of a target policy using offline data collected from potentially different policies. In real-world applications, however, logged data often suffers from missingness. While OPE has been extensively studied in the literature, a theoretical understanding of how missing data affects OPE results remains unclear. In this paper, we investigate OPE in the presence of monotone missingness and theoretically demonstrate that the value estimates remain unbiased under ignorable missingness but can be biased under nonignorable (informative) missingness. To retain the consistency of value estimation, we propose an inverse probability weighting value estimator and conduct statistical inference to quantify the uncertainty of the estimates. Through a series of numerical experiments, we empirically demonstrate that our proposed estimator yields a more reliable value inference under missing data.","How can we evaluate the effectiveness of a treatment or recommendation when some of the most important outcomes are missing, and worse, missing for reasons we cannot observe? This is a common issue in real-world data, especially in healthcare, where patients may drop out of a study over time due to recovery or death. If not properly handled, such “informative” missingness can seriously distort evaluations of treatment effectiveness.Our method tackles this problem. We introduce a novel approach that works even when the missing data depends on unseen outcomes—a setting that no existing method handles in long-term decision problems. The key idea is to use a shadow variable, a clever proxy that holds information about the missing outcome. By incorporating this into our estimation process, we can effectively correct for the hidden bias and recover accurate evaluations. We also provide a way to quantify how confident we are in the results, ensuring reliability.This breakthrough makes it possible to evaluate policies, like a new drug treatment plan or a personalized recommendation system, with far greater accuracy, even when the data is messy or incomplete. Our work lays a foundation for safer clinical decisions and smarter AI systems in the face of real-world uncertainty."
