type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $\alpha$-$\beta$-Divergence,https://ICML.cc//virtual/2025/poster/43650,"Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qianqian Xu, Qingming Huang","Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student model by minimizing the divergence between their output distributions, typically using forward Kullback-Leibler divergence (FKLD) or reverse KLD (RKLD). It has become an effective training paradigm due to the broader supervision information provided by the teacher distribution compared to one-hot labels. We identify that the core challenge in KD lies in balancing two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}} effect, which refers to focusing on modes with large errors, and the \textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on modes with high student confidence. Through an analysis of how probabilities are reassigned during gradient updates, we observe that these two effects are entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too weak in FKLD, causing the student to fail to concentrate on the target class. In contrast, both are too strong in RKLD, causing the student to overly emphasize the target class while ignoring the broader distributional information from the teacher. To address this imbalance, we propose ABKD, a generic framework with $\alpha$-$\beta$-divergence. Our theoretical results show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving a better trade-off between these effects. Extensive experiments on 17 language/vision datasets with 12 teacher-student settings confirm its efficacy.","Knowledge Distillation (KD) is a method where a smaller student AI model learns from a larger, more powerful teacher model by imitating its output predictions. This process allows the student to benefit from the richer information in the teacher’s output, which goes beyond simple correct/incorrect labels.However, the effectiveness of KD depends heavily on how the difference between teacher and student outputs is measured. Most methods use either forward Kullback-Leibler divergence (FKLD) or reverse KL divergence (RKLD), but both have issues: FKLD is too relaxed, making the student spread its focus too widely, while RKLD is too aggressive, forcing the student to over-focus on a few predictions and ignore useful information.We find that this problem stems from two competing effects: hardness-concentration (focusing on hard-to-predict examples) and confidence-concentration (focusing on what the student already predicts with confidence). FKLD and RKLD treat these effects in extreme ways.To solve this, we introduce ABKD, a new framework based on α-β divergence, which lets us balance both effects more flexibly. By tuning just the loss function, ABKD improves performance across 17 language and vision tasks, showing strong results without needing additional model changes."
Poster,ABNet: Adaptive explicit-Barrier Net for Safe and Scalable Robot Learning,https://ICML.cc//virtual/2025/poster/43514,"Wei Xiao, Johnson Tsun-Hsuan Wang, Chuang Gan, Daniela Rus","Safe learning is central to AI-enabled robots where a single failure may lead to catastrophic results. Existing safe learning methods are not scalable, inefficient and hard to train, and tend to generate unstable signals under noisy inputs that are challenging to be deployed for robots. To address these challenges, we propose Adaptive explicit-Barrier Net (ABNet) in which barriers explicitly show up in the closed-form model that guarantees safety. The ABNet has the potential to incrementally scale toward larger safe foundation models.  Each head of ABNet could learn safe control policies from different features and focuses on specific part of the observation. In this way, we do not need to directly construct a large model for complex tasks, which significantly facilitates the training of the model while ensuring its stable output. Most importantly, we can still formally prove the safety guarantees of the ABNet. We demonstrate the efficiency and strength of ABNet in 2D robot obstacle avoidance, safe robot manipulation, and vision-based end-to-end autonomous driving, with results showing much better robustness and guarantees over existing models.","How to make an AI model/robot learn like a human being is a long-standing challenge. The human-style of learning is in a scalable and reliable way. In other words, a human always safely learns skills one by one instead of learning all the skills at once, which is contradictory to existing large foundation models that require huge data and extremely expensive computation resource. This work marks a first step towards safe and scalable learning using our proposed ABNet. ABNet is efficient, scalable, and most importantly with provable safety guarantees, and it inspires us to further explore learning beyond safety requirements. ABNet will speed up the grounding of AI on real robots, safely and efficiently assisting humans in physical and cognitive tasks."
Poster,A Bregman Proximal Viewpoint on Neural Operators,https://ICML.cc//virtual/2025/poster/44179,"Abdel-Rahim Mezidi, Jordan Patracone, Saverio Salzo, Amaury Habrard, Massimiliano Pontil, Rémi Emonet, Marc Sebban","We present several advances on neural operators by viewing the action of operator layers as the minimizers of Bregman regularized optimization problems over Banach function spaces. The proposed framework allows interpreting the activation operators as Bregman proximity operators from dual to primal space. This novel viewpoint is general enough to recover classical neural operators as well as a new variant, coined Bregman neural operators, which includes the inverse activation operator and features the same expressivity of standard neural operators. Numerical experiments support the added benefits of the Bregman variant of Fourier neural operators for training deeper and more accurate models.","Computer programs can struggle when simulating physics phenomena like fluid flow or climate modeling, as it requires learning complex patterns. Recent approaches—called neural operators—demonstrated potential but often hit performance limits when they get bigger, restricting their reliability.Using ideas from optimization theory, we designed a new perspective on how neural operators work. This viewpoint led us to develop ""Bregman Neural Operators"", which—unlike conventional approaches—improve as they get larger rather than degrading. Our approach produced more accurate results across a range of challenging physical simulations.Our work bridges mathematical theory with practical machine learning. This helps scientists run more reliable simulations of the real world and it provides new theoretical insights into how these programs learn, opening pathways for further innovations."
Poster,A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment,https://ICML.cc//virtual/2025/poster/43959,"Raanan Yehezkel Rohekar, Yaniv Gurwicz, Sungduk Yu, Estelle Aflalo Guez, Vasudev Lal","Are generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learning a world model from which sequences are generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT and presenting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences, and introduce a corresponding confidence score. Empirical tests were conducted in controlled environments using the setups of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, was tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases where it generates illegal moves, it also fails to capture a causal structure.","GPT models, like those used in chatbots, are becoming increasingly widespread, with applications expanding beyond natural language understanding into a variety of new domains. But as these models are applied to different fields, a crucial question arises: Can a GPT model---trained simply to predict the next item in a sequence---actually learn the underlying mechanisms of a domain, or is it merely guessing based on patterns in data? To investigate this, we discovered a surprising mathematical connection between the attention mechanism at the heart of GPT models and a framework that scientists use to represent cause-and-effect relationships. Building on this insight, we created a new method to uncover the hidden cause-and-effect structure within a sequence---without needing any extra training or examples. We tested our findings in the controlled environments of the strategy games Chess and Othello and found that the models could internalize the underlying game mechanics, rather than simply mimicking observed sequences. This suggests GPT models may be capable of a deeper understanding than previously thought, paving the way for applying them to uncover the underlying mechanisms in challenging scientific areas such as protein folding, drug design, and the design of new materials."
Poster,Accelerated Diffusion Models via Speculative Sampling,https://ICML.cc//virtual/2025/poster/46111,"Valentin De Bortoli, Alexandre Galashov, Arthur Gretton, Arnaud Doucet","Speculative sampling is a popular technique for accelerating inference in Large Language Models by generating candidate tokens using a fast draft model and then accepting or rejecting them based on the target model's distribution. While speculative sampling was previously limited to discrete sequences, we extend it to diffusion models, which generate samples via continuous, vector-valued Markov chains. In this context, the target model is a high-quality but computationally expensive diffusion model. We propose various drafting strategies, including a simple and effective approach that does not require training a draft model and is applicable out-of-the-box to any diffusion model. We demonstrate significant generation speedup on various diffusion models, halving the number of function evaluations while generating exact samples from the target model. Finally, we also show how this procedure can be used to accelerate Langevin diffusions to sample unnormalized distributions.",We propose an accelerated method for the sampling of diffusion models and Langevin diffusions leveraging ideas from Large Language Models.
Poster,Accelerating Large Language Model Reasoning via Speculative Search,https://ICML.cc//virtual/2025/poster/44040,"Zhihai Wang, Jie Wang, Jilai Pan, Xilin Xia, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Feng Wu","Tree-search-based reasoning methods have significantly enhanced the reasoning capability of large language models (LLMs) by facilitating the exploration of multiple intermediate reasoning steps, i.e., thoughts. However, these methods suffer from substantial inference latency, as they have to generate numerous reasoning thoughts, severely limiting LLM applicability. To address this challenge, we propose a novel Speculative Search (SpecSearch) framework that significantly accelerates LLM reasoning by optimizing thought generation. Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts. The major pillar of SpecSearch is a novel quality-preserving rejection mechanism, which effectively filters out thoughts whose quality falls below that of the large model's outputs. Moreover, we show that SpecSearch preserves comparable reasoning quality to the large model. Experiments on both the Qwen and Llama models demonstrate that SpecSearch significantly outperforms state-of-the-art approaches, achieving up to 2.12$\times$ speedup with comparable reasoning quality.","Large language models, like ChatGPT, are great at solving complex problems by thinking through different possible steps — a bit like how a person might work through a puzzle. But for the computer to try out many possible ways of solving a problem, it usually needs to spend a lot of time thinking, which makes these models slow to use.To solve this, we created a new method called Speculative Search (SpecSearch). Our approach speeds up the thinking process by letting a smaller, faster program work together with the larger, smarter model. The small model quickly generates possible steps, and then the large model only spends time checking and keeping the high-quality ones. This way, the system avoids wasting time on ideas that wouldn’t be helpful anyway.Our experiments show that SpecSearch makes language models much faster — over twice as fast in some cases — without losing their ability to reason well. We have shared our code at https://github.com/MIRALab-USTC/LLMReasoning-SpecSearch, so others can use and build on our method for making AI smarter and faster."
Poster,Accelerating Linear Recurrent Neural Networks for the Edge with Unstructured Sparsity,https://ICML.cc//virtual/2025/poster/45121,"Alessandro Pierro, Steven Abreu, Jonathan Timcheck, Philipp Stratmann, Andreas Wild, Sumit Shrestha","Linear recurrent neural networks enable powerful long-range sequence modeling with constant memory usage and time-per-token during inference. These architectures hold promise for streaming applications at the edge, but deployment in resource-constrained environments requires hardware-aware optimizations to minimize latency and energy consumption. Unstructured sparsity offers a compelling solution, enabling substantial reductions in compute and memory requirements--when accelerated by compatible hardware platforms. In this paper, we conduct a scaling study to investigate the Pareto front of performance and efficiency across inference compute budgets.We find that highly sparse linear RNNs *consistently* achieve better efficiency-performance trade-offs than dense baselines, with $2\times$ less compute and $36$\% less memory at iso-accuracy.Our models achieve state-of-the-art results on a real-time streaming task for audio denoising.By quantizing our sparse models to fixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic chip for real-time processing, we translate model compression into tangible gains of $42\times$ lower latency and $149\times$ lower energy consumption compared to a dense model on an edge GPU.Our findings showcase the transformative potential of unstructured sparsity, paving the way for highly efficient recurrent neural networks in real-world, resource-constrained environments.","Novel computing architectures show great promise in bringing energy-efficient AI capabilities to the edge. In this paper, we demonstrate how sparse models, which have a large portion of their parameters set to zero, can achieve the same accuracy as dense models in audio denoising and keyword spotting while requiring a fraction of the compute and memory. Moreover, we show that the Intel Loihi 2 neuromorphic research chip can leverage this sparsity to run models with $42\times$ lower latency and $149\times$ lower energy consumption compared to a dense model on an edge GPU."
Poster,Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies,https://ICML.cc//virtual/2025/poster/43675,"Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Gaurav Jain, Oren Pereg, Moshe Wasserblat, David Harel","Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms demonstrate significant speedups of up to 2.8x over standard autoregressive decoding. By enabling any off-the-shelf model to serve as a drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.","Making large language models (like those that power chatbots) generate text faster is a big challenge. A technique called ""speculative decoding"" can speed things up by having a smaller, faster ""drafter"" model predict several words ahead, which the larger ""target"" model then checks at once. However, this usually only works if both models use the exact same dictionary of words (vocabulary), which is often not the case and can mean needing to build a new drafter model from scratch.We've developed three new ways for speculative decoding to work even when the drafter and target models have different vocabularies. These methods don't change the quality of the text generated by the large model and can use existing, off-the-shelf models without any extra training.Our experiments demonstrate that our techniques can make language models run up to 2.8 times faster on tasks like summarizing text, writing computer code, and understanding long documents. This makes it easier and more practical to use speculative decoding with a much wider variety of models, speeding up AI applications without needing to create specialized drafter models."
Poster,Accelerating PDE-Constrained Optimization by the Derivative of Neural Operators,https://ICML.cc//virtual/2025/poster/45599,"Ze Cheng, Zhuoyu Li, Wang Xiaoqiang, Jianing Huang, Zhizhou Zhang, Zhongkai Hao, Hang Su","PDE-Constrained Optimization (PDECO) problems can be accelerated significantly by employing gradient-based methods with surrogate models like neural operators compared to traditional numerical solvers. However, this approach faces two key challenges:(1) **Data inefficiency**: Lack of efficient data sampling and effective training for neural operators, particularly for optimization purpose.(2) **Instability**: High risk of optimization derailment due to inaccurate neural operator predictions and gradients.To address these challenges, we propose a novel framework: (1) **Optimization-oriented training**: we leverage data from full steps of traditional optimization algorithms and employ a specialized training method for neural operators. (2) **Enhanced derivative learning**: We introduce a **Virtual-Fourier** layer to enhance derivative learning within the neural operator, a crucial aspect for gradient-based optimization. (3) **Hybrid optimization**: We implement a hybrid approach that integrates neural operators with numerical solvers, providing robust regularization for the optimization process.Our extensive experimental results demonstrate the effectiveness of our model in accurately learning operators and their derivatives. Furthermore, our hybrid optimization approach exhibits robust convergence.","Imagine you're trying to **optimize** something complex – like making a car part as light as possible while still being strong, or designing a chemical process to maximize output. Often, the behavior of these systems is governed by complex physics described by **Partial Differential Equations (PDEs)**. When we try to find the *best* design or process under these rules, we're doing what's called **PDE-Constrained Optimization (PDECO)**. Solving these optimization problems using traditional computer simulations is incredibly slow and computationally expensive.---### The Need for Speed (and Trust)Our research aims to **accelerate** these PDECO problems using **AI models** called ""neural operators."" These AI tools can learn system behaviors much faster. However, two main challenges arise:1.  **Data Inefficiency:** AI models usually need vast amounts of data. How do we train them effectively without endless slow simulations?2.  **Instability:** AI predictions can be inaccurate, risking faulty designs or optimization failures. We need reliability.---### Our Solution: Smart, Reliable AIWe developed a new framework to address this:1.  **Smarter Training:** We train our AI by showing it how reliable, traditional PDECO methods work step-by-step, making it learn efficiently from less data.2.  **Accurate Calculations:** For optimization, precise calculations (like ""derivatives"" which show how things change) are crucial. We added a special ""**Virtual-Fourier**"" layer to our AI to ensure it learns these accurately.3.  **Hybrid Approach:** We combine our fast AI with robust traditional solvers. The AI speeds things up, but if it makes a questionable prediction, the reliable traditional solver steps in to maintain accuracy and prevent errors. It's like having a fast AI assistant checked by an expert.---### The OutcomeOur results show our model accurately learns complex behaviors and their derivatives. Crucially, our hybrid approach ensures **robust and reliable convergence**, meaning faster, more confident solutions for these challenging PDECO problems."
Poster,Accelerating Quantum Reinforcement Learning with a Quantum Natural Policy Gradient Based Approach,https://ICML.cc//virtual/2025/poster/45600,"Yang Xu, Vaneet Aggarwal","We address the problem of quantum reinforcement learning (QRL) under model-free settings with quantum oracle access to the Markov Decision Process (MDP). This paper introduces a Quantum Natural Policy Gradient (QNPG) algorithm, which replaces the random sampling used in classical Natural Policy Gradient (NPG) estimators with a deterministic gradient estimation approach, enabling seamless integration into quantum systems. While this modification introduces a bounded bias in the estimator, the bias decays exponentially with increasing truncation levels. This paper demonstrates that the proposed QNPG algorithm achieves a sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-1.5})$ for queries to the quantum oracle, significantly improving the classical lower bound of $\tilde{\mathcal{O}}(\epsilon^{-2})$ for queries to the MDP.","Reinforcement-learning agents often need huge amounts of experience before they behave well; for the widely used natural policy-gradient method, the theoretical lower bound of sample complexity is $\tilde{O}(\epsilon^{-2})$ in the classical setting, which is prohibitive in data-hungry fields like robotics and finance.We show that a future quantum computer can do better. Our Quantum Natural Policy Gradient (QNPG) algorithm prepares many candidate trajectories in quantum superposition, replaces random-length sampling with a fixed-length deterministic trick that is friendly to quantum hardware, and applies a quantum variance-reduction routine. Together these ideas yield a provably correct model-free algorithm whose sample complexity scales as $\tilde{O}(\epsilon^{-1.5})$, beating the best classical bound and that works with large, parameterized policies rather than just tiny tabular ones."
