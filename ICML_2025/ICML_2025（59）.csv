type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Contour Integration Underlies Human-Like Vision,https://ICML.cc//virtual/2025/poster/44500,"Ben Lonnqvist, Elsa Scialom, Abdulkadir Gokce, Zehra Merchant, Michael Herzog, Martin Schrimpf","Despite the tremendous success of deep learning in computer vision, models still fall behind humans in generalizing to new input distributions. Existing benchmarks do not investigate the specific failure points of models by analyzing performance under many controlled conditions. Our study systematically dissects where and why models struggle with contour integration - a hallmark of human vision -- by designing an experiment that tests object recognition under various levels of object fragmentation. Humans (n=50) perform at high accuracy, even with few object contours present. This is in contrast to models which exhibit substantially lower sensitivity to increasing object contours, with most of the over 1,000 models we tested barely performing above chance. Only at very large scales ($\sim5B$ training dataset size) do models begin to approach human performance. Importantly, humans exhibit an integration bias - a preference towards recognizing objects made up of directional fragments over directionless fragments. We find that not only do models that share this property perform better at our task, but that this bias also increases with model training dataset size, and training models to exhibit contour integration leads to high shape bias. Taken together, our results suggest that contour integration is a hallmark of object vision that underlies object recognition performance, and may be a mechanism learned from data at scale.","Computer vision is not as robust as human vision. Why? One reason is that humans adapt better to unseen circumstances -- for example, humans can recognize familiar objects even when only scattered pieces of their outlines are visible, but cutting-edge AI vision systems often fail under these conditions. To understand this phenomenon in humans and AI models, we showed 50 people and over a thousand AI models images in which objects were broken into disconnected fragments at many levels of difficulty. This allowed us to study why and where humans and models face the greatest difficulties. While humans stayed highly accurate even when the edges of the object were highly fragmented, most AI models performed near chance unless trained on extremely large image datasets. We also discovered that both people and the biggest models excel when fragments align along the object’s true contour—a “gap-filling” ability known as contour integration. This work reveals that piecing together broken outlines is fundamental to robust vision and can emerge in AI purely through massive data exposure."
Poster,Contract Design Under Approximate Best Responses,https://ICML.cc//virtual/2025/poster/46004,"Francesco Bacchiocchi, Jiarui Gan, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti","Principal-agent problems model scenarios where a principal aims at incentivizing an agent to take costly, unobservable actions through the provision of payments. Such interactions are ubiquitous in several real-world applications, ranging from blockchain to the delegation of machine learning tasks. In this paper, we initiate the study of hidden-action principal-agent problems under approximate best responses, in which the agent may select any action that is not too much suboptimal given the principal's payment scheme (a.k.a. contract). Our main result is a polynomial-time algorithm to compute an optimal contract under approximate best responses. This is perhaps surprising, as computing an optimal commitment under approximate best responses is known to be computationally intractable in Stackelberg games. We also investigate the learnability of contracts under approximate best responses, by providing a no-regret learning algorithm for a natural application scenario where the principal does not know anything about the environment.","Contract design provides a powerful framework for capturing the strategic interaction between a principal and an agent, where the principal aims to incentivize the agent to undertake desirable actions. Yet, in many real-world settings, agents may choose actions that are suboptimal—or even deliberately misaligned—with the principal’s objectives, leading to significant consequences for the principal. In this paper, we formalize and characterize such scenarios by allowing the agent to take actions that are suboptimal for them by at most a parameter $\delta > 0$. We provide a polynomial-time algorithm to compute an optimal contract in this setting and a no-regret learning algorithm for cases where the principal has no prior knowledge of the environment."
Poster,Contradiction Retrieval via Contrastive Learning with Sparsity,https://ICML.cc//virtual/2025/poster/45043,"Haike Xu, Zongyu Lin, Kai-Wei Chang, Yizhou Sun, Piotr Indyk","Contradiction retrieval refers to identifying and extracting documents that explicitly disagree with or refute the content of a query, which is important to many downstream applications like fact checking and data cleaning. To retrieve contradiction argument to the query from large document corpora, existing methods such as similarity search and cross-encoder models exhibit different limitations.To address these challenges, we introduce a novel approach: SparseCL that leverages specially trained sentence embeddings designed to preserve subtle, contradictory nuances between sentences. Our method utilizes a combined metric of cosine similarity and a sparsity function to efficiently identify and retrieve documents that contradict a given query. This approach dramatically enhances the speed of contradiction detection by reducing the need for exhaustive document comparisons to simple vector calculations. We conduct contradiction retrieval experiments on Arguana, MSMARCO, and HotpotQA, where our method produces an average improvement of $11.0\%$ across different models. We also validate our method on downstream tasks like natural language inference and cleaning corrupted corpora.This paper outlines a promising direction for non-similarity-based information retrieval which is currently underexplored.","Contradiction retrieval refers to identifying and extracting documents that explicitly disagree with or refute the content of a query, which is crucial for downstream applications such as fact-checking and data cleaning. To tackle these challenges, we introduce SparseCL, a novel approach that utilizes specially trained sentence embeddings designed to capture subtle, contradictory nuances between sentences. Our method combines cosine similarity with a sparsity function to efficiently identify and retrieve documents that contradict a given query. This approach dramatically enhances the speed of contradiction detection by reducing the need for exhaustive document comparisons to simple vector calculations. This paper outlines a promising direction for non-similarity-based information retrieval, which is currently underexplored."
Poster,Contrastive Learning with Simplicial Convolutional Networks for Short-Text Classification,https://ICML.cc//virtual/2025/poster/45750,"Liang Huang, Benedict Lee, Daniel Ng, Kelin Xia","Text classification is a fundamental task in Natural Language Processing (NLP). Short text classification has recently captured much attention due to its increased amount from various sources with limited labels and its inherent challenges for its sparsity in words and semantics. Recent studies have adopted self-supervised contrastive learning across different representations to improve performance. However, most of the current models face several challenges. Firstly, the augmentation step might not be able to generate positive and negative samples that are semantically similar and dissimilar to the anchor respectively. Secondly, the text data could be enhanced with external auxiliary information that might introduce noise to the sparse text data. In addition, they are limited in capturing higher-order information such as group-wise interactions. In this work, we propose a novel document simplicial complex construction based on text data for a higher-order message-passing mechanism. We enhance the short text classification performance by contrasting the structural representation with the sequential representation generated by the transformer mechanism for improved outcomes and mitigated issues. The proposed framework, Contrastive Learning with Simplicial Convolutional Networks (C-SCN), leverages the expressive power of graph neural networks, models higher-order information beyond pair-wise relations and enriches features through contrastive learning. Experimental results on four benchmark datasets demonstrate the capability of C-SCN to outperform existing models in analysing sequential and complex short-text data.","Classifying short texts is challenging due to their limited word count, sparse meaning, and limited labels. Recent methods utilise contrastive learning, which involves comparing similar and dissimilar text samples, to improve performance. However, many struggle to generate meaningful comparisons, add noisy external data, or miss higher-level patterns. We propose C-SCN, a novel model that combines contrastive learning with simplicial convolutional networks to capture deeper relationships in text. Unlike traditional methods, C-SCN represents documents as structured networks in simplicial complexes, enabling richer analysis beyond word pairs. It contrasts these structures with transformer-generated text embeddings, improving accuracy while reducing noise. Tests on four benchmark datasets show C-SCN outperforms existing models, proving its strength in handling short, complex texts. This approach could enhance applications like social media analysis, customer feedback sorting, or real-time content tagging."
Poster,Contrastive Localized Language-Image Pre-Training,https://ICML.cc//virtual/2025/poster/43845,"Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, Zhe Gan","CLIP has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, it has been widely adopted as the vision backbone of multimodal large language models (MLLMs). The success of CLIP relies on aligning web-crawled noisy text annotations at image levels. However, such criteria may be insufficient for downstream tasks in need of fine-grained vision representations, especially when understanding region-level is demanding for MLLMs. We improve the localization capability of CLIP with several advances. Our proposed pre-training method, Contrastive Localized Language-Image Pre-training (CLOC), complements CLIP with region-text contrastive loss and modules. We formulate a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, we design a visually-enriched and spatially-localized captioning framework to effectively generate region-text labels. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks.","Modern AI systems that understand images and text—like those that can describe a photo or answer questions about a picture—often rely on a popular model called CLIP. CLIP learns by matching images with their captions. Such pairs of data are collected on a billion-scale from the internet. While this works well for many tasks, it struggles when more detailed understanding is needed, such as identifying specific objects or regions within an image.To improve this, we developed a new method called CLOC (Contrastive Localized Language-Image Pre-training). Unlike CLIP, CLOC doesn’t just look at whole images and their captions. Instead, it also teaches the model to connect smaller image regions with detailed text descriptions. We introduced a concept called ''promptable embeddings'', which lets the model easily adapt its understanding to different image areas when given location hints.We also created a pipeline to automatically generate fine-grained region descriptions at scale. With this, we trained CLOC on billions of images. Our results show that CLOC helps AI systems perform better on tasks requiring more precise image understanding, like pinpointing objects being referred to in a sentence."
Poster,Contrastive Private Data Synthesis via Weighted Multi-PLM Fusion,https://ICML.cc//virtual/2025/poster/44062,"Tianyuan Zou, Yang Liu, Peng Li, Yufei Xiong, Jianqing Zhang, Jingjing Liu, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang","Substantial quantity and high quality are the golden rules of making a good training dataset with sample privacy protection equally important. Generating synthetic samples that resemble high-quality private data while ensuring Differential Privacy (DP), a formal privacy guarantee, promises scalability and practicality. However, existing methods relying on pre-trained models for data synthesis often struggle in data-deficient scenarios, suffering from limited sample size, inevitable generation noise and existing pre-trained model bias. To address these challenges, we propose a novel contr**A**stive private data **S**ynthesis via **W**eighted multiple **P**re-trained generative models framework, named as **WASP**. WASP utilizes limited private samples for more accurate private data distribution estimation via a Top-*Q* voting mechanism, and leverages low-quality synthetic samples for contrastive generation via collaboration among dynamically weighted multiple pre-trained models. Extensive experiments on 6 well-developed datasets with 6 open-source and 3 closed-source PLMs demonstrate the superiority of WASP in improving model performance over diverse downstream tasks. Code is available at https://github.com/LindaLydia/WASP.","How can we create useful training data without risking anyone’s privacy? This was the question we set out to explore by studying how to generate synthetic data that mimics real private datasets containing limited samples, while revealing as little as possible about the individuals behind them.Our work introduces a method called **WASP**, which combines the strengths of multiple AI models to produce realistic and privacy-preserving synthetic data. Unlike existing approaches that rely on a single model or large amounts of real data, **WASP** uses a collaborative strategy: it asks different models to generate data, scores the results using limited private examples, and then learns to trust the best-performing models more in future rounds. It also learns not just from good examples but from bad ones as well, by contrasting them during training.We found that this strategy leads to better synthetic data even when only limited real examples are available. This is important, as many real-world applications, like healthcare and finance, must work with sensitive data of small amount that cannot be freely shared. Our results suggest a promising path forward for training AI models in data-scarce, privacy-sensitive environments."
Poster,Contrastive Visual Data Augmentation,https://ICML.cc//virtual/2025/poster/43550,"Yu Zhou, Bingxuan Li, Mohan Tang, Xiaomeng Jin, Te-Lin Wu, Kuan-Hao Huang, Heng Ji, Kai-Wei Chang, Nanyun Peng","Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA extracts key contrastive textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of CoDA on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect NovelSpecies, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains in accuracy.","AI models with image understanding ability still mix up rare or brand-new objects because their training images lack the subtle cues humans notice. Our method automatically finds the key traits that distinguish a new concept from the look-alike it’s confused with, then generates a few synthetic images that highlight those cues. After filtering for quality, a single synthetic image can be used to teach image understanding AI models to improve recognition accuracy by 12 % on unseen animal species and by 5–6 % on two standard benchmarks. CoDA offers a fast, low-cost way to keep AI systems current."
Poster,Control and Realism: Best of Both Worlds in Layout-to-Image without Training,https://ICML.cc//virtual/2025/poster/46493,"Bonan Li, Yinhan Hu, Songhua Liu, Xinchao Wang","Layout-to-Image generation aims to create complex scenes with precise control over the placement and arrangement of subjects. Existing works have demonstrated that pre-trained Text-to-Image diffusion models can achieve this goal without training on any specific data; however, they often face challenges with imprecise localization and unrealistic artifacts. Focusing on these drawbacks, we propose a novel training-free method, WinWinLay. At its core, WinWinLay presents two key strategies—Non-local Attention Energy Function and Adaptive Update—that collaboratively enhance control precision and realism. On one hand, we theoretically demonstrate that the commonly used attention energy function introduces inherent spatial distribution biases, hindering objects from being uniformly aligned with layout instructions. To overcome this issue, non-local attention prior is explored to redistribute attention scores, facilitating objects to better conform to the specified spatial conditions. On the other hand, we identify that the vanilla backpropagation update rule can cause deviations from the pre-trained domain, leading to out-of-distribution artifacts. We accordingly introduce a Langevin dynamics-based adaptive update scheme as a remedy that promotes in-domain updating while respecting layout constraints. Extensive experiments demonstrate that WinWinLay excels in controlling element placement and achieving photorealistic visual fidelity, outperforming the current state-of-the-art methods.","In this paper, a novel training-free method based on the diffusion model, WinWinLay, is proposed by revisiting attention backward guidance and introducing modifications to tackle existing drawbacks."
Poster,Controllable Data Generation with Hierarchical Neural Representations,https://ICML.cc//virtual/2025/poster/44257,"Sheyang Tang, xiaoyu xu, Jiayan Qiu, Zhou Wang","Implicit Neural Representations (INRs) represent data as continuous functions using the parameters of a neural network, where data information is encoded in the parameter space. Therefore, modeling the distribution of such parameters is crucial for building generalizable INRs. Existing approaches learn a joint distribution of these parameters via a latent vector to generate new data, but such a flat latent often fails to capture the inherent hierarchical structure of the parameter space, leading to entangled data semantics and limited control over the generation process. Here, we propose a **C**ontrollable **H**ierarchical **I**mplicit **N**eural **R**epresentation (**CHINR**) framework, which explicitly models conditional dependencies across layers in the parameter space. Our method consists of two stages: In Stage-1, we construct a Layers-of-Experts (LoE) network, where each layer modulates distinct semantics through a unique latent vector, enabling disentangled and expressive representations. In Stage-2, we introduce a Hierarchical Conditional Diffusion Model (HCDM) to capture conditional dependencies across layers, allowing for controllable and hierarchical data generation at various semantic granularities. Extensive experiments across different modalities demonstrate that CHINR improves generalizability and offers flexible hierarchical control over the generated content.","We proposed a new AI algorithm that learns to generate images, shapes, or motions in a more controllable way. Our method builds on a class of AI models called Implicit Neural Representations (INRs), which represent data not as pixels or points, but as continuous functions with the parameters of a neural network. The key idea is that these parameters naturally follow a layered structure, where earlier layers capture broad features like shape or pose, and later layers refine the details such as texture or expression.To take advantage of this structure, we designed a two-step method: first, we learn a model to represent data with layer-wise control codes; then, we train another model to understand how these layer-wise codes depend on each other. This lets us generate new content by adjusting one layer at a time, giving us fine-grained control over what gets created, for example, changing facial expressions without changing the face's overall shape. Our system works across different types of data, making it easier to steer AI generation in meaningful ways."
Poster,Controlled Generation with Equivariant Variational Flow Matching,https://ICML.cc//virtual/2025/poster/44911,"Floor Eijkelboom, Heiko Zimmermann, Sharvaree Vadgama, Erik Bekkers, Max Welling, Christian Andersson Naesseth, Jan-Willem van de Meent","We derive a controlled generation objective within the framework of Variational Flow Matching (VFM),which casts flow matching as a variational inference problem.We demonstrate that controlled generation can be implemented two ways: (1) by way of end-to-end training of conditional generative models, or (2) as a Bayesian inference problem, enabling post hoc control of unconditional models without retraining. Furthermore, we establish the conditions required for equivariant generation and provide an equivariant formulation of VFM tailored for molecular generation, ensuring invariance to rotations, translations, and permutations. We evaluate our approach on both uncontrolled and controlled molecular generation, achieving state-of-the-art performance on uncontrolled generation and outperforming state-of-the-art models in controlled generation, both with end-to-end training and in the Bayesian inference setting. This work strengthens the connection between flow-based generative modeling and Bayesian inference, offering a scalable and principled framework for constraint-driven and symmetry-aware generation.","This paper presents a new way to guide AI models to generate data that follows specific rules, like making molecules with certain properties. The method, called Variational Flow Matching (VFM), works by smoothly transforming random noise into useful data. It can either be trained to follow rules from the start, or it can adjust an existing model afterward without retraining. We also make sure the model respects important symmetries—like a molecule still being the same if you rotate it. Our works well on several molecule-building tasks, is flexible, and runs efficiently."
