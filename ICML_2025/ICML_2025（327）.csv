type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales,https://ICML.cc//virtual/2025/poster/45832,"Drew Prinster, Xing Han, Anqi Liu, Suchi Saria","Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but also continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Methods for nonparametric sequential testing---especially conformal test martingales (CTMs) and anytime-valid inference---offer promising tools for this monitoring task. However, existing approaches are restricted to monitoring limited hypothesis classes or ``alarm criteria'' (e.g., detecting data shifts that violate certain exchangeability or IID assumptions), do not allow for online adaptation in response to shifts, and/or cannot diagnose the cause of degradation or alarm. In this paper, we address these limitations by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that adapt online to mild covariate shifts (in the marginal input distribution), quickly detect harmful shifts, and diagnose those harmful shifts as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.","Responsibly deploying artificial intelligence (AI) systems in high-stakes settings—such as to assist doctors in medical diagnosis or AI chatbot agents used at internet-wide scale—requires continual monitoring of the AI’s performance, to quickly detect and address any unsafe behavior. It turns out, however, that AI monitoring is very difficult: many current approaches are too slow to detect truly harmful changes, or they may raise many false alarms and so risk creating a “boy who cried wolf” effect.To address these challenges, we introduce a novel framework called “WATCH” (**W**eighted **A**daptive **T**esting for **C**hangepoint **H**ypotheses), which monitors AI deployments with three main goals:&nbsp;&nbsp;&nbsp;&nbsp;(1) **Detection:** First, WATCH quickly detects harmful shifts, and it catches them faster than directly tracking standard risk metrics.&nbsp;&nbsp;&nbsp;&nbsp;(2) **Adaptation:** Second, WATCH adapts in real-time to mild or benign changes (which are commonplace but not harmful) to reduce unnecessary alarms and improve the utility of the AI’s outputs for end-users.&nbsp;&nbsp;&nbsp;&nbsp;(3) **Root-Cause Analysis:** Lastly, WATCH diagnoses the cause of degradation (ie, as a severe change in the data inputs or a fundamental “concept shift” in input-output relationships) to inform appropriate recovery (eg, retraining) of the AI.WATCH charts a path toward safer and more responsible deployment of AI systems via adaptive monitoring."
Poster,Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models,https://ICML.cc//virtual/2025/poster/43674,"Tianjie Ju, Yi Hua, Hao Fei, Zhenyu Shao, Yubin Zheng, Haodong Zhao, Mong-Li Lee, Wynne Hsu, Zhuosheng Zhang, Gongshen Liu","Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at https://github.com/illusionhi/ProbingPrivacy.","Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at https://github.com/illusionhi/ProbingPrivacy."
Poster,WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting,https://ICML.cc//virtual/2025/poster/45318,"Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang","We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results.","Predicting future events from past data is important in fields like finance and weather forecasting. We developed WAVE attention, a new deep learning method inspired by statistical ARMA models. WAVE effectively separates short-term (recent events) and long-term (historical trends) influences within time series data. By combining statistical insights with advanced Transformer models, WAVE significantly improves forecasting accuracy without extra complexity. Our approach provides clearer predictions by explicitly distinguishing these different temporal impacts, achieving better performance in accurate and interpretable time series forecasting."
Poster,Weakly Supervised Anomaly Detection via Dual-Tailed Kernel,https://ICML.cc//virtual/2025/poster/44385,"Walid Durani, Tobias Nitzl, Claudia Plant, Christian Böhm","Detecting anomalies with limited supervision is challenging due to the scarcity of labeled anomalies, which often fail to capture the diversity of abnormal behaviors. We propose Weakly Supervised Anomaly Detection via Dual-Tailed Kernel (WSAD-DT), a novel framework that learns robust latent representations to distinctly separate anomalies from normal samples under weak supervision. WSAD-DT introduces two centroids—one for normal samples and one for anomalies—and leverages a dual-tailed kernel scheme: a light-tailed kernel to compactly model in-class points and a heavy-tailed kernel to main- tain a wider margin against out-of-class instances. To preserve intra-class diversity, WSAD-DT in- corporates kernel-based regularization, encouraging richer representations within each class. Furthermore, we devise an ensemble strategy that partition unlabeled data into diverse subsets, while sharing the limited labeled anomalies among these partitions to maximize their impact. Empirically, WSAD-DT achieves state-of-the-art performance on several challenging anomaly detection benchmarks, outperforming leading ensemble-based methods such as XGBOD.","Anomaly detection is hard when only a few examples of unusual behavior are labeled—like a handful of fraudulent transactions in a sea of normal ones. We tackle this challenge by designing a system that learns from just a few labeled anomalies and a large amount of mostly normal, unlabeled data.Our method, called WSAD-DT, gives the data two “home bases” in its internal map—one for normal and one for abnormal points. Two mathematical lenses—a fast-shrinking “light” tail and a slower-shrinking “heavy” tail—help pull each point toward the right base and push it away from the wrong one, keeping everyday patterns compact and anomalies distinct. To prevent everything from collapsing into a single spot, we add a diversity check. We also train a small ensemble of models to boost robustness.WSAD-DT detects anomalies more accurately than existing methods, even with very limited labels, and has the potential to improve fraud detection, fault diagnosis, and medical screening."
Poster,Weakly-Supervised Contrastive Learning for Imprecise Class Labels,https://ICML.cc//virtual/2025/poster/44934,"Zi-Hao Zhou, Jun-Jie Wang, Tong Wei, Min-Ling Zhang","Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at [https://github.com/Speechless-10308/WSC](https://github.com/Speechless-10308/WSC).","Real-world data often comes with imperfect labels—like vague tags or mistakes—that confuse AI models. Traditional methods, which rely on precise labels to learn patterns, struggle because they can’t tell which items truly belong together.We created a new AI training method that uses ""soft"" similarity scores instead of rigid labels. Imagine connecting dots (data points) with lines whose thickness shows how alike they are. Our system adjusts these connections over time, even with messy labels, to better capture true relationships.This approach boosts accuracy in tough scenarios, like when labels are 90% noisy, and works across tasks like image recognition. It’s flexible, easy to add to existing tools, and backed by math to ensure reliability. This makes AI more practical for real-world data, where perfect labels are rare but learning still needs to happen."
Poster,"Weak-to-Strong Generalization Even in Random Feature Networks, Provably",https://ICML.cc//virtual/2025/poster/45441,"Marko Medvedev, Kaifeng Lyu, Dingli Yu, Sanjeev Arora, Zhiyuan Li, Nati Srebro","Weak-to-Strong Generalization (Burns et al.,2024) is the phenomenon whereby a strong student, say GPT-4, learns a task from a weak teacher, say GPT-2, and ends up significantly outperforming the teacher. We show that this phenomenon does not require a complex and pretrained learner like GPT-4, can arise even in simple non-pretrained models, simply due to the size advantage of the student. But, we also show that there are inherint limits to the extent of such weak to strong generalization. We consider students and teachers that are random feature models, described by two-layer networks with a random and fixed bottom layer and trained top layer. A ‘weak’ teacher, with a small number of units (i.e. random features), is trained on the population, and a ‘strong’ student, with a much larger number of units (i.e. random features), is trained only on labels generated by the weak teacher. We demonstrate, prove, and understand how the student can outperform the teacher, even though trained only on data labeled by the teacher. We also explain how such weak-to-strong generalization is enabled by early stopping. We then show the quantitative limits of weak-to-strong generalization in this model, and in fact in a much broader class of models, for arbitrary teacher and student feature spaces and a broad class of learning rules, including when the student features are pre-trained or otherwise more informative. In particular, we show that in such models the student’serror can only approach zero if the teacher’s error approaches zero, and a strong student cannot “boost” a slightly-better-then-chance teacher to obtain a small error.","Weak-to-Strong Generalization (Burns et al.,2024) is the phenomenon whereby a strong student, say GPT-4, learns a task from a weak teacher, say GPT-2, and ends up significantly outperforming the teacher. We show that this phenomenon does not require a complex and pretrained learner like GPT-4, can arise even in simple non-pretrained models, simply due to the size advantage of the student. But, we also show that there are inherint limits to the extent of such weak to strong generalization. We consider students and teachers that are random feature models, described by two-layer networks with a random and fixed bottom layer and trained top layer. We demonstrate, prove, and understand how the student can outperform the teacher in our setup, even though trained only on data labeled by the teacher."
Poster,Weak-to-Strong Jailbreaking on Large Language Models,https://ICML.cc//virtual/2025/poster/46335,"Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Wang","Large language models (LLMs) are vulnerable to jailbreak attacks -- resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the **weak-to-strong** jailbreaking attack, an efficient inference time attack for aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99\% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong.","Large language models (LLMs), like those powering AI chatbots, are trained to avoid producing harmful or dangerous content. However, our study shows that these safety measures can be easily bypassed—even without using powerful computers or advanced technical skills. We introduce a new method called ""weak-to-strong jailbreaking"", where a small, misaligned AI model (one not trained to be safe) is used to subtly influence a much larger and safer model during text generation. This influence can trick the bigger model into saying things it normally would refuse to. We tested this attack across several widely used AI models and found that it could make even the most advanced models generate unsafe content over 99% of the time. While we also propose a partial defense strategy, our findings reveal serious gaps in current safety systems. This work highlights the urgent need to develop stronger protections as AI becomes more powerful and widely available."
Poster,WeGeFT: Weight‑Generative Fine‑Tuning for Multi‑Faceted Efficient Adaptation of Large Models,https://ICML.cc//virtual/2025/poster/45660,"Chinmay Savadikar, Xi Song, Tianfu Wu","Fine-tuning large pretrained Transformer models can focus on either introducing a small number of new learnable parameters (parameter efficiency) or editing representations of a small number of tokens using lightweight modules (representation efficiency). While the pioneering method LoRA (Low-Rank Adaptation) inherently balances parameter, compute, and memory efficiency, many subsequent variants trade off compute and memory efficiency and/or performance to further reduce fine-tuning parameters. To address this limitation and unify parameter-efficient and representation-efficient fine-tuning, we propose Weight-Generative Fine-Tuning (WeGeFT, pronounced  *wee-gift*), a novel approach that **learns to generate fine-tuning weights directly from the pretrained weights**. WeGeFT employs a simple low-rank formulation consisting of two linear layers, either shared across multiple layers of the pretrained model or individually learned for different layers. This design achieves multi-faceted efficiency in parameters, representations, compute, and memory, while maintaining or exceeding the performance of LoRA and its variants. Extensive experiments on commonsense reasoning, arithmetic reasoning, instruction following, code generation, and visual recognition verify the effectiveness of our proposed WeGeFT.","Modern AI language models are extremely capable, but adapting them to new tasks can be resource-heavy — requiring lots of memory, computing power, and to change the many internal parameters. To make this easier, researchers have developed techniques that aim to update only a small number of these parameters, making the fine-tuning process more efficient.One popular method, called LoRA (Low-Rank Adaptation), strikes a strong balance: it keeps the number of new parameters low and remains efficient in terms of memory, speed, and performance. However, many newer methods reduce the number of added parameters even further — but at the cost of using more memory, more computation, or losing accuracy.We created WeGeFT (short for Weight-Generative Fine-Tuning, and pronounced as wee-gift), a new approach that keeps LoRA’s broad efficiency benefits while reducing the number of added parameters even more. It learns how to generate the necessary updates directly from the original model’s knowledge, using a simple and compact design. Despite being lightweight, WeGeFT matches or outperforms LoRA on a wide range of tasks — from arithmetic and commonsense reasoning, following instructions to coding and image recognition — making it a powerful and efficient tool for tuning AI models."
Poster,Weight matrices compression based on PDB model in deep neural networks,https://ICML.cc//virtual/2025/poster/46571,"Xiaoling Wu, Junpeng Zhu, Zeng Li","Weight matrix compression has been demonstrated to effectively reduce overfitting and improve the generalization performance of deep neural networks. Compression is primarily achieved by filtering out noisy eigenvalues of the weight matrix. In this work, a novel **Population Double Bulk (PDB) model** is proposed to characterize the eigenvalue behavior of the weight matrix, which is more general than the existing Population Unit Bulk (PUB) model. Based on PDB model and Random Matrix Theory (RMT), we have discovered a new **PDBLS algorithm** for determining the boundary between noisy eigenvalues and information. A **PDB Noise-Filtering algorithm** is further introduced to reduce the rank of the weight matrix for compression. Experiments show that our PDB model fits the empirical distribution of eigenvalues of the weight matrix better than the PUB model, and our compressed weight matrices have lower rank at the same level of test accuracy. In some cases, our compression method can even improve generalization performance when labels contain noise. The code is avaliable at https://github.com/xlwu571/PDBLS.","Deep neural networks can overfit when their weight matrices contain too much noise. A common solution is to compress these matrices by removing noisy eigenvalues. However, existing models that guide this process often fail to match real eigenvalue distributions. We propose a new model, Population Double Bulk (PDB), which better captures the eigenvalue structure. Based on this, we develop PDBLS, an algorithm to detect and remove noise-dominated components. Our method reduces the matrix rank while preserving accuracy, and can even improve generalization when labels are noisy. This provides a more effective and robust approach to weight matrix compression in deep learning. The approach can be readily applied to various network architectures."
Poster,Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win,https://ICML.cc//virtual/2025/poster/46332,"Lorenz Kummer, Samir Moustafa, Anatol Ehrlich, Franka Bause, Nikolaus Suess, Wilfried Gansterer, Nils M. Kriege","The lottery ticket hypothesis (LTH) is well-studied for convolutional neural networks but has been validated only empirically for graph neural networks (GNNs), for which theoretical findings are largely lacking. In this paper, we identify the expressivity of sparse subnetworks, i.e. their ability to distinguish non-isomorphic graphs, as crucial for finding winning tickets that preserve the predictive performance.We establish conditions under which the expressivity of a sparsely initialized GNN matches that of the full network, particularly when compared to the Weisfeiler-Leman test, and in that context put forward and prove a Strong Expressive Lottery Ticket Hypothesis. We subsequently show that an increased expressivity in the initialization potentially accelerates model convergence and improves generalization. Our findings establish novel theoretical foundations for both LTH and GNN research, highlighting the importance of maintaining expressivity in sparsely initialized GNNs. We illustrate our results using examples from drug discovery.","Graph Neural Networks (GNNs) analyze complex relational data, powering breakthroughs in areas like drug discovery and social media analysis. A key idea -- called the Lottery Ticket Hypothesis -- claims that large neural networks contain smaller subnetworks that alone can achieve top performance. We extend this idea to GNNs by identifying the role of ""expressivity,"" the network’s ability to distinguish between different graph structures, in finding these subnetworks. Our theoretical analysis proves that maintaining expressivity is crucial when pruning networks. Empirically, we show expressive subnetworks perform significantly better after training, even with fewer parameters. This not only provides novel insights for theoretical research but also practical guidelines for building efficient, powerful GNNs that reliably differentiate critical structures, such as toxic versus non-toxic molecules."
