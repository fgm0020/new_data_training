type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior,https://ICML.cc//virtual/2025/poster/45491,"Ching-Hua Lee, Chouchang Yang, Jaejin Cho, Yashas Malur Saidutta, Rakshith Sharma Srinivasa, Yilin Shen, Hongxia Jin","Denoising diffusion probabilistic models (DDPMs) can be utilized to recover a clean signal from its degraded observation(s) by conditioning the model on the degraded signal. The degraded signals are themselves contaminated versions of the clean signals; due to this correlation, they may encompass certain useful information about the target clean data distribution. However, existing adoption of the standard Gaussian as the prior distribution in turn discards such information when shaping the prior, resulting in sub-optimal performance. In this paper, we propose to improve conditional DDPMs for signal restoration by leveraging a more informative prior that is jointly learned with the diffusion model. The proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the variational autoencoder (VAE) framework, taking advantage of the correlation between the degraded and clean signals to encode a better diffusion prior. On speech and image restoration tasks, we show that RestoreGrad demonstrates faster convergence (5-10 times fewer training steps) to achieve better quality of restored signals over existing DDPM baselines and improved robustness to using fewer sampling steps in inference time (2-2.5 times fewer), advocating the advantages of leveraging jointly learned prior for efficiency improvements in the diffusion process.","Restoring clean signals—like clear speech or undistorted images—from their contaminated or degraded versions is a long-standing challenge in machine learning and signal processing. While diffusion-based generative models have shown strong potential for this task, they often rely on oversimplified assumptions about the data distribution (such as assuming the prior noise follows a standard Gaussian distribution), which hinders training efficiency and limits restoration quality.Our paper presents RestoreGrad, a novel framework that enhances diffusion-based signal restoration by learning a more informative representation of the latent noise—known as the ""prior""—in tandem with the diffusion model. Unlike previous methods that use fixed or manually designed priors, RestoreGrad automatically learns this prior through a pair of encoder networks, effectively combining the generative strength of diffusion models with the modeling efficiency of variational autoencoders.RestoreGrad speeds up training by up to 10× and requires fewer inference steps, making it more efficient and practical. It improves restoration quality for both speech and images, generalizes well to unseen data, and introduces only minor computational overhead. This makes RestoreGrad a practical and scalable solution for real-world audio and visual enhancement—and potentially other signal restoration problems."
Poster,Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach,https://ICML.cc//virtual/2025/poster/46448,"Jiancong Xiao, Bojian Hou, Zhanliang Wang, Ruochen Jin, Qi Long, Weijie Su, Li Shen","One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuningwith domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.","Large language models (LLMs), like ChatGPT, often predict how confident they are in an answer—but after being trained to follow human preferences, these models can become overconfident, even when they're wrong. This is a serious problem in real-world applications like healthcare or law, where trusting a wrong answer could lead to harmful consequences.Our research investigates why this happens and how to fix it. We discovered that during preference alignment—when a model is trained to generate human-preferred answers—it can lose its ability to judge uncertainty accurately. We then designed a new method called Calibration-Aware Fine-Tuning (CFT) to correct this issue without hurting the model’s overall performance.Our experiments show that CFT dramatically improves calibration, making the model’s confidence better reflect reality, and even boosts accuracy in some cases. This means users can better trust what the model says—and how confident it is—especially in high-stakes scenarios.By restoring this critical property, our work helps make aligned LLMs safer and more reliable."
Poster,Rethink GraphODE Generalization within Coupled Dynamical System,https://ICML.cc//virtual/2025/poster/44104,"Guancheng Wan, Zijie Huang, Wanjia Zhao, Xiao Luo, Yizhou Sun, Wei Wang","Coupled dynamical systems govern essential phenomena across physics, biology, and engineering, where components interact through complex dependencies. While Graph Ordinary Differential Equations (GraphODE) offer a powerful framework to model these systems, their **generalization** capabilities degrade severely under limited observational training data due to two fundamental flaws: (i) the entanglement of static attributes and dynamic states in the initialization process, and (ii) the reliance on context-specific coupling patterns during training, which hinders performance in unseen scenarios. In this paper, we propose a Generalizable GraphODE with disentanglement and regularization (GREAT) to address these challenges. Through systematic analysis via the Structural Causal Model, we identify backdoor paths that undermine generalization and design two key modules to mitigate their effects. The *Dynamic-Static Equilibrium Decoupler (DyStaED)* disentangles static and dynamic states via orthogonal subspace projections, ensuring robust initialization. Furthermore, the *Causal Mediation for Coupled Dynamics (CMCD)* employs variational inference to estimate latent causal factors, reducing spurious correlations and enhancing universal coupling dynamics. Extensive experiments across diverse dynamical systems demonstrate that ours outperforms state-of-the-art methods within both in-distribution and out-of-distribution.","Many phenomena in science and engineering, from interacting particles in physics to networks in biology, involve interconnected parts evolving together over time. We use powerful AI tools called Graph Ordinary Differential Equations (GraphODEs) to model these complex ""coupled dynamical systems."" However, these models often struggle to predict new situations accurately when they've only learned from limited data. We discovered two main reasons: First, the models tend to mix up unchanging properties (like an object's material) with changing states (like its movement). Second, they often learn interaction patterns that are specific only to the training data, rather than the universal underlying laws. To fix this, we developed ""GREAT,"" a new GraphODE framework. Using causal reasoning, we designed two key components: one module carefully separates the static properties from the dynamic states, ensuring a clean start. The other helps the model learn the true, universal rules of interaction, ignoring misleading patterns from the training data. Our experiments show that GREAT significantly outperforms existing methods, especially when predicting unseen scenarios. This work offers a more robust way to understand and forecast the behavior of complex, interconnected systems in the real world."
Poster,Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding,https://ICML.cc//virtual/2025/poster/43612,"Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason Lee, Pan Li, Zhangyang “Atlas” Wang","Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset. To address this, we propose con**T**extualized equivari**A**nt **P**osition **E**ncoding (**TAPE**), a novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving robustness and adaptability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments show that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques.","Transformers (the AI behind ChatGPT) struggle with tasks requiring precise ""positional awareness"" – like solving math problems or finding specific details in long documents. This happens because their current methods for tracking word positions create rigid patterns, like forcing nearby words to always interact more strongly. This limits their ability to handle long texts or adapt to different tasks needing flexible positional understanding.We developed TAPE (conTextualized equivariAnt Position Encoding), a smarter way to encode position. Unlike fixed methods, TAPE dynamically adjusts how it represents positions based on the actual content of the text as it processes it layer by layer. Crucially, it uses mathematical principles (""equivariance"") to ensure these position updates stay stable and maintain correct relationships between words, even when the sequence order changes.TAPE significantly boosts performance on tasks heavily reliant on position, like long addition (21.6% better accuracy than previous best) and retrieving hidden information from very long texts (near-perfect accuracy up to about 6,000 words). It also improves general language modeling for long contexts and can be easily added to existing models like Llama 2 with minimal extra cost. This makes transformers more capable and efficient for complex reasoning and long-context understanding."
Poster,Rethinking Aleatoric and Epistemic Uncertainty,https://ICML.cc//virtual/2025/poster/46057,"Freddie Bickford Smith, Jannik Kossen, Eleanor Trollope, Mark van der Wilk, Adam Foster, Tom Rainforth","The ideas of aleatoric and epistemic uncertainty are widely used to reason about the probabilistic predictions of machine-learning models. We identify incoherence in existing discussions of these ideas and suggest this stems from the aleatoric-epistemic view being insufficiently expressive to capture all the distinct quantities that researchers are interested in. To address this we present a decision-theoretic perspective that relates rigorous notions of uncertainty, predictive performance and statistical dispersion in data. This serves to support clearer thinking as the field moves forward. Additionally we provide insights into popular information-theoretic quantities, showing they can be poor estimators of what they are often purported to measure, while also explaining how they can still be useful in guiding data acquisition.","If a machine-learning model has some predictive uncertainty, we might want to reason about where that uncertainty comes from. A popular thought is that we can break down predictive uncertainty into two parts: an ""aleatoric"" (or ""chance"") part relating to a sense of inherent unpredictability in the world, and an ""epistemic"" (or ""knowledge"") part relating to the model's lack of knowledge. We show that this perspective is too simplistic in the context of machine learning, preventing an appropriately nuanced understanding of key ideas. To address this we revisit foundational ideas from decision theory and use them to provide a new perspective on uncertainty in machine learning. Our hope is that this will help avoid some of the confusions that we believe have been caused by the existing perspective, and in turn improve researchers' ability to understand and design machine-learning methods."
Poster,Rethinking Benign Overfitting in Two-Layer Neural Networks,https://ICML.cc//virtual/2025/poster/45109,"Ruichen Xu, Kexin Chen","Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) revealed a sharp phase transition from benign to harmful overfitting when thenoise-to-feature ratio exceeds a threshold—a situation common in long-tailed data distributions where atypical data is prevalent. However, such harmful overfitting rarely happens in overparameterized neural networks. Further experimental results suggested that memorization is necessary for achieving near-optimal generalization error in long-tailed data distributions (Feldman & Zhang, 2020). We argue that this discrepancy between theoretical predictions and empirical observations arises because previous feature-noise data models overlook the heterogeneous nature of noise across different data classes. In this paper, we refine the feature-noise data model by incorporating class-dependent heterogeneous noise and re-examine the overfitting phenomenon in neural networks. Through a comprehensive analysis of the training dynamics, we establish test loss bounds for the refined model. Our findings reveal that neural networks can leverage ""data noise"" to learn implicit features that improve the classification accuracy for long-tailed data. Our analysis also provides a training-free metric for evaluating data influence on test performance. Experimental validation on both synthetic and real-world datasets supports our theoretical results.","Scientists were puzzled by a contradiction in AI research. Theories predicted that when powerful AI models are trained on datasets with much noise, they should learn the random noise in the data and perform poorly—a problem called ""harmful overfitting."" However, in practice, this rarely happens.The researchers in this study argue that previous theories were missing a key detail: they assumed the ""noise"" in the data is the same for all categories. This paper suggests that the noise is different depending on the data class (e.g., the visual ""noise"" in pictures of rare birds is different from that in common dogs).By creating a more realistic model that includes this varied noise, they discovered that the AI doesn't just ignore the noise; it actually leverages it. The network learns hidden features from what appears to be random data noise, which in turn helps it get better at correctly identifying the rare items.As a practical result, the team also developed a new metric that can evaluate how much a piece of data will influence the AI's performance without needing to go through the entire training process. Their findings were confirmed on both computer-generated and real-world data."
Poster,Rethinking Causal Ranking: A Balanced Perspective on Uplift Model Evaluation,https://ICML.cc//virtual/2025/poster/44364,"Minqin Zhu, Zexu Sun, Ruoxuan Xiong, Anpeng Wu, Baohong Li, Caizhi Tang, JUN ZHOU, Fei Wu, Kun Kuang","Uplift modeling is crucial for identifying individuals likely to respond to a treatment in applications like marketing and customer retention, but evaluating these models is challenging due to the inaccessibility of counterfactual outcomes in real-world settings.In this paper, we identify a fundamental limitation in existing evaluation metrics, such as the uplift and Qini curves, which fail to rank individuals with binary negative outcomes accurately.This can lead to biased evaluations, where biased models receive higher curve values than unbiased ones, resulting in suboptimal model selection.To address this, we propose the Principled Uplift Curve (PUC), a novel evaluation metric that assigns equal curve values of individuals with both positive and negative binary outcomes, offering a more balanced and unbiased assessment. We then derive the Principled Uplift Loss (PUL) function from the PUC and integrate it into a new uplift model, the Principled Treatment and Outcome Network (PTONet), to reduce bias during uplift model training.Experiments on both simulated and real-world datasets demonstrate that the PUC provides less biased evaluations, while PTONet outperforms existing methods. The source code is available at: https://github.com/euzmin/PUC.","In this paper, we fundamentally identify the limitations of conventional evaluation metrics in individual ranking based on causal effects, particularly their inability to accurately rank individuals with binary negative outcomes. This limitation can result in biased evaluations, where models with systematic biases receive higher curve values than unbiased ones, ultimately leading to suboptimal model selection. To address this issue, we propose a novel and effective metric that treats individuals with both positive and negative binary outcomes equally in curve construction. Furthermore, we introduce a strategy for leveraging this metric to guide the optimization of uplift models."
Poster,Rethinking Chain-of-Thought from the Perspective of Self-Training,https://ICML.cc//virtual/2025/poster/44231,"Zongqian Wu, Baoduo Xu, Ruochen Cui, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng","Chain-of-thought (CoT) reasoning has emerged as an effective approach for activating latent capabilities in LLMs. Interestingly, we observe that both CoT reasoning and self-training share the core objective: iteratively leveraging model-generated information to progressively reduce prediction uncertainty. Building on this insight, we propose a novel CoT framework to improve reasoning performance. Our framework integrates two key components: (i) a task-specific prompt module that optimizes the initial reasoning process, and (ii) an adaptive reasoning iteration module that dynamically refines the reasoning process and addresses the limitations of previous CoT approaches, i.e., over-reasoning and high similarity between consecutive reasoning iterations. Extensive experiments show that the proposed method achieves significant advantages in both performance and computational efficiency. Our code is available at: https://github.com/zongqianwu/ST-COT.","Large language models (LLMs), like ChatGPT, have shown impressive abilities in answering complex questions. One popular method to improve their thinking process is called chain-of-thought reasoning, where the model is encouraged to think step-by-step before giving a final answer. In this work, we find that this thinking process is similar to how humans learn by reviewing their own answers and improving over time. Based on this idea, we propose a new approach that helps LLMs think more effectively. It includes a smart way to guide the model at the start and a strategy to improve its thinking with each step."
Poster,Rethinking Confidence Scores and Thresholds in Pseudolabeling-based SSL,https://ICML.cc//virtual/2025/poster/43641,"Harit Vishwakarma, Yi Chen, Satya Sai Srinath Namburi GNVV, Sui Jiet Tay, Ramya Vinayak, Frederic Sala","Modern semi-supervised learning (SSL) methods rely on pseudolabeling and consistency regularization.  Pseudolabeling is typically performed by comparing the model's confidence scores and a predefined threshold. While several heuristics have been proposed to improve threshold selection, the underlying issues of overconfidence and miscalibration in confidence scores remain largely unaddressed, leading to inaccurate pseudolabels, degraded test accuracy, and prolonged training. We take a first-principles approach to learn confidence scores and thresholds with an explicit knob for error. This flexible framework addresses the fundamental question of optimal scores and threshold selection in pseudolabeling. Moreover, it gives practitioners a principled way to control the quality and quantity of pseudolabels. Such control is vital in SSL, where balancing pseudolabel quality and quantity directly affects model performance and training efficiency. Our experiments show that, by integrating this framework with modern SSL methods, we achieve significant improvements in accuracy and training efficiency. In addition, we provide novel insights on the trade-offs between the choices of the error parameter and the end model's performance.","Modern AI systems often learn from a mix of labeled and unlabeled data. A common approach is to let the model guess labels for the unlabeled data, a process called pseudolabeling, and then train on those guesses (pseudolabels). But deciding which pseudolabels to trust is tricky. Most methods rely on the model’s confidence, using a fixed rule: if the confidence is above a certain threshold, accept the guess. Unfortunately, ad hoc choices of confidence scores and thresholds can be unreliable, leading to many wrong guesses and inefficient training.In our work, we take a more principled approach. Instead of using the common choices, we introduce a procedure to obtain better confidence scores and thresholds that reflect how much error you're willing to tolerate while pseudolabeling as many points as possible. This gives users direct control over the trade-off between making more guesses and making better guesses — a key challenge in this type of learning. When added to existing methods, our approach improves both accuracy and provides new insights into how this balance affects final performance."
Poster,Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning,https://ICML.cc//virtual/2025/poster/44213,"Zeyu Gan, Yun Liao, Yong Liu","Test-time scaling, which is also often referred to as *slow-thinking*, has been demonstrated to enhance multi-step reasoning in large language models (LLMs). However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood. This paper explores the mechanisms of external slow-thinking from a theoretical standpoint. We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory. Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability. We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships. Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model's internal reasoning capacity may yield more sustained improvements in the long term. We open-source our code at https://github.com/ZyGan1999/Snowball-Errors-and-Probability.","LLMs often perform better on complex, step-by-step reasoning tasks when using techniques known as test-time scaling or ""slow-thinking."" These methods are widely used because they boost accuracy, but the fundamental reason why they are effective has been poorly understood.To uncover why slow-thinking, especially those without extra training (we call external slow-thinking), works, our research delves into the underlying mechanics. We introduced and analyzed the concept of a ""snowball error effect,"" where small initial mistakes can accumulate and grow during the LLM's reasoning process. Using information theory, we show that external slow-thinking methods can be understood as strategies that effectively reduce the probability of these errors escalating. We also compared several popular slow-thinking approaches based on this understanding.This work provides a theoretical explanation for how making LLMs ""think slower"" improves their reasoning. Our findings suggest that the specific technique used might be less critical. For more significant long-term gains in LLM reasoning, focusing on expanding the LLM's search for solutions or enhancing its core internal reasoning capacity might be more fruitful directions."
