type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Value-Based Deep RL Scales Predictably,https://ICML.cc//virtual/2025/poster/45893,"Oleh Rybkin, Michal Nauman, Preston Fu, Charlie Snell, Pieter Abbeel, Sergey Levine, Aviral Kumar","Scaling data and compute is critical in modern machine learning. However, scaling also demands _predictability_: we want methods to not only perform well with more compute or data, but also have their performance be predictable from low compute or low data runs, without ever running the large-scale experiment. In this paper, we show predictability of value-based off-policy deep RL. First, we show that data and compute requirements to reach a given performance level lie on a _Pareto frontier_, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can extrapolate data requirements into a higher compute regime, and  compute requirements into a higher data regime. Second, we determine the optimal allocation of total _budget_ across data and compute to obtain given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling behavior is enabled by first estimating predictable relationships between different _hyperparameters_, which is used to counteract effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance.","A reinforcement learning agent is an AI that takes actions and makes decisions. Robots and 'thinking' LLMs are reinforcement learning agents. However, these are trained using expensive techniques relying on 'policy gradients', whereas in this paper we study scaling value-based RL that could make these more efficient and versatile. We study how to train large-scale value-based reinforcement learning agents by providing rules on how to select the amount of resources spent, such as data and compute. We observe that such rules are possible to establish with cheap experiments, which improves the performance of the larger scale, more expensive experiment."
Poster,Variance as a Catalyst: Efficient and Transferable Semantic Erasure Adversarial Attack for Customized Diffusion Models,https://ICML.cc//virtual/2025/poster/45874,"Jiachen Yang, Yusong Wang, Yanmei Fang, Yunshu Dai, Fangjun Huang","Latent Diffusion Models (LDMs) enable fine-tuning with only a few images and have become widely used on the Internet. However, it can also be misused to generate fake images, leading to privacy violations and social risks. Existing adversarial attack methods primarily introduce noise distortions to generated images but fail to completely erase identity semantics. In this work, we identify the variance of VAE latent code as a key factor that influences image distortion. Specifically, larger variances result in stronger distortions and ultimately erase semantic information. Based on this finding, we propose a Laplace-based (LA) loss function that optimizes along the fastest variance growth direction, ensuring each optimization step is locally optimal. Additionally, we analyze the limitations of existing methods and reveal that their loss functions often fail to align gradient signs with the direction of variance growth. They also struggle to ensure efficient optimization under different variance distributions. To address these issues, we further propose a novel Lagrange Entropy-based (LE) loss function.Experimental results demonstrate that our methods achieve state-of-the-art performance on CelebA-HQ and VGGFace2. Both proposed loss functions effectively lead diffusion models to generate pure-noise images with identity semantics completely erased. Furthermore, our methods exhibit strong transferability across diverse models and efficiently complete attacks with minimal computational resources. Our work provides a practical and efficient solution for privacy protection.","Modern AI models can recreate a person’s face using just a few photos, raising serious privacy concerns. These models may be misused to generate fake images that resemble real people. We asked how to prevent the model from learning and replicating someone's identity in the first place. Our approach adds carefully crafted noise to the input image. These small changes are almost invisible to the human eye but are designed to disrupt how the model learns. By breaking the connection between the input image and the generated output, the AI model fails to capture identity-related features. As a result, it produces random, meaningless noise instead of a recognizable face. This method is fast, effective, and works across different AI models. It offers a practical way to protect individuals from the unauthorized use of their images and can also help artists defend their work from being copied by AI. Our work supports safer and more responsible use of generative AI technologies."
Poster,Variance-Reduced Forward-Reflected-Backward Splitting Methods for Nonmonotone Generalized Equations,https://ICML.cc//virtual/2025/poster/43579,Quoc Tran-Dinh,"We develop two novel stochastic variance-reduction methods to approximate solutions of a class of nonmonotone [generalized] equations. Our algorithms leverage a new combination of ideas from the forward-reflected-backward splitting method and a class of unbiased variance-reduced estimators. We construct two new stochastic estimators within this class, inspired by the well-known SVRG and SAGA estimators. These estimators significantly differ from existing approaches used in minimax and variational inequality problems. By appropriately choosing parameters, both algorithms achieve state-of-the-art oracle complexity of $\mathcal{O}(n + n^{2/3} \epsilon^{-2})$ for obtaining an $\epsilon$-solution in terms of the operator residual norm for a class of nonmonotone problems, where $n$ is the number of  summands and $\epsilon$ signifies the desired accuracy. This complexity aligns with the best-known results in SVRG and SAGA methods for stochastic nonconvex optimization.  We test our algorithms on some numerical examples and compare them with existing methods. The results demonstrate promising improvements offered by the new methods compared to their competitors.","We develop two new variance-reduced algorithms based on the forward-reflected-backward splitting method to tackle a class of nonmonotone root-finding problems. These methods encompass both SVRG and SAGA estimators as special cases. By carefully selecting the parameters, our algorithms achieve the state-of-the-art oracle complexity for attaining an $\epsilon$-solution, matching the state-of-the-art complexity bounds observed in nonconvex optimization methods using SVRG and SAGA. While the first scheme resembles a stochastic variant of the optimistic gradient method, the second one is entirely novel and distinct from existing approaches, even their deterministic counterparts. We validate our methods through numerical examples, and the results demonstrate promising performance compared to existing techniques under careful parameter selections."
Poster,Variational Control for Guidance in Diffusion Models,https://ICML.cc//virtual/2025/poster/44885,"Kushagra Pandey, Farrin Marouf Sofian, Felix Draxler, Theofanis Karaletsos, Stephan Mandt","Diffusion models exhibit excellent sample quality, but existing guidance methods often require additional model training or are limited to specific tasks. We revisit guidance in diffusion models from the perspective of variational inference and control, introducing \emph{Diffusion Trajectory Matching (DTM)} that enables guiding pretrained diffusion trajectories to satisfy a terminal cost. DTM unifies a broad class of guidance methods and enables novel instantiations. We introduce a new method within this framework that achieves state-of-the-art results on several linear, non-linear, and blind inverse problems without requiring additional model training or specificity to pixel or latent space diffusion models. Our code will be available at https://github.com/czi-ai/oc-guidance.","Diffusion models power most state-of-the-art breakthroughs in text-to-image and video synthesis. However, in some applications, the user may be interested in generating samples that adhere to some external constraints. For instance, the user may be interested in generating samples that borrow style from a reference image. Additionally, since training a new model for each such conditional task can be cumbersome, the goal is to leverage existing large-scale pretrained diffusion models as powerful priors for such downstream tasks. In this work, we propose a method to achieve this goal using ideas from optimal control and Bayesian inference."
Poster,Variational Counterfactual Intervention Planning to Achieve Target Outcomes,https://ICML.cc//virtual/2025/poster/44461,"Xin Wang, Shengfei Lyu, Luo Chi, Xiren Zhou, Huanhuan Chen","A key challenge in personalized healthcare is identifying optimal intervention sequences to guide temporal systems toward target outcomes, a novel problem we formalize as counterfactual target achievement. In addressing this problem, directly adopting counterfactual estimation methods face compounding errors due to the unobservability of counterfactuals. To overcome this, we propose Variational Counterfactual Intervention Planning (VCIP), which reformulates the problem by modeling the conditional likelihood of achieving target outcomes, implemented through variational inference. By leveraging the g-formula to bridge the gap between interventional and observational log-likelihoods, VCIP enables reliable training from observational data. Experiments on both synthetic and real-world datasets show that VCIP significantly outperforms existing methods in target achievement accuracy.",We introduce a novel counterfactual target achievement problem in temporal systems and propose VCIP to find optimal interventions by modeling achievement probability.
Poster,Variational Learning of Fractional Posteriors,https://ICML.cc//virtual/2025/poster/45695,"Kian Ming Chai, Edwin V. Bonilla","We introduce a novel one-parameter variational objective that lower bounds the data evidence and enables the estimation of approximate fractional posteriors. We extend this framework to hierarchical construction and Bayes posteriors, offering a versatile tool for probabilistic modelling. We demonstrate two cases where gradients can be obtained analytically and a simulation study on mixture models showing that our fractional posteriors can be used to achieve better calibration compared to posteriors from the conventional variational bound. When applied to variational autoencoders (VAEs), our approach attains higher evidence bounds and enables learning of high-performing approximate Bayes posteriors jointly with fractional posteriors. We show that VAEs trained with fractional posteriors produce decoders that are better aligned for generation from the prior.","We need more tools to approximate solutions for problems that are too hard to solve exactly in machine learning and statistical inference.This paper expands the space of possible approximate solutions to include a particular class of solutions, beyond what is currently available.This gives more flexibility for machine learning and statistical inference."
Poster,Variational Phylogenetic Inference with Products over Bipartitions,https://ICML.cc//virtual/2025/poster/43853,"Evan Sidrow, Alexandre Bouchard-Côté, Lloyd Elliott","Bayesian phylogenetics is vital for understanding evolutionary dynamics, and requires accurate and efficient approximation of posterior distributions over trees. In this work, we develop a variational Bayesian approach for ultrametric phylogenetic trees. We present a novel variational family based on coalescent times of a single-linkage clustering and derive a closed-form density for the resulting distribution over trees. Unlike existing methods for ultrametric trees, our method performs inference over all of tree space, it does not require any Markov chain Monte Carlo subroutines, and our variational family is differentiable. Through experiments on benchmark genomic datasets and an application to the viral RNA of SARS-CoV-2, we demonstrate that our method achieves competitive accuracy while requiring significantly fewer gradient evaluations than existing state-of-the-art techniques.","Understanding how species evolve over time often involves building evolutionary trees, or phylogenies, which show how different organisms are related. To do this accurately, scientists use statistical methods to estimate the most likely shapes of these trees based on genetic data. One popular but complex method is called Bayesian phylogenetics, which typically relies on slow and computationally intensive techniques.In recent years, researchers have developed faster alternatives using a method called variational inference, which approximates the range of possible tree shapes without relying on traditional, slower simulation methods. However, many of these existing approaches are still quite complex or limited in scope.In this study, we introduce a simpler and more efficient variational inference method for estimating evolutionary trees. Our technique models the timing of how species split from common ancestors and smoothly explores all possible tree shapes. It avoids the need for complicated sampling steps and can be easily optimized using modern tools.When tested on real-world genetic data—including data from the virus that causes COVID-19—the method achieved comparable accuracy to leading tools, while requiring much less computing power. This makes evolutionary analysis quicker and more accessible for researchers working with large genetic datasets."
Poster,Variational Rectified Flow Matching,https://ICML.cc//virtual/2025/poster/45265,"Pengsheng Guo, Alex Schwing","We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. In contrast,  variational rectified flow matching learns and samples from multi-modal flow directions. We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results.","(1) At the heart of recent advances in diffusion and flow matching methods lies a major assumption: Existing models assume a single deterministic velocity at each point in the data-time space. (2) With Variational Rectified Flow Matching (VRFM), we introduce a new framework that models the multi-modal distribution over velocity directions, unlocking a richer class of generative models. (3) VRFM is a shift in how we think about learning continuous dynamics. We demonstrate consistent, strong performance across datasets (Synthetic, MNIST, CIFAR-10, ImageNet) and architectures (UNet, DiT). This opens the door to more expressive, uncertainty-aware generative modeling."
Poster,VCT: Training Consistency Models with Variational Noise Coupling,https://ICML.cc//virtual/2025/poster/46068,"Gianluigi Silvestri, Luca Ambrogioni, Chieh-Hsin Lai, Yuhta Takida, Yuki Mitsufuji","Consistency Training (CT) has recently emerged as a strong alternative to diffusion models for image generation. However, non-distillation CT often suffers from high variance and instability, motivating ongoing research into its training dynamics. We propose Variational Consistency Training (VCT), a flexible and effective framework compatible with various forward kernels, including those in flow matching. Its key innovation is a learned noise-data coupling scheme inspired by Variational Autoencoders, where a data-dependent encoder models noise emission. This enables VCT to adaptively learn noise-to-data pairings, reducing training variance relative to the fixed, unsorted pairings in classical CT. Experiments on multiple image datasets demonstrate significant improvements: our method surpasses baselines, achieves state-of-the-art FID among non-distillation CT approaches on CIFAR-10, and matches SoTA performance on ImageNet 64x64 with only two sampling steps. Code is available at https://github.com/sony/vct.","Consistency models learn to generate data in one or few sampling steps, but training them from scratch can be both unstable and slow. In traditional approaches, each data point is paired with randomly sampled noise during training. This strategy, while principled, contributes to the high training variance and can lead to suboptimal results.To address this, we introduce Variational Consistency Training (VCT), which shares similarities with Variational Autoencoders. Instead of using fixed Gaussian noise, VCT adds a small encoder that learns a data-dependent distribution over the noise. By letting the model itself learn what noise to inject, learning becomes smoother and more stable, because the model receives a better training signal.In practice, VCT improved results over equivalent baselines with minimal extra cost, achieving SOTA FID on 2-step CIFAR-10, and  competitive performance on class-conditional ImageNet at 64×64. Crucially, the encoder adds only a small overhead to training time and leaves one-step sampling speed unchanged, making VCT a simple yet powerful upgrade for fast, high-quality generation."
Poster,Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision,https://ICML.cc//virtual/2025/poster/43574,"Marco Cipriano, Moritz Feuerpfeil, Gerard de Melo","Scalable Vector Graphics (SVG) is a popular format on the web and in the design industry. However, despite the great strides made in generative modeling, SVG has remained underexplored due to the discrete and complex nature of such data. We introduce GRIMOIRE, a text-guided SVG generative model that is comprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster images onto a discrete codebook by reconstructing them as vector shapes, and an Auto-Regressive Transformer (ART) models the joint probability distribution over shape tokens, positions and textual descriptions, allowing us to generate vector graphics from natural language. Unlike existing models that require direct supervision from SVG data, GRIMOIRE learns shape image patches using only raster image supervision which opens up vector generative modeling to significantly more data. We demonstrate the effectiveness of our method by fitting GRIMOIRE for closed filled shapes on the MNIST and Emoji, and for outline strokes on icon and font data, surpassing previous image-supervised methods in generative quality and vector-supervised approach in flexibility.","Grimoire is a new AI model that can create vector drawings like icons or simple illustrations from text descriptions. Vector graphics is the image format used in design and web applications, but teaching computers to make them is challenging because of their complex format. Unlike many existing methods, Grimoire does not need data in vector format to learn from. Instead, we train the model using regular raster images, which are far more common. First, the model learns to reconstruct simple parts of the images as vector data after mapping them into a discrete codebook; then, given a text description, the model learns to predict the correct sequence and position of codes to generate new SVGs. This approach lets Grimoire be very flexible in supporting new SVG attributes while outperforming previous raster supervision methods. We tested our model on handwritten numbers, emojis, and icons, showing it can handle solid shapes and outlines."
