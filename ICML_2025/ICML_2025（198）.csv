type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,NExtLong: Toward Effective Long-Context Training without Long Documents,https://ICML.cc//virtual/2025/poster/45351,"Chaochen Gao, Xing W, Zijia Lin, Debing Zhang, Songlin Hu","Large language models (LLMs) with extended context windows have made significant strides yet remain a challenge due to the scarcity of long documents. Existing methods tend to synthesize long-context data but lack a clear mechanism to reinforce the long-range dependency modeling. To address this limitation, we propose NExtLong, a novel framework for synthesizing long-context data through Negative document Extension. NExtLong decomposes a document into multiple meta-chunks and extends the context by interleaving hard negative distractors retrieved from pretraining corpora. This approach compels the model to discriminate long-range dependent context from distracting content, enhancing its ability to model long-range dependencies. Extensive experiments demonstrate that NExtLong achieves significant performance improvements on the HELMET and RULER benchmarks compared to existing long-context synthesis approaches and leading models, which are trained on non-synthetic long documents. These findings highlight NExtLong's ability to reduce reliance on non-synthetic long documents, making it an effective framework for developing advanced long-context LLMs.","Most large language models struggle to understand information spread across very long documents, partly because high-quality training data of that length is rare. Existing methods try to create longer inputs by simply stitching together short texts, but this often leads to irrelevant or incoherent content. To solve this, we introduce NExtLong, a new way to train models on longer contexts. Instead of just concatenating texts, we deliberately insert carefully chosen ""distracting"" content between important parts of a document. This forces the model to learn how to identify and focus on meaningful information—even when it's buried in noise. Our method mimics the challenges real long documents present, helping models better handle long-range dependencies. In testing, NExtLong significantly outperformed previous approaches on multiple long-document tasks. This suggests it could help train more capable language models without needing massive amounts of real long-form data."
Poster,NICE Data Selection for Instruction Tuning in LLMs with Non-differentiable Evaluation Metric,https://ICML.cc//virtual/2025/poster/46560,"Jingtan Wang, Xiaoqiang Lin, Rui Qiao, Pang Wei Koh, Chuan-Sheng Foo, Bryan Kian Hsiang Low","Curating data for instruction tuning is crucial for enhancing the performance of large language models (LLMs). This work aims to select training data for instruction tuning to improve the LLM performance on specific tasks. Existing methods often rely on next-token prediction (NTP) loss as a proxy for target task performance due to the non-differentiable nature of performance evaluation metrics. They select training data points that are most helpful in reducing validation loss. However, there is a discrepancy between minimizing NTP loss and maximizing performance (e.g., code pass rate in code generation). To remedy this, we introduce a novel Non-differentiable evaluation metric-based InfluenCe Estimation (NICE), which leverages the policy gradient to select the training data that improves the performance. Moreover, NICE can perform data selection in the absence of labels (ground-truth responses) when the evaluation metrics do not require labels (e.g., a reward model can output reward scores without supervision from labels). Experimental results show that our approach outperforms existing data selection baselines that use NTP loss in diverse and realistic scenarios. Notably, subsets selected by NICE often produce models that outperform those trained on the full dataset. Our code is available at https://github.com/JTWang2000/NICE.","Training large language models (LLMs) to follow instructions well depends on selecting the right training data. Existing methods often rely on loss functions like next-token prediction loss (how well a model predicts next tokens). However, loss is not necessarily strongly correlated with performance on real-world tasks, like writing working code.We introduce a new method called NICE that selects training data based on how much it improves actual task performance, measured by the evaluation metrics. It uses policy gradients (from reinforcement learning) to estimate which examples are most helpful for improving the performance instead of the loss. Unlike prior methods, NICE can even work when labeled data isn't available, as long as the evaluation metric doesn’t need ground-truth labels.Experiments show NICE consistently outperforms existing approaches, often producing better models with less data, showing that smart data selection matters."
Poster,NMA-tune: Generating Highly Designable and Dynamics Aware Protein Backbones,https://ICML.cc//virtual/2025/poster/46578,"Urszula Julia Komorowska, Francisco Vargas, Alessandro Rondina, Pietro Lió, Mateja Jamnik","Protein's backbone flexibility is a crucial property that heavily influences its functionality. Recent work in the field of protein diffusion probabilistic modelling has leveraged Normal Mode Analysis (NMA) and, for the first time, introduced information about large scale protein motion into the generative process. However, obtaining molecules with both the desired dynamics and designable quality has proven challenging. In this work, we present NMA-tune, a new method that introduces the dynamics information to the protein design stage. NMA-tune uses a trainable component to condition the backbone generation on the lowest normal mode of oscillation. We implement NMA-tune as a plug-and-play extension to RFdiffusion, show that the proportion of samples with high quality structure and the desired dynamics is improved as compared to other methods without the trainable component, and we show the presence of the targeted modes in the Molecular Dynamics simulations.","One of the most promising directions for Generative AI applications in medicine is designing novel proteins. Those biomolecules are essential for many processes happening in nature, including processes within human bodies. Proteins' flexibility is a crucial property that heavily influences their exact functionality. In many cases the collective motions of proteins' building blocks can be explained by the Normal Mode Analysis (NMA), which is the theory of harmonic oscillations within a protein. Recent work in the field leveraged NMA to introduce information about large scale protein motion into the design process, which was the first step towards dynamics-informed design. However, obtaining molecules with both the desired dynamics and high structure quality that obeys physical constraints has proven challenging. In this work, we present NMA-tune, a new method that introduces the dynamics information to the protein design stage. NMA-tune uses a trainable component (a deep neural network) in order to condition the backbone generation on the lowest normal mode of oscillation. We implement NMA-tune as a plug-and-play extension to well-know generative model that was shown to generate realistic protein structures. We show that the proportion of samples with high quality structure and the desired dynamics is improved as compared to other methods without the trainable component, and we demostrate the presence of the targeted modes in the high quality, physics-based dynamics computer simulations."
Poster,No Free Lunch from Random Feature Ensembles: Scaling Laws and Near-Optimality Conditions,https://ICML.cc//virtual/2025/poster/43492,"Benjamin Ruben, William Tong, Hamza Chaudhry, Cengiz Pehlevan","Given a fixed budget for total model size, one must choose between training a single large model or combining the predictions of multiple smaller models. We investigate this trade-off for ensembles of random-feature ridge regression models in both the overparameterized and underparameterized regimes. Using deterministic equivalent risk estimates, we prove that when a fixed number of parameters is distributed among $K$ independently trained models, the ridge-optimized test risk increases with $K$.Consequently, a single large model achieves optimal performance.  We then ask when ensembles can achieve *near*-optimal performance.In the overparameterized regime, we show that, to leading order, the test error depends on ensemble size and model size only through the total feature count, so that overparameterized ensembles consistently achieve near-optimal performance.To understand underparameterized ensembles, we derive scaling laws for the test risk as a function of total parameter count when the ensemble size and parameters per ensemble member are jointly scaled according to a ``growth exponent'' $\ell$. While the optimal error scaling is always achieved by increasing model size with a fixed ensemble size, our analysis identifies conditions on the kernel and task eigenstructure under which near-optimal scaling laws can be obtained by joint scaling of ensemble size and model size.","Training large machine-learning models is costly and time-consuming, so practitioners often train several smaller models and average their predictions. This method, known as “ensembling,” lets computations run in parallel across many machines—but is it always the best choice? We explore that question in a clean test bed where each model is just a collection of random features followed by linear regression. In this setting we prove a clear rule: if the total feature budget is fixed, the lowest possible error comes from putting all features into one model rather than spreading them across an ensemble.Next we ask when an ensemble can perform almost as well as a single larger model. The key is how model size compares to the size of the dataset. If each model already has more features than there are training examples, then averaging several such models works nearly as well as simply enlarging a single model. But when data outnumber features—as in modern language models trained on a web-scale corpus—individual model size becomes the limiting factor to performance. In this setting, researchers track progress by how accuracy improves as models grow, or ``scale.’’  For ensembles, scaling up their size can mean adding more ensemble members, adding more features to each ensemble member, or a combination of both.  We show that even in this data-rich regime, ensembles can keep pace with a single larger model, provided the learning task is sufficiently easy and every member widens quickly enough as total size increases."
Poster,Noise Conditional Variational Score Distillation,https://ICML.cc//virtual/2025/poster/45111,"Xinyu Peng, Ziyang Zheng, Yaoming Wang, Han Li, Nuowen Kan, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong","We propose Noise Conditional Variational Score Distillation (NCVSD), a novel method for distilling pretrained diffusion models into generative denoisers. We achieve this by revealing that the unconditional score function implicitly characterizes the score function of denoising posterior distributions. By integrating this insight into the Variational Score Distillation (VSD) framework, we enable scalable learning of generative denoisers capable of approximating samples from the denoising posterior distribution across a wide range of noise levels. The proposed generative denoisers exhibit desirable properties that allow fast generation while preserve the benefit of iterative refinement: (1) fast one-step generation through sampling from pure Gaussian noise at high noise levels; (2) improved sample quality by scaling the test-time compute with multi-step sampling; and (3) zero-shot probabilistic inference for flexible and controllable sampling. We evaluate NCVSD through extensive experiments, including class-conditional image generation and inverse problem solving. By scaling the test-time compute, our method outperforms teacher diffusion models and is on par with consistency models of larger sizes. Additionally, with significantly fewer NFEs than diffusion-based methods, we achieve record-breaking LPIPS on inverse problems.","Generating realistic images and solving image-related problems with artificial intelligence often uses ""diffusion models."" These models are powerful but can be slow, requiring many steps to create a clear image from noise. While some methods speed this up, they often lose the ability to refine images, impacting quality and flexibility.Our paper presents Noise Conditional Variational Score Distillation (NCVSD) to make these models faster and more versatile. NCVSD efficiently condenses large diffusion models into smaller, quicker ""generative denoisers."" This works by recognizing that the model's core mathematical function implicitly guides how to clean up noisy images.NCVSD offers several advantages: it generates high-quality images in a single step, significantly speeding up the process. Unlike other fast methods, it still allows for multiple refinement steps to improve quality, giving users control over speed versus precision. Additionally, NCVSD is flexible, enabling tasks like deblurring or completing images without specific retraining. Experiments show NCVSD performs on par with or better than existing models, with reduced computational steps."
Poster,Noise-Guided Predicate Representation Extraction and Diffusion-Enhanced Discretization for Scene Graph Generation,https://ICML.cc//virtual/2025/poster/43766,"Guoqing Zhang, Shichao Kan, Fanghui Zhang, Wanru Xu, Yue Zhang, Yigang Cen","Scene Graph Generation (SGG) is a fundamental task in visual understanding, aimed at providing more precise local detail comprehension for downstream applications. Existing SGG methods often overlook the diversity of predicate representations and the consistency among similar predicates when dealing with long-tail distributions. As a result, the model's decision layer fails to effectively capture details from the tail end, leading to biased predictions. To address this, we propose a Noise-Guided Predicate Representation Extraction and Diffusion-Enhanced Discretization (NoDIS) method. On the one hand, expanding the predicate representation space enhances the model's ability to learn both common and rare predicates, thus reducing prediction bias caused by data scarcity. We propose a conditional diffusion model to reconstructs features and increase the diversity of representations for same category predicates. On the other hand, independent predicate representations in the decision phase increase the learning complexity of the decision layer, making accurate predictions more challenging. To address this issue, we introduce a discretization mapper that learns consistent representations among similar predicates, reducing the learning difficulty and decision ambiguity in the decision layer. To validate the effectiveness of our method, we integrate NoDIS with various SGG baseline models and conduct experiments on multiple datasets. The results consistently demonstrate superior performance.","To address the widely existing issue of data imbalance (long-tailed distribution), we propose a feature enhancement method based on diffusion models and discretization mapping. This approach leverages the generative capability of diffusion models to perform online feature augmentation, while the discretization mapping aggregates representations of semantically similar predicates to alleviate decision-level pressure. When applied to the scene graph generation task, our method effectively mitigates biased predictions caused by long-tailed distributions and achieves strong performance across multiple datasets."
Poster,Noisy SIGNSGD Is More Differentially Private Than You (Might) Think,https://ICML.cc//virtual/2025/poster/43769,"Richeng Jin, Huaiyu (David) Dai","The prevalent distributed machine learning paradigm faces two critical challenges: communication efficiency and data privacy. SIGNSGD provides a simple-to-implement approach with improved communication efficiency by requiring workers to share only the signs of the gradients. However, it fails to converge in the presence of data heterogeneity, and a simple fix is to add Gaussian noise before taking the signs, which leads to the Noisy SIGNSGD algorithm that enjoys competitive performance while significantly reducing the communication overhead. Existing results suggest that Noisy SIGNSGD with additive Gaussian noise has the same privacy guarantee as classic DP-SGD due to the post-processing property of differential privacy, and logistic noise may be a good alternative to Gaussian noise when combined with the sign-based compressor. Nonetheless, discarding the magnitudes in Noisy SIGNSGD leads to information loss, which may intuitively amplify privacy. In this paper, we make this intuition rigorous and quantify the privacy amplification of the sign-based compressor. Particularly, we analytically show that Gaussian noise leads to a smaller estimation error than logistic noise when combined with the sign-based compressor and may be more suitable for distributed learning with heterogeneous data. Then, we further establish the convergence of Noisy SIGNSGD. Finally, extensive experiments are conducted to validate the theoretical results.","Training AI models across many devices faces two big hurdles: 1) Sending huge amounts of data back and forth is slow and expensive, and 2) keeping each user's private data safe is crucial. SIGNSGD tackles the communication problem by having devices share only whether the model adjustments (gradients) are positive or negative (the ""sign"") and reduces the communication overhead. However, it struggles when the data on different devices varies too much. To fix this convergence problem, existing works added Gaussian noise to the adjustments before taking the sign, which leads to Noisy SIGNSGD. While adding Gaussian noise is the de facto method for differential privacy, we theoretically show in this paper that discarding the actual size of the adjustments (the ""magnitude"") might boost privacy protection even further than expected."
Poster,NoLiMa: Long-Context Evaluation Beyond Literal Matching,https://ICML.cc//virtual/2025/poster/46685,"Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A Rossi, Seunghyun Yoon, Hinrich Schuetze","Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a ""needle"" (relevant information) from a ""haystack"" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 13 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 11 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. Even models enhanced with reasoning capabilities or CoT prompting struggle to maintain performance in long contexts.We publicly release the dataset and evaluation code at https://github.com/adobe-research/NoLiMa.","Large language models (LLMs) are increasingly developed to handle very long documents — ranging from articles to entire books. A common way to evaluate this capability is by adding a key piece of information (a ""needle"") within a large amount of unrelated text (a ""haystack""), then asking the model to retrieve it. However, in many existing tests, the needle closely resembles the question, allowing models to succeed through simple pattern matching rather than true understanding.To address this, we introduce NoLiMa, a new benchmark that makes the task more challenging by minimizing the overlap in wording between the question and the relevant information. This requires models to go beyond surface-level matching and instead identify deeper, more abstract connections.We tested 13 well-known language models that advertise support for long inputs. While they perform strongly on short texts, we observe a substantial decline in accuracy as the context length increases. Even state-of-the-art models like GPT-4o show significant drops when literal clues are removed. These findings highlight current limitations in long-context reasoning and point to the need for further research."
Poster,No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets,https://ICML.cc//virtual/2025/poster/44954,"Corinna Coupette, Jeremy Wayland, Emily Simons, Bastian Rieck","Benchmark datasets have proved pivotal to the success of graph learning, and *good* benchmark datasets are crucial to guide the development of the field. Recent research has highlighted problems with graph-learning datasets and benchmarking practices—revealing, for example, that methods which ignore the graph structure can outperform graph-based approaches. Such findings raise two questions: (1) What makes a good graph-learning dataset, and (2) how can we evaluate dataset quality in graph learning? Our work addresses these questions. As the classic evaluation setup uses datasets to evaluate models, it does not apply to dataset evaluation. Hence, we start from first principles. Observing that graph-learning datasets uniquely combine two modes—graph structure and node features—, we introduce RINGS, a flexible and extensible *mode-perturbation framework* to assess the quality of graph-learning datasets based on *dataset ablations*—i.e., quantifying differences between the original dataset and its perturbed representations. Within this framework, we propose two measures—*performance separability* and *mode complementarity*—as evaluation tools, each assessing the capacity of a graph dataset to benchmark the power and efficacy of graph-learning methods from a distinct angle. We demonstrate the utility of our framework for dataset evaluation via extensive experiments on graph-level tasks and derive actionable recommendations for improving the evaluation of graph-learning methods. Our work opens new research directions in data-centric graph learning, and it constitutes a step toward the systematic *evaluation of evaluations*.","To assess the quality of new machine-learning *models*, researchers typically evaluate the performance of these models on a number of standard datasets. But how do we know that the *datasets* used for evaluation are any good? Our work addresses this question, focusing on a setting in which the data points are *graphs* and our task is to predict some of their graph-level properties (""graph learning""). In this setting, two types of information can be exploited, and a good graph-learning dataset should require both of them to be considered in solving a given learning task. We introduce a framework for measuring the extent to which this desirable property holds, finding that many popular datasets fail to meet our new quality standard. Our work highlights problems with established evaluation practices in (graph) machine learning, and it provides tools to improve these practices. Thus, we contribute to the development of more reliable machine-learning models."
Poster,Non-Asymptotic and Non-Lipschitzian Bounds on Optimal Values in Stochastic Optimization Under Heavy Tails,https://ICML.cc//virtual/2025/poster/44978,"Jindong Tong, Hongcheng Liu, Johannes Royset","This paper focuses on non-asymptotic confidence bounds (CB) for the optimal values of stochastic optimization (SO) problems. Existing approaches often rely on two conditions that may be restrictive: The need for a global Lipschitz constant and the assumption of light-tailed distributions. Beyond either of the conditions, it remains largely unknown whether computable CBs can be constructed. In view of this literature gap, we provide three key findings below: (i) Based on the conventional formulation of sample average approximation (SAA), we derive non-Lipschitzian CBs for convex SP problems under heavy tails. (ii) We explore diametrical risk minimization (DRM)---a recently introduced modification to SAA---and attain non-Lipschitzian CBs for nonconvex SP problems in light-tailed settings. (iii) We extend our analysis of DRM to handle heavy-tailed randomness by utilizing properties in formulations for training over-parameterized classification models.","Training machine learning models often involves solving stochastic optimization problems based on noisy or unpredictable data. But how confident can we be in how well the model will perform in the real world? Most existing results for estimating this confidence rely on strong assumptions about randomness and data behavior.Our work develops new ways to estimate confidence without these strict assumptions. By building on traditional methods and their robust extensions, we proposed better estimations of this confidence even when the data contains many outliers or extreme values.This allows machine learning decisions to be validated more reliably in real-world applications, especially when dealing with volatile or unpredictable data."
