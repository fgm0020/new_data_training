type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Score-of-Mixture Training: One-Step Generative Model Training Made Simple via Score Estimation of Mixture Distributions,https://ICML.cc//virtual/2025/poster/43459,"Tejas Jayashankar, Jongha (Jon) Ryu, Gregory Wornell","We propose *Score-of-Mixture Training* (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the$\alpha$-skew Jensen–Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels.Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call *Score-of-Mixture Distillation* (SMD).It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64×64 show that SMT/SMD are competitive with and can even outperform existing methods.","Generative modeling enables the exploration of the statistical structures inherent in data by learning to produce rich, diverse, and realistic samples. In this paper, we develop a method for efficient one-step generative modeling, where high-quality samples are produced in a single model execution.Recently diffusion models have become popular for generation, but they require many iterative steps to transform noise into structure. Recent efforts to enable one-step generation typically rely on distilling such pre-trained diffusion models, an approach that is computationally expensive. Alternatives that train one-step models from scratch often suffer from instability or expensive simulation.We show that one-step generative models can be trained from scratch without costly pre-training or distillation. Our method centers on learning a model that estimates the gradient of the mixture distribution of real and generated data. Inspired by advances in diffusion modeling, we introduce a novel, stable, and efficient training scheme for one-step generation that is purely based on ensuring distributional overlap between real and generated samples using distribution matching principles from information theory."
Poster,scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data,https://ICML.cc//virtual/2025/poster/44286,"Olga Ovcharenko, Florian Barkmann, Philip Toma, Imant Daunhawer, Julia Vogt, Sebastian Schelter, Valentina Boeva","Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL methods. Our evaluation spans nine datasets and focuses on three common downstream tasks: batch correction, cell type annotation, and missing modality prediction. Furthermore, we systematically assess various data augmentation strategies. Our analysis reveals task-specific trade-offs: the specialized single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at uni-modal batch correction, while generic SSL methods, such as VICReg and SimCLR, demonstrate superior performance in cell typing and multi-modal data integration. Random masking emerges as the most effective augmentation technique across all tasks, surpassing domain-specific augmentations. Notably, our results indicate the need for a specialized single-cell multi-modal data integration framework. scSSL-Bench provides a standardized evaluation platform and concrete recommendations for applying SSL to single-cell analysis, advancing the convergence of deep learning and single-cell genomics.","Scientists today study individual cells in unprecedented detail, but analyzing this complex data requires sophisticated methods. Researchers have struggled to identify which methods work best for different purposes/tasks. This study created a comprehensive benchmark, scSSL-Bench, to compare nineteen different ML methods across nine datasets containing different modalities of cell data and focus on three key tasks: fixing technical errors when combining data from different experiments, identifying cell types, and filling in missing modalities. We discovered that specialized cell analysis methods excel at fixing technical errors, while general-purpose ML approaches perform better at identifying cell types. By providing evidence-based recommendations for choosing the right cell analysis methods and identifying gaps where new specialized methods are needed, this work enhances reproducibility and gives scientists a roadmap for more effective single-cell analysis, ultimately accelerating discoveries in medicine and biology."
Poster,SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations,https://ICML.cc//virtual/2025/poster/46692,"Grigory Bartosh, Dmitry Vetrov, Christian Andersson Naesseth","The Latent Stochastic Differential Equation (SDE) is a powerful tool for time series and sequence modeling. However, training Latent SDEs typically relies on adjoint sensitivity methods, which depend on simulation and backpropagation through approximate SDE solutions, which limit scalability. In this work, we propose SDE Matching, a new simulation-free method for training Latent SDEs. Inspired by modern Score- and Flow Matching algorithms for learning generative dynamics, we extend these ideas to the domain of stochastic dynamics for time series modeling, eliminating the need for costly numerical simulations. Our results demonstrate that SDE Matching achieves performance comparable to adjoint sensitivity methods while drastically reducing computational complexity.","The Latent Stochastic Differential Equation (SDE) is a powerful tool for time series and sequence modeling. However, training Latent SDEs typically relies on adjoint sensitivity methods, which depend on simulation and backpropagation through approximate SDE solutions, which limit scalability. In this work, we propose SDE Matching, a new simulation-free method for training Latent SDEs. Inspired by modern Score- and Flow Matching algorithms for learning generative dynamics, we extend these ideas to the domain of stochastic dynamics for time series modeling, eliminating the need for costly numerical simulations. Our results demonstrate that SDE Matching achieves performance comparable to adjoint sensitivity methods while drastically reducing computational complexity."
Poster,SDMG: Smoothing Your Diffusion Models for Powerful Graph Representation Learning,https://ICML.cc//virtual/2025/poster/44201,"Junyou Zhu, Langzhou He, Chao Gao, Dongpeng Hou, Zhen Su, Philip Yu, Juergen Kurths, Frank Hellmann","Diffusion probabilistic models (DPMs) have recently demonstrated impressive generative capabilities. There is emerging evidence that their sample reconstruction ability can yield meaningful representations for recognition tasks. In this paper, we demonstrate that the objectives underlying generation and representation learning are not perfectly aligned. Through a spectral analysis, we find that minimizing the mean squared error (MSE) between the original graph and its reconstructed counterpart does not necessarily optimize representations for downstream tasks. Instead, focusing on reconstructing a small subset of features, specifically those capturing global information, proves to be more effective for learning powerful representations. Motivated by these insights, we propose a novel framework, the Smooth Diffusion Model for Graphs (SDMG), which introduces a multi-scale smoothing loss and low-frequency information encoders to promote the recovery of global, low-frequency details, while suppressing irrelevant high-frequency noise. Extensive experiments validate the effectiveness of our method, suggesting a promising direction for advancing diffusion models in graph representation learning.","Graphs describe many things we care about, from social networks to protein molecules, yet teaching computers to read them usually needs lots of hand-labeled examples. A new family of “diffusion” AI models can learn without labels by first adding noise to a graph and then learning to undo it—but they try to rebuild every tiny detail, including random clutter, which paradoxically weakens the final predictions.We created SDMG (Smooth Diffusion Model for Graphs). This training recipe guides the model to focus on the broad, slow changing patterns in a graph, like spotting the outline of a coastline from an airplane rather than counting every pebble on the beach. Simple smoothing rules reward the recovery of these big-picture signals and allow the model to ignore distracting high-frequency noise.With this small change, SDMG produces sharper graph insight: on standard benchmarks it outperforms previous self-supervised methods at tasks like classifying research papers or identifying molecule types, all while using the same data. Better, label-efficient graph understanding could speed up drug discovery, improve fault detection in power grids, and help scientists make sense of complex climate networks—bringing the benefits of machine learning to areas where labeled data are scarce."
Poster,SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming,https://ICML.cc//virtual/2025/poster/46410,"Hong-Ming Chiu, Hao Chen, Huan Zhang, Richard Zhang","Neural network verifiers based on linear bound propagation scale impressively to massive models but can be surprisingly loose when neuron coupling is crucial. Conversely, semidefinite programming (SDP) verifiers capture inter-neuron coupling naturally, but their cubic complexity restricts them to only small models. In this paper, we propose SDP-CROWN, a novel hybrid verification framework that combines the tightness of SDP relaxations with the scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new linear bound---derived via SDP principles---that explicitly captures $\ell_{2}$-norm-based inter-neuron coupling while adding only one extra parameter per layer. This bound can be integrated seamlessly into any linear bound-propagation pipeline, preserving the inherent scalability of such methods yet significantly improving tightness. In theory, we prove that our inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional per-neuron bounds. In practice, when incorporated into the state-of-the-art $\alpha$-CROWN verifier, we observe markedly improved verification performance on large models with up to 65 thousand neurons and 2.47 million parameters, achieving tightness that approaches that of costly SDP-based methods.","Verifying that neural networks behave reliably, especially when faced with small and unexpected changes to their inputs, is a key challenge in making AI systems safe. Some fast methods can check large networks but miss important internal relationships between neurons, leading to overly cautious results. Other more accurate methods, like those based on semidefinite programming (SDP), capture these relationships well but are too slow for big networks.In this work, we introduce SDP-CROWN, a new technique that combines the best of both worlds. It keeps the speed of scalable methods while adding a way to model how neurons influence each other without the heavy computation of traditional SDP. Our approach can be integrated into existing tools and is both theoretically stronger and practically more accurate. In tests on large neural networks, it significantly improves verification quality while remaining efficient."
Poster,SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space,https://ICML.cc//virtual/2025/poster/45141,"Xupeng Zhu, Fan Wang, Robin Walters, Jane Shi","Diffusion Policies are effective at learning closed-loop manipulation policies from human demonstrations but generalize poorly to novel arrangements of objects in 3D space, hurting real-world performance. To address this issue, we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion policy that adapts trajectories according to 3D transformations of the scene. Such equivariance is achieved by embedding the states, actions, and the denoising process in spherical Fourier space. Additionally, we employ novel spherical FiLM layers to condition the action denoising process equivariantly on the scene embeddings. Lastly, we propose a spherical denoising temporal U-net that achieves spatiotemporal equivariance with computational efficiency. In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization across transformed 3D scenes. SDP demonstrates a large performance improvement over strong baselines in 20 simulation tasks and 5 physical robot tasks including single-arm and bi-manual embodiments. Code is available at https://github.com/amazon-science/Spherical_Diffusion_Policy.",(1) Current deep learning methods that control robotic manipulators do not generalize to the variations on the object’s 3D poses. (2) We propose an end-to-end policy learning method that leverages geometries and provably adapts to random 3D object poses. (3) The resulting method achieves significantly better performance than the SOTA baseline in 20 simulation tasks and 5 physical robotic tasks.
Poster,SEAD: Unsupervised Ensemble of Streaming Anomaly Detectors,https://ICML.cc//virtual/2025/poster/46199,"Saumya Gaurang Shah, Abishek Sankararaman, Balakrishnan Narayanaswamy, Vikramank Singh","Can we efficiently choose the best Anomaly Detection (AD) algorithm for a data-stream without requiring anomaly labels? Streaming anomaly detection is hard. SOTA AD algorithms are sensitive to their hyperparameters and no single method works well on all datasets. The best algorithm/hyper-parameter combination for a given data-stream can change over time with data drift. 'What is an anomaly?' is often application, context and dataset dependent. We propose SEAD (Streaming Ensemble of Anomaly Detectors), the first model selection algorithm for streaming, unsupervised AD. All prior AD model selection algorithms are either supervised, or only work in the offline setting when all data from the test set  is available upfront. We show that SEAD is {\em(i)}  unsupervised, i.e., requires no true anomaly labels, {\em(ii)}  efficiently implementable in a streaming setting,  {\em (iii)}  agnostic to the choice of the base algorithms among which it chooses from, and {\em (iv)}  adaptive to non-stationarity in the data-stream. Experiments on 14 non-trivial public datasets and an internal dataset corroborate our claims.","Real-time anomaly detection systems are important in applications ranging from cybersecurity to healthcare. These systems a) do not have access to labels, and b) need to adapt to changes in input data over time. Although many such methods exist for real-time anomaly detection, no single method works well for all applications. We ask the question: Is it possible to select the best model for the given application at every timestamp without using labels? We select the best models at each point in time by giving higher preference to models that have generated fewer detections in the past, using the intuition that anomalies are inherently rare. Our research presents the first model selection method for real-time anomaly detection without using labels."
Poster,Secant Line Search for Frank-Wolfe Algorithms,https://ICML.cc//virtual/2025/poster/44401,"Deborah Hendrych, Sebastian Pokutta, Mathieu Besançon, David Martinez-Rubio","We present a new step-size strategy based on the secant method for Frank-Wolfe algorithms. This strategy, which requires mild assumptions about the function under consideration, can be applied to any Frank-Wolfe algorithm. It is as effective as full line search and, in particular, allows for adapting to the local smoothness of the function, such as in (Pedregosa et al., 2020), but comes with a significantly reduced computational cost, leading to higher effective rates of convergence. We provide theoretical guarantees and demonstrate the effectiveness of the strategy through numerical experiments.","We’ve developed a new way to make a certain type of optimization method (called the Frank-Wolfe algorithm) work faster and more efficiently. Think of it like finding the best path down a mountain - our method helps the computer take better steps without having to check every possible direction, which saves a lot of time and computing power. This matters in machine learning because many models rely on solving these kinds of optimization problems to learn from data — making this process faster means training models more quickly and using less computing power.The great part is that it works with many versions of this method and still finds solutions just as well as older, more time-consuming approaches. But because our method is much quicker, it reaches good solutions faster in practice.We've proven mathematically that this method works well, and we've tested it with real examples to show that it performs as promised."
Poster,SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding,https://ICML.cc//virtual/2025/poster/44319,"Peihua Mai, Youlong Ding, Ziyan Lyu, Minxin Du, Yan (James) Pang","Federated recommender system (FedRec) has emerged as a solution to protect user data through collaborative training techniques. A typical FedRec involves transmitting the full model and entire weight updates between edge devices and the server, causing significant burdens to edge devices with limited bandwidth and computational power. The sparsity of embedding updates provides opportunity for payload optimization, while existing sparsity-aware federated protocols generally sacrifice privacy for efficiency. A key challenge in designing a secure sparsity-aware efficient protocol is to protect the rated item indices from the server. In this paper, we propose a lossless secure recommender systems with on sparse embedding updates (SecEmb). SecEmb reduces user payload while ensuring that the server learns no information about both rated item indices and individual updates except the aggregated model. The protocol consists of two correlated modules: (1) a privacy-preserving embedding retrieval module that allows users to download relevant embeddings from the server, and (2) an update aggregation module that securely aggregates updates at the server. Empirical analysis demonstrates that SecEmb reduces both download and upload communication costs by up to 90x and decreases user-side computation time by up to 70x compared with secure FedRec protocols. Additionally, it offers non-negligible utility advantages compared with lossy message compression methods.","The paper introduces SecEmb, a system designed to improve privacy, efficiency, and security for personalized recommender systems that use Federated Learning (FL), especially on devices with limited resources like smartphones. SecEmb is a new protocol for training personalized recommendation systems on devices using federated learning that leverages the sparsity of user interactions to achieve dramatic improvements in communication and computation efficiency, while strongly protecting user privacy by ensuring the server does not learn sensitive information like which specific items a user has rated."
Poster,SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding,https://ICML.cc//virtual/2025/poster/45215,"Woohyeon Park, Woojin Kim, Jaeik Kim, Jaeyoung Do","Despite significant advancements in Vision-Language Models (VLMs), the performance of existing VLMs remains hindered by object hallucination, a critical challenge to achieving accurate visual understanding. To address this issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach that enables VLMs to effectively leverage multi-scale visual information with an object-centric manner, closely aligning with human visual perception. SECOND progressively selects and integrates multi-scale visual information, facilitating a more precise interpretation of images. By contrasting these visual information iteratively, SECOND significantly reduces perceptual hallucinations and outperforms a wide range of benchmarks. Our theoretical analysis and experiments highlight the largely unexplored potential of multi-scale application in VLMs, showing that prioritizing and contrasting across scales outperforms existing methods.","Vision-Language Models (VLMs), which combine image understanding with language generation, have made impressive progress in tasks like image captioning or visual question answering. However, they often suffer from perceptual hallucination — generating descriptions that either overlook objects clearly present in the image or mention objects that are not there at all. This leads to confusing or incomplete answers and limits their trustworthiness in critical applications.We developed SECOND: Selective and Contrastive Decoding, a new method that helps these models better focus on what’s actually visible in an image. Unlike other approaches that treat all image areas equally, SECOND imitates how humans look at images: starting with a rough overview, then zooming in on important parts. SECOND gradually filters and refines visual details through multiple stages, helping the model ignore irrelevant parts of the image and focus on meaningful ones.SECOND works without any extra training and is compatible with existing models. It also includes a novel comparison step, where outputs at different stages are contrasted to reduce errors. Across multiple benchmarks, our method significantly reduced hallucination and improved object recognition.By making AI models see more like humans, SECOND moves us one step closer to trustworthy and accurate visual reasoning."
