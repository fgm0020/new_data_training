type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport,https://ICML.cc//virtual/2025/poster/44922,"Jayadev Naram, Fredrik Hellström, Ziming Wang, Rebecka Jörnsten, Giuseppe Durisi","In many scenarios of practical interest, labeled data from a target distribution are scarce while labeled data from a related source distribution are abundant. One particular setting of interest arises when the target label space is a subset of the source label space, leading to the framework of partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a domain alignment term and a weighted empirical loss on the source data, with the aim of transferring knowledge between domains. However, a theoretical basis for this procedure is lacking, and in particular, most existing weighting schemes are heuristic. In this work, we derive generalization bounds for the PDA problem based on partial optimal transport. These bounds corroborate the use of the partial Wasserstein distance as a domain alignment term, and lead to theoretically motivated explicit expressions for the empirical source loss weights. Inspired by these bounds, we devise a practical algorithm for PDA, termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT is competitive with recent approaches, and that our proposed weights improve on existing schemes.","Machine learning systems often rely on large amounts of labeled data to learn effectively. But in many real-world situations, such data are only available in related—but not identical—contexts. For example, a model trained to recognize a wide range of objects may need to be adapted to a new setting where only a smaller set of objects is relevant, and labeled examples in the new setting are scarce. This challenge is known as partial domain adaptation.In our work, we take a fresh look at how to best transfer knowledge from one setting to another in these cases. We provide new theoretical insights into how and why certain adaptation techniques work, especially those that try to align patterns between the old and new data. Based on our findings, we propose a practical new method, called WARMPOT, that learns more effectively which parts of the old data are actually useful in the new setting.Our experiments show that WARMPOT performs as well as or better than current top methods, and can even reach state-of-the-art results on a widely used benchmark. Overall, this work helps make machine learning models more adaptable and reliable when working with limited data in real-world applications."
Poster,The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning,https://ICML.cc//virtual/2025/poster/44777,"Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Shijie Xu, weihongluo, Yuhua Li, xiuqiang He, Ruixuan Li","To improve the training efficiency of federated learning (FL), previous research has employed low-rank decomposition techniques to reduce communication overhead. In this paper, we seek to enhance the performance of these low-rank decomposition methods. Specifically, we focus on three key issues related to decomposition in FL: what to decompose, how to decompose, and how to aggregate. Subsequently, we introduce three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD), each targeting a specific issue. These techniques are complementary and can be applied simultaneously to achieve optimal performance. Additionally, we provide a rigorous theoretical analysis to ensure the convergence of the proposed MUD. Extensive experimental results show that our approach achieves faster convergence and superior accuracy compared to relevant baseline methods. The code is available at https://github.com/Leopold1423/fedmud-icml25.","Federated Learning (FL) allows devices like smartphones to collaboratively train models without sharing their private data. However, frequent communication of model updates between devices and the central server creates a major bottleneck, especially with large models or slow networks. Researchers have tried compressing these updates using a technique called low-rank decomposition, but existing methods still suffer from inefficiencies and approximation errors.In our work, we identify three fundamental questions: what to compress, how to compress, and how to combine the compressed results. Further, we propose three new techniques, Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD), that target each of these challenges. Together, they reduce communication costs while preserving model accuracy.Our approach not only speeds up training but also achieves better results than current state-of-the-art methods. This has the potential to make privacy-preserving federated training more practical and efficient, especially in real-world settings with limited bandwidth or energy constraints."
Poster,The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret,https://ICML.cc//virtual/2025/poster/45208,"Lukas Fluri, Leon Lang, Alessandro Abate, Patrick Forré, David Krueger, Joar Skalse","In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by *learning* the reward function. However, a learned reward model may have a low error on the data distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an *error-regret mismatch*. The main source of an error-regret mismatch is the distributional shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any *fixed* expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as RLHF. We hope our results stimulate the theoretical and empirical study of improved methods to learn reward models, and better ways to measure their quality reliably.","Teaching AI complex tasks often involves first using examples to train a ""reward function"" – the AI’s guide for good and bad actions. However, the reward function might look accurate on the examples (low error) but still encourage poor AI decisions in practice (high regret). This ""error-regret mismatch"" occurs when the AI can exploit loopholes where the reward function is wrong.We mathematically investigated why and when this mismatch happens. We discovered that even if a reward function seems well-trained, the AI can still perform badly if the reward function’s training examples have certain ""unsafe"" characteristics – essentially, when they don’t cover important scenarios. Our work precisely defines what makes a set of training examples ""unsafe"" and shows that common safety techniques, like adding penalties to discourage extreme behaviors, don't always fix this fundamental issue.As AI systems increasingly learn their tasks from data, understanding this error-regret mismatch is crucial. Our findings explain a key reason why an AI might act in undesirable ways. This knowledge helps pave the way for developing AI systems that learn more reliably and behave more consistently with our intended goals, which is vital for building AI we can trust."
Poster,The Polynomial Stein Discrepancy for Assessing Moment Convergence,https://ICML.cc//virtual/2025/poster/46584,"Narayan Srinivasan, Matthew Sutton, Christopher Drovandi, Leah South","We propose a novel method for measuring the discrepancy between a set of samples and a desired posterior distribution for Bayesian inference. Classical methods for assessing sample quality like the effective sample size are not appropriate for scalable Bayesian sampling algorithms, such as stochastic gradient Langevin dynamics, that are asymptotically biased. Instead, the gold standard is to use the kernel Stein Discrepancy (KSD), which is itself not scalable given its quadratic cost in the number of samples. The KSD and its faster extensions also typically suffer from the curse-of-dimensionality and can require extensive tuning. To address these limitations, we develop the polynomial Stein discrepancy (PSD) and an associated goodness-of-fit test. While the new test is not fully convergence-determining, we prove that it detects differences in the first $r$ moments for Gaussian targets. We empirically show that the test has higher power than its competitors in several examples, and at a lower computational cost. Finally, we demonstrate that the PSD can assist practitioners to select hyper-parameters of Bayesian sampling algorithms more efficiently than competitors.","Modern challenges in statistics and machine learning involve the ability to process large amounts of data and complex mathematical models, which can be computationally intensive.  To overcome this computational challenge, approximate models are often used in practice.  However, it is difficult to assess whether samples generated from approximate models are consistent with the complex model of interest (i.e. whether they are fit-for-purpose).Traditional ways of checking or measuring the quality of simulated samples often miss important issues, especially when the approximate simulation process introduces some bias or error. Some newer techniques try to fix this, but they can be slow with large datasets, struggle with complex data, and often need a lot of fine-tuning to work properly. They can also miss key differences in basic features of the samples, like the average or the spread.To address these problems, we introduce a new, faster approach that focuses directly on checking whether the important summary features of the samples—like averages and variability—match what we expect. We show through examples that this method works well across a range of common statistical tasks."
Poster,The Power of Random Features and the Limits of Distribution-Free Gradient Descent,https://ICML.cc//virtual/2025/poster/43774,"Ari Karchmer, Eran Malach","We study the relationship between gradient-based optimization of parametric models (e.g., neural networks) and optimization of linear combinations of random features. Our main result shows that if a parametric model can be learned using mini-batch stochastic gradient descent (bSGD) without making assumptions about the data distribution, then with high probability, the target function can also be approximated using a polynomial-sized combination of random features. The size of this combination depends on the number of gradient steps and numerical precision used in the bSGD process. This finding reveals fundamental limitations of distribution-free learning in neural networks trained by gradient descent, highlighting why making assumptions about data distributions is often crucial in practice.Along the way, we also introduce a new theoretical framework called average probabilistic dimension complexity (adc), which extends the probabilistic dimension complexity developed by Kamath et al. (2020). We prove that adc has a polynomial relationship with statistical query dimension, and use this relationship to demonstrate an infinite separation between adc and standard dimension complexity.","This paper reveals a fundamental limitation of training neural networks without making assumptions about the data. The paper describes a discovery that when machine learning algorithms try to work on any possible distribution of data (called ""distribution-free"" learning), they essentially become no more powerful than much simpler methods that just combine randomly chosen features. In other words, if you want your neural network to work well on absolutely any kind of data without knowing anything about what that data looks like, you're severely limiting what the network can actually learn. This helps explain why successful machine learning in practice almost always involves making smart assumptions about the type of data you're working with, rather than trying to build completely general-purpose systems. The finding possibly suggests that the capabilities seen in modern AI come not just from powerful algorithms, but from carefully tailoring those algorithms to the specific characteristics of the data they'll encounter."
Poster,The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in Equivariant Tensor Products,https://ICML.cc//virtual/2025/poster/45917,"YuQing Xie, Ameya Daigavane, Mit Kotak, Tess Smidt","$E(3)$-equivariant neural networks have demonstrated success across a wide range of 3D modelling tasks. A fundamental operation in these networks is the tensor product, which interacts two geometric features in an equivariant manner to create new features. Due to the high computational complexity of the tensor product, significant effort has been invested to optimize the runtime of this operation. For example, Luo et al. (2024) recently proposed the Gaunt tensor product (GTP) which promises a significant speedup. In this work, we provide a careful, systematic analysis of a number of tensor product operations. In particular, we emphasize that different tensor products are not performing the same operation. The reported speedups typically come at the cost of expressivity. We introduce measures of expressivity and interactability to characterize these differences. In addition, we realized the original implementation of GTP can be greatly simplified by directly using a spherical grid at no cost in asymptotic runtime. This spherical grid approach is faster on our benchmarks and in actual training of the MACE interatomic potential by 30\%. Finally, we provide the first systematic microbenchmarks of the various tensor product operations. We find that the theoretical runtime guarantees can differ wildly from empirical performance, demonstrating the need for careful application-specific benchmarking. Code is available at https://github.com/atomicarchitects/PriceofFreedom","Neural networks that understand 3D geometry are becoming increasingly important for tasks like predicting how molecules interact or how objects move in space. These networks often rely on a complex mathematical operation called the tensor product to combine 3D information. However, this operation is usually slow and uses a lot of computing power.Recently, researchers have looked into developing faster versions. One such example is the Gaunt Tensor Product (GTP). In our work, we took a closer look at several of these fast methods to see how they really compare. We found that while these methods are faster, they do so by losing expressivity. We propose new ways to measure the expressivity/speed tradeoff for these alternative tensor product operations.We also discovered a simpler and even faster way to implement GTP by using a spherical grid, which cuts down training time in a real-world example by 30%. Lastly, we tested all the methods in detail and found that how fast they are in theory doesn’t always match up with how they perform in practice. So it’s important to test them carefully depending on the task.Our findings provide researchers a useful too for understanding the right balance between speed and accuracy when building powerful 3D AI models."
Poster,The Price of Linear Time: Error Analysis of Structured Kernel Interpolation,https://ICML.cc//virtual/2025/poster/44750,"Alexander Moreno, Justin Xiao, Jonathan Mei","Structured Kernel Interpolation (SKI) scales Gaussian Processes (GPs) by approximating the kernel matrix via inducing point interpolation, achieving linear computational complexity. However, it lacks rigorous theoretical error analysis. This paper bridges this gap by proving error bounds for the SKI Gram matrix and examining their effect on hyperparameter estimation and posterior inference. We further provide a practical guide to selecting the number of inducing points under convolutional cubic interpolation: they should grow as \(n^{d/3}\) for error control. Crucially, we identify two dimensionality regimes for the SKI Gram matrix spectral norm error vs. complexity trade-off. For \(d<3\), \textit{any} error tolerance can achieve linear time for sufficiently large sample size. For \(d\geq 3\), the error must \textit{increase} with sample size for our guarantees to hold. Our analysis provides key insights into SKI's scalability-accuracy trade-offs, establishing precise conditions for achieving linear-time GP inference with controlled error.","We use powerful computational methods called Gaussian Processes (GPs) for tasks like prediction, but these can be slow with large datasets. A technique called Structured Kernel Interpolation (SKI) offers a way to speed them up significantly. Our research fills a crucial gap by carefully analyzing the potential errors introduced by this SKI speed-up. We provide practical guidelines on how to use SKI effectively, revealing how the amount of data and its underlying complexity (or 'dimensionality') influence the trade-off between calculation speed and prediction accuracy. For simpler data, our work shows it's possible to achieve both fast computations and high accuracy. However, for more complex, high-dimensional data, our findings indicate that to maintain speed with very large datasets, a slight increase in potential error might be an unavoidable consequence. This analysis helps researchers make more informed decisions when using SKI, allowing them to harness its speed benefits while understanding and managing the accuracy implications."
Poster,The Relationship Between No-Regret Learning and Online Conformal Prediction,https://ICML.cc//virtual/2025/poster/45708,"Ramya Ramalingam, Shayan Kiyani, Aaron Roth","Existing algorithms for online conformal prediction---guaranteeing marginal coverage in adversarial settings---are variants of online gradient descent (OGD), but their analyses of worst-case coverage do not follow from the regret guarantee of OGD. What is the relationship between no-regret learning and online conformal prediction? We observe that although standard regret guarantees imply marginal coverage in i.i.d. settings, this connection fails as soon as we either move to adversarial environments or ask for group conditional coverage. On the other hand, we show a tight connection between *threshold calibrated* coverage and swap-regret in adversarial settings, which extends to group-conditional (multi-valid) coverage. We also show that algorithms in the *follow the regularized leader* family of no regret learning algorithms (which includes online gradient descent) can be used to give group-conditional coverage guarantees in adversarial settings for arbitrary grouping functions. Via this connection we analyze and conduct experiments using a multi-group generalization of the ACI algorithm of Gibbs & Candes (2021).","Some learning algorithms are designed to work even in unpredictable environments by learning from mistakes over time to limit a quantity called *regret* - a technique known as no-regret learning. We noticed that some of these algorithms, although not originally designed for it, also perform well on a seemingly different task: generating prediction sets that reliably include the correct answer.In this paper, we explore when and why this connection holds. Our first finding is a close relationship between two properties: strong coverage guarantees for prediction sets, and a strong no-regret condition called swap regret. This means any algorithm that satisfies one will automatically satisfy the other. We then study a broad class of algorithms and show they simultaneously achieve weaker versions of both properties - but for different reasons, and without a tight link between them.Finally, we use these insights to develop a straightforward, easy-to-implement algorithm that produces prediction sets that are not only reliable overall, but also within any subgroups the user specifies - such as demographic categories of interest."
Poster,The Ripple Effect: On Unforeseen Complications of Backdoor Attacks,https://ICML.cc//virtual/2025/poster/43805,"Rui Zhang, Yun Shen, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Yuan Zhang, Guowen Xu, Yang Zhang","Recent research highlights concerns about the trustworthiness of third-party Pre-Trained Language Models (PTLMs) due to potential backdoor attacks.These backdoored PTLMs, however, are effective only for specific pre-defined downstream tasks.In reality, these PTLMs can be adapted to many other unrelated downstream tasks.Such adaptation may lead to unforeseen consequences in downstream model outputs, consequently raising user suspicion and compromising attack stealthiness.We refer to this phenomenon as backdoor complications.In this paper, we undertake the first comprehensive quantification of backdoor complications.Through extensive experiments using 4 prominent PTLMs and 16 text classification benchmark datasets, we demonstrate the widespread presence of backdoor complications in downstream models fine-tuned from backdoored PTLMs.The output distribution of triggered samples significantly deviates from that of clean samples.Consequently, we propose a backdoor complication reduction method leveraging multi-task learning to mitigate complications without prior knowledge of downstream tasks.The experimental results demonstrate that our proposed method can effectively reduce complications while maintaining the efficacy and consistency of backdoor attacks.","Language models, widely used today, can be secretly tampered with through backdoor attacks before they are made public. These attacks insert hidden triggers intended to cause specific malicious behavior. However, these models are often adapted and used for many tasks beyond the attacker's original plan. We discovered that when a backdoored model is used in these unexpected ways, the hidden attack often leads to unpredictable and strange errors in the model's output – a phenomenon we call *backdoor complications*. These complications can make the attack less stealthy by causing noticeable anomalies. We performed the first large-scale study to measure how often and how severely these complications occur across different models and tasks, confirming they are widespread. Based on this, we developed a method using multi-task learning that can reduce these complications. Our results show this method helps backdoored models behave more consistently across tasks, which, while potentially making backdoors harder to spot through their errors, offers crucial insights into controlling backdoor behavior for future detection and defense strategies."
Poster,Thermalizer: Stable autoregressive neural emulation of spatiotemporal chaos,https://ICML.cc//virtual/2025/poster/46342,"Chris Pedersen, Laure Zanna, Joan Bruna","Autoregressive surrogate models (or *emulators*) of spatiotemporal systems provide an avenue for fast, approximate predictions, with broad applications across science and engineering. At inference time however, these models are generally unable to provide predictions over long time rollouts due to accumulation of errors leading to diverging trajectories. In essence, emulators operate out of distribution, and controlling the online distribution quickly becomes intractable in large-scale settings. To address this fundamental issue, and focusing on time-stationary systems admitting an invariant measure, we leverage diffusion models to obtain an implicit estimator of the score of this invariant measure. We show that this model of the score function can be used to stabilize autoregressive emulator rollouts by applying on-the-fly denoising during inference, a process we call *thermalization*. Thermalizing an emulator rollout is shown to extend the time horizon of stable predictions by two orders of magnitude in complex systems exhibiting turbulent and chaotic behavior, opening up a novel application of diffusion models in the context of neural emulation.","AI simulations of fluid flows have the potential to revolutionise many areas of science and engineering due to immense computational speedups. However current models tend to only be reliable over short timescales, as they build up error and the predicted state degrades over time. This is due to the fact that the AI models are approximations, and so small errors introduced at each step accumulate over many timesteps.We tackle this problem by using an algorithm from the field of image generative modelling called diffusion models, to ""denoise"" the AI-simulated fluid flow on the fly - a process we call *thermalization*. This algorithm is able to only remove the errors introduced by the AI-simulator, without disrupting the temporal dynamics of the fluid flow. We demonstrate this approach on multiple flows and multiple AI-simulators. In every case we are able extend the length of reliable rollouts by a factor of 100 when applying the thermalizer to the AI-simulated flow. While we demonstrate thermalization on fluid flows, in principle it could be applied to many other kinds of AI models, including language modelling."
