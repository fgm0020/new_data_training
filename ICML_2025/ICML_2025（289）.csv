type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Sundial: A Family of Highly Capable Time Series Foundation Models,https://ICML.cc//virtual/2025/poster/45591,"Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, Mingsheng Long","We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundial's pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: https://github.com/thuml/Sundial.","Predicting the future from past observations (time series forecasting) is crucial for many real-world applications. However, traditional methods often struggle because the variations of time series are complex, and the future value can be nondeterministic. While powerful AI models exist, they typically need massive amounts of specific training data and tough tuning.To overcome these limitations, we developed a new family of powerful AI tools, called time series foundation models. These models are based on generative AI, the same technology behind creating realistic images or text. Crucially, they were pre-trained on an enormous dataset: one trillion real-world time points from diverse domains. This extensive pre-training allows them to deeply understand complex patterns in time series.Our model significantly outperforms previous methods. It generates predictions that are not only more accurate but also present a wider range of possible future outcomes, providing better insights into uncertainty. This versatility allows it to excel at both specific point forecasters and probabilistic forecasters. The best part? We released this model as a ready-to-use forecaster. It requires no training and delivers top-tier, state-of-the-art results on well-established forecasting benchmarks, making reliable predictions within milliseconds."
Poster,Supercharging Graph Transformers with Advective Diffusion,https://ICML.cc//virtual/2025/poster/45539,"Qitian Wu, Chenxiao Yang, Kaipeng Zeng, Michael Bronstein","The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models, i.e., the generalization of common graph neural networks in continuous space. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions","The ability of machine learning models to perform well on new data is very important. For data that comes in the form of networks (like social networks or molecules), this involves understanding the underlying connections and patterns—what we call “topological structures”. Previous works fail to well understand how machine learning models perform when the network patterns change, which we call ""topological shifts"" in this paper. We propose AdvDIFFormer, a new model inspired by advective diffusion equations, a PDE model describing a class of natural processes that spread and mix physical quantities (like how fluids flow and mix). This model can control errors that happen when the network patterns change, which previous models cannot do. Experiment results show that this new model works better for many prediction tasks in real-world graph data, such as social networks, molecular screening and protein interactions."
Poster,Super Deep Contrastive Information Bottleneck for Multi-modal Clustering,https://ICML.cc//virtual/2025/poster/45133,"Zhengzheng Lou, Ke Zhang, Yucong Wu, Shizhe Hu","In an era of increasingly diverse information sources, multi-modal clustering (MMC) has become a key technology for processing multi-modal data. It can apply and integrate the feature information and potential relationships of different modalities. Although there is a wealth of research on MMC, due to the complexity of datasets, a major challenge remains in how to deeply explore the complex latent information and interdependencies between modalities. To address this issue, this paper proposes a method called super deep contrastive information bottleneck (SDCIB) for MMC, which aims to explore and utilize all types of latent information to the fullest extent. Specifically, the proposed SDCIB explicitly introduces the rich information contained in the encoder's hidden layers into the loss function for the first time, thoroughly mining both modal features and the hidden relationships between modalities. Moreover, the proposed SDCIB performs dual optimization by simultaneously considering consistency information from both the feature distribution and clustering assignment perspectives, the proposed SDCIB significantly improves clustering accuracy and robustness. We conducted experiments on 4 multi-modal datasets and the accuracy of the method on the ESP dataset improved by 9.3\%. The results demonstrate the superiority and clever design of the proposed SDCIB. The source code is available on https://github.com/ShizheHu.","Computers can analyze many types of data, such as images, text, and speech, clustering them into different groups. We want to further explore whether the intermediate information generated by the computer in the process of analyzing this data can in turn help it better clustering the data. We designed a ""command center"" (technically called a loss function) to ""guide"" the computer in analyzing the data. During analysis, the intermediate information generated is sent back to the ""command center"" to ""guide"" the computer in better analyzing the data and achieving improved data partition. We found the use of intermediate information in the ""command center"" leads to better performance in guiding the computer to analyze and segment the data.Our results show that the intermediate information generated during data analysis contains rich representations, enabling better data partition, which in turn facilitates the discovery of the intrinsic structure and patterns within the data."
Poster,Supervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching,https://ICML.cc//virtual/2025/poster/46594,"Joan Serrà, Recep Oguz Araz, Dmitry Bogdanov, Yuki Mitsufuji","Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Because of the ground truth nature, existing approaches match musical versions at the track level (e.g., whole song). However, most applications require to match them at the segment level (e.g., 20s chunks). In addition, existing approaches resort to classification and triplet losses, disregarding more recent losses that could bring meaningful improvements. In this paper, we propose a method to learn from weakly annotated segments, together with a contrastive loss variant that outperforms well-studied alternatives. The former is based on pairwise segment distance reductions, while the latter modifies an existing loss following decoupling, hyper-parameter, and geometric considerations. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching.","Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Traditional approaches match musical versions at the track level (e.g., whole song). However, most applications require to match them at the segment level (e.g., 20s chunks). In addition, existing approaches resort to classical loss functions, disregarding more recent ones that could bring meaningful improvements. In this paper, we propose a method to learn from whole-song-annotated segments, together with a new loss function that outperforms well-studied alternatives. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching."
Poster,Surrogate Prompt Learning: Towards Efficient and Diverse Prompt Learning for Vision-Language Models,https://ICML.cc//virtual/2025/poster/43460,"Liangchen Liu, Nannan Wang, Xi Yang, Xinbo Gao, Tongliang Liu","Prompt learning is a cutting-edge parameter-efficient fine-tuning technique for pre-trained vision-language models (VLMs). Instead of learning a single text prompt, recent works have revealed that learning diverse text prompts can effectively boost the performances on downstream tasks, as the diverse prompted text features can comprehensively depict the visual concepts from different perspectives. However, diverse prompt learning demands enormous computational resources. This efficiency issue still remains unexplored. To achieve efficient and diverse prompt learning, this paper proposes a novel \textbf{Surrogate Prompt Learning (SurPL)} framework. Instead of learning diverse text prompts, SurPL directly generates the desired prompted text features via a lightweight \textbf{Surrogate Feature Generator (SFG)}, thereby avoiding the complex gradient computation procedure of conventional diverse prompt learning. Concretely, based on a basic prompted text feature, SFG can directly and efficiently generate diverse prompted features according to different pre-defined conditional signals. Extensive experiments indicate the effectiveness of the surrogate prompted text features, and show compelling performances and efficiency of SurPL on various benchmarks.","We introduce Surrogate Prompt Learning (SurPL), a new method that makes adapting large vision–language models much more efficient. Traditional prompt learning requires optimizing many separate text prompts, which takes an expensive process in both time and compute. Instead of fine-tuning multiple prompts, SurPL uses a lightweight Surrogate Feature Generator to directly produce diverse prompted embeddings on demand, cutting out most of the heavy computation. This generator learns to take a single “base” prompt embedding and, guided by simple control signals, output many variations instantly.  By shifting the work from gradient-based tuning to a small auxiliary network, SurPL achieves comparable performance on standard vision benchmarks while using far fewer resources."
Poster,Survival Analysis via Density Estimation,https://ICML.cc//virtual/2025/poster/43491,"Hiroki Yanagisawa, Shunta Akiyama","This paper introduces a novel framework for survival analysis by reinterpreting it as a form of density estimation. Our algorithm post-processes density estimation outputs to derive survival functions, enabling the application of any density estimation model to effectively estimate survival functions. This approach broadens the toolkit for survival analysis and enhances the flexibility and applicability of existing techniques. Our framework is versatile enough to handle various survival analysis scenarios, including competing risk models for multiple event types. It can also address dependent censoring when prior knowledge of the dependency between event time and censoring time is available in the form of a copula. In the absence of such information, our framework can estimate the upper and lower bounds of survival functions, accounting for the associated uncertainty.","Our paper introduces a novel algorithm for survival analysis, a regression task that estimates the probability distribution of a target label from a feature vector. A key aspect of this analysis is that some target labels are censored, meaning only a lower bound of the true target label is available. Survival analysis is crucial in fields such as healthcare and engineering, where accurate predictions can have significant impacts. We propose transforming survival analysis into a multiclass classification problem, specifically through density estimation. This transformation allows for the use of advanced models like LightGBM, which is known for its speed and enhanced prediction quality. Furthermore, we demonstrate that if the density estimates are close to the true values, the resulting survival predictions will be accurate, ensuring reliable results."
Poster,SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training,https://ICML.cc//virtual/2025/poster/46065,"Chao Ma, Wenbo Gong, Meyer Scetbon, Edward Meeds","Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the success of large language models. However, they often require maintaining optimizer states throughout training, which can result in memory requirements several times greater than the model footprint. This overhead imposes constraints on scalability and computational efficiency. Stochastic Gradient Descent (SGD), in contrast, is a stateless optimizer, as it does not track state variables during training. Consequently, it achieves optimal memory efficiency. However, its capability in LLM training is limited (Zhao et al., 2024b)  In this work, we show that pre-processing SGD using normalization and whitening in a stateless manner can achieve similar performance as Adam for LLM training, while maintaining the same memory footprint of SGD. Specifically, we show that normalization stabilizes gradient distributions, and whitening counteracts the local curvature of the loss landscape. This results in SWAN (SGD with Whitening And Normalization), a stochastic optimizer that eliminates the need to store any optimizer states. Empirically, SWAN achieves ~50% reduction on total end-to-end memory compared to Adam. Under the memory-efficienct LLaMA training benchmark of (Zhao et al., 2024a), SWAN reaches the same evaluation perplexity using half as many tokens for 350M and 1.3B model.","Training giant AI language models is very memory-intensive. One reason is that popular training methods (like one called Adam) store a lot of information about past updates to help the model learn, which uses up huge amounts of memory. Our research introduces a new approach called SWAN that eliminates this issue. SWAN doesn’t keep any past data. Instead, at each learning step it makes two quick adjustments (one to keep the step size stable, and another to steer the step in the right direction) so the model learns smoothly without extra memory. In tests, SWAN trained large language models just as accurately as Adam while needing only about half the training data and far less memory. This is early proof that we can train very powerful AI systems efficiently without heavy memory demands, potentially making advanced AI development faster, cheaper, and more accessible."
Poster,SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?,https://ICML.cc//virtual/2025/poster/43573,"Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke","We introduce SWE-Lancer, a benchmark of over 1400 freelance software engineering tasks from Upwork, valued at \\\$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks — ranging from \\\$50 bug fixes to \\\$32000 feature implementations — and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split. By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.","Large language models like ChatGPT are getting better at coding, but could they perform real software engineering work? The SWE-Lancer benchmark aims to measure this. We collected nearly 1500 actual freelance software engineering jobs from Upwork that are worth $1 million USD in total payouts, and measured model performance on these tasks. SWE-Lancer tasks consist of both independent engineering work — like bug fixes and feature implementations — and managerial tasks, where models act as a tech lead and pick the best implementation solution for a given problem. We evaluated multiple state-of-the-art models, and all were unable to solve the majority of tasks, indicating they do not surpass human performance on real-world software engineering work. To facilitate future research, we open-sourced our evaluation code and a public evaluation split so anyone can run SWE-Lancer. By mapping model performance to real dollars, we hope SWE-Lancer enables greater research into the economic impact of AI model development."
Poster,Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales,https://ICML.cc//virtual/2025/poster/44897,"Ju-Seung Byun, Andrew Perrault","Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) introduce additional challenges. For instance, diverse preferences complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. These RL challenges create confusion about whether the probability of an action for a given state should be increased or decreased, similar to the noise in labels for classification tasks. In this work, we focus on RL algorithms that share learning difficulties with cross-entropy loss, especially for low-probability predictions. To enhance stability, we adapt reverse cross-entropy (RCE) from supervised learning for noisy data, defining a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO). Notably, SPPO shows strong performance across different hyperparameters. Furthermore, we validate the symmetric RL loss in the RLHF framework using PPO for natural language processing tasks such as IMDB positive sentiment and TL;DR summarization.","Reinforcement learning (RL), where AI learns by trial and error, is often less stable than supervised learning. This instability becomes even greater when learning from human or AI feedback, such as in RLHF or RLAIF, because varied preferences or misleading reward signals introduce noise, making it difficult for the AI to identify which actions are truly beneficial. To address this, we adapted reverse cross-entropy, a method from supervised learning known for handling noisy data, to create a symmetric RL loss. This approach makes the RL learning process more stable and reliable. Our method demonstrated strong and consistent performance improvements across diverse tasks, including video games, robotic control, and natural language processing with RLHF. This research contributes to building more robust AI systems that can learn effectively from complex, imperfect feedback."
Poster,Symmetry-Aware GFlowNets,https://ICML.cc//virtual/2025/poster/45704,"Hohyun Kim, Seunggeun Lee, Min-hwan Oh","Generative Flow Networks (GFlowNets) offer a powerful framework for sampling graphs in proportion to their rewards. However, existing approaches suffer from systematic biases due to inaccuracies in state transition probability computations. These biases, rooted in the inherent symmetries of graphs, impact both atom-based and fragment-based generation schemes. To address this challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that incorporates symmetry corrections into the learning process through reward scaling. By integrating bias correction directly into the reward structure, SA-GFN eliminates the need for explicit state transition computations. Empirical results show that SA-GFN enables unbiased sampling while enhancing diversity and consistently generating high-reward graphs that closely match the target distribution.","This paper tackles a challenge in how computers learn to build graphs—structures made of points and connections that can represent things like molecules. A popular method for this, called Generative Flow Networks (GFlowNets), is designed to explore many different graph possibilities while favoring those that are more useful or valuable. However, the current techniques often make systematic mistakes because they don't fully account for the natural symmetries in graphs—situations where two graphs look different but actually represent the same thing. To fix this, the paper introduce a new approach called Symmetry-Aware GFlowNets (SA-GFN). This method adjusts how the computer learns, so it automatically corrects for these symmetry-related mistakes without needing complicated calculations. The result is a system that explores a wider variety of graphs more accurately and reliably finds the best ones. This could be especially helpful in areas like drug discovery, where finding the right molecular structure can make a big difference."
