type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Natural Perturbations for Black-box Training of Neural Networks by Zeroth-Order Optimization,https://ICML.cc//virtual/2025/poster/45125,"Hiroshi Sawada, Kazuo Aoyama, Yuya Hikima","This paper proposes a novel concept of natural perturbations for black-box training of neural networks by zeroth-order optimization. When a neural network is implemented directly in hardware, training its parameters by backpropagation ends up with an inaccurate result due to the lack of detailed internal information. We instead employ zeroth-order optimization, where the sampling of parameter perturbations is of great importance. The sampling strategy we propose maximizes the entropy of perturbations with a regularization that the probability distribution conditioned by the neural network does not change drastically, by inheriting the concept of natural gradient. Experimental results show the superiority of our proposal on diverse datasets, tasks, and architectures.","Neural networks are generally implemented on ordinary computers with CPUs or GPUs and memory, and are trained using the well-known backpropagation algorithm, which requires detailed internal information stored in memory. In contrast, there has been growing interest in developing methods that train neural networks without detailed internal information.We employ a black-box optimization method, where we perturb neural network parameters slightly and observe how the training loss function changes. While most existing methods perturb each parameter independently, our method considers parameter correlations and perturbs them so that the neural network's output does not change drastically. We call such generated perturbations *natural perturbations*. The term *natural* has the same meaning as *natural gradient* used when detailed internal information is available.The experimental results show that our method clearly outperforms existing methods. Our contribution accelerates research on emerging methods that train neural networks directly implemented on hardware or in memory-constrained environments."
Poster,Navigating Conflicting Views: Harnessing Trust for Learning,https://ICML.cc//virtual/2025/poster/43734,"Jueqing Lu, Wray Buntine, Yuanyuan Qi, Joanna Dipnall, Belinda Gabbe, Lan Du","Resolving conflicts is critical for improving the reliability of multi-view classification.While prior work focuses on learning consistent and informative representations across views, it often assumes perfect alignment and equal importance of all views, an assumption rarely met in real-world scenarios, as some views may express distinct information. To address this, we develop a computational trust-based discounting method that enhances the Evidential Multi-view framework by accounting for the instance-wise reliability of each view through a probability-sensitive trust mechanism.We evaluate our method on six real-world datasets using Top-1 Accuracy, Fleiss’ Kappa, and a new metric, Multi-View Agreement with Ground Truth, to assess prediction reliability. We also assess the effectiveness of uncertainty in indicating prediction correctness via AUROC.Additionally, we test the scalability of our method through end-to-end training on a large-scale dataset.The experimental results show that computational trust can effectively resolve conflicts, paving the way for more reliable multi-view classification models in real-world applications.Codes available at: https://github.com/OverfitFlow/Trust4Conflict","(1) Problem: In real-world settings, decisions often need to be made based on multiple perspectives, or “views”—such as in healthcare diagnostics or autonomous driving. However, these views can sometimes conflict, and current multi-view classification methods typically assume that all views are equally reliable, which isn’t always true.(2) Solution: We introduce a computational trust-based method that learns to weigh each view based on how realiable its predictions are. By modeling trust as a dynamic, learnable value for each instance-view pair, our method adjusts how much influence each view has when fusing their predictions. This mechanism is built on a principled foundation using Subjective Logic and enhances existing Evidential Multi-view classification models.(3) Impact: Our approach not only improves classification accuracy but also boosts the consistency among views, especially when they disagree. Tested on six benchmark datasets and a large-scale real-world dataset, it consistently outperformed existing baselines. This has important implications for safety-critical domains, providing more reliable decision-making."
Poster,Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning,https://ICML.cc//virtual/2025/poster/45558,"Fangwen Wu, Lechao Cheng, Shengeng Tang, Xiaofeng Zhu, Chaowei Fang, Dingwen Zhang, Meng Wang","Class-incremental learning (CIL) seeks to enable a model to sequentially learn new classes while retaining knowledge of previously learned ones. Balancing flexibility and stability remains a significant challenge, particularly when the task ID is unknown. To address this, our study reveals that the gap in feature distribution between novel and existing tasks is primarily driven by differences in mean and covariance moments. Building on this insight, we propose a novel semantic drift calibration method that incorporates mean shift compensation and covariance calibration. Specifically, we calculate each class's mean by averaging its sample embeddings and estimate task shifts using weighted embedding changes based on their proximity to the previous mean, effectively capturing mean shifts for all learned classes with each new task. We also apply Mahalanobis distance constraint for covariance calibration, aligning class-specific embedding covariances between old and current networks to mitigate the covariance shift. Additionally, we integrate a feature-level self-distillation approach to enhance generalization. Comprehensive experiments on commonly used datasets demonstrate the effectiveness of our approach. The source code is available at https://github.com/fwu11/MACIL.git.","Modern AI systems typically learn all categories at once, but in real-world scenarios, they must continually absorb new classes over time without forgetting previously acquired knowledge—a challenge known as class-incremental learning (CIL). This becomes particularly difficult when the model lacks task identity information, as internal feature statistics—specifically the mean (""center"") and covariance (""spread"")—shift in ways that cause representations of older classes to drift.To address this, we propose a semantic drift calibration method. First, we compensate for mean shifts by tracking how new sample embeddings move relative to existing class centers, capturing task-level shifts as each new class is introduced. Next, we align class-specific covariance using a Mahalanobis distance constraint, effectively calibrating the feature spread between the previous and current models.Evaluated on standard CIL benchmarks, our method significantly improves accuracy across both old and new classes. By directly correcting the statistical roots of semantic drift, this work moves us closer to AI systems capable of continuous, robust, and graceful adaptation in dynamic environments."
Poster,Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46651,"Cheol Kim, Jai Moondra, Shresth Verma, Madeleine Pollack, Lingkai Kong, Milind Tambe, Swati Gupta","In many real-world applications of Reinforcement Learning (RL), deployed policies have varied impacts on different stakeholders, creating challenges in reaching consensus on how to effectively aggregate their preferences. Generalized $p$-means form a widely used class of social welfare functions for this purpose, with broad applications in fair resource allocation, AI alignment, and decision-making. This class includes well-known welfare functions such as Egalitarian, Nash, and Utilitarian welfare. However, selecting the appropriate social welfare function is challenging for decision-makers, as the structure and outcomes of optimal policies can be highly sensitive to the choice of $p$. To address this challenge, we study the concept of an $\alpha$-approximate portfolio in RL, a set of policies that are approximately optimal across the family of generalized $p$-means for all $p \in [-\infty, 1]$. We propose algorithms to compute such portfolios and provide theoretical guarantees on the trade-offs among approximation factor, portfolio size, and computational efficiency. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of our approach in summarizing the policy space induced by varying $p$ values, empowering decision-makers to navigate this landscape more effectively.","In many real-world applications of reinforcement learning, a single policy can affect different stakeholders in very different ways. This creates a challenge: how do we fairly aggregate these diverse preferences when making policy deployment decisions? A common approach is to use a social welfare function, which combines the utilities of all stakeholders into a single measure of overall benefit. However, there is no universally ""right"" social welfare function. Each one reflects a different notion of fairness, and choosing among them can be difficult. To address this challenge, we develop an algorithm that constructs a small portfolio of policies that are approximately optimal across an entire family of social welfare functions known as the generalized $p$-means. This allows decision-makers to choose from a concise set of high-quality policies without committing to a single fairness principle in advance. Our method helps visualize how stakeholder outcomes shift as fairness preferences change, enabling more transparent and informed decision-making."
Poster,NBDI: A Simple and Effective Termination Condition for Skill Extraction from Task-Agnostic Demonstrations,https://ICML.cc//virtual/2025/poster/46269,"Myunsoo Kim, Hayeong Lee, Seong-Woong Shim, JunHo Seo, Byung-Jun Lee","Intelligent agents are able to make decisions based on different levels of granularity and duration. Recent advances in skill learning enabled the agent to solve complex, long-horizon tasks by effectively guiding the agent in choosing appropriate skills. However, the practice of using fixed-length skills can easily result in skipping valuable decision points, which ultimately limits the potential for further exploration and faster policy learning.In this work, we propose to learn a simple and effective termination condition that identifies decision points through a state-action novelty module that leverages agent experience data.Our approach, Novelty-based Decision Point Identification (NBDI), outperforms previous baselines in complex, long-horizon tasks, and remains effective even in the presence of significant variations in the environment configurations of downstream tasks, highlighting the importance of decision point identification in skill learning.","In reinforcement learning, skills (i.e., sequences of low-level actions) help agents solve long-horizon tasks. However, most prior methods rely on fixed-length skills, which can miss critical decision points, such as crossroads in navigation tasks. This limits exploration and slows down policy learning in downstream tasks.We introduce NBDI (Novelty-Based Decision point Identification), a simple yet effective method that uses state-action novelty to detect when a skill should terminate. By quantifying the novelty of a state-action pair, NBDI adaptively assigns variable-length skills using task-agnostic demonstrations, without requiring prior knowledge of the downstream task.Our method enables agents to make new decisions at meaningful points, improving their ability to generalize to new, more complex environments. Empirical results demonstrate that NBDI improves performance across diverse domains, including maze navigation and robotic manipulation, and maintains its effectiveness even under significant changes in the configuration of long-horizon downstream tasks."
Poster,Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback,https://ICML.cc//virtual/2025/poster/46174,"Qiwei Di, Jiafan He, Quanquan Gu","Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction.To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm namely robust contextual dueling bandits ($\texttt{RCDB}$), which is based on uncertainty-weighted maximum likelihood estimation.  Our algorithm achieves an $\tilde O(d\sqrt{T}/\kappa+dC/\kappa)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, $\kappa$ is the lower bound of the derivative of the link function, and $  0 \le C \le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Our work is the first to achieve nearly minimax optimal regret for dueling bandits in the presence of adversarial preference feedback. Additionally, for the sigmoid link function, we develop a novel algorithm that takes into account the effect of local derivatives into maximum likelihood estimation (MLE) analysis through a refined method for estimating the link function's derivative. This method helps us to eliminate the $\kappa$ dependence in the leading term with respect to $T$, which reduces the exponential dependence on the parameter radius $B$ to a polynomial dependence. We conduct experiments to evaluate our proposed algorithm $\texttt{RCDB}$ against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.","Learning from human feedback is vital for aligning AI models like large language models (LLMs) with human values, but adversaries can provide misleading feedback to manipulate these models. We introduce a robust learning approach, RCDB, that effectively handles deceptive feedback by weighing uncertainty and adapting accordingly. Our method achieves near-optimal performance, both in theory and practice, even under adversarial conditions. For scenarios where the link function is sigmoid, we further propose RCDB-S, which offers improved theoretical guarantees."
Poster,Nearly Optimal Sample Complexity for Learning with Label Proportions,https://ICML.cc//virtual/2025/poster/43817,"Robert Busa-Fekete, Travis Dick, Claudio Gentile, Haim Kaplan, Tomer Koren, Uri Stemmer","We investigate Learning from Label Proportions (LLP), a partial information setting where examples in a training set are grouped into bags, and only aggregate label values in each bag are available. Despite the partial observability, the goal is still to achieve small regret at the level of individual examples. We give results on the sample complexity of LLP under square loss, showing that our sample complexity is essentially optimal. From an algorithmic viewpoint, we rely on carefully designed variants of Empirical Risk Minimization, and Stochastic Gradient Descent algorithms, combined with ad hoc variance reduction techniques. On one hand, our theoretical results improve in important ways on the existing literature on LLP, specifically in the way the sample complexity depends on the bag size. On the other hand, we validate our  algorithmic solutions on several datasets, demonstrating improved empirical performance (better accuracy for less samples)  against recent baselines.","We all know that for a machine learning model to learn, it needs labeled examples. For instance, if you want a system to recognize cats, you feed it many pictures, each clearly marked ""cat"" or ""not cat."" But what if you don't have individual labels for every picture? What if you only know the percentage of cats in a group of pictures? This is the core challenge in Learning from Label Proportions (LLP). Imagine you have ""bags"" of photos, and for each bag, you're just told, ""This bag has 60% cats."" You don't know which specific photos are cats. Even with this limited information, our goal is to train a model that can accurately identify individual cats (or non-cats) when given new, unseen photos.In this paper, we dive into LLP, focusing on a critical question: How much data do you need to train a good model when you only have these ""bag-level"" labels? We look at what's called ""square loss,"" which is a common way to measure how well a model is doing. Our findings show that our method needs a nearly perfect (optimal) amount of data, meaning it's very efficient.From a practical side, our algorithms are built using smart variations of standard machine learning techniques like Empirical Risk Minimization (ERM) and Stochastic Gradient Descent (SGD). The key innovation is special variance reduction techniques that we've carefully designed.What Makes Our Work Stand Out?1. Theoretical Improvements: Our research significantly advances the theory of LLP. We've shown that our approach needs less data than previous methods, especially considering the large bag size scenario. This means our models can learn effectively even when the label information is very sparse.2. Real-World Performance: We've also tested our algorithms on various real-world datasets. Our results demonstrate that our solutions perform better than recent, comparable methods. This means you can get more accurate results with less training data, which is a big win for practical applications.In essence, we're making it much more feasible to train powerful machine learning models even when detailed, individual labels are scarce, opening doors for applications where such precise labeling is expensive or impossible."
Poster,NEAR: Neural Electromagnetic Array Response,https://ICML.cc//virtual/2025/poster/43524,"Yinyan Bu, Jiajie Yu, Kai Zheng, Xinyu Zhang, Piya Pal","We address the challenge of achieving angular super-resolution in multi-antenna radar systems that are widely used for localization, navigation, and automotive perception. A multi-antenna radar achieves very high resolution by computationally creating a large virtual sensing system using very few physical antennas. However, practical constraints imposed by hardware, noise, and a limited number of antennas can impede its performance. Conventional supervised learning models that rely on extensive pre-training with large datasets, often exhibit poor generalization in unseen environments. To overcome these limitations, we propose NEAR, an untrained implicit neural representation (INR) framework that predicts radar responses at unseen locations from sparse measurements, by leveraging latent harmonic structures inherent in radar wave propagation. We establish new theoretical results linking antenna array response to expressive power of INR architectures, and develop a novel physics-informed and latent geometry-aware regularizer. Our approach integrates classical signal representation with modern implicit neural learning, enabling high-resolution radar sensing that is both interpretable and generalizable. Extensive simulations and real-world experiments using radar platforms demonstrate NEAR's effectiveness and its ability to adapt to unseen environments.","MIMO antenna systems have emerged as a crucial sensing modality for advanced sensing tasks such as driver assistance systems (ADAS) and autonomous vehicles, especially due to its robustness to adverse weather conditions. However, practical constraints imposed by hardware, noise, limited budget of antennas, and dynamic environment can impede its sensing performance. Can we achieve super-resolution sensing using single-shot data under sample-starved conditions and stringent hardware constraints?Instead of building new hardware, we developed new algorithms. Inspired by implicit neural representations (INRs), we propose NEAR, a method that models each coordinate in the antenna space as a continuous function representing the expected EM wave response at that point. Guided by the physics of planar wave propagation and the underlying harmonic structure, NEAR learns these functions directly from single-shot measurements collected by sparse antennas, and predicts the EM response at arbitrary, unseen locations.Overall, we believe our findings contribute to advancing research in INRs and their unique applications in super-resolution active sensing. Our work also marks the first step towards leveraging INRs for predicting unseen antenna responses in MIMO antenna sensing, paving the way for new opportunities to enhance the performance of future sensing and localization systems."
Poster,Near Optimal Best Arm Identification for Clustered Bandits,https://ICML.cc//virtual/2025/poster/46538,"Yash Kheshwani, Avishek Ghosh, Nikhil Karamchandani","This work investigates the problem of best arm identification for multi-agent multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where each cluster solves a stochastic bandit problem. The mapping between agents and bandits is \textit{a priori} unknown. Each bandit is associated with $K$ arms, and the goal is to identify the best arm for each agent under a $\delta$-probably correct ($\delta$-PC) framework, while minimizing sample complexity and communication overhead. We propose two novel algorithms: \emph{Clustering then Best Arm Identification} (\texttt{Cl-BAI}) and \emph{Best Arm Identification then Clustering} (\texttt{BAI-Cl}). \texttt{Cl-BAI} employs a two-phase approach that first clusters agents based on the bandit problems they are learning, followed by identifying the best arm for each cluster. \texttt{BAI-Cl} reverses the sequence by identifying the best arms first and then clustering agents accordingly. Both algorithms exploit the successive elimination framework to ensure computational efficiency and high accuracy. Theoretical analysis establishes $\delta$-PC guarantees for both methods, derives bounds on their sample complexity, and provides a lower bound for the problem class. Moreover, when $M$ is small (a constant), we show that the sample complexity of (a variant of) \texttt{BAI-Cl} is (order-wise) minimax optimal. Experiments on synthetic and real-world (Movie Lens, Yelp) data demonstrates the superior performance of the proposed algorithms in terms of sample and communication efficiency, particularly in settings where $M \ll N$.",We address the problem where multiple agents are grouped into clusters based on the task they are handling. We study this in the sequential framework where data is arriving one-by-one. We propose algorithms to solve this sequential clustering problem and validated them via experiments.
Poster,Near-Optimal Consistency-Robustness Trade-Offs for Learning-Augmented Online Knapsack Problems,https://ICML.cc//virtual/2025/poster/46157,"Mohammadreza Daneshvaramoli, Helia Karisani, Adam Lechowicz, Bo Sun, Cameron Musco, Mohammad Hajiesmaili","This paper introduces a family of learning-augmented algorithms for online knapsack problems that achieve near Pareto-optimal consistency-robustness trade-offs through a simple combination of trusted learning-augmented and worst-case algorithms. Our approach relies on succinct, practical predictions—single values or intervals estimating the minimum value of any item in an offline solution. Additionally, we propose a novel fractional-to-integral conversion procedure, offering new insights for online algorithm design.","This work tackles a classic challenge: how to make good decisions when items arrive one at a time and we must act immediately. We study this in the context of the online knapsack problem, where the goal is to select high-value items without exceeding a fixed budget.Our key idea is to use simple machine learning predictions—just a single value or a range—to help guide decisions. We design new algorithms that are robust when predictions are wrong and high-performing when predictions are accurate. These algorithms come close to the best possible trade-off between the two.We also show how to adapt our methods for realistic cases and demonstrate their effectiveness on both synthetic and real-world data."
