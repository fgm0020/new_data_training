type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Representative Language Generation,https://ICML.cc//virtual/2025/poster/46100,"Charlotte Peale, Vinod Raman, Omer Reingold","We introduce ""representative generation,"" extending the theoretical framework for generation proposed by Kleinberg et al. (2024) and formalized by Li et al. (2024), to additionally address diversity and bias concerns in generative models. Our notion requires outputs of a generative model to proportionally represent groups of interest from the training data.  We characterize representative uniform and non-uniform generation, introducing the ``group closure dimension'' as a key combinatorial quantity. For representative generation in the limit, we analyze both information-theoretic and computational aspects, demonstrating feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but proving a negative result for computability using only membership queries. This contrasts with Kleinberg et al.'s (2024) positive results for standard generation in the limit. Our findings provide a rigorous foundation for developing more diverse and representative generative models.","Modern generative AI models, like large language models, can produce convincing text or images but often fail to reflect the true diversity of their training data. For example, even if trained on data representing many groups, they might only generate outputs from a narrow subset—like only showing cats from a dataset of various animals. This paper introduces a new concept called representative generation, which requires models not just to be accurate, but also to reflect the proportions of different groups seen in training. We develop a theoretical framework to determine when such representative generation is possible and show it’s sometimes harder than standard generation. We also prove that achieving representation with limited computational tools is fundamentally impossible in some cases. This work lays a foundation for designing fairer and more inclusive generative AI systems that better represent the diversity present in real-world data."
Poster,Representative Ranking for Deliberation in the Public Sphere,https://ICML.cc//virtual/2025/poster/45374,"Manon Revel, Smitha Milli, Tyler Lu, Jamelle Watson-Daniels, Maximilian Nickel","Online comment sections, such as those on news sites or social media, have the potential to foster informal public deliberation, However, this potential is often undermined by the frequency of toxic or low-quality exchanges that occur in these settings. To combat this, platforms increasingly leverage algorithmic ranking to facilitate higher-quality discussions, e.g., by using civility classifiers or forms of prosocial ranking. Yet, these interventions may also inadvertently reduce the visibility of legitimate viewpoints, undermining another key aspect of deliberation: representation of diverse views. We seek to remedy this problem by introducing guarantees of representation into these methods. In particular, we adopt the notion of *justified representation* (JR) from the social choice literature and incorporate a JR constraint into the comment ranking setting. We find that enforcing JR leads to greater inclusion of diverse viewpoints while still being compatible with optimizing for user engagement or other measures of conversational quality.","Online comment sections on news sites and social media can encourage public discussions, but they often suffer from toxic or low-quality interactions. To improve the quality of these discussions, algorithms are used to rank comments, focusing on civility and positive interactions. However, these methods can unintentionally limit the diversity of opinions represented. To address this, we propose a new approach that ensures diverse viewpoints are included by using a concept from social choice theory called justified representation (JR). By applying JR to comment ranking, our research shows that it significantly enhances the representation and inclusivity of different views without negatively affecting the quality of conversations or user engagement."
Poster,ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation,https://ICML.cc//virtual/2025/poster/44554,"Angxiao Yue, Zichong Wang, Hongteng Xu","Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications.Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency.In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format.We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves on-par performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37$\times$ faster than RFDiffusion and 63$\times$ faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency.","Imagine creating a 3D sculpture with small interconnected pieces. This process is similar to designing new proteins. Proteins are essential for life and significant for many biological and medical applications. Generating the skeleton or ""backbone"" of proteins is a crucial step, but existing computer methods are often slow and can't produce high-quality designs.Our ReQFlow teaches the computer to design protein backbones more efficiently and accurately, with a clever mathematical approach (based on ""quaternions"" and a ""rectified flow"" technique). The results show that we are up to 63 times faster than some current strong methods—and produce better quality protein backbones, even for very long ones that were previously difficult to design.Our findings could speed up the discovery of new drugs, help create enzymes for biocatalysis, and unlock the potential in designing advanced biological materials."
Poster,Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger,https://ICML.cc//virtual/2025/poster/46017,"Qi Yang, Chenghao Zhang, Lubin Fan, Kun Ding, Jieping Ye, Shiming Xiang","Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns.  We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples.  This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs.","Visual Question Answering systems, which answer questions about images, often struggle when they lack enough examples showing how to reason through complex questions. Even when they find relevant examples, their answers can be inconsistent or unreliable.To solve this, we developed a new framework called RCTS that helps AI models better understand and use existing knowledge. Our method builds a richer knowledge base by identifying and reinforcing consistent reasoning patterns. We also introduced a smart search technique, inspired by game-playing strategies, to pick the most helpful examples for answering each question.This approach significantly improves the accuracy and consistency of AI-generated answers on a variety of image-based question-answering tasks. Our results show that RCTS outperforms current leading methods, offering a promising step forward in making AI systems more reliable when interpreting visual content and responding to natural language questions."
Poster,ResearchTown: Simulator of Human Research Community,https://ICML.cc//virtual/2025/poster/46055,"Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, Jiaxuan You","Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. In this work, we propose ResearchTown, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified as an agent-data graph, where researchers and papers are represented as agent-type and data-type nodes, respectively, and connected based on their collaboration relationships. We also introduce TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research community simulation, we present ResearchBench, a benchmark that uses a node-masking prediction task for scalable and objective assessment based on similarity. Our experiments reveal three key findings: (1) ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) ResearchTown can maintain robust simulation with multiple researchers and diverse papers; (3) ResearchTown can generate interdisciplinary research ideas that potentially inspire pioneering research directions.","Scientific discovery is often driven by collaboration—researchers reading, writing, and reviewing papers together. But can we simulate human research communities with LLMs?We introduce ResearchTown, a simulation framework that represents the research community as a graph of agents (researchers) and data (papers). Each LLM-powered agent engages in key research activities—reading, writing, and reviewing papers—while collaborating with other agents. To formalize these interactions, we develop TextGNN, a text-based message-passing framework that captures all collaborative activities through a unified modeling approach.To evaluate the quality of the simulation, we build a benchmark that measures whether the simulator can generate realistic research content. Our results show that ResearchTown can accurately model collaborative research workflows and even produce novel interdisciplinary ideas—for example, combining insights from natural language processing and criminology. This work opens up new opportunities to study how scientific ideas emerge and offers a path toward AI-assisted research innovation."
Poster,Residual Matrix Transformers: Scaling the Size of the Residual Stream,https://ICML.cc//virtual/2025/poster/45278,"Brian Mak, Jeffrey Flanigan","The residual stream acts as a memory bus where transformer layers both store and access features (Elhage et al., 2021). We consider changing the mechanism for retrieving and storing information in the residual stream, and replace the residual stream of the transformer with an outer product memory matrix (Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix Transformer (RMT). We find that the RMT enjoys a number of attractive properties: 1) the size of the residual stream can be scaled independently of compute and model size, improving performance, 2) the RMT can achieve the same loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41% fewer training tokens tokens, and 3) the RMT outperforms the transformer on downstream evaluations. We theoretically analyze the transformer and the RMT, and show that the RMT allows for more efficient scaling of the residual stream, as well as improved variance propagation properties.","Traditional AI language models are based on the transformer architecture. The transformer uses a ""residual stream"" - essentially a memory highway where different parts of the model store and retrieve information. However, this memory system is limited in how much information it can store and it is expensive to scale up, requiring more computational power and parameters.  We replaced this traditional memory highway with a new ""outer product memory matrix"" system, creating what we call the Residual Matrix Transformer (RMT). This new approach allows the size of the ""residual stream"" to scale independently from the overall model size and computation requirements. Our RMT achieves the same performance as traditional transformers while using 58% fewer computational operations, 25% fewer parameters, and 41% less training data. This breakthrough could make powerful AI language models more efficient and accessible, reducing the computational costs and energy requirements for training and running advanced AI systems."
Poster,Residual TPP: A Unified Lightweight Approach for Event Stream Data Analysis,https://ICML.cc//virtual/2025/poster/46171,"Ruoxin Yuan, Guanhua Fang","This work introduces Residual TPP, a novel, unified, and lightweight approach for analyzing event stream data. It leverages the strengths of both simple statistical TPPs and expressive neural TPPs to achieve superior performance. Specifically, we propose the Residual Events Decomposition (RED) technique in temporal point processes, which defines a weight function to quantify how well the intensity function captures the event characteristics. The RED serves as a flexible, plug-and-play module that can be integrated with any TPP model in a wide range of tasks. It enables the identification of events for which the intensity function provides a poor fit, referred to as residual events. By combining RED with a Hawkes process, we capture the self-exciting nature of the data and identify residual events. Then an arbitrary neural TPP is employed to take care of residual events. Extensive experimental results demonstrate that Residual TPP consistently achieves state-of-the-art goodness-of-fit and prediction performance in multiple domains and offers significant computational advantages as well.","Many systems rely on analyzing sequences of events over time, such as online shopping activity, social media interactions, or financial transactions. Traditional models used to fit and predict these events are either simple but limited, or powerful but computationally heavy.We introduce Residual TPP, a new method that combines the strengths of both. It starts with a simple model to capture overall patterns like periodicity and self-excitation, then identifies events where the base model fails to explain, referred to as residual events. These residuals are then handled by a more expressive neural network, which can learn deeper patterns. This approach is both flexible and lightweight, making it suitable for a wide range of data without requiring heavy computation.Experiments show that Residual TPP not only outperforms existing methods in terms of goodness-of-fit and prediction accuracy but also runs faster. This makes it a powerful tool for researchers and industries working with complex event stream data."
Poster,ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral Residuals,https://ICML.cc//virtual/2025/poster/45196,"Yuanchao Xu, Kaidi Shao, Nikos Logothetis, Zhongwei Shen","Analyzing the long-term behavior of high-dimensional nonlinear dynamical systems remains a significant challenge. While the Koopman operator framework provides a powerful global linearization tool, current methods for approximating its spectral components often face theoretical limitations and depend on predefined dictionaries. Residual Dynamic Mode Decomposition (ResDMD) advanced the field by introducing the \emph{spectral residual} to assess Koopman operator approximation accuracy; however, its approach of only filtering precomputed spectra prevents the discovery of the operator's complete spectral information, a limitation known as the `spectral inclusion' problem. We introduce ResKoopNet (Residual-based Koopman-learning Network), a novel method that directly addresses this by explicitly minimizing the \emph{spectral residual} to compute Koopman eigenpairs. This enables the identification of a more precise and complete Koopman operator spectrum. Using neural networks, our approach provides theoretical guarantees while maintaining computational adaptability. Experiments on a variety of physical and biological systems show that ResKoopNet achieves more accurate spectral approximations than existing methods, particularly for high-dimensional systems and those with continuous spectra, which demonstrates its effectiveness as a tool for analyzing complex dynamical systems.","Understanding how complex systems change over time is a major scientific challenge. Current methods try to find underlying simple patterns in these systems but often get misled by missing crucial information. We've developed a new data-driven approach called ResKoopNet that acts like an intelligent pattern-finder. It automatically learns the best way to view a complex system to reveal its hidden properties and constantly works to minimize any errors in the patterns it detects. We tested ResKoopNet on various complex systems, from physical models to biological brain activities, and found it significantly more accurate at uncovering the true, complete patterns. This could lead to better ways to analyze and predict the behavior of many complex systems in science and engineering."
Poster,Resolving Lexical Bias in Model Editing,https://ICML.cc//virtual/2025/poster/44799,"Hammad Rizwan, Domenic Rosati, Ga Wu, Hassan Sajjad","Model editing aims to modify the outputs of large language models after they are trained. Previous approaches have often involved direct alterations to model weights, which can result in model degradation. Recent techniques avoid making modifications to the model's weights by using an adapter that applies edits to the model when triggered by semantic similarity in the representation space. We demonstrate that current adapter methods are *critically vulnerable* to strong lexical biases, leading to issues such as applying edits to irrelevant prompts with overlapping words. This paper presents a principled approach to learning a disentangled representation space that facilitates precise localization of edits by maintaining distance between irrelevant prompts while preserving proximity among paraphrases. In our empirical study, we show that our method (Projector Editor Networks for Model Editing - PENME) achieves state-of-the-art model editing results while being more computationally efficient during inference than previous methods and adaptable across different architectures.","Language models constantly need to learn new information, but directly rewriting or retraining them can accidentally make them ""forget"" things they already knew. Imagine trying to update a textbook by erasing and rewriting pages, but without realising it, you accidentally erase other important information.To fix this, a common approach to updating these models is to store each correction separately, and while looking at how the model processes the new prompt, apply it when it seems **similar to a past correction**. Our paper shows that this “similarity test"" often falls short. It tends to be tricked by lexical bias, meaning it focuses too much on shared words rather than the actual meaning. For example, if you corrected the model on ""The twin city of Portsmouth,"" it might mistakenly apply that correction to ""The twin city of Pittsburgh"" just because both sentences contain ""The twin city of,"" even though they refer to different places. This leads to misfires, where the model applies the wrong edit.To solve this issue, we introduce **Projector Editor Networks for Model Editing (PENME)**. PENME uses a small ""projector"" that transforms a prompt into a space where **meaning outranks wording**. This way, the model becomes much better at recognising when a new prompt truly relates to a past correction, rather than being tricked by similar words.Our system helps language models learn and remember new information better, without forgetting or confusing what they already know. PENME is also easy to use with different language models, offering a fast and reliable way to keep them up to date."
Poster,ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals,https://ICML.cc//virtual/2025/poster/46466,"Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang","Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g.~8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers.  We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama and Qwen2.5 families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33\% lower perplexity on Wikitext than the next best method SpinQuant, and upto 3X speedup over 16-bit baseline. Anonymous code repository available  at https://anonymous.4open.science/r/project-resq-2142.","ResQ: Making AI Models Smaller, Faster, and SmarterLarge language models (LLMs) like ChatGPT and Google Bard are incredibly powerful, but they’re also massive and resource-hungry. Running these models quickly and efficiently is a big challenge, especially on limited hardware like laptops or smartphones. That’s where ResQ comes in. ResQ is a new method that makes these giant models much smaller and faster without sacrificing their brainpower. It does this by a smart trick: it figures out which parts of the model need more attention and keeps those in high detail, while safely shrinking the less important parts. By combining mathematical techniques (like PCA and random rotations) with efficient hardware implementation, ResQ can shrink models to just 4-bit precision - 4 times smaller compared to the usual 16 bits - while keeping performance strong. It results in up to 5× faster processing and much lower memory use, all while keeping AI models smart enough for tasks like language understanding, reasoning, and even multi-modal tasks (like combining text with images). ResQ’s approach is also robust as it works across different models like LLaMA and Qwen, and can even run huge models on a single graphics card instead of needing multiple high-end servers. This could help make AI more accessible, running efficiently on everyday devices. In summary, ResQ makes AI models smaller, faster, and more efficient, paving the way for smarter AI everywhere."
