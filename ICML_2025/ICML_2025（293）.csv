type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks,https://ICML.cc//virtual/2025/poster/44998,"Jeongmo Kim, Yisak Park, Minung Kim, Seungyul Han","Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments. Our code is available at https://github.com/JM-Kim-94/tavt.git.","Can an agent that has learned only the forward movement move in a different direction? Agents that be applied in real world shuld be able to adapt to changing environments or various tasks. In particular, it is very challenging for agent to adapt to unseen tasks. To overcome this, the TAVT algorithm proposes two main methods. First, an agent learns the training task set based on the newly defined task measurement (Task-Aware structure). Second, it creates virtual tasks based on the learned task set and uses them for agent learning (Virtual Training structure). Due to this virtual training scheme, the agent trained by the TAVT can adapt to unseen tasks that have not actually been experienced."
Poster,Task-Gated Multi-Expert Collaboration Network for Degraded Multi-Modal Image Fusion,https://ICML.cc//virtual/2025/poster/45432,"Yiming Sun, Xin Li, Pengfei Zhu, Qinghua Hu, Dongwei Ren, Huiying Xu, Xinzhong Zhu","Multi-modal image fusion aims to integrate complementary information from different modalities to enhance perceptual capabilities in applications such as rescue and security. However, real-world imaging often suffers from degradation issues, such as noise, blur, and haze in visible imaging, as well as stripe noise in infrared imaging, which significantly degrades model performance. To address these challenges, we propose a task-gated multi-expert collaboration network (TG-ECNet) for degraded multi-modal image fusion. The core of our model lies in the task-aware gating and multi-expert collaborative framework, where the task-aware gating operates in two stages: degradation-aware gating dynamically allocates expert groups for restoration based on degradation types, and fusion-aware gating guides feature integration across modalities to balance information retention between fusion and restoration tasks. To achieve this, we design a two-stage training strategy that unifies the learning of restoration and fusion tasks. This strategy resolves the inherent conflict in information processing between the two tasks, enabling all-in-one multi-modal image restoration and fusion. Experimental results demonstrate that TG-ECNet significantly enhances fusion performance under diverse complex degradation conditions and improves robustness in downstream applications. The code is available at https://github.com/LeeX54946/TG-ECNet.","This paper proposes a unified framework for degraded multi-modal image restoration and fusion, which bridges different tasks together through a two-stage training strategy to learn inter-task information while avoiding mutual interference, enabling all-in-one processing.This paper proposes the task-aware gating and multi-expert collaboration module. The degradation-aware gating adapts to different degradation types and selects the optimal expert group for image restoration; while the fusion-aware gating dynamically balances the information retention between fusion and restoration tasks to achieve better fusion performance.This paper constructs a large-scale degraded multi-modal image fusion benchmark, DeMMI-RF, which contains more than $30,000$ multi-modal data of different degradation types, including those from UAVs and driving viewpoints. Results on multiple datasets validate the superior performance of the model in complex degraded scenarios and robustness for downstream applications."
Poster,Task Generalization with Autoregressive Compositional Structure: Can Learning from $D$ Tasks Generalize to $D^T$ Tasks?,https://ICML.cc//virtual/2025/poster/44350,"Amirhesam Abedsoltan, Huaqing Zhang, Kaiyue Wen, Hongzhou Lin, Jingzhao Zhang, Misha Belkin","Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations. This raises a fundamental question: When can learning from a small set of tasks  generalize to a large task family? In this paper, we investigate task generalization through the lens of autoregressive compositional structure, where each task is a composition of T operations, and each operation is among a finite family of D subtasks. This yields a total class of size~D^T. We first show that generalization to all D^T  tasks is theoretically achievable by training on only \tilde{O}(D) tasks. Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via In-context Learning (ICL) and chain-of-thought (CoT) reasoning. We further demonstrate this exponential generalization in arithmetic and language translation, extending beyond parity functions.","Large language models (LLMs) exhibit a striking ability to solve tasks they were never explicitly trained on. Unlike classical supervised learning—which typically assumes that test data comes from the same distribution as training data—LLMs can generalize to entirely new task distributions given only a few demonstrations. This phenomenon is known as in-context learning (ICL).We study this capability of LLMs through the lens of autoregressive compositionality, focusing on tasks that can be decomposed into $T$ operations, each selected from only $D$ possibilities. Although the total number of tasks grows exponentially $D^T$, we prove—mathematically—that a model trained on $D\cdot\ln(D)$ randomly chosen tasks can generalize to the entire task family.Our experiments with Transformer models support this theory. When prompted using chain-of-thought before answering, the model learns from solving a small set of training tasks to correctly handling many of new ones. We demonstrate this across parity problems, arithmetic, and multi-step language translation."
Poster,TCP-Diffusion: A Multi-modal Diffusion Model for Global Tropical Cyclone Precipitation Forecasting with Change Awareness,https://ICML.cc//virtual/2025/poster/46214,"Cheng Huang, Pan Mu, Cong Bai, Peter Watson","Deep learning methods have made significant progress in regular rainfall forecasting, yet the more hazardous tropical cyclone (TC) rainfall has not received the same attention. While regular rainfall models can offer valuable insights for designing TC rainfall forecasting models, most existing methods suffer from cumulative errors and lack physical consistency. Additionally, these methods overlook the importance of meteorological factors in TC rainfall and their integration with the numerical weather prediction (NWP) model. To address these issues, we propose Tropical Cyclone Precipitation Diffusion (TCP-Diffusion), a multi-modal model for forecasting of TC precipitation given an existing TC in any location globally. It forecasts rainfall around the TC center for the next 12 hours at 3 hourly resolution based on past rainfall observations and multi-modal environmental variables. Adjacent residual prediction (ARP) changes the training target from the absolute rainfall value to the rainfall trend and gives our model the capability of rainfall change awareness, reducing cumulative errors and ensuring physical consistency. Considering the influence of TC-related meteorological factors and the useful information from NWP model forecasts, we propose a multi-model framework with specialized encoders to extract richer information from environmental variables and results provided by NWP models. The results of extensive experiments show that our method outperforms other DL methods and the NWP method from the European Centre for Medium-Range Weather Forecasts (ECMWF).","Tropical cyclones (TCs), such as typhoons and hurricanes, often bring extreme rainfall that can lead to devastating floods. Predicting this rainfall in advance is crucial for disaster preparedness, but remains a major scientific challenge. Most deep learning models that predict regular rainfall don’t work well in the case of TCs — they often accumulate errors over time and ignore important weather patterns.In this work, we introduce TCP-Diffusion, a machine learning model designed specifically for forecasting rainfall caused by tropical cyclones. Our model looks at past rainfall and a wide range of environmental conditions to predict where and how much it will rain around a cyclone over the next 12 hours. A key innovation is that our model learns to predict how rainfall will change, rather than its exact value — this helps reduce long-term errors and ensures the predictions make physical sense. We also combine our model with outputs from traditional weather forecasting systems to boost its accuracy.Our method outperforms both standard deep learning models and advanced numerical weather prediction systems in forecasting TC rainfall."
Poster,Teaching Language Models to Critique via Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45115,"Zhihui Xie, Jie chen, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong","Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide *accurate judgments* and *actionable suggestions*. In this work, we study LLM critics for code generation and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic $\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models.Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1\% relative improvements across challenging code generation benchmarks.","Large language models (LLMs) struggle to effectively critique their own work, lacking the ability to provide accurate and actionable feedback needed for self-improvement, especially in areas like code generation.We introduce CTRL, a technique that trains an LLM critic using reinforcement learning—without direct human examples—to become skilled at providing clear, corrective feedback on code.These CTRL-trained critics significantly boost code accuracy and reduce accumulated errors, even when assisting more powerful AI models, enabling AI systems to improve more efficiently through cycles of feedback and revision."
Poster,Teaching Physical Awareness to LLMs through Sounds,https://ICML.cc//virtual/2025/poster/46139,"Weiguo Wang, Andy Nie, Wenrui Zhou, Yi Kai, Chengchen Hu","Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, yet they fundamentally lack physical awareness--understanding of real-world physical phenomena.In this work, we present ACORN, a framework that teaches LLMs physical awareness through sound, focusing on fundamental physical phenomena like the Doppler effect, multipath effect, and spatial relationships. To overcome data scarcity, ACORN introduce a physics-based simulator combining real-world sound sources with controlled physical channels to generate diverse training data. Using this simulator, we build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an audio encoder that processes both magnitude and phase information. By connecting our audio encoder to state-of-the-art LLMs, we demonstrate reasonable results in both simulated and real-world tasks, such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, paving the way for enabling LLMs to understand physical world.","Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, but they struggle with a fundamental challenge: understanding the physical world. In contrast, humans rely heavily on sound to perceive their surroundings—we can hear if someone is nearby, whether a car is moving toward us, or if a voice is coming from behind a wall.We present ACORN, a framework that teaches LLMs to understand the physical world through sound and develop human-like hearing. Instead of collecting expensive real-world recordings, we simulate how sound travels through space—capturing physical effects like motion, echo, and direction—and generate a large dataset of sound-based questions and answers. We also design a new audio encoder that mimics how humans process both the intensity and timing of sound. Our results show that LLMs can learn to detect whether a voice is blocked by a wall, estimate motion via Doppler shifts, and locate where sounds are coming from. This opens the door to safer voice-controlled systems, smarter robots, and AI that responds more naturally to the real world—making them not just smart, but also physically aware."
Poster,Teaching Transformers Causal Reasoning through Axiomatic Training,https://ICML.cc//virtual/2025/poster/46158,"Aniket Vashishtha, Abhinav Kumar, Atharva Pandey, Abbavaram Gowtham Reddy, Kabir Ahuja, Vineeth N Balasubramanian, Amit Sharma","For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since interventional data is costly to generate, we study to what extent an agent can learn  causal reasoning from passive data. Specifically, we consider an axiomatic training setup where an agent learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the agent would learn to generalize from the axiom demonstrations to new scenarios. For example, if a transformer model is trained on demonstrations of the causal transitivity axiom over small graphs, would it generalize to applying the transitivity axiom over large graphs? Our results, based on a novel axiomatic training scheme, indicate that such generalization is possible.  We consider the task of inferring whether a variable causes another variable, given a causal graph structure. We find that a 67 million parameter transformer model, when trained on linear causal chains (along with some noisy variations) can generalize well to new kinds of graphs, including longer causal chains, causal chains with reversed order, and graphs with branching; even when it is not explicitly trained for such settings. Our model performs at par (or even better) than many larger language models  such as GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework provides a new paradigm of learning causal reasoning from passive data that can be used to learn arbitrary axioms, as long as sufficient demonstrations can be generated.","Collecting the costly “what-if” data that usually teaches AI systems cause-and-effect is tricky, so we tried a simpler route: we showed a mid-sized transformer (67 million parameters) many short, synthetic examples of one basic causal rule: transitivity (“if A causes B and B causes C, then A causes C”). Remarkably, after this “axiomatic training” with variety in training data and changes in model while teaching it this rule, the model could spot causes in much larger and more complicated graphs it had never seen before, including longer chains, reversed chains, and branching structures, and it matched or even outperformed far bigger models like GPT-4, Gemini Pro, and Phi-3. The results suggest that by feeding AI enough clear demonstrations of any causal axiom, we can cheaply equip it with robust causal-reasoning skills important for real-world decision-making."
Poster,TeDS: Joint Learning of Diachronic and Synchronic Perspectives in Quaternion Space for Temporal Knowledge Graph Completion,https://ICML.cc//virtual/2025/poster/43888,"Jiujiang Guo, Mankun Zhao, Wenbin Zhang, Tianyi Xu, Linying Xu, Yu Jian, Yu Mei, Yu Ruiguo","Existing research on temporal knowledge graph completion treats temporal information as supplementary, without simulating various features of facts from a temporal perspective. This work summarizes features of temporalized facts from both diachronic and synchronic perspectives: (1) Diachronicity. Facts often exhibit varying characteristics and trends across different temporal domains; (2) Synchronicity. In specific temporal contexts, various relations between entities influence each other, generating latent semantics. To track above issues, we design a quaternion-based model, TeDS, which divides timestamps into diachronic and synchronic timestamps to support dual temporal perception: (a) Two composite quaternions fusing time and relation information are generated by reorganizing synchronic timestamp and relation quaternions, and Hamilton operator achieves their interaction. (b) Each time point is sequentially mapped to an angle and converted to scalar component of a quaternion using trigonometric functions to build diachronic timestamps. We then rotate relation by using Hamilton operator between it and diachronic timestamp. In this way, TeDS achieves deep integration of relations and time while accommodating different perspectives. Empirically, TeDS significantly outperforms SOTA models on six benchmarks.","(1) High-quality knowledge graphs (KGs) enable faster and more accurate extraction of key information, enhancing the efficiency of intelligent retrieval, reasoning, and decision-making. However, the quality of KGs often heavily depends on the methods of information acquisition, the scope of coverage, and the timeliness of information. These factors can lead to fluctuations in quality, which significantly affect the effectiveness and scalability of KGs in real-world applications. Therefore, it is essential to develop more refined and intelligent construction and completion mechanisms to support the sustainable development and efficient use of high-quality KGs. (2) This study abstracts the characteristic rules of factual knowledge in the physical world by drawing from real-world patterns: (a) **Diachronicity** Facts often exhibit different features and developmental trends across various time periods. (b) **Synchronicity** In specific temporal contexts, multiple relations between entities interact with each other, thereby generating latent semantics. Furthermore, we propose a quaternion-based model, TeDS, which divides timestamps into diachronic and synchronic timestamps to support dual temporal perception. (3) This will help simulate and understand knowledge graph structures from a real-world perspective, enhance the interpretability and expressiveness of knowledge representation, and further provide sustainable solutions for knowledge graph completion and construction, empowering practical applications."
Poster,Telling Peer Direct Effects from Indirect Effects in Observational Network Data,https://ICML.cc//virtual/2025/poster/43930,"Xiaojing Du, Jiuyong Li, Debo Cheng, Lin Liu, Wentao Gao, XIONGREN CHEN, Ziqi Xu","Estimating causal effects is crucial for decision-makers in many applications, but it is particularly challenging with observational network data due to peer interactions. Some algorithms have been proposed to estimate causal effects involving network data, particularly peer effects, but they often fail to tell apart diverse peer effects. To address this issue, we propose a general setting which considers both peer direct effects and peer indirect effects, and the effect of an individual's own treatment, and provide the identification conditions of these causal effects. To differentiate these effects, we leverage causal mediation analysis and tailor it specifically for network data. Furthermore, given the inherent challenges of accurately estimating effects in networked environments, we propose to incorporate attention mechanisms to capture the varying influences of different neighbors and to explore high-order neighbor effects using multi-layer graph neural networks (GNNs). Additionally, we employ the Hilbert-Schmidt Independence Criterion (HSIC) to further enhance the model’s robustness and accuracy. Extensive experiments on two semi-synthetic datasets derived from real-world networks and on a dataset from a recommendation system confirm the effectiveness of our approach. Our findings have the potential to improve intervention strategies in networked systems, particularly in social networks and public health.","People in networks—such as friends, classmates, or online connections—can influence us in two ways: their actions affect us directly, and their resulting outcomes affect us indirectly. Unfortunately, standard observational data cannot separate these two paths of influence. We propose gDIS, an approach that learns which connections matter most, automatically combines information across the network, and uses a statistical check to avoid misleading correlations. We tested gDIS on two semi-synthetic social networks and a dataset from a recommendation system, and it distinguished direct and indirect influences more accurately than existing methods. By clearly separating these pathways, our method can help health officials decide who to vaccinate first and marketers choose early adopters, leading to more efficient intervention strategies."
Poster,TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching,https://ICML.cc//virtual/2025/poster/46231,"Yue Meng, Chuchu Fan","Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF","Many robot tasks have complex instructions that include both timing and the goals, like ""reach the red zone within 5 seconds, but avoid obstacles at all times"". These instructions can be written in a formal language called Signal Temporal Logic (STL). But current AI models can only handle simple or fixed STLs. We developed TeLoGraF, a novel approach to learn solutions for general STL forms. We created a large dataset of 200K STL instructions paired with example solutions, and used a graph-based neural network to help the AI reason about the logic and generate the solutions."
