type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Towards the Causal Complete Cause of Multi-Modal Representation Learning,https://ICML.cc//virtual/2025/poster/46215,"Jingyao Wang, Siyu Zhao, Wenwen Qiang, Jiangmeng Li, Changwen Zheng, Fuchun Sun, Hui Xiong","Multi-Modal Learning (MML) aims to learn effective representations across modalities for accurate predictions. Existing methods typically focus on modality consistency and specificity to learn effective representations. However, from a causal perspective, they may lead to representations that contain insufficient and unnecessary information. To address this, we propose that effective MML representations should be causally sufficient and necessary. Considering practical issues like spurious correlations and modality conflicts, we relax the exogeneity and monotonicity assumptions prevalent in prior works and explore the concepts specific to MML, i.e., Causal Complete Cause ($C^3$). We begin by defining $C^3$, which quantifies the probability of representations being causally sufficient and necessary. We then discuss the identifiability of $C^3$ and introduce an instrumental variable to support identifying $C^3$ with non-exogeneity and non-monotonicity. Building on this, we conduct the $C^3$ measurement, i.e., $C^3$ risk. We propose a twin network to estimate it through (i) the real-world branch: utilizing the instrumental variable for sufficiency, and (ii) the hypothetical-world branch: applying gradient-based counterfactual modeling for necessity. Theoretical analyses confirm its reliability. Based on these results, we propose $C^3$ Regularization, a plug-and-play method that enforces the causal completeness of the learned representations by minimizing $C^3$ risk. Extensive experiments demonstrate its effectiveness.","This work explores how to improve multi-modal learning (MML) from a causal perspective. Existing methods typically define good MML representations from two perspectives: modality consistency and modality specificity. However, this work points out that these methods may lead models to learn representations that are either incomplete or contain irrelevant information. To address this, this work proposes a new standard for high-quality representations: they should be causally complete, that is, both necessary and sufficient for making correct predictions. Simply put, the information learned by the model should be not only enough to make the right decision (sufficiency), but also essential, removing it would lead to wrong results (necessity). To make this possible in real-world settings where data can be noisy or conflicting, this work proposes a relaxed and more practical way to measure and enforce causal completeness. It defines a new concept called Causal Complete Cause ($C^3$), and shows how to measure the quality of learned representations using the proposed $C^3$ risk. A novel twin network structure is introduced to compute this risk through (i) a real-world branch, which utilizes the instrumental variable for sufficiency, and (ii) a hypothetical-world branch, which applies gradient-based counterfactual modeling for necessity. Finally, the work proposes a plug-and-play technique called $C^3$ Regularization that can be embedded into any MML model to help it learn more reliable and causally complete representations. Extensive experiments across various benchmarks demonstrate the advantages of this method."
Poster,Towards the Efficient Inference by Incorporating Automated Computational Phenotypes under Covariate Shift,https://ICML.cc//virtual/2025/poster/43887,"chao ying, Jun Jin, Yi Guo, Xiudi Li, Muxuan Liang, Jiwei Zhao","Collecting gold-standard phenotype data via manual extraction is typically labor-intensive and slow, whereas automated computational phenotypes (ACPs) offer a systematic and much faster alternative.However, simply replacing the gold-standard with ACPs, without acknowledging their differences, could lead to biased results and misleading conclusions.Motivated by the complexity of incorporating ACPs while maintaining the validity of downstream analyses, in this paper, we consider a semi-supervised learning setting that consists of both labeled data (with gold-standard) and unlabeled data (without gold-standard), under the covariate shift framework.We develop doubly robust and semiparametrically efficient estimators that leverage ACPs for general target parameters in the unlabeled and combined populations. In addition,we carefully analyze the efficiency gains achieved by incorporating ACPs, comparing scenarios with and without their inclusion.Notably, we identify that ACPs for the unlabeled data, instead of for the labeled data, drive the enhanced efficiency gains. To validate our theoretical findings, we conduct comprehensive synthetic experiments and apply our method to multiple real-world datasets, confirming the practical advantages of our approach.","In this paper, we explore the benefits of incorporating automated computational phenotypes (ACPs) in a semi-supervised learning framework under covariate shift, particularly in terms of improving estimation efficiency.We propose an estimator that is both doubly robust and semiparametrically efficient. Additionally, our method allows for a rigorous quantification of the efficiency gain through closed-form expressions."
Poster,Towards Theoretical Understanding of Sequential Decision Making with Preference Feedback,https://ICML.cc//virtual/2025/poster/45202,"Simone Drago, Marco Mussi, Alberto Maria Metelli","The success of sequential decision-making approaches, such as *reinforcement learning* (RL), is closely tied to the availability of a reward feedback. However, designing a reward function that encodes the desired objective is a challenging task. In this work, we address a more realistic scenario: sequential decision making with preference feedback provided, for instance, by a human expert. We aim to build a theoretical basis linking *preferences*, (non-Markovian) *utilities*, and (Markovian) *rewards*, and we study the connections between them. First, we model preference feedback using a partial (pre)order over trajectories, enabling the presence of incomparabilities that are common when preferences are provided by humans but are surprisingly overlooked in existing works. Second, to provide a theoretical justification for a common practice, we investigate how a preference relation can be approximated by a multi-objective utility. We introduce a notion of preference-utility compatibility and analyze the computational complexity of this transformation, showing that constructing the minimum-dimensional utility is NP-hard. Third, we propose a novel concept of preference-based policy dominance that does not rely on utilities or rewards and discuss the computational complexity of assessing it. Fourth, we develop a computationally efficient algorithm to approximate a utility using (Markovian) rewards and quantify the error in terms of the suboptimality of the optimal policy induced by the approximating reward. This work aims to lay the foundation for a principled approach to sequential decision making from preference feedback, with promising potential applications in RL from human feedback.","The success of sequential decision-making approaches, such as Reinforcement Learning, is closely tied to the availability of a numerical reward function, which must capture the desired agent behavior. However, in complex real-world application, defining such a reward function is challenging. As such, an alternative, more realistic feedback has emerged in the literature: preference among trajectories. Such preferences can be provided, e.g., by a human expert who knows what goal the learning process aims to achieve. In this paper, we study the link between preference feedback and numerical reward functions, passing by trajectory utilities. We model preferences as partial orders, to capture the multi-dimensionality that can arise in a real-world problem. From this model, we define the concepts of dominance and optimality in terms of behavior, i.e., policies, and discuss their computational properties. Moreover, we study whether a reward function can be recovered from the observed preferences, we define a method to approximate it when exact recovery is not possible, and provide a bound on the performance difference that such an approximation introduces."
Poster,Towards Trustworthy Federated Learning with Untrusted Participants,https://ICML.cc//virtual/2025/poster/45372,"Youssef Allouah, Rachid Guerraoui, John Stephan","Resilience against malicious participants and data privacy are essential for trustworthy federated learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of participants shares a randomness seed unknown to others.  In a setting where malicious participants may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, using shared randomness between participants.We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted.  Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.","Federated learning lets many users train a machine learning model together without sharing their personal data, helping to protect privacy. However, most current methods rely on a central server that must be trusted to act honestly and keep data secure. Our research introduces a new method called CafCor that removes the need to trust this central server. Instead, it uses a small amount of shared random information that participants exchange privately with each other before training—information that remains hidden from the server. This shared randomness protects user privacy, though it can make it harder to guard against dishonest participants. CafCor carefully balances these trade-offs, achieving better privacy and performance than existing approaches that don’t assume any trust, and approaching the effectiveness of those that trust all participants and the server. Our experiments show that it's possible to train models securely and reliably, even when the server is not trusted."
Poster,Towards Understanding Catastrophic Forgetting in Two-layer Convolutional Neural Networks,https://ICML.cc//virtual/2025/poster/44997,"Boqi Li, Youjun Wang, Weiwei Liu","Continual learning (CL) focuses on the ability of models to learn sequentially from a stream of tasks. A major challenge in CL is catastrophic forgetting (CF). CF is a phenomenon where the model experiences significant performance degradation on previously learned tasks after training on new tasks. Although CF is commonly observed in convolutional neural networks (CNNs), the theoretical understanding about CF within CNNs remains limited. To fill the gap, we present a theoretical analysis of CF in a two-layer CNN. By employing a multi-view data model, we analyze the learning dynamics of different features throughout CL and derive theoretical insights. The findings are supported by empirical results from both simulated and real-world datasets.","When AI systems learn many tasks one after another, they often ""forget"" what they learned before — a problem known as catastrophic forgetting. While this is common in convolutional neural networks (CNNs), the underlying causes remain poorly understood.We develop a theoretical framework to study this phenomenon using a simplified CNN trained on multi-view data. By analyzing how the model updates its internal features over time, we show that gradient descent tends to learn features with stronger signals, even if they are less robust. As a result, more stable but weaker features may fail to be learned.We also show that features useful for earlier tasks can be inherently unrelated to the labels of later tasks. As the model adapts to new data, these task-specific features are naturally discarded — leading to forgetting. Our theoretical findings are supported by experiments on both synthetic and real-world datasets."
Poster,Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis,https://ICML.cc//virtual/2025/poster/46507,"Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou","Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in *Mechanistic Interpretability (MI)*. Unlike previous studies (Prakash et al. 2024, Chhabra et al. 2024) that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, bringing the setup closer to real-world scenarios. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, contrasting with previous work (Prakash et al. 2024, Chhabra et al. 2024) that reported only small circuit additions after fine-tuning. Based on these observations, we develop a **circuit-aware Low-Rank Adaptation (LoRA)** method that assigns ranks to layers according to edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA achieves an average improvement of 2.46% over standard LoRA with comparable parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, offering new insights into task design and deepening our understanding of circuit dynamics and fine-tuning mechanisms.","Fine-tuning large language models often boosts their performance, but it’s unclear how their internal workings adapt to new tasks. In our work, we use circuit analysis methods to study the changes in the model’s internal circuits during fine-tuning, hoping to find the phenomenon behind the increase in accuracy of LLM during fine-tuning. We find that most of the key pieces stay intact, but the strength and pattern of their connections change significantly. Leveraging this insight, we introduce a circuit-aware Low-Rank Adaptation (LoRA) method that devotes extra capacity to the most altered connections, achieving better accuracy with fewer added parameters. We further demonstrate that merging the connection patterns from simpler subtasks can effectively handle compositional tasks without finding circuit for this task again. By revealing and harnessing these hidden pathways, our method makes fine-tuning faster, more efficient, and more transparent. We hope this clearer view into the “black box” will lead to more trustworthy and adaptable AI systems."
Poster,Towards Understanding Gradient Dynamics of the Sliced-Wasserstein Distance via Critical Point Analysis,https://ICML.cc//virtual/2025/poster/45417,"Christophe Vauthier, Anna Korba, Quentin Mérigot","In this paper, we investigate the properties of the Sliced Wasserstein Distance (SW) when employed as an objective functional. The SW metric has gained significant interest in the optimal transport and machine learning literature, due to its ability to capture intricate geometric properties of probability distributions while remaining  computationally tractable, making it a valuable tool for various applications, including generative modeling and domain adaptation.Our study aims to provide a rigorous analysis of the critical points arising from the optimization of the SW objective. By computing explicit perturbations, we establish that stable critical points of SW cannot concentrate on segments. This stability analysis is crucial for understanding the behaviour of optimization algorithms for models trained using the SW objective. Furthermore, we investigate the properties of the SW objective, shedding light on the existence and convergence behavior of critical points. We illustrate our theoretical results through numerical experiments.","Generative modeling consists of training a computer to create new content based on a set of existing data (for example, new pictures of birds based on a dataset of existing pictures). To do so, it is generally useful to be able to quantify how far away the content generated by the program is from the actual data. In other words, we need to be able to calculate a distance between datasets, and it turns out that there is a mathematical theory which is well-suited to this task : the theory of optimal transport.In our work, we focus on one of the tools of optimal transport, the so-called ""Sliced-Wasserstein distance"", which, thanks to its many advantageous properties (notably its ease of calculation), has recently garnered much interest in the machine learning community. We uncover theoretical and empirical evidence that machine learning algorithms that make use of this distance will work seamlessly, and avoid getting ""stuck"" at some state where the computer becomes unable to improve its performance."
Poster,Towards Understanding Parametric Generalized Category Discovery on Graphs,https://ICML.cc//virtual/2025/poster/45645,"Bowen Deng, Lele Fu, Jialong Chen, Sheng Huang, Tianchi Liao, Zhang Tao, Chuan Chen","Generalized Category Discovery (GCD) aims to identify both known and novel categories in unlabeled data by leveraging knowledge from old classes. However, existing methods are limited to non-graph data; lack theoretical foundations to answer *When and how known classes can help GCD*. We introduce the Graph GCD task; provide the first rigorous theoretical analysis of *parametric GCD*. By quantifying the relationship between old and new classes in the embedding space using the Wasserstein distance W, we derive the first provable GCD loss bound based on W. This analysis highlights two necessary conditions for effective GCD. However, we uncover, through a Pairwise Markov Random Field perspective, that popular graph contrastive learning (GCL) methods inherently violate these conditions. To address this limitation, we propose SWIRL, a novel GCL method for GCD. Experimental results validate our (theoretical) findings and demonstrate SWIRL's effectiveness.","In real-world networks like social media or recommendation systems, new types of users or items can appear that AI models have never seen before. We study how AI can automatically discover both known and unknown categories in such networks.We analyze how knowledge from known categories helps recognize new ones and show that many current methods are not well-suited for this task. Based on our findings, we develop a new method called SWIRL , which improves the ability of AI to identify unknown categories in complex network data."
Poster,Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings,https://ICML.cc//virtual/2025/poster/45500,"Rong-Xi Tan, Ming Chen, Ke Xue, Yao Wang, Yaoyuan Wang, Fu Sheng, Chao Qian","The pursuit of universal black-box optimization (BBO) algorithms is a longstanding goal. However, unlike domains such as language or vision, where scaling structured data has driven generalization, progress in offline BBO remains hindered by the lack of unified representations for heterogeneous numerical spaces. Thus, existing offline BBO approaches are constrained to single-task and fixed-dimensional settings, failing to achieve cross-domain universal optimization. Recent advances in language models (LMs) offer a promising path forward: their embeddings capture latent relationships in a unifying way, enabling universal optimization across different data types possible. In this paper, we discuss multiple potential approaches, including an end-to-end learning framework in the form of next-token prediction, as well as prioritizing the learning of latent spaces with strong representational capabilities. To validate the effectiveness of these methods, we collect offline BBO tasks and data from open-source academic works for training. Experiments demonstrate the universality and effectiveness of our proposed methods. Our findings suggest that unifying language model priors and learning string embedding space can overcome traditional barriers in universal BBO, paving the way for general-purpose BBO algorithms. The code is provided at https://github.com/lamda-bbo/universal-offline-bbo.","Most computer programs that try to find optimal solutions can only work on one specific type of problem at a time - like finding the best recipe ingredients or the most efficient robot movements. This limitation means we need different optimization programs for different problems, which is inefficient.We discovered that modern AI language models, which are great at understanding relationships between words and concepts, could help solve this challenge. By converting different types of optimization problems into a format that language models can understand, we create a universal system that can tackle many different kinds of optimization tasks.We test our approach on a variety of problems collected from academic research and find it work well across different domains. This breakthrough means that instead of needing separate specialized programs for each type of optimization problem, we can have a single, versatile system that handles them all - similar to how human experts can apply their problem-solving skills across different situations."
Poster,Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation,https://ICML.cc//virtual/2025/poster/44642,"Fanqing Meng, Jiaqi Liao, Xinyu Tan, Quanfeng Lu, WENQI SHAO, Kaipeng Zhang, Yu Cheng, Dianqi Li, Ping Luo","Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive \textbf{Phy}sics \textbf{Gen}eration \textbf{Ben}chmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench","This paper explores the important question of whether current video generation models conform to physical laws, which is of great significance for future applications of video generation models in embodied intelligence and related fields. Considering that existing benchmarks and evaluation metrics have difficulty reflecting physical realism, this paper establishes PhyGenBench and PhyGenEval, which achieve a high degree of consistency with human judgment in assessing physical realism. In addition, this paper provides a thorough discussion on the physical realism of video generation models and points out possible directions for improvement."
