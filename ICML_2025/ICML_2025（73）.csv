type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Diff-MoE: Diffusion Transformer with Time-Aware and Space-Adaptive Experts,https://ICML.cc//virtual/2025/poster/45706,"Kun Cheng, Xiao He, Lei Yu, Zhijun Tu, Mingrui Zhu, Nannan Wang, Xinbo Gao, Jie Hu","Diffusion models have transformed generative modeling but suffer from scalability limitations due to computational overhead and inflexible architectures that process all generative stages and tokens uniformly. In this work, we introduce Diff-MoE, a novel framework that combines Diffusion Transformers with Mixture-of-Experts to exploit both temporarily adaptability and spatial flexibility. Our design incorporates expert-specific timestep conditioning, allowing each expert to process different spatial tokens while adapting to the generative stage, to dynamically allocate resources based on both the temporal and spatial characteristics of the generative task. Additionally, we propose a globally-aware feature recalibration mechanism that amplifies the representational capacity of expert modules by dynamically adjusting feature contributions based on input relevance. Extensive experiments on image generation benchmarks demonstrate that Diff-MoE significantly outperforms state-of-the-art methods. Our work demonstrates the potential of integrating diffusion models with expert-based designs, offering a scalable and effective framework for advanced generative modeling.","(1) Diffusion models excel at generating high-quality data but face scalability challenges due to heavy computation and one-size-fits-all processing across timesteps and spatial tokens.(2) We introduce Diff‑MoE, which augments Diffusion Transformers with a Mixture‑of‑Experts architecture: each expert receives its own timestep conditioning and handles different spatial tokens, enabling dynamic allocation of compute based on where and when it’s most needed.(3) We further enhance expert modules with a globally‑aware feature recalibration mechanism that amplifies relevant signals on the fly. Extensive image generation experiments show Diff‑MoE outperforms current state‑of‑the‑art methods, demonstrating a practical path to more scalable and adaptable generative modeling."
Poster,DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra,https://ICML.cc//virtual/2025/poster/45918,"Montgomery Bohde, Mrunali Manjrekar, Runzhong Wang, Shuiwang Ji, Connor Coley","Mass spectrometry plays a fundamental role in elucidating the structures of unknown molecules and subsequent scientific discoveries. One formulation of the structure elucidation task is the conditional *de novo* generation of molecular structure given a mass spectrum. Toward a more accurate and efficient scientific discovery pipeline for small molecules, we present DiffMS, a formula-restricted encoder-decoder generative network that achieves state-of-the-art performance on this task. The encoder utilizes a transformer architecture and models mass spectra domain knowledge such as peak formulae and neutral losses, and the decoder is a discrete graph diffusion model restricted by the heavy-atom composition of a known chemical formula. To develop a robust decoder that bridges latent embeddings and molecular structures, we pretrain the diffusion decoder with fingerprint-structure pairs, which are available in virtually infinite quantities, compared to structure-spectrum pairs that number in the tens of thousands. Extensive experiments on established benchmarks show that DiffMS outperforms existing models on *de novo* molecule generation. We provide several ablations to demonstrate the effectiveness of our diffusion and pretraining approaches and show consistent performance scaling with increasing pretraining dataset size. DiffMS code is publicly available at https://github.com/coleygroup/DiffMS.","Identifying unknown small molecules is a major bottleneck in scientific discovery. When scientists analyze a sample, they often use a technique called mass spectrometry, which breaks molecules apart then measures the weights and frequencies of the pieces, resulting in a mass spectrum that represents the structure. The challenge is that it's incredibly difficult to work backward from this spectrum to the original molecule's structure, especially since many different molecules can produce similar spectra.To tackle this, we created DiffMS, an machine learning algorithm that learns to generate a molecule's structure given its mass spectrum and its chemical formula (which is usually easier to determine). DiffMS uses a specialized ""encoder"" to extract information from the spectrum and a ""decoder"" that takes the structural information from the encoder and generates candidate molecules that may match the observed spectrum. Our findings show that while DiffMS doesn't always generate the correct molecule, it is pretty good at generating similar molecules, which are still useful for chemists to manually determine the true structure."
Poster,Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces,https://ICML.cc//virtual/2025/poster/46155,"Kevin Rojas, Yuchen Zhu, Sichen Zhu, Felix Ye, Molei Tao","Diffusion models have demonstrated remarkable performance in generating unimodal data across various tasks, including image, video, and text generation. On the contrary, the joint generation of multimodal data through diffusion models is still in the early stages of exploration. Existing approaches heavily rely on external preprocessing protocols, such as tokenizers and variational autoencoders, to harmonize varied data representations into a unified, unimodal format. This process heavily demands the high accuracy of encoders and decoders, which can be problematic for applications with limited data. To lift this restriction, we propose a novel framework for building multimodal diffusion models on arbitrary state spaces, enabling native generation of coupled data across different modalities. By introducing an innovative decoupled noise schedule for each modality, we enable both unconditional and modality-conditioned generation within a single model simultaneously. We empirically validate our approach for text-image generation and mixed-type tabular data synthesis, demonstrating that it achieves competitive performance.","Many AI tools today can generate impressive results — like realistic images, videos, or text — but usually only focus on one type of data at a time. Creating models that can generate multiple types of data together, such as matching text with images, is still a big challenge. Current methods often rely on complex tools to convert different types of data into a single format the model can understand. But these tools can struggle, especially when there isn't much training data. Our research introduces a new way to train AI systems to work directly with different data types — like text and images — without needing to squeeze them into the same format first. We developed a method that lets the model handle each type of data in its own way, which helps the model learn more naturally. This makes it possible to generate, for example, an image from a caption or even generate both together from scratch. We tested our method and found that it works well across different types of data. This could help build more flexible and reliable AI systems that understand and generate information more like humans do — across multiple forms, not just one."
Poster,Diffusion Adversarial Post-Training for One-Step Video Generation,https://ICML.cc//virtual/2025/poster/46184,"Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang","The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model can generate two-second, 1280x720, 24fps videos in real-time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.",We propose a technique to convert a slow video diffusion model into a one-step generator. Our model can generate 1280x720 24fps videos using only a single step. It can run in real time on 8xh100 GPUs.
Poster,Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain,https://ICML.cc//virtual/2025/poster/46096,"Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, Qingming Huang","The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image.For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.","We propose a new method to counteract adversarial attacks on images. Traditional methods attempt to remove adversarial noise, but often end up damaging important features of the image. Our approach looks at images in the frequency domain (focusing on the image’s frequency components, such as the amplitude and phase spectra). We find that adversarial noise increases with frequency, so by concentrating on the less affected low-frequency components, we can better preserve the original content and structure of the image. We improve the purification process by carefully modifying these low-frequency parts, both in terms of amplitude and phase spectra. Extensive experiments show that our method significantly outperforms existing defense methods."
Poster,Diffusion Counterfactual Generation with Semantic Abduction,https://ICML.cc//virtual/2025/poster/44995,"Rajat Rasal, Avinash Kori, Fabio De Sousa Ribeiro, Tian Xia, Ben Glocker","Counterfactual image generation presents significant challenges, including preserving identity, maintaining perceptual quality, and ensuring faithfulness to an underlying causal model. While existing auto-encoding frameworks admit semantic latent spaces which can be manipulated for causal control, they struggle with scalability and fidelity. Advancements in diffusion models present opportunities for improving counterfactual image editing, having demonstrated state-of-the-art visual quality, human-aligned perception and representation learning capabilities. Here, we present a suite of diffusion-based causal mechanisms, introducing the notions of spatial, semantic and dynamic abduction. We propose a general framework that integrates semantic representations into diffusion models through the lens of Pearlian causality to edit images via a counterfactual reasoning process. To the best of our knowledge, ours is the first work to consider high-level semantic identity preservation for diffusion counterfactuals and to demonstrate how semantic control enables principled trade-offs between faithful causal control and identity preservation.","Counterfactual image editing answers ""what if"" questions about an image by making precise changes while preserving all other details, e.g. what if this person’s expression changed, they wore sunglasses, or they were older? In each case, we want to modify only the requested attribute while keeping the person’s identity, background, and other unrelated features unchanged.Our work uses powerful image generation models, called diffusion models, to create counterfactual images. We guide diffusion models so that causally related properties of the image are modified together, i.e. smiling will cause your mouth to open, while unrelated properties remain unchanged. We find that providing a compact summary of the original image, called a semantic embedding, as an input to the diffusion model further improves identity preservation in the counterfactual.Diffusion models became popular for generating images from random text prompts, as seen in tools like MidJourney and DALL-E. We believe that a counterfactual perspective is essential to adapting diffusion models for broader, real-world applications. Our approach could help diffusion models better generalise to tasks that require creative reasoning and a deeper understanding of cause and effect."
Poster,Diffusion Instruction Tuning,https://ICML.cc//virtual/2025/poster/46009,"Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare","We introduce *Lavender*, a simple supervised fine-tuning (SFT) method that boosts the performance of advanced vision-language models (VLMs) by leveraging state-of-the-art image generation models such as Stable Diffusion. Specifically, Lavender aligns the text-vision attention in the VLM transformer with the equivalent used by Stable Diffusion during SFT, instead of adapting separate encoders. This alignment enriches the model’s visual understanding and significantly boosts performance across in- and out-of-distribution tasks. Lavender requires just 0.13 million training examples---2.5\% of typical large-scale SFT datasets---and fine-tunes on standard hardware (8 GPUs) in a single day. It consistently improves state-of-the-art open-source multimodal LLMs (e.g., Llama-3.2-11B, MiniCPM-Llama3-v2.5), achieving up to 30\% gains and a 68\% boost on challenging out-of-distribution medical QA tasks. By efficiently transferring the visual expertise of image generators with minimal supervision, Lavender offers a scalable solution for more accurate vision-language systems. Code, training data, and models are available on the [project page](https://astrazeneca.github.io/vlm/).","AI models that can understand both images and text are powerful, but they are often held back by a lack of training data. We wondered if we could improve these vision-language models (VLMs) by borrowing knowledge from a different kind of AI: one that is an expert at *generating* images, like Stable Diffusion.We noticed that image generators have a very precise internal map of how words correspond to specific image regions. Our method, which we call *Lavender*, uses these highly accurate ""attention maps"" as a guide to fine-tune the VLM. Essentially, we teach the VLM to ""look"" at the image with the same focus as the expert image generator, enriching its visual understanding.This alignment provides a powerful and efficient training signal. The results were significant: models improved by up to 30% on general benchmarks and even 68% on a challenging medical question-answering task. Remarkably, this was achieved with just 2.5% of the data typically used for this kind of training and in only a single day. Lavender offers a scalable way to build more robust and capable vision-language systems by bridging two expert AI paradigms."
Poster,Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Auto Speculation,https://ICML.cc//virtual/2025/poster/44125,"Hengyuan Hu, Aniket Das, Dorsa Sadigh, Nima Anari","Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful tools for generative modeling. However, their sequential computation requirements lead to significant inference-time bottlenecks. In this work, we utilize the connection between DDPMs and Stochastic Localization to prove that, under an appropriate reparametrization, the increments of DDPM satisfy an exchangeability property. This general insight enables near-black-box adaptation of various performance optimization techniques from autoregressive models to the diffusion setting. To demonstrate this, we introduce _Autospeculative Decoding_ (ASD), an extension of the widely used speculative decoding algorithm to DDPMs that does not require any auxiliary draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O}(K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM. We also demonstrate that a practical implementation of autospeculative decoding accelerates DDPM inference significantly in various domains.","Generative AI tools called Denoising Diffusion Probabilistic Models (DDPMs) are excellent for tasks like creative new images or videos, but they're often very slow because they generate content step-by-step. Our research discovers a hidden property of DDPMs: the order of some internal calculation steps in these DDPMs can actually be rearranged without changing the final result. We use this insight to develop a new method called Autospeculative Decoding (ASD). ASD cleverly allows many of these steps to be performed simultaneously, or in parallel. We mathematically prove that this makes DDPMs significantly faster at generating content, without needing any added components. We complement our proofs with practical demonstrations to show that this leads to 2-4x speedups across various applications."
Poster,Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors,https://ICML.cc//virtual/2025/poster/44707,"Emile Pierret, Bruno Galerne","Diffusion or score-based models recently showed high performance in image generation.They rely on a forward and a backward stochastic differential equations (SDE). The sampling of a data distribution is achieved by numerically solving the backward SDE or its associated flow ODE.Studying the convergence of these models necessitates to control four different types of error: the initialization error, the truncation error, the discretization error and the score approximation.In this paper, we theoretically study the behavior of diffusion models and their numerical implementation when the data distribution is Gaussian.Our first contribution is to derive the analytical solutions of the backward SDE and the probability flow ODE and to prove that these solutions and their discretizations are all Gaussian processes.Our second contribution is to compute the exact Wasserstein errors between the target and the numerically sampled distributions for any numerical scheme.This allows us to monitor convergence directly in the data space, while experimental works limit their empirical analysis to Inception features.An implementation of our code is available online.","Diffusion models, a type of machine learning model, have recently become very good at generating images. These models work by running a process forward to add noise to data, and then reversing it to recover or generate new data. This forward and backward process is described using mathematical tools called stochastic differential equations (SDEs).To understand how well these models work, we need to look at four types of errors that can happen during this process: errors at the start (initialization), from stopping too early (truncation), from using step-by-step calculations (discretization), and from estimating certain model parts (score approximation).In this paper, we focus on a simpler case where the data follows a Gaussian distribution, which allows us to study the math more precisely. Our first main result is that we find exact mathematical solutions for the reverse process used in diffusion models and show that both the original and approximated versions behave like Gaussian processes.Our second main result is that we calculate the exact differences (using a metric called Wasserstein distance) between the ideal results and the ones produced by the numerical simulations. This helps us measure how well the model is working."
Poster,Diffusion on Language Model Encodings for Protein Sequence Generation,https://ICML.cc//virtual/2025/poster/43588,"Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov","Protein *sequence* design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present *DiMA*, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only (ESM-2, ESMc), dual-decodable (CHEAP), and multimodal (SaProt) representations using the same architecture and training approach. We conduct extensive evaluation of existing methods alongside *DiMA* using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. *DiMA* consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design, despite being trained solely on sequence data. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios.  Code is released at [GitHub](https://github.com/MeshchaninovViacheslav/DiMA).","Proteins are fundamental biological molecules that drive cellular processes and hold tremendous potential for advancing medicine, biotechnology, and sustainable materials. However, designing new proteins remains challenging due to the vast number of possible amino acid sequences and the complex relationship between sequence and function.We present DiMA, an artificial intelligence framework that generates novel protein sequences by learning from existing protein data. Unlike previous methods, DiMA operates efficiently across different protein representations while maintaining both quality and diversity in generated sequences. Our approach uses significantly fewer computational resources than comparable systems while achieving superior performance.Comprehensive testing demonstrates that DiMA produces structurally viable proteins that are genuinely novel rather than variations of known sequences. The system can be directed to generate proteins with specific characteristics, such as particular functional motifs or structural properties.This work addresses a key bottleneck in protein engineering by making high-quality protein design more accessible and computationally efficient. The implications span drug discovery, industrial biotechnology, and fundamental biological research, potentially accelerating the development of therapeutic proteins, sustainable enzymes, and novel biomaterials."
