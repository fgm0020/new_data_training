type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models,https://ICML.cc//virtual/2025/poster/44942,"Yiming Chen, yuan zhang, Yin Liu, Kun Yuan, Zaiwen Wen","The memory challenges associated with training Large Language Models (LLMs) have become a critical concern, particularly when using the Adam optimizer. To address this issue, numerous memory-efficient techniques have been proposed, with GaLore standing out as a notable example designed to reduce the memory footprint of optimizer states. However, these approaches do not alleviate the memory burden imposed by activations, rendering them unsuitable for scenarios involving long context sequences or large mini-batches. Moreover, their convergence properties are still not well-understood in the literature. In this work, we introduce a Randomized Subspace Optimization framework for pre-training and fine-tuning LLMs. Our approach decomposes the high-dimensional training problem into a series of lower-dimensional subproblems. At each iteration, a random subspace is selected, and the parameters within that subspace are optimized. This structured reduction in dimensionality allows our method to simultaneously reduce memory usage for both activations and optimizer states. We establish comprehensive convergence guarantees and derive rates for various scenarios, accommodating different optimization strategies to solve the subproblems. Extensive experiments validate the superior memory and communication efficiency of our method, achieving performance comparable to GaLore and Adam.","Training large AI models like ChatGPT often requires huge amounts of computer memory, especially when using common methods like the Adam optimizer. While some recent techniques help reduce part of the memory load, they still struggle with other major sources of memory usage—like the data temporarily stored during training. These limitations make it hard to train models on long texts or with large batches of data. In our work, we propose a new training method that reduces memory usage more effectively. Instead of training the full model at once, we break the process into smaller, manageable parts and focus on a random piece of the model each time. This strategy saves memory in multiple ways and still achieves strong performance. We also provide mathematical proof that our method works reliably. Experiments show that our approach can match the performance of popular methods while using much less memory and communication."
Poster,A Meta-learner for Heterogeneous Effects in Difference-in-Differences,https://ICML.cc//virtual/2025/poster/44708,"Hui Lan, Chang, Eleanor W Dillon, Vasilis Syrgkanis","We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines.","Understanding how treatment effects vary across different population is essential in policy evaluation, especially when using observational data. We address this by developing a new method for estimating heterogeneous treatment effects using panel data, where we have repeated outcome observations across time. At the core of our approach is a meta-learning framework, which transforms the causal task into structured prediction problems, which can be solved using any machine learning approach. Importantly, our method is doubly robust, meaning it remains reliable even if some parts of the model are inaccurate. Moreover, it can also extend to more complex settings like treatment non-compliance and shifting data distributions. Empirical results show that our method consistently outperforms existing alternatives. This makes it easier for researchers to understand not just whether a treatment works, but for whom and under what conditions."
Poster,A Mixed-Curvature based Pre-training Paradigm for Multi-Task Vehicle Routing Solver,https://ICML.cc//virtual/2025/poster/45668,"Suyu Liu, Zhiguang Cao, Shanshan Feng, Yew Soon ONG","Solving various types of vehicle routing problems (VRPs) using a unified neural solver has garnered significant attentions in recent years. Despite their effectiveness, existing neural multi-task solvers often fail to account for the geometric structures inherent in different tasks, which may result in suboptimal performance. To address this limitation, we propose a curvature-aware pre-training framework. Specifically, we leverage mixed-curvature spaces during the feature fusion stage, encouraging the model to capture the underlying geometric properties of each instance. Through extensive experiments, we evaluate the proposed pre-training strategy on existing neural multi-task solvers across a variety of testing scenarios. The results demonstrate that the curvature-aware pre-training approach not only enhances the generalization capabilities of existing neural VRP solvers on synthetic datasets but also improves solution quality on real-world benchmarks.","The neural network based solvers, have emerged as powerful tools for solving various kinds of Vehicle Routing Problems (VRPs). While effective, current neural solvers often overlook a crucial piece of information: the unique geometric structures in the delivery map: whether the delivery points are spread out on a broad area or clustered in a small region with winding roads. These complex patterns can confuse the solver and finally lead into sub-optimal routes.Our research introduces a novel way to pre-train these neural models. We call it ""curvature-aware pre-training"" because it teaches the model to better perceive the underlying spatial arrangements. By exposing the model to various types of geometric subspaces with adaptive curvatures during its training stage, we enable it to capture the true nature of each instance in a more concrete way. Our experiments show that this new pre-training approach significantly improves existing neural solvers. It helps them perform better on unseen scenarios and find higher-quality, more accurate solutions for both simulated and real-world routing problems."
Poster,A Mixture-Based Framework for Guiding Diffusion Models,https://ICML.cc//virtual/2025/poster/45786,"Yazid Janati, Badr MOUFAD, Mehdi Qassime, Alain Oliviero Durmus, Eric Moulines, Jimmy Olsson","Denoising diffusion models have driven significant progress in the field of Bayesian inverse problems. Recent approaches use pre-trained diffusion models as priors to solve a wide range of such problems, only leveraging inference-time compute and thereby eliminating the need to retrain task-specific models on the same dataset. To approximate the posterior of a Bayesian inverse problem, a diffusion model samples from a sequence of intermediate posterior distributions, each with an intractable likelihood function. This work proposes a novel mixture approximation of these intermediate distributions. Since direct gradient-based sampling of these mixtures is infeasible due to intractable terms, we propose a practical method based on Gibbs sampling. We validate our approach through extensive experiments on image inverse problems, utilizing both pixel- and latent-space diffusion priors, as well as on source separation with an audio diffusion model. The code is available at \url{https://www.github.com/badr-moufad/mgdm}.","This work introduces a new method for solving inverse problems, which are tasks where one tries to recover a hidden cause from observed data—like reconstructing a clear image from a blurry one, or separating individual sounds from a mixed audio recording.Traditionally, solving these problems in a Bayesian way (which means estimating not just one answer, but a distribution of likely answers) has been very computationally expensive. Recent advances use diffusion models—a powerful class of generative AI models—to do this more efficiently by treating them as flexible prior assumptions about what realistic signals (like images or audio) look like.However, diffusion models don’t directly provide the solution: they sample from a sequence of gradually refined guesses. This process relies on complex intermediate distributions that are hard to work with mathematically. To address this, the authors propose a new way of approximating these distributions using mixtures (combinations of simpler components), and introduce a practical technique—based on Gibbs sampling—to generate samples from them."
Poster,A Model of Place Field Reorganization During Reward Maximization,https://ICML.cc//virtual/2025/poster/46112,"M Ganesh Kumar, Blake Bordelon, Jacob A Zavatone-Veth, Cengiz Pehlevan","When rodents learn to navigate in a novel environment, a high density of place fields emerges at reward locations, fields elongate against the trajectory, and individual fields change spatial selectivity while demonstrating stable behavior. Why place fields demonstrate these characteristic phenomena during learning remains elusive. We develop a normative framework using a reward maximization objective, whereby the temporal difference (TD) error drives place field reorganization to improve policy learning. Place fields are modeled using Gaussian radial basis functions to represent states in an environment, and directly synapse to an actor-critic for policy learning. Each field's amplitude, center, and width, as well as downstream weights, are updated online at each time step to maximize rewards. We demonstrate that this framework unifies three disparate phenomena observed in navigation experiments. Furthermore, we show that these place field phenomena improve policy convergence when learning to navigate to a single target and relearning multiple new targets. To conclude, we develop a simple normative model that recapitulates several aspects of hippocampal place field learning dynamics and unifies mechanisms to offer testable predictions for future experiments.","A place field is a localized area in an environment in which a place cell in the hippocampus fires. Their population activity allows one to decode the location, resembling a biological global position system. As animals navigate in a novel environment, randomly distributed place fields change during learning: clustering near rewards, elongating along paths, and changing their spatial selectivity (""drift"") even after behavior stabilizes. Why these changes occur in individual cells, and how they aid learning remains unclear.  We developed a simple computational model where place fields can adapt to maximize rewards. Using the feedback during trial-and-error based learning, the model adjusts each field's property—such as location, size, and strength. This reorganization mimics how animals refine their understanding of their environment i.e. location of the goal and home. Importantly, our model explains all three observed phenomena: reward clustering, field elongation, and drift.  By linking individual place field changes to reward-driven learning, our work offers a unified theory for how cells in the brain optimizes its encoding of the environment for navigation. Additionally, we show that changing the encoding improves the speed of learning, compared to using a default random representation. Furthermore, the model suggests that field drift, once thought random, may help animals to quickly adapt to new goals. This could inspire more flexible AI navigation systems and guide experiments to test brain learning mechanisms."
Poster,AMPO: Active Multi Preference Optimization for Self-play Preference Selection,https://ICML.cc//virtual/2025/poster/45443,"Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravanakumar Rajmohan","Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, making it computationally infeasible to include all of them in the training objective. We propose Active Multi-Preference Optimization (AMPO), which combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses, then pick a small but informative subset—covering reward extremes and distinct semantic clusters—for preference optimization.The resulting contrastive-training scheme identifies not only the best and worst answers but also subtle, underexplored modes crucial for robust alignment. Theoretically, we provide guarantees of expected reward maximization using our active selection method. Empirically, AMPO achieves state-of-the-art results on AlpacaEval with Llama 8B and Mistral 7B. We release our datasets [here](https://huggingface.co/Multi-preference-Optimization).","Large language models (LLMs), like those used in chatbots or virtual assistants, often generate multiple possible answers to a question. But teaching these models to consistently choose the best response is tricky — especially when most training methods compare only two answers at a time. This limited approach misses valuable signals from the many other responses the model could have considered.Our research introduces Active Multi-Preference Optimization (AMPO) — a new way to train language models that looks at groups of good and bad answers instead of just pairs. We let the model generate its own possible answers, then carefully select a few that are diverse and informative. These selected responses help the model learn not only what a great answer looks like, but also what makes an answer unclear, vague, or subtly misleading.This smarter training method makes language models more accurate and aligned with human expectations. In tests on popular benchmarks, our method outperforms existing techniques — and we’ve made our data and code publicly available to help others build more reliable AI systems."
Poster,A Multi-Region Brain Model to Elucidate the Role of Hippocampus in Spatially Embedded Decision-Making,https://ICML.cc//virtual/2025/poster/43834,"Yi Xie, Jaedong Hwang, Carlos Brody, David Tank, Ila R. Fiete","Brains excel at robust decision-making and data-efficient learning. Understanding the architectures and dynamics underlying these capabilities can inform inductive biases for deep learning. We present a multi-region brain model that explores the normative role of structured memory circuits in a spatially embedded binary decision-making task from neuroscience.We counterfactually compare the learning performance and neural representations of reinforcement learning (RL) agents with brain models of different interaction architectures between grid and place cells in the entorhinal cortex and hippocampus, coupled with an action-selection cortical recurrent neural network. We demonstrate that a specific architecture--where grid cells receive and jointly encode self-movement velocity signals and decision evidence increments--optimizes learning efficiency while best reproducing experimental observations relative to alternative architectures.Our findings thus suggest brain-inspired structured architectures for efficient RL. Importantly, the models make novel, testable predictions about organization and information flow within the entorhinal-hippocampal-neocortical circuit: we predict that grid cells must conjunctively encode position and evidence for effective spatial decision-making, directly motivating new neurophysiological experiments.","Everyday choices—like deciding when to turn down a hallway—rely on two skills: knowing **where** we are and adding up **clues** about what to do next. Yet, it remains a mystery how the brain merges these streams of information across multiple brain regions to accomplish this everyday task of making decisions in physical space.We built a “virtual brain” inside a reinforcement‑learning agent that links three key brain circuits: grid cells that track location, the hippocampus that stores memories, and a small cortical network that picks actions. The agent practises a classic mouse task: walking down a T‑shaped maze lined with visual towers (""evidence"") on both sides, then turning at the end toward the side with more towers (“decision”).After testing all the possible brain circuit designs, we found that learning was fastest—and runs were shortest—when each grid cell encoded both position **and** a running tower count. This “dual‑coding” design also reproduced the unusual firing patterns recorded in the real mouse hippocampus during the same task, whereas other designs did not.For neuroscience, our results predict that biological grid cells combine “Where am I?” with “How much evidence have I gathered?”—a hypothesis that future neurophysiology experiments can directly test. For machine learning, adding such structured memory maps to learning agents can reduce training demands, enabling lighter, more efficient AI that makes decisions in the real world."
Poster,An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures,https://ICML.cc//virtual/2025/poster/44205,"Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Picard, Thomas Massena, Mathieu Serrurier","Orthogonal convolutional layers are valuable components in multiple areas of machine learning, such as adversarial robustness, normalizing flows, GANs, and Lipschitz-constrained models. Their ability to preserve norms and ensure stable gradient propagation makes them valuable for a large range of problems. Despite their promise, the deployment of orthogonal convolution in large-scale applications is a significant challenge due to computational overhead and limited support for modern features like strides, dilations, group convolutions, and transposed convolutions. In this paper, we introduce **AOC** (Adaptive Orthogonal Convolution), a scalable method that extends a previous method (BCOP), effectively overcoming existing limitations in the construction of orthogonal convolutions. This advancement unlocks the construction of architectures that were previously considered impractical. We demonstrate through our experiments that our method produces expressive models that become increasingly efficient as they scale. To foster further advancement, we provide an open-source python package implementing this method, called **Orthogonium**.","Orthogonal layers have become essential in machine learning because they stabilize training, enhance robustness against adversarial attacks, and improve performance in generative models like normalizing flows and GANs. Specifically, orthogonal convolutional layers help models maintain consistent gradient norms, ensuring more reliable learning and greater stability. Despite these advantages, traditional orthogonal convolution methods are computationally demanding and lack flexibility for modern features, limiting their use in large-scale, practical scenarios.We created Adaptive Orthogonal Convolution (AOC), an approach that solves these limitations, making orthogonal convolutions practical for advanced use-cases. AOC supports modern features like strides, group convolutions, and dilations— which are critical for some tasks. Our method allows to train larger and and more powerful models, without the usual cost of orthogonal constraints.We share our solution through an easy-to-use, open-source library. Allowing researchers and developers to easily build expressive, stable models, significantly expanding the potential applications of orthogonal convolutional layers in fields like generative models, robust prediction, and secure AI systems."
Poster,An All-Atom Generative Model for Designing Protein Complexes,https://ICML.cc//virtual/2025/poster/46160,"Ruizhe Chen, Dongyu Xue, Xiangxin Zhou, Zaixiang Zheng, xiangxiang Zeng, Quanquan Gu","Proteins typically exist in complexes, interacting with other proteins or biomolecules to perform their specific biological roles. Research on single-chain protein modeling has been extensively and deeply explored, with advancements seen in models like the series of ESM and AlphaFold2. Despite these developments, the study and modeling of multi-chain proteins remain largely uncharted, though they are vital for understanding biological functions. Recognizing the importance of these interactions, we introduce APM (all-Atom Protein generative Model), a model specifically designed for modeling multi-chain proteins. By integrating atom-level information and leveraging data on multi-chain proteins, APM is capable of precisely modeling inter-chain interactions and designing protein complexes with binding capabilities from scratch. It also performs folding and inverse-folding tasks for multi-chain proteins. Moreover, APM demonstrates versatility in downstream applications: it achieves enhanced performance through supervised fine-tuning (SFT) while also supporting zero-shot sampling in certain tasks, achieving state-of-the-art results. We released our code at https://github.com/bytedance/apm.","Proteins often work in groups called complexes to perform certain biological tasks. While we understand single proteins well, how these complexes form and function is less clear but equally important.We introduce APM, an AI that designs these multi-protein complexes with atomic precision. Unlike older methods focusing on single proteins or simpler views, APM models the detailed atomic interactions between protein chains.APM can create novel complexes, predict their formation, and design sequences for desired structures. It has shown superior performance in tasks like antibody and therapeutic peptide design. This work offers new avenues for creating custom protein complexes for medicine and biotech"
Poster,AnalogGenie-Lite: Enhancing Scalability and Precision in Circuit Topology Discovery through Lightweight Graph Modeling,https://ICML.cc//virtual/2025/poster/45643,"Jian Gao, Weidong Cao, Xuan Zhang","The sustainable performance improvements of integrated circuits (ICs) drive the continuous advancement of nearly all transformative technologies. Since its invention, IC performance enhancements have been dominated by scaling the semiconductor technology. Yet, as Moore's law tapers off, a crucial question arises: ***How can we sustain IC performance in the post-Moore era?*** Creating new circuit topologies has emerged as a promising pathway to address this fundamental need. This work proposes AnalogGenie-Lite, a decoder-only transformer that discovers novel analog IC topologies with significantly enhanced scalability and precision via lightweight graph modeling.AnalogGenie-Lite makes several unique contributions, including concise device-pin representations (i.e., advancing the best prior art from $O\left(n^2\right)$ to $O\left(n\right)$), frequent sub-graph mining, and optimal sequence modeling. Compared to state-of-the-art circuit topology discovery methods, it achieves $5.15\times$ to $71.11\times$ gains in scalability and 23.5\% to 33.6\% improvements in validity. Case studies on other domains' graphs are also provided to show the broader applicability of the proposed graph modeling approach. Source code: https://github.com/xz-group/AnalogGenie-Lite.","Integrated circuits (ICs) have powered decades of technological breakthroughs, enabling innovations from medical devices to quantum computing. Historically, IC improvements relied on making semiconductor components smaller, following Moore's Law. However, as physical limits are approached, simply shrinking components isn't enough.AnalogGenie-Lite addresses this by discovering new analog circuit topologies—critical components that process continuous signals and bridge physical devices with digital systems. It uses a specialized AI model based on a simplified graph representation to accurately and efficiently explore new circuit designs. By reducing complexity, AnalogGenie-Lite achieves substantial improvements: it can handle larger circuits, generate valid designs with fewer errors, and discover novel designs that are unseen by humans.This novel approach offers a practical path forward for maintaining performance advancements in electronics, even as traditional scaling methods reach their limits. Additionally, the techniques developed in AnalogGenie-Lite have potential applications beyond electronics, including areas like protein generation, personalized recommendation, and 3D object recognition."
