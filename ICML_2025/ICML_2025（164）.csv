type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Learning Mean Field Control on Sparse Graphs,https://ICML.cc//virtual/2025/poster/44932,"Christian Fabian, Kai Cui, Heinz Koeppl","Large agent networks are abundant in applications and nature and pose difficult challenges in the field of multi-agent reinforcement learning (MARL) due to their computational and theoretical complexity. While graphon mean field games and their extensions provide efficient learning algorithms for dense and moderately sparse agent networks, the case of realistic sparser graphs remains largely unsolved. Thus, we propose a novel mean field control model inspired by local weak convergence to include sparse graphs such as power law networks with coefficients above two. Besides a theoretical analysis, we design scalable learning algorithms which apply to the challenging class of graph sequences with finite first moment. We compare our model and algorithms for various examples on synthetic and real world networks with mean field algorithms based on Lp graphons and graphexes. As it turns out, our approach outperforms existing methods in many examples and on various networks due to the special design aiming at an important, but so far hard to solve class of MARL problems.","Networks are the building blocks of countless systems in nature and of human-made structures such as the human brain or social networks. Despite their high relevance in real world applications, it is hard to learn optimal behavior in large networks because one has to consider millions or even billions of agents.The concept of graphon mean field games (GMFGs) and corresponding extensions provide tools to understand and learn in large networks by reducing the large number of individuals to a few representative ones. While GMFGs and their extensions give valuable scientific insights, their assumptions on the average number of connections of an individual are often far from what is observed in real world networks.In this paper, we overcome these sometimes unrealistic assumptions with our local weak mean field control (LWMFC) model. The LWMFC approach divides all individuals according to their number of connections. Then, we can efficiently learn behavior in large networks and also provide mathematical insights. We illustrate the advantages of LWMFC with different examples, such as two epidemic models and the spreading of rumors in a social network."
Poster,Learning Minimum-Size BDDs: Towards Efficient Exact Algorithms,https://ICML.cc//virtual/2025/poster/44802,"Christian Komusiewicz, André Schidler, Frank Sommer, Manuel Sorge, Luca Staus","Binary decision diagrams (BDDs) are widely applied tools to compactly represent labeled data as directed acyclic graphs; for efficiency and interpretability reasons small BDDs are preferred.Given labeled data, minimizing BDDs is NP-complete and thus recent research focused on the influence of parameters such as the solution size $s$ on the complexity [Ordyniak et al., AAAI 2024].Our main positive result is an algorithm that is efficient if in particular $s$, the domain size $D$, and the Hamming distance between any two data points is small, improving on previous running-time bounds.This algorithm is inspired by the witness-tree paradigm that was recently successful for computing decision trees [Komusiewicz et al., ICML 2023], whose extension to BDDs was open.We extend our algorithmic results to the case where we allow a small number of misclassified data points and complement them with lower bounds that show that the running times are tight from multiple points of view.We show that our main algorithm holds practical promise by providing a proof-of-concept implementation.","This paper introduces a new algorithm, called WitBDD, for efficiently building small binary decision diagrams (BDDs), which are models used in data classification and AI. BDDs, a generalization of decision trees, are popular due to their simplicity and understandability. Keeping BDDs small is important to ensure efficiency and for allowing the model to be interpreted by humans. The problem of finding a smallest binary decision diagram is, however, very hard (NP-complete).We adapt a method known as the ""witness paradigm""—previously successful with decision trees—to BDDs. This approach gradually refines a BDD by identifying misclassified data points and adding necessary corrections, greatly reducing the number of possibilities the algorithm has to explore. We prove mathematically that the algorithm is fast when the desired BDD size and data differences are small and also extend it to allow for classification errors. In experiments, our new algorithm outperforms existing SAT-based methods for small BDDs (sizes $\leq 4$), solving problems faster in many cases. For larger BDDs, existing methods are still better.In short, our work makes computing small, optimal BDDs more practical and opens the door for further optimization and use in explainable AI."
Poster,Learning Mixtures of Experts with EM: A Mirror Descent Perspective,https://ICML.cc//virtual/2025/poster/43609,"Quentin Fruytier, Aryan Mokhtari, Sujay Sanghavi","Classical Mixtures of Experts (MoE) are Machine Learning models that involve partitioning the input space, with a separate ""expert"" model trained on each partition. Recently, MoE-based model architectures have become popular as a means to reduce training and inference costs. There, the partitioning function and the experts are both learnt jointly via gradient descent-type methods on the log-likelihood. In this paper we study theoretical guarantees of the Expectation Maximization (EM) algorithm for the training of MoE models. We first rigorously analyze EM for MoE where the conditional distribution of the target and latent variable conditioned on the feature variable belongs to an exponential family of distributions and show its equivalence to projected Mirror Descent with unit step size and a Kullback-Leibler Divergence regularizer. This perspective allows us to derive new convergence results and identify conditions for local linear convergence; In the special case of mixture of 2 linear or logistic experts, we additionally provide guarantees for linear convergence based on the signal-to-noise ratio. Experiments on synthetic and (small-scale) real-world data supports that EM outperforms the gradient descent algorithm both in terms of convergence rate and the achieved accuracy.","Machine learning models called Mixtures of Experts (MoE) work by dividing up the input space and assigning a specialized model—or ""expert""—to each part. These models have recently become popular for their ability to reduce the cost of training and making predictions, especially in large-scale applications like Large Language Models (LLM). Typically, both the way the input is divided and the experts themselves are learned using a technique called gradient descent.In our work, we revisit a classic but often overlooked alternative: the Expectation-Maximization (EM) algorithm. We show that, in the context of MoE training, EM has a connection to a modern optimization technique called Mirror Descent, and we use this link to better understand how and when EM works well for training MoE models. In particular, we identify conditions where EM can converge quickly and reliably.We also provide mathematical guarantees for this behavior in simpler models, and our experiments confirm that EM often performs better than gradient descent—not only learning faster, but also achieving higher accuracy. This highlights EM as a strong and theoretically grounded option for training expert-based models."
Poster,Learning Monotonic Probabilities with a Generative Cost Model,https://ICML.cc//virtual/2025/poster/45065,"Yongxiang Tang, Yanhua Cheng, Xiaocheng Liu, chenchen Jiao, Yanxiang Zeng, Ning Luo, Pengjia Yuan, Xialong Liu, Peng Jiang","In many machine learning tasks, it is often necessary for the relationship between input and output variables to be monotonic, including both strictly monotonic and implicitly monotonic relationships. Traditional methods for maintaining monotonicity mainly rely on construction or regularization techniques, whereas this paper shows that the issue of strict monotonic probability can be viewed as a partial order between an observable revenue variable and a latent cost variable. This perspective enables us to reformulate the monotonicity challenge into modeling the latent cost variable. To tackle this, we introduce a generative network for the latent cost variable, termed the Generative Cost Model (**GCM**), which inherently addresses the strict monotonic problem, and propose the Implicit Generative Cost Model (**IGCM**) to address the implicit monotonic problem. We further validate our approach with a numerical simulation of quantile regression and conduct multiple experiments on public datasets, showing that our method significantly outperforms existing monotonic modeling techniques. The code for our experiments can be found at [https://github.com/tyxaaron/GCM](https://github.com/tyxaaron/GCM).","In many machine learning tasks, ensuring a monotonic relationship between input and output variables is crucial. Traditional methods often struggle with maintaining this monotonicity effectively.To address this challenge, we reformulated the problem by viewing it as a relationship between an observable revenue variable and a latent cost variable. This led us to develop a new approach focusing on modeling the latent cost variable. We introduced the Generative Cost Model (GCM) to handle strict monotonic relationships and the Implicit Generative Cost Model (IGCM) for more implicit monotonic relationships. Our models inherently address the monotonicity issue without relying on traditional construction or regularization techniques.Our research is significant because it offers a novel probabilistic perspective and a generative solution to the monotonic problem in machine learning, making it a valuable contribution to the field."
Poster,Learning Multi-Level Features with Matryoshka Sparse Autoencoders,https://ICML.cc//virtual/2025/poster/44178,"Bart Bussmann, Noa Nabeshima, Adam Karvonen, Neel Nanda","Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting neural networks by extracting the concepts represented in their activations. However, choosing the size of the SAE dictionary (i.e. number of learned concepts) creates a tension: as dictionary size increases to capture more relevant concepts, sparsity incentivizes features to be split or absorbed into more specific features, leaving high-level features missing or warped. We introduce Matryoshka SAEs, a novel variant that addresses these issues by simultaneously training multiple nested dictionaries of increasing size, forcing the smaller dictionaries to independently reconstruct the inputs without using the larger dictionaries. This organizes features hierarchically - the smaller dictionaries learn general concepts, while the larger dictionaries learn more specific concepts, without incentive to absorb the high-level features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find superior performance on sparse probing and targeted concept erasure tasks, more disentangled concept representations, and reduced feature absorption. While there is a minor tradeoff with reconstruction performance, we believe Matryoshka SAEs are a superior alternative for practical tasks, as they enable training arbitrarily large SAEs while retaining interpretable features at different levels of abstraction.","Modern AI systems can be like black boxes and we don't know what concepts they've learned internally. Researchers use tools called sparse autoencoders to peek inside and extract interpretable concepts, but these tools have a major flaw: when we try to capture more detailed concepts, they lose track of broader categories. For example, a system might learn about specific punctuation marks like periods and commas but forget the general concept of ""punctuation marks"" altogether.We developed Matryoshka Sparse Autoencoders, named after Russian nesting dolls. Our system trains several sparse autoencoders at once, nested inside each other with levels of increasing detail. The smaller, core ""dolls"" are trained to capture the most general, high-level concepts. Then, the larger, surrounding ""dolls"" learn more specific details, building upon the general concepts without overwriting or damaging them.This layered approach allows Matryoshka Sparse Autoencoders to represent both the forest (big-picture ideas) and the trees (fine-grained details) effectively. As a result, researchers can get a more complete and interpretable view of how an AI model understands information at multiple levels of abstraction. This ultimately helps researchers in getting a better understanding of how powerful AI systems work, which may be crucial to ensure that they are safe."
Poster,Learning multivariate Gaussians with imperfect advice,https://ICML.cc//virtual/2025/poster/46687,"Arnab Bhattacharyya, Davin Choo, Philips George John, Themistoklis Gouleakis","We revisit the problem of distribution learning within the framework of learning-augmented algorithms.In this setting, we explore the scenario where a probability distribution is provided as potentially inaccurate advice on the true, unknown distribution. Our objective is to develop learning algorithms whose sample complexity decreases as the quality of the advice improves, thereby surpassing standard learning lower bounds when the advice is sufficiently accurate. Specifically, we demonstrate that this outcome is achievable for the problem of learning a multivariate Gaussian distribution $N(\mu, \Sigma)$ in the PAC learning setting. Classically, in the advice-free setting, $\widetilde{\Theta}(d^2/\varepsilon^2)$ samples are sufficient and worst case necessary to learn $d$-dimensional Gaussians up to TV distance $\varepsilon$ with constant probability. When we are additionally given a parameter $\widetilde{\Sigma}$ as advice, we show that $\widetilde{\mathcal{O}}(d^{2-\beta}/\varepsilon^2)$ samples suffices whenever $|| \widetilde{\Sigma}^{-1/2} \Sigma \widetilde{\Sigma}^{-1/2} - I_d ||_1 \leq \varepsilon d^{1-\beta}$ (where $||\cdot||_1$ denotes the entrywise $\ell_1$ norm) for any $\beta > 0$, yielding a polynomial improvement over the advice-free setting.","Estimating the mean and covariance of a multivariate Gaussian distribution is a well-known problem in machine learning. In the worst case, it requires a number of samples that grows quadratically with the number of variates/features. We study a new setting where, in addition to data samples, we are given imperfect advice in the form of predictions/guesses for the mean and covariance. These predictions may come from prior models or expert knowledge, but we have no guarantees about their accuracy. We design an algorithm that first tests whether the advice is reliable. If it is, we use it to reduce the number of samples needed, applying tools from convex optimization. If it isn’t, we default to standard estimators. Our method is always correct and provably uses fewer samples when the advice is good. We also show that the trade-off between advice quality and sample efficiency is close to the best possible."
Poster,Learning Optimal Multimodal Information Bottleneck Representations,https://ICML.cc//virtual/2025/poster/46426,"Qilong Wu, Yiyang Shao, Jun Wang, Xiaobo Sun","Leveraging high-quality joint representations from multimodal data can greatly enhance model performance in various machine-learning based applications. Recent multimodal learning methods, based on the multimodal information bottleneck (MIB) principle, aim to generate optimal MIB with maximal task-relevant information and minimal superfluous information via regularization. However, these methods often set regularization weights in an *ad hoc* manner and overlook imbalanced task-relevant information across modalities, limiting their ability to achieve optimal MIB. To address this gap, we propose a novel multimodal learning framework, Optimal Multimodal Information Bottleneck (OMIB), whose optimization objective guarantees the achievability of optimal MIB by setting the regularization weight within a theoretically derived bound. OMIB further addresses imbalanced task-relevant information by dynamically adjusting regularization weights per modality, ensuring the inclusion of all task-relevant information. Moreover, we establish a solid information-theoretical foundation for OMIB's optimization and implement it under the variational approximation framework for computational efficiency. Finally, we empirically validate the OMIB’s theoretical properties on synthetic data and demonstrate its superiority over the state-of-the-art benchmark methods in various downstream tasks.","Machines can learn better when they use information from multiple sources, like combining what they hear and see. However, blending this information well is tricky, especially when one source is more useful than the others.Our research tackles this problem by developing a new method that helps computers find the right balance: keeping the useful information while filtering out what’s unnecessary. Unlike previous approaches, our method sets this balance using a mathematically sound rule rather than trial-and-error. It can even adjust how much each source matters, depending on how helpful it is.We tested our approach on both simulated and real-world data, and it consistently outperformed other leading methods. This could make future AI systems smarter and more adaptable, especially in complex situations that require understanding information from different perspectives."
Poster,Learning-Order Autoregressive Models with Application to Molecular Graph Generation,https://ICML.cc//virtual/2025/poster/45939,"Zhe Wang, Jiaxin Shi, Nicolas Heess, Arthur Gretton, Michalis Titsias","Autoregressive models (ARMs) have become the workhorse for sequence generation tasks, since many problems can be modeled as next-token prediction. While there appears to be a natural ordering for text (i.e., left-to-right), for many data types, such as graphs, the canonical ordering is less obvious. To address this problem, we introduce a variant of ARM that generates high-dimensional data using a probabilistic ordering that is sequentially inferred from data. This model incorporates a trainable probability distribution, referred to as an order-policy, that dynamically decides the autoregressive order in a state-dependent manner. To train the model, we introduce a variational lower bound on the exact log-likelihood, which we optimize with stochastic gradient estimation. We demonstrate experimentally that our method can learn meaningful autoregressive orderings in image and graph generation. On the challenging domain of molecular graph generation, we achieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated using the Fréchet ChemNet Distance (FCD), Synthetic Accessibility Score (SAS), Quantitative Estimate of Drug-likeness (QED).","We have developed an AI model that learns the optimal step-by-step sequence to build complex structures like molecules. While standard models can easily generate text with its predictable order, they struggle with molecules where the best construction path is not obvious. Our model intelligently overcomes this by deciding the most effective piece to add at each stage, such as the next atom or chemical bond. When tested on the challenging task of designing new molecules, this method achieved state-of-the-art results on key industry benchmarks, producing molecules highly rated for their validity, stability, and potential as useful drugs."
Poster,Learning Parametric Distributions from Samples and Preferences,https://ICML.cc//virtual/2025/poster/45822,"Marc Jourdan, Gizem Yüce, Nicolas Flammarion","Recent advances in language modeling have underscored the role of preference feedback in enhancing model performance. This paper investigates the conditions under which preference feedback improves parameter estimation in classes of continuous parametric distributions. In our framework, the learner observes pairs of samples from an unknown distribution along with their relative preferences depending on the same unknown parameter. We show that preferences-based M-estimators achieve a better asymptotic variance than sample-only M-estimators, further improved by deterministic preferences. Leveraging the hard constraints revealed by deterministic preferences, we propose an estimator achieving an estimation error scaling of $\mathcal{O}(1/n)$---a significant improvement over the $\Theta(1/\sqrt{n})$ rate attainable with samples alone. Next, we establish a lower bound that matches this accelerated rate; up to problem-dependent constants. While the assumptions underpinning our analysis are restrictive, they are satisfied by notable cases such as Gaussian or Laplace distributions for preferences based on the log-probability reward.","Many AI systems today learn from examples, but newer methods are starting to use preferences---comparisons between options---to improve learning. We asked: When and how can preference feedback help models learn faster?In our study, we built a framework where a learner sees pairs of examples and is told which one is better. We found that this kind of feedback can significantly sharpen the model's estimates, especially when preferences are consistent and deterministic. In the best case, learning becomes much faster: the error drops much more quickly than when using examples alone.We also proved that this speed-up is the best possible under certain conditions. While our results rely on specific assumptions, they hold in important practical cases---like when AI models use scores based on how likely something is to happen."
Poster,Learning Policy Committees for Effective Personalization in MDPs with Diverse Tasks,https://ICML.cc//virtual/2025/poster/44698,"Luise Ge, Michael Lanier, Anindya Sarkar, Bengisu Guresti, Chongjie Zhang, Yevgeniy Vorobeychik","Many dynamic decision problems, such as robotic control, involve a series of tasks, many of which are unknown at training time.Typical approaches for these problems, such as multi-task and meta reinforcement learning, do not generalize well when the tasks are diverse. On the other hand, approaches that aim to tackle task diversity, such as using task embedding as policy context and task clustering, typically lack performance guarantees and require a large number of training tasks. To address these challenges, we propose a novel approach for learning a policy committee that includes at least one near-optimal policy with high probability for tasks encountered during execution. While we show that this problem is in general inapproximable, we present two practical algorithmic solutions.The first yields provable approximation and task sample complexity guarantees when tasks are low-dimensional (the best we can do due to inapproximability), whereas the second is a general and practical gradient-based approach. In addition, we provide a provable sample complexity bound for few-shot learning. Our experiments on MuJoCo and Meta-World show that the proposed approach outperforms state-of-the-art multi-task, meta-, and task clustering baselines in training, generalization, and few-shot learning, often by a large margin. Our code is available at https://github.com/CERL-WUSTL/PACMAN.","When designing intelligent systems that learn to make decisions, such as like robots or personalized digital assistants, we would like them to perform well for as many users or situations as possible. However, tasks in the real world are often very different from each other, making it challenging to train a single system that works well across the board.Our research introduces a new way to address this challenge by building a committee of decision-making policies. Instead of searching for one perfect solution, we construct a small group of policies so that, for almost every task, at least one of them performs nearly as well as possible. This idea is formalized through what we call (ε, 1−δ) coverage, which means we guarantee that most people—or tasks—will get near-optimal performance (within ε), for at least 1−δ fraction of all cases.We developed practical algorithms to create such committees, with strong theoretical guarantees and efficient learning from just a few examples. Our approach not only improves fairness—ensuring many users or downstream tasks get high-quality outcomes—but also outperforms leading AI methods in experiments involving diverse robotic control tasks."
