type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures,https://ICML.cc//virtual/2025/poster/44131,"Dongzhe Zheng, Wenjie Mei","Learning unknown dynamics under environmental (or external) constraints is fundamental to many fields (e.g., modern robotics), particularly challenging when constraint information is only locally available and uncertain. Existing approaches requiring global constraints or using probabilistic filtering fail to fully exploit the geometric structure inherent in local measurements (by using, e.g., sensors) and constraints. This paper presents a geometric framework unifying measurements, constraints, and dynamics learning through a fiber bundle structure over the state space. This naturally induced geometric structure enables measurement-aware Control Barrier Functions that adapt to local sensing (or measurement) conditions. By integrating Neural ODEs, our framework learns continuous-time dynamics while preserving geometric constraints, with theoretical guarantees of learning convergence and constraint satisfaction dependent on sensing quality. The geometric framework not only enables efficient dynamics learning but also suggests promising directions for integration with reinforcement learning approaches. Extensive simulations demonstrate significant improvements in both learning efficiency and constraint satisfaction over traditional methods, especially under limited and uncertain sensing conditions.","Learning unknown dynamics under safety-critical constraints--exemplified by autonomous vehicles in unmapped environments--represents a foundational robotics challenge. Traditional methods decouple dynamics learning from sensor-state relationships, yielding brittle solutions.Our framework uses differential geometry to unify sensor measurements, constraints, and dynamic learning. Modeling robot state-sensor interactions as fiber bundles reveals that sensor uncertainty follows structured physical laws. For example, LiDAR precision depends on obstacle proximity while safety boundaries adapt geometrically. The geometric connection encodes how motion alters measurement reliability--coupling dynamics and sensor physics.Key Advances: We establish (1) geometric unification through differential structures, (2) symmetry-aware learning via equivariant architectures, and (3) adaptive safety certification responding to sensing quality. Simulations show significant improvements in learning efficiency and safety compliance under real-world constraints. This establishes a rigorous geometric foundation for autonomous systems in uncertain environments, with applications in robotics and self-driving vehicles."
Poster,Learning Efficient Robotic Garment Manipulation with Standardization,https://ICML.cc//virtual/2025/poster/44005,"zhou changshi, Feng Luan, hujiarui, Shaoqiang Meng, Zhipeng Wang, Yanchao Dong, Yanmin Zhou, Bin He","Garment manipulation is a significant challenge for robots due to the complex dynamics and potential self-occlusion of garments. Most existing methods of efficient garment unfolding overlook the crucial role of standardization of flattened garments, which could significantly simplify downstream tasks like folding, ironing, and packing. This paper presents APS-Net, a novel approach to garment manipulation that combines unfolding and standardization in a unified framework. APS-Net employs a dual-arm, multi-primitive policy with dynamic fling to quickly unfold crumpled garments and pick-and-place(p&p)  for precise alignment. The purpose of garment standardization during unfolding involves not only maximizing surface coverage but also aligning the garment’s shape and orientation to predefined requirements. To guide effective robot learning, we introduce a novel factorized reward function for standardization, which incorporates garment coverage (Cov), keypoint distance (KD), and intersection-over-union (IoU) metrics. Additionally, we introduce a spatial action mask and an Action Optimized Module to improve unfolding efficiency by selecting actions and operation points effectively. In simulation, APS-Net outperforms state-of-the-art methods for long sleeves, achieving 3.9% better coverage, 5.2% higher IoU, and a 0.14 decrease in KD (7.09% relative reduction). Real-world folding tasks further demonstrate that standardization simplifies the folding process. Project page: https://hellohaia.github.io/APS/","Robots often struggle to work with clothes because garments are soft, crumpled, and tricky to handle. We developed a method that helps robots not only unfold clothes quickly, but also lay them out neatly in a consistent way. It combines simple actions like tossing and placing to make the garment flat and aligned, which makes later tasks like folding or packing easier. We also taught the robot what a “well-arranged” piece of clothing should look like using new training method. Our approach showed better performance than earlier techniques in both simulated and real-world tests, and it could make robotic laundry systems more useful at home and in industry."
Poster,Learning Event Completeness for Weakly Supervised Video Anomaly Detection,https://ICML.cc//virtual/2025/poster/45685,"Yu Wang, Shiwei Chen","Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing temporal intervals containing anomalous events within untrimmed videos, utilizing only video-level annotations. However, a significant challenge arises due to the absence of dense frame-level annotations, often leading to incomplete localization in existing WS-VAD methods. To address this issue, we present a novel LEC-VAD, Learning Event Completeness for Weakly Supervised Video Anomaly Detection, which features a dual structure designed to encode both category-aware and category-agnostic semantics between vision and language. Within LEC-VAD, we devise semantic regularities that leverage an anomaly-aware Gaussian mixture to learn precise event boundaries, thereby yielding more complete event instances. Besides, we develop a novel memory bank-based prototype learning mechanism to enrich concise text descriptions associated with anomaly-event categories. This innovation bolsters the text's expressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates remarkable advancements over the current state-of-the-art methods on two benchmark datasets XD-Violence and UCF-Crime.","Imagine you’re monitoring security cameras but only know if something unusual happened in a long video, not when or what exactly. This is the challenge of weakly supervised video anomaly detection (WS-VAD). Most existing methods struggle to precisely locate anomalies because they lack detailed, frame-by-frame labels. Our solution, LEC-VAD, acts like a detective trained to notice both broad ""vibes"" (e.g., ""something’s off"") and specific clues (e.g., ""a fight breaks out"").LEC-VAD uses two smart tools:1. Anomaly ""GPS"": By modeling anomalies as a mix of typical patterns (like a Gaussian “blur” of possibilities), LEC-VAD sharpens the edges of suspicious events, avoiding vague or incomplete guesses.2. Text Memory Bank: It stores concise descriptions of anomalies (e.g., ""explosion,"" ""loitering"") to refine how the model ""talks"" about events, making detection sharper.By combining visual clues with language, LEC-VAD outperforms current methods on real-world datasets like XD-Violence and UCF-Crime. This approach could improve surveillance systems, content moderation, or even self-driving cars by spotting dangers faster and with less human effort."
Poster,Learning Extrapolative Sequence Transformations from Markov Chains,https://ICML.cc//virtual/2025/poster/46418,"Sophia Hager, Aleem Khan, Andrew Wang, Nicholas Andrews","Most successful applications of deep learning involve similar training and test conditions. However, tasks such as biological sequence design involve searching for sequences that improve desirable properties beyond previously known values, which requires novel hypotheses that \emph{extrapolate} beyond training data. In these settings, extrapolation may be achieved by using random search methods such as Markov chain Monte Carlo (MCMC), which, given an initial state, sample local transformations to approximate a target density that rewards states with the desired properties. However, even with a well-designed proposal, MCMC may struggle to explore large structured state spaces efficiently. Rather than relying on stochastic search, it would be desirable to have a model that greedily optimizes the properties of interest, successfully extrapolating in as few steps as possible. We propose to learn such a model from the Markov chains resulting from MCMC search. Specifically, our approach uses selected states from Markov chains as a source of training data for an autoregressive model, which is then able to efficiently generate novel sequences that extrapolate along the sequence-level properties of interest. The proposed approach is validated on three problems: protein sequence design, text sentiment control, and text anonymization. We find that the autoregressive model can extrapolate as well or better than MCMC, but with the additional benefits of scalability and significantly higher sample efficiency.","Neural networks have trouble ""extrapolating"", or generating sequences with qualities more extreme than they've seen in the past. For instance, a model trained on 2-4 star reviews is unlikely to generate a 5-star review. In some use cases, this ability would be very useful. It's possible to extrapolate by repeatedly changing one small part of a sequence, checking if the model thinks it's going to score better, and keeping the change if it does. However, because this involves so many small changes, it takes a long time to run for any sequence, which makes it impractical. We take these long chains of changed sequences and choose a handful of examples, then train a model on those examples to learn a model that generates much shorter chains which still extrapolate. We look at three settings and find that our model accomplishes extrapolation in much less time than generating the long chain, and in some cases extrapolates better than the long chain."
Poster,Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning,https://ICML.cc//virtual/2025/poster/44162,"Lianbo Ma, Jianlun Ma, Yuee Zhou, Guoyang Xie, Qiang He, Zhichao Lu","Mixed Precision Quantization (MPQ) has become an essential technique for optimizing neural network by determining the optimal bitwidth per layer. Existing MPQ methods, however, face a major hurdle: they require a computationally expensive search for quantization strategies on large-scale datasets. To resolve this issue, we introduce a novel approach that first searches for quantization strategies on small datasets and then generalizes them to large-scale datasets. This approach simplifies the process, eliminating the need for large-scale quantization fine-tuning and only necessitating model weight adjustment. Our method is characterized by three key techniques: sharpness-aware minimization for enhanced quantized model generalization, implicit gradient direction alignment to handle gradient conflicts among different optimization objectives, and an adaptive perturbation radius to accelerate optimization. It offers advantages such as no intricate computation of feature maps and high search efficiency. Both theoretical analysis and experimental results validate our approach. Using the CIFAR10 dataset (just 0.5\% the size of ImageNet training data) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a significantly lower computational cost, while improving efficiency by up to 150\% over the baselines.","We propose an innovative method named Adaptive Sharpness-Aware Gradient Aligning (ASGA) to address inefficiencies in optimizing neural networks through mixed-precision quantization. Traditional methods face high computational costs when optimizing neural networks on large datasets, while we trained models on small proxy datasets using sharpness-aware minimization and adaptive gradient alignment techniques. By stabilizing model training processes, this approach achieves comparable accuracy on large-scale datasets like ImageNet, while accelerating optimization search speeds by up to 150% and reducing data requirements to 0.5% of the target dataset's demands. This advancement enables faster and more accessible deployment of AI optimization for edge devices and resource-constrained scenarios."
Poster,Learning from others' mistakes: Finetuning machine translation models with span-level error annotations,https://ICML.cc//virtual/2025/poster/46127,"Lily Zhang, Hamid Dadkhahi, Mara Finkelstein, Firas Trabelsi, Jiaming Luo, Markus Freitag","Despite growing interest in incorporating feedback to improve language models, most efforts focus only on sequence-level annotations. In this work, we explore the potential of utilizing fine-grained span-level annotations from offline datasets to improve model quality. We develop a simple finetuning algorithm, called Training with Annotations (TWA), to directly train machine translation models on such annotated data. TWA utilizes targeted span-level error information while also flexibly learning what to penalize within a span. Moreover, TWA considers the overall trajectory of a sequence when deciding which non-error spans to utilize as positive signals. Experiments on English-German and Chinese-English machine translation show that TWA outperforms baselines such as supervised finetuning on sequences filtered for quality and Direct Preference Optimization on pairs constructed from the same data.","Language models have gotten so good that it can the difficult to improve them using the standard practice of collecting high-quality examples to train models to emulate. Instead, researchers are increasingly turning towards improving models using various forms of feedback, such as whether text is good or bad, or which option is better within a pair. However, most current methods only use broad feedback—like rating an entire sentence. But what if we could use more detailed feedback that highlights exactly which parts of a sentence are wrong? In our work, we introduce a simple training method called Training with Annotations (TWA) that helps machine translation models learn directly from this kind of precise, span-level feedback. TWA takes advantage of these detailed notes by learning from both the mistakes and correct parts of a sentence. When we tested TWA on English-German and Chinese-English translations, it outperformed existing techniques that rely on broader feedback. This shows that training models with targeted feedback can make them more accurate and effective."
Poster,Learning from Sample Stability for Deep Clustering,https://ICML.cc//virtual/2025/poster/44446,"Zhixin Li, Yuheng Jia, Hui LIU, Junhui Hou","Deep clustering, an unsupervised technique independent of labels, necessitates tailored supervision for model training. Prior methods explore supervision like similarity and pseudo labels, yet overlook individual sample training analysis. Our study correlates sample stability during unsupervised training with clustering accuracy and network memorization on a per-sample basis. Unstable representations across epochs often lead to mispredictions, indicating difficulty in memorization and atypicality. Leveraging these findings, we introduce supervision signals for the first time based on sample stability at the representation level. Our proposed strategy serves as a versatile tool to enhance various deep clustering techniques. Experiments across benchmark datasets showcase that incorporating sample stability into training can improve the performance of deep clustering. The code is available at https://github.com/LZX-001/LFSS.","In this paper, we introduced a new approach to grouping data that uses the idea of how consistent the representation of each data point behaves during the learning process. After running many tests, we found that how stable a data point is closely relates to how accurately it can be grouped and how well the model remembers it.  Based on these observations, we developed a new method, which takes advantage of data point stability at both the individual and group levels to improve the overall performance of deep learning-based grouping techniques."
Poster,Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network,https://ICML.cc//virtual/2025/poster/46235,"Jijia Liu, Feng Gao, Qingmin Liao, Chao Yu, Yu Wang","Reinforcement learning (RL) for continuous control often requires large amounts of online interaction data. Value-based RL methods can mitigate this burden by offering relatively high sample efficiency. Some studies further enhance sample efficiency by incorporating offline demonstration data to “kick-start” training, achieving promising results in continuous control. However, they typically compute the Q-function independently for each action dimension, neglecting interdependencies and making it harder to identify optimal actions when learning from suboptimal data, such as non-expert demonstration and online-collected data during the training process.To address these issues, we propose Auto-Regressive Soft Q-learning (ARSQ), a value-based RL algorithm that models Q-values in a coarse-to-fine, auto-regressive manner.First, ARSQ decomposes the continuous action space into discrete spaces in a coarse-to-fine hierarchy, enhancing sample efficiency for fine-grained continuous control tasks. Next, it auto-regressively predicts dimensional action advantages within each decision step, enabling more effective decision-making in continuous control tasks. We evaluate ARSQ on two continuous control benchmarks, RLBench and D4RL, integrating demonstration data into online training. On D4RL, which includes non-expert demonstrations, ARSQ achieves an average 1.62× performance improvement over SOTA value-based baseline. On RLBench, which incorporates expert demonstrations, ARSQ surpasses various baselines, demonstrating its effectiveness in learning from suboptimal online-collected data.","Teaching robots new skills is most effective when they can learn both from trial-and-error and by watching humans. However, people don’t always demonstrate tasks perfectly, which makes it hard for robots to know the best way to act, slowing down their learning. To tackle this, we created a new method that helps robots learn effectively even from less-than-perfect examples. Our approach lets robots choose their actions in stages, starting with simple decisions and gradually making them more precise—similar to learning a dance step by step. We also designed our method so robots can learn each part of a movement in order, such as moving each joint one at a time, which helps them better understand and use messy demonstrations. When we tested this approach on popular robot learning tasks, it outperformed existing methods, especially when the training data was flawed. This means robots can pick up useful skills faster and with fewer mistakes, even if the examples they learn from aren’t perfect."
Poster,Learning from True-False Labels via Multi-modal Prompt Retrieving,https://ICML.cc//virtual/2025/poster/46070,"Zhongnian Li, Jinghao Xu, Peng Ying, Meng Wei, Xinzheng Xu","Pre-trained **V**ision-**L**anguage **M**odels (VLMs) exhibit strong zero-shot classification abilities, demonstrating great potential for generating weakly supervised labels. Unfortunately, existing weakly supervised learning methods are short of ability in generating accurate labels via VLMs. In this paper, we propose a novel weakly supervised labeling setting, namely **T**rue-**F**alse **L**abels (TFLs) which can achieve high accuracy when generated by VLMs. The TFL indicates whether an instance belongs to the label, which is randomly and uniformly sampled from the candidate label set. Specifically, we theoretically derive a risk-consistent estimator to explore and utilize the conditional probability distribution information of TFLs. Besides, we propose a convolutional-based **M**ulti-modal **P**rompt **R**etrieving (MRP) method to bridge the gap between the knowledge of VLMs and target learning tasks. Experimental results demonstrate the effectiveness of the proposed TFL setting and MRP learning method. The code to reproduce the experiments is at https://github.com/Tranquilxu/TMP.","Pre-trained Vision-Language Models (VLMs) exhibit strong zero-shot classification abilities, demonstrating great potential for generating weakly supervised labels. In this paper, we propose a novel weakly supervised setting, True-False Label (TFL), which generates reliable labels using vision-language models. Besides, we introduce a risk-consistent method to learn from TFLs via multi-modal prompt retrieving. Our findings show that pre-trained models can improve themselves without introducing additional labeling."
Poster,Learning Fused State Representations for Control from Multi-View Observations,https://ICML.cc//virtual/2025/poster/44691,"Zeyu Wang, Yao-Hui Li, Xin Li, Hongyu Zang, Romain Laroche, Riashat Islam","Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose **M**ulti-view **F**usion **S**tate for **C**ontrol (**MFSC**), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC’s robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance. The project code is available at [https://github.com/zpwdev/MFSC](https://github.com/zpwdev/MFSC).","Robots often rely on cameras and sensors to understand the world around them. Using multiple views can improve their perception, but it also brings new challenges—such as too much redundant data, distractions from irrelevant details, or missing angles altogether.Our method, called MFSC (Multi-view Fusion State for Control), helps robots make sense of all these different views. It condenses the visual information into a compact summary that focuses only on what’s important for making decisions. This helps robots ignore the noise and concentrate on what truly matters.We tested MFSC in complex environments, where some views were missing or noisy. Experimental results show that robots using MFSC maintained better decision-making performance in scenarios with missing or noisy views."
