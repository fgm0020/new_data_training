type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Large Language-Geometry Model: When LLM meets Equivariance,https://ICML.cc//virtual/2025/poster/45326,"Zongzhao Li, Jiacheng Cen, Bing Su, Tingyang Xu, Yu Rong, Deli Zhao, Wenbing Huang","Accurately predicting 3D structures and dynamics of physical systems is crucial in scientific applications. Existing approaches that rely on geometric Graph Neural Networks (GNNs) effectively enforce $\mathrm{E}(3)$-equivariance, but they often fail in leveraging extensive broader information. While direct application of Large Language Models (LLMs) can incorporate external knowledge, they lack the capability for spatial reasoning with guaranteed equivariance. In this paper, we propose EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates $\mathrm{E}(3)$-equivariance with LLM capabilities. Specifically, EquiLLM comprises four key components: geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adapter. Essentially, the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adapter modules. Experimental results demonstrate that EquiLLM delivers significant improvements over previous methods across molecular dynamics simulation, human motion simulation, and antibody design, highlighting its promising generalizability.","Predicting how things move and interact in 3D, like molecules or people, is really important for many scientific fields. Current AI methods either excel at understanding 3D geometry but miss important background knowledge, or they have vast knowledge but struggle with spatial reasoning.We developed EquiLLM, a new AI framework that combines the best of both worlds. Our approach uses large language models to tap into extensive scientific knowledge, while specialized components handle the complex 3D geometry calculations. The framework works by having the language model process scientific information and context, while dedicated modules ensure all spatial calculations remain mathematically consistent.EquiLLM significantly improves capabilities in areas like simulating how molecules move, understanding human motion, and even designing antibodies. This shows that EquiLLM is a versatile tool that can be applied to many different scientific challenges."
Poster,Large Language Model-driven Large Neighborhood Search for Large-Scale MILP Problems,https://ICML.cc//virtual/2025/poster/43770,"Huigen Ye, Hua Xu, An Yan, Yaoyang Cheng","Large Neighborhood Search (LNS) is a widely used method for solving large-scale Mixed Integer Linear Programming (MILP) problems. The effectiveness of LNS crucially depends on the choice of the search neighborhood. However, existing strategies either rely on expert knowledge or computationally expensive Machine Learning (ML) approaches, both of which struggle to scale effectively for large problems. To address this, we propose LLM-LNS, a novel Large Language Model (LLM)-driven LNS framework for large-scale MILP problems. Our approach introduces a dual-layer self-evolutionary LLM agent to automate neighborhood selection, discovering effective strategies with scant small-scale training data that generalize well to large-scale MILPs. The inner layer evolves heuristic strategies to ensure convergence, while the outer layer evolves evolutionary prompt strategies to maintain diversity. Experimental results demonstrate that the proposed dual-layer agent outperforms state-of-the-art agents such as FunSearch and EOH. Furthermore, the full LLM-LNS framework surpasses manually designed LNS algorithms like ACP, ML-based LNS methods like CL-LNS, and large-scale solvers such as Gurobi and SCIP. It also achieves superior performance compared to advanced ML-based MILP optimization frameworks like GNN&GBDT and Light-MILPopt, further validating the effectiveness of our approach.","Solving real-world planning tasks—like delivery routing or factory scheduling—often involves tackling Mixed Integer Linear Programs (MILPs), which become extremely hard as they grow. Large Neighborhood Search (LNS) is a common technique that improves solutions by repeatedly focusing on parts of the problem. But deciding which part to focus on is difficult and usually requires domain expertise or costly AI methods that don’t scale well. We introduce LLM-LNS, a new system that uses Large Language Models (LLMs)—the same kind of AI behind ChatGPT—to guide this process automatically. Our approach features a dual-layer self-evolving LLM agent: one layer explores diverse strategies, while the other refines them to boost performance. Remarkably, it learns from small problems and generalizes to much larger ones. LLM-LNS consistently outperforms existing methods, including expert-designed strategies, other AI systems, and industry-standard solvers like Gurobi. It delivers faster and better solutions, offering major efficiency gains for industries that rely on solving large-scale optimization problems, such as logistics and manufacturing."
Poster,Large Language Models are Demonstration Pre-Selectors for Themselves,https://ICML.cc//virtual/2025/poster/44899,"Jiarui Jin, Yuwei Wu, Haoxuan Li, Xiaoting He, Weinan Zhang, Yiming Yang, Yong Yu, Jun Wang, Mengyue Yang","In-context learning with large language models (LLMs) delivers strong few-shot performance by choosing few-shot demonstrations from the entire training dataset. However, previous few-shot in-context learning methods, which calculate similarity scores for choosing demonstrations, incur high computational costs by repeatedly retrieving large-scale datasets for each query. This is due to their failure to recognize that not all demonstrations are equally informative, and many less informative demonstrations can be inferred from a core set of highly informative ones. To this end, we propose FEEDER (FEw yet Essential Demonstration prE-selectoR), a novel \emph{pre-selection} framework that identifies a core subset of demonstrations containing the most informative examples. This subset, referred to as the FEEDER set, consists of demonstrations that capture both the ''sufficiency'' and ''necessity'' information to infer the entire dataset. Notice that FEEDER is selected before the few-shot in-context learning, enabling more efficient few-shot demonstrations choosing in a smaller set. To identify FEEDER, we propose a novel effective tree based algorithm. Once selected, it can replace the original dataset, leading to improved efficiency and prediction accuracy in few-shot in-context learning. Additionally, FEEDER also benefit fine-tuning LLMs, we propose a bi-level optimization method enabling more efficient training without sacrificing performance when datasets become smaller. Our experiments are on 6 text classification datasets, 1 reasoning dataset, and 1 semantic-parsing dataset, across 6 LLMs (ranging from 335M to 7B parameters), demonstrate that: (i) In few-shot inference, FEEDER achieves superior (or comparable) performance while utilizing only half the input training data. (ii) In fine-tuning, FEEDER significantly boosts the performance of LLMs.","Large language models (LLMs), like those behind chatbots and AI assistants, get better at new tasks when shown a few example questions and answers. However, picking the best examples from a huge pile can be slow and computationally expensive. We found that many examples do not add much value — most of the useful information comes from a small, carefully chosen set. To address this, we created FEEDER (FEw yet Essential Demonstration prE-selectoR), a method that selects the smallest set of essential examples needed for the AI to learn as much as possible. FEEDER identifies these core examples before training, using a novel tree-based approach. This makes it much faster and cheaper to pick examples for the model, and actually helps the AI make better predictions. We tested FEEDER across several language tasks and different AIs. We found it can match or beat previous methods while using only half the examples, and it also improves training for smaller datasets. This could help make large language models more efficient and accessible."
Poster,Large Language Models to Diffusion Finetuning,https://ICML.cc//virtual/2025/poster/46370,"Edoardo Cetin, Tianyu Zhao, Yujin Tang","We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is applicable to any foundation model pre-trained with cross-entropy and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method can be more effective and is fully compatible with traditional finetuning and search approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks.","We developed a new method to enhance how large language models (like ChatGPT, Claude, and DeepSeek) answer questions by letting them dynamically increase their processing power at test time. Normally, language models respond instantly or produce longer explanations by formulating their thoughts in plain text. Instead, our technique allows them to gradually refine their answers by going beyond the space of language, much like a person reasoning about a problem beyond saying out loud every word in their thoughts.Using a process called ""diffusion,"" our models steadily improve their accuracy the more computation they use, which can be specified by the user on demand. This means our method could allow users to allocate increasingly more thinking time for tough questions until the model returns an appropriate answer. Our models can even intelligently decide how many thinking steps are necessary automatically, saving resources when simpler questions arise. Overall, our approach helps language models provide smarter answers in a new orthogonal way that provides users with a new level of agency."
Poster,Larger or Smaller Reward Margins to Select Preferences for LLM Alignment?,https://ICML.cc//virtual/2025/poster/44101,"Kexin Huang, Junkang Wu, Ziqian Chen, xue wang, Jinyang Gao, Bolin Ding, Jiancan Wu, Xiangnan He, Xiang Wang","Preference learning is critical for aligning large language models (LLMs) with human values, with the quality of preference datasets playing a crucial role in this process. While existing metrics primarily assess data quality based on either *explicit* or *implicit* reward margins, their single-margin focus often leads to contradictory evaluations for the same data.To address this issue, we propose a new metric of *alignment potential*, $M_{AP}$, which integrates both margins to quantifythe gap from the model's *current implicit* reward margin to the *target explicit* reward margin, thereby estimating the model's potential to align on the preference data.Empirical results demonstrate that training on the data selected by $M_{AP}$ consistently enhances alignment performance, surpassing existing metrics across different base models and optimization objectives.Furthermore, our method can be extended to self-play data generation frameworks, where we use this metric to identify high-quality data within the self-generated content by LLMs. Under this data generation scenario, our method surpasses current state-of-the-artmethods across various training settings and demonstrates continuous improvementswith increasing dataset size and training iterations.","When teaching AI language models to understand human preferences, researchers face a challenge: existing methods for evaluating training data quality often provide conflicting assessments, making it difficult to select the most effective data for training. Our research introduces a new measurement approach that bridges this gap by considering both what the AI system currently understands and what we want it to learn. This helps us identify which training examples will be most valuable for teaching the AI to better align with human values. Experiments show that training examples selected by our method consistently outperform existing metrics under various training settings, enabling more efficient training of AI systems that better understand and respect human preferences."
Poster,LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence,https://ICML.cc//virtual/2025/poster/43466,"Zhuoling Li, Xiaogang Xu, Zhenhua Xu, Ser-Nam Lim, Hengshuang Zhao","Recent embodied agents are primarily built based on reinforcement learning (RL) or large language models (LLMs). Among them, RL agents are efficient for deployment but only perform very few tasks. By contrast, giant LLM agents (often more than 1000B parameters) present strong generalization while demanding enormous computing resources. In this work, we combine their advantages while avoiding the drawbacks by conducting the proposed referee RL on our developed large auto-regressive model (LARM). Specifically, LARM is built upon a lightweight LLM (fewer than 5B parameters) and directly outputs the next action to execute rather than text. We mathematically reveal that classic RL feedbacks vanish in long-horizon embodied exploration and introduce a giant LLM based referee to handle this reward vanishment during training LARM. In this way, LARM learns to complete diverse open-world tasks without human intervention. Especially, LARM successfully harvests enchanted diamond equipment in Minecraft, which demands significantly longer decision-making chains than the highest achievements of prior best methods.","This paper proposes the referee reinforcement learning algorithm, which can fine-tune a large language model into a promising embodied policy, termed as large auto-regressive model in this work."
Poster,La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation,https://ICML.cc//virtual/2025/poster/46630,"Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, lulu hu","Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (**La**yerwise **Ro**tated **S**parse **A**ctivation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40\% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30× wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54\%, while surpassing TEAL by 1.77\% and CATS by 17.14\%.","Large Language Models (LLMs) are powerful but can be resource-intensive, slowing down their performance. One way to make them more efficient is by reducing the amount of active computations, a process called activation sparsity. However, current methods either require additional training, which is time-consuming, or rely on unreliable techniques that can lead to inconsistent performance.Our new approach, LaRoSA, aims to address these issues by enhancing the efficiency of LLMs without the need for extra training or unstable methods. We use a clever technique that involves rotating the data within each layer of the model, making it easier to identify and keep only the most important parts for processing. This ensures that the model remains fast and efficient without losing much accuracy.LaRoSA has been tested on various LLMs and shown to maintain high performance while speeding up the processing time significantly. For example, when applied to a specific model, LLaMA2-7B, LaRoSA reduces the processing time by 1.30 times while maintaining a high level of accuracy, outperforming other existing methods. This makes it a promising solution for improving the efficiency of LLMs in real-world applications."
Poster,LASER: Attention with Exponential Transformation,https://ICML.cc//virtual/2025/poster/44345,"Sai Surya Duvvuri, Inderjit Dhillon","Transformers have had tremendous impact for several sequence related tasks, largely due to their ability to retrieve from any part of the sequence via softmax based dot-product attention. This mechanism plays a crucial role in Transformer's performance.  We analyze the gradients backpropagated through the softmax operation in the attention mechanism and observe that these gradients can often be small. This poor gradient signal backpropagation can lead to inefficient learning of parameters preceeding the attention operations. To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal. We show that LASER attention can be implemented by making small modifications to existing attention implementations. We conduct experiments on autoregressive large language models (LLMs) with upto 7.7 billion parameters with an average improvement of upto 1.44% over standard attention on downstream evaluations and 1.65% finetuning improvements. Additionally, LASER demonstrates generalization performance improvement across a variety of tasks (vision, text and speech):Vision Transformer (ViT) on Imagenet, Conformer on the Librispeech speech-to-text and BERT with 2.2 billion parameters.","We identified a key bottleneck in the attention mechanism used by transformers, which weakens the backpropagation signal and makes training inefficient. Our solution, LASER, applies a simple exponential transformation to the representations before the attention step, which strengthens the gradient signal. This method requires only minimal code changes and results in consistent performance improvements across text, image, and speech models."
Poster,LAST SToP for Modeling Asynchronous Time Series,https://ICML.cc//virtual/2025/poster/45155,"Shubham Gupta, Thibaut Durand, Graham Taylor, Lilian Bialokozowicz","We present a novel prompt design for Large Language Models (LLMs) tailored to **Asynchronous Time Series**. Unlike regular time series, which assume values at evenly spaced time points, asynchronous time series consist of timestamped events occurring at irregular intervals, each described in natural language. Our approach effectively utilizes the rich natural language of event descriptions, allowing LLMs to benefit from their broad world knowledge for reasoning across different domains and tasks. This allows us to extend the scope of asynchronous time series analysis beyond forecasting to include tasks like anomaly detection and data imputation. We further introduce **Stochastic Soft Prompting**, a novel prompt-tuning mechanism that significantly improves model performance, outperforming existing finetuning methods such as QLORA. Through extensive experiments on real-world datasets, we demonstrate that our approach achieves state-of-the-art performance across different tasks and datasets.","Most AI systems analyze data that arrives at regular intervals, like daily stock prices or hourly temperature readings. But many real-world events happen unpredictably — like medical emergencies, social media posts, or equipment failures — and are described in natural language rather than just numbers. Traditional methods struggle with this ""asynchronous time series"" data because they can't handle irregular timing and rich text descriptions together.We developed LASTS, a new approach that uses Large Language Models  to analyze these irregular event sequences. Instead of forcing events into rigid categories, our method preserves their natural language descriptions, allowing the AI to use its understanding of language and world knowledge. We also created ""Stochastic Soft Prompting,"" a finetuning technique that helps the LLMs understand our specific domain data much better than other famous finetuning techniques.Our approach significantly outperforms existing methods across multiple real-world datasets. This makes sophisticated time series analysis more accessible and could improve applications in healthcare monitoring, financial analysis, and social media understanding, helping organizations better predict and respond to irregular but important events."
Poster,Latent Action Learning Requires Supervision in the Presence of Distractors,https://ICML.cc//virtual/2025/poster/46574,"Alexander Nikulin, Ilya Zisman, Denis Tarasov, Nikita Lyubaykin, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov","Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by **8x**, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by **4.2x** on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.","One of the most promising ways to scale up Embodied AI is to use videos from the internet, given that they are extremely large and diverse, encompassing many complex, real-world human activities. However, video data cannot be used immediately due to a lack of action annotations. Recently, approaches that infer latent proxy actions have demonstrated remarkable pre-training efficiency. These approaches, however, have focused on distractor-free data, which unfortunately does not reflect real-world videos, which may contain action-correlated distractors, such as people moving in the background. In this study, we empirically investigate the effect of such distractors on latent action learning. We demonstrate that, without additional supervision in the form of hints indicating what is relevant to the action, these approaches are ineffective. Our findings indicate that existing methods will not be able to utilize the available videos on the internet effectively, and that novel approaches are needed."
