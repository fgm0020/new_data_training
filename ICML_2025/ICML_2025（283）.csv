type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"Spurious Correlations in High Dimensional Regression: The Roles of Regularization, Simplicity Bias and Over-Parameterization",https://ICML.cc//virtual/2025/poster/46042,"Simone Bombari, Marco Mondelli","Learning models have been shown to rely on spurious correlations between non-predictive features and the associated labels in the training data, with negative implications on robustness, bias and fairness.In this work, we provide a statistical characterization of this phenomenon for high-dimensional regression, when the data contains a predictive *core* feature $x$ and a *spurious* feature $y$. Specifically, we quantify the amount of spurious correlations $\mathcal C$ learned via linear regression, in terms of the data covariance and the strength $\lambda$ of the ridge regularization.As a consequence, we first capture the simplicity of $y$ through the spectrum of its covariance, and its correlation with $x$ through the Schur complement of the full data covariance. Next, we prove a trade-off between $\mathcal C$ and the in-distribution test loss $\mathcal L$, by showing that the value of $\lambda$ that minimizes $\mathcal L$ lies in an interval where $\mathcal C$ is increasing. Finally, we investigate the effects of over-parameterization via the random features model, by showing its equivalence to regularized linear regression.Our theoretical results are supported by numerical experiments on Gaussian, Color-MNIST, and CIFAR-10 datasets.","Machine learning models often learn misleading patterns from their training data. For example, a neural network might think that a photo of a person swimming in the ocean is a boat, simply because the network (when training) used to see water only as the background of boats. These patterns are known as spurious correlations, and can negatively impact the fairness, accuracy, and reliability of AI models. In this study, we theoretically explore why and how these misleading correlations happen. We look at the mathematically tractable setting of linear regression, and we provide insights that could be statistically relevant also for more complex models. For example, we attempt to formalize the known fact that neural networks prefer to learn ""easy"" patterns: intuitively, a blue background is easier to recognize than a heterogeneously-shaped object (e.g., a boat). Furthermore we see that, unfortunately, there is sometimes a trade-off between maximizing the accuracy of a model, and minimizing the amount of spurious correlations. Finally, we extend our theoretical results to the case where models become larger and larger, as in the setting of modernly used deep neural networks."
Poster,Square$\chi$PO: Differentially Private and Robust $\chi^2$-Preference Optimization in Offline Direct Alignment,https://ICML.cc//virtual/2025/poster/44581,"Xingyu Zhou, Yulian Wu, Wenqian Weng, Francesco Orabona","In this paper, we theoretically study the offline alignment of language models with human preference feedback, under both preference label corruption and privacy protections. To this end, we propose a variant of \texttt{$\chi$PO} -- \texttt{Square}\texttt{$\chi$PO}, which is a simple one-line change of \texttt{$\chi$PO}  with the standard log-loss replaced by a new square loss over probability. Thanks to the inherent nice properties of this new loss, we have advanced the state-of-the-art of differentially private and robust alignment. Specifically, for the local model of label privacy, \texttt{Square}\texttt{$\chi$PO}  is the first one that attains optimal rate based on single-policy concentrability even with general function approximations. It also gives the first result under the central model of privacy protection over both prompts (responses) and labels. On the robustness side against Huber label corruption, \texttt{Square}\texttt{$\chi$PO}  is the first alignment method that has a meaningful theoretical guarantee under general function approximations. More importantly, \texttt{Square}\texttt{$\chi$PO}  can address privacy protection and corruption \emph{simultaneously}, where an interesting separation is observed, implying that the order of privacy and corruption matters. Furthermore, we show that \texttt{Square}\texttt{$\chi$PO}  can also be easily extended to handle the scenario of the general preference model with state-of-the-art guarantees under corruption and privacy. Last but not least, all of our theoretical guarantees enjoy a unified analysis, building upon a new result on the generalization error bounds of least-square regression under corruption and privacy constraints, which we believe is of independent interest to the community.","Training large language models to align with human preferences is a key challenge in AI. But what if the feedback from humans is noisy—or needs to be kept private? In this work, we introduce a new, simple method to improve how language models learn from such feedback, even when it's imperfect or protected. Our approach replaces the usual training loss with a new version that makes learning more stable and accurate. As a result, our method is the first to offer strong guarantees when both privacy and noise are present, covering cases where either the feedback, the user prompts, or both need to stay private. We also show it works well even when the model has to make complex decisions. One surprising insight from our work is that whether privacy protections are applied before or after data is corrupted makes a big difference. Overall, our findings not only improve current techniques, but also provide new theoretical tools for building more trustworthy and robust AI systems."
Poster,SSHR: More Secure Generative Steganography with High-Quality Revealed Secret Images,https://ICML.cc//virtual/2025/poster/45392,"Jiannian Wang, Yao Lu, Guangming Lu","Image steganography ensures secure information transmission and storage by concealing secret messages within images. Recently, the diffusion model has been incorporated into the generative image steganography task, with text prompts being employed to guide the entire process. However, existing methods are plagued by three problems: (1) the restricted control exerted by text prompts causes generated stego images resemble the secret images and seem unnatural, raising the severe detection risk; (2) inconsistent intermediate states between Denoising Diffusion Implicit Models and its inversion, coupled with limited control of text prompts degrade the revealed secret images; (3) the descriptive text of images(i.e. text prompts) are also deployed as the keys, but this incurs significant security risks for both the keys and the secret images.To tackle these drawbacks, we systematically propose the SSHR, which joints the Reference Images with the adaptive keys to govern the entire process, enhancing the naturalness and imperceptibility of stego images. Additionally, we methodically construct an Exact Reveal Process to improve the quality of the revealed secret images. Furthermore, adaptive Reference-Secret Image Related Symmetric Keys are generated to enhance the security of both the keys and the concealed secret images. Various experiments indicate that our model outperforms existing methods in terms of recovery quality and secret image security.","Image steganography ensures secure information transmission and storage by concealing secret messages within images. Recently, the diffusion model has been incorporated into the generative image steganography task, with text prompts being employed to guide the entire process. However, existing methods are plagued by three problems: (1) the restricted control exerted by text prompts causes generated stego images resemble the secret images and seem unnatural, raising the severe detection risk; (2) inconsistent intermediate states between Denoising Diffusion Implicit Models and its inversion, coupled with limited control of text prompts degrade the revealed secret images; (3) the descriptive text of images(i.e. text prompts) are also deployed as the keys, but this incurs significant security risks for both the keys and the secret images.To tackle these drawbacks, we systematically propose the SSHR, which joints the Reference Images with the adaptive keys to govern the entire process, enhancing the naturalness and imperceptibility of stego images. Additionally, we methodically construct an Exact Reveal Process to improve the quality of the revealed secret images. Furthermore, adaptive Reference-Secret Image Related Symmetric Keys are generated to enhance the security of both the keys and the concealed secret images. Various experiments indicate that our model outperforms existing methods in terms of recovery quality and secret image security."
Poster,Stability and Generalization Analysis of Decentralized SGD: Sharper Bounds Beyond Lipschitzness and Smoothness,https://ICML.cc//virtual/2025/poster/44488,"Shuang Zeng, Yunwen Lei","Decentralized SGD (D-SGD) is a popular optimization method to train large-scale machine learning models. In this paper, we study the generalization behavior of D-SGD for both smooth and nonsmooth problems by leveraging the algorithm stability. For convex and smooth problems, we develop stability bounds involving the training errors to show the benefit of optimization in generalization. This improves the existing results by removing the Lipschitzness assumption and implying fast rates in a low-noise condition. We also develop the first optimal stability-based generalization bounds for D-SGD applied to nonsmooth problems. We further develop optimization error bounds which imply minimax optimal excess risk rates. Our novelty in the analysis consists of an error decomposition to use the co-coercivity of functions as well as the control of a neighboring-consensus error.","Training large machine learning models often necessitates data from numerous devices. However, in cases where there is no central server to consolidate this data, Decentralized Stochastic Gradient Descent (D-SGD) offers a solution. In this approach, each device works on its individual data and collaborates with neighboring devices. This study explores the efficacy of models trained using D-SGD on new data, a concept known as generalization. Our research reveals that D-SGD demonstrates strong generalization capabilities across various problem types, whether the functions are smooth (such as curves without abrupt changes) or nonsmooth with sharp corners. Furthermore, we have introduced novel mathematical tools that provide more robust generalization guarantees based on less stringent assumptions. This finding underscores the reliability and effectiveness of decentralized learning, particularly in scenarios where data is dispersed, and inter-device communication is limited."
Poster,Stability and Generalization Capability of Subgraph Reasoning Models for Inductive Knowledge Graph Completion,https://ICML.cc//virtual/2025/poster/45513,"Minsung Hwang, Jaejun Lee, Joyce Whang","Inductive knowledge graph completion aims to predict missing triplets in an incomplete knowledge graph that differs from the one observed during training. While subgraph reasoning models have demonstrated empirical success in this task, their theoretical properties, such as stability and generalization capability, remain unexplored. In this work, we present the first theoretical analysis of the relationship between the stability and the generalization capability for subgraph reasoning models. Specifically, we define stability as the degree of consistency in a subgraph reasoning model's outputs in response to differences in input subgraphs and introduce the Relational Tree Mover’s Distance as a metric to quantify the differences between the subgraphs. We then show that the generalization capability of subgraph reasoning models, defined as the discrepancy between the performance on training data and test data, is proportional to their stability. Furthermore, we empirically analyze the impact of stability on generalization capability using real-world datasets, validating our theoretical findings.","Knowledge graphs (KGs) represent real-world facts using triplets, but they often include missing triplets. Subgraph reasoning models predict missing triplets using the subgraph around each triplet. These models are useful even in inductive Knowledge Graph Completion (KGC), where a KG that appears during inference contains new entities. However, their theoretical properties, such as the relationship between stability and generalization capability, remain unexplored.We analyze stability which is the degree of output consistency with respect to the change of the input subgraph, measured by the Relational Tree Mover's Distance. The generalization capability is measured by the generalization bound which is the discrepancy between performance on training and test data. We theoretically prove that more stable subgraph reasoning models tend to exhibit a higher generalization capability. We validate our theoretical findings on real-world inductive KGC benchmarks. Our analysis highlights the importance of designing stable subgraph reasoning models to enhance generalization capability in inductive KGC."
Poster,Stabilizing Sample Similarity in Representation via Mitigating Random Consistency,https://ICML.cc//virtual/2025/poster/45864,"Jieting Wang, ZhangZelong Zhang, Feijiang Li, Yuhua Qian, Xinyan Liang","Deep learning excels at capturing complex data representations, yet quantifying the discriminative quality of these representations remains challenging. While unsupervised metrics often assess pairwise sample similarity, classification tasks fundamentally require class-level discrimination. To bridge this gap, we propose a novel loss function that evaluates representation discriminability via the Euclidean distance between the learned similarity matrix and the true class adjacency matrix.We identify random consistency—an inherent bias in Euclidean distance metrics—as a key obstacle to reliable evaluation, affecting both fairness and discrimination. To address this, we derive the expected Euclidean distance under uniformly distributed label permutations and introduce its closed-form solution, the Pure Square Euclidean Distance (PSED), which provably eliminates random consistency. Theoretically, we demonstrate that PSED satisfies heterogeneity and unbiasedness guarantees, and establish its generalization bound via the exponential Orlicz norm, confirming its statistical learnability.Empirically, our method surpasses conventional loss functions across multiple benchmarks, achieving significant improvements in accuracy, $F_1$ score, and class-structure differentiation. (Code is published in https://github.com/FeijiangLi/ICML2025-PSED)","Deep learning is powerful because it can learn meaningful patterns from data. Traditionally, researchers have measured this ability by comparing how similar individual samples are to each other. However, for tasks like classification, what matters more is whether the model can distinguish between entire categories of data—not just individual examples. In this paper, we propose a new way to evaluate deep learning models by measuring how well their learned patterns align with the true class structure of the data. We also identify and correct for random biases that can skew these evaluations. Our method provides a mathematically sound and unbiased measure, leading to more reliable model assessments. Experiments show that it improves accuracy and helps models better separate different classes."
Poster,Stable Fair Graph Representation Learning with Lipschitz Constraint,https://ICML.cc//virtual/2025/poster/44069,"Qiang Chen, Zhongze Wu, Xiu Su, Xi Lin, Zhe Qu, Shan You, Shuo Yang, Chang Xu","Group fairness based on adversarial training has gained significant attention on graph data, which was implemented by masking sensitive attributes to generate fair feature views. However, existing models suffer from training instability due to uncertainty of the generated masks and the trade-off between fairness and utility. In this work, we propose a stable fair Graph Neural Network (SFG) to maintain training stability while preserving accuracy and fairness performance. Specifically, we first theoretically derive a tight upper Lipschitz bound to control the stability of existing adversarial-based models and employ a stochastic projected subgradient algorithm to constrain the bound, which operates in a block-coordinate manner. Additionally, we construct the uncertainty set to train the model, which can prevent unstable training by dropping some overfitting nodes caused by chasing fairness. Extensive experiments conducted on three real-world datasets demonstrate that SFG is stable and outperforms other state-of-the-art adversarial-based methods in terms of both fairness and utility performance. Codes are available at https://github.com/sh-qiangchen/SFG.","Ensuring fairness in graph neural networks (GNNs) is a critical yet challenging task. Existing methods often rely on masking sensitive attributes and using adversarial learning to promote fair representations. However, these methods frequently suffer from unstable training, which undermines their reliability and trustworthiness.In this work, we propose a new method called Stable Fair GNN (SFG), which aims to enhance training stability while preserving utility and fairness. We theoretically derive a tight Lipschitz bound of graph fair models with generator to constrain the weight fluctuations. Additionally, we introduce a distributionally robust optimization (DRO) framework that enhances the model’s resilience to mask variations in the fair representations, helping it avoid overfitting to fairness-related noise.Experiments on multiple real-world datasets demonstrate that SFG consistently preserving existing methods in both accuracy and fairness, while significantly reducing training instability. Our findings highlight that achieving trustworthy and effective graph learning requires not only improving fairness, but also explicitly controlling training stability."
Poster,Stable Offline Value Function Learning with Bisimulation-based Representations,https://ICML.cc//virtual/2025/poster/45250,"Brahma Pavse, Yudong Chen, Qiaomin Xie, Josiah Hanna","In reinforcement learning, offline value function learning is the procedure of using an offline dataset to estimate the expected discounted return from each state when taking actions according to a fixed target policy. The stability of this procedure, i.e., whether it converges to its fixed-point, critically depends on the representations of the state-action pairs. Poorly learned representations can make value function learning unstable, or even divergent. Therefore, it is critical to stabilize value function learning by explicitly shaping the state-action representations. Recently, the class of bisimulation-based algorithms have shown promise in shaping representations for control. However, it is still unclear if this class of methods can \emph{stabilize} value function learning. In this work, we investigate this question and answer it affirmatively. We introduce a bisimulation-based algorithm called kernel representations for offline policy evaluation (\textsc{krope}). \textsc{krope} uses a kernel to shape state-action representations such that state-action pairs that have similar immediate rewards and lead to similar next state-action pairs under the target policy also have similar representations. We show that \textsc{krope}: 1) learns stable representations and 2) leads to lower value error than baselines. Our analysis provides new theoretical insight into the stability properties of bisimulation-based methods and suggests that practitioners can use these methods to improve the stability and accuracy of offline evaluation of reinforcement learning agents.","Our work is concerned with reinforcement learning (RL) agents. RL agents are data-driven decision-making agents that make decisions with long-term consequences. For example, a self-driving car’s reaching a particular location is dependent on the previous decisions it made since the driving session started. As we seek to deploy such decision-making agents in the real world, it is increasingly important that we can determine whether an agent will make good or bad decisions.One way to assess whether an RL agent will make decisions that lead to negative outcomes is to actually deploy it in the real world. However, doing so is naturally risky since it may make unsafe decisions. A safer, alternative approach is to estimate how the RL agent may perform using data collected by other agents that may have already been deployed. This approach (known as offline policy evaluation) asks the counterfactual question: “If this new RL agent had been deployed, how well would it have performed?”. Answering counterfactual questions is challenging since it involves reasoning about events that did not occur. Prior methods that have tried to counterfactually evaluate an RL agent tend to produce estimates that are unreliable, which makes them practically futile.To improve previous methods, we explored the idea of using abstractions. At a given moment, RL agents often make decisions based on many factors, but not all of them are important (e.g., clouds do not matter when driving). We wondered if these irrelevant factors hurt the accuracy of offline policy evaluation, and found that they do. To address this limitation, we created an algorithm that learns to abstract away irrelevant factors and focus only on relevant factors. We found that our algorithm made offline policy evaluation more accurate and reliable on simulated robotic tasks. We also provided mathematical explanations for why it works. We believe that our approach brings us closer to deploying more trustworthy RL systems."
Poster,Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization,https://ICML.cc//virtual/2025/poster/45168,"Xinyu Luo, Cedar Site Bai, Bolian Li, Petros Drineas, Ruqi Zhang, Brian Bullins","While popular optimization methods such as SGD, AdamW, and Lion depend on steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there remains a critical gap in handling the non-Euclidean structure observed in modern deep networks training. In this work, we address this need by introducing a new accelerated $\ell_p$ steepest descent algorithm, called Stacey, which uses interpolated primal-dual iterate sequences to effectively navigate non-Euclidean smooth optimization tasks. In addition to providing novel theoretical guarantees for the foundations of our algorithm, we empirically compare our approach against these popular methods on tasks including image classification and language model (LLM) pretraining, demonstrating both faster convergence and higher final accuracy. We further evaluate different values of $p$ across various models and datasets, underscoring the importance and efficiency of non-Euclidean approaches over standard Euclidean methods. Code can be found at https://github.com/xinyuluo8561/Stacey.","Many popular training methods, such as SGD and Adam, rely on problem geometries not always reflected in modern deep learning.  We introduce Stacey, a new primal-dual steepest descent algorithm that combines updates in different geometries to further accelerate optimization. Stacey is both theoretically and empirically justified, outperforming existing methods on tasks like image classification and language model pretraining."
Poster,Staged and Physics-Grounded Learning Framework with Hyperintensity Prior for Pre-Contrast MRI Synthesis,https://ICML.cc//virtual/2025/poster/45409,"Dayang Wang, Srivathsa Pasumarthi Venkata, Ajit Shankaranarayanan, Greg Zaharchuk","Contrast-enhanced MRI enhances pathological visualization but often necessitates Pre-Contrast images for accurate quantitative analysis and comparative assessment. However, Pre-Contrast images are frequently unavailable due to time, cost, or safety constraints, or they may suffer from degradation, making alignment challenging. This limitation hinders clinical diagnostics and the performance of tools requiring combined image types. To address this challenge, we propose a novel staged, physics-grounded learning framework with a hyperintensity prior to synthesize Pre-Contrast images directly from Post-Contrast MRIs. The proposed method can generate high-quality Pre-Contrast images, thus, enabling comprehensive diagnostics while reducing the need for additional imaging sessions, costs, and patient risks. To the best of our knowledge, this is the first Pre-Contrast synthesis model capable of generating images that may be interchangeably used with standard-of-care Pre-Contrast images. Extensive evaluations across multiple datasets, sites, anatomies, and downstream tasks demonstrate the model’s robustness and clinical applicability, positioning it as a valuable tool for contrast-enhanced MRI workflows.","MRI scans often use contrast agents to highlight important tissues in the body, helping doctors detect things like tumors or vascular problems. However, these scans usually require a baseline image known as a Pre-Contrast scan for accurate comparison and measurement. Unfortunately, Pre-Contrast scans are sometimes missing, unavailable, or degraded due to time, cost, or patient safety concerns. To solve this issue, we developed a new AI-based method that can recreate high-quality Pre-Contrast MRI images using only the contrast-enhanced scan. The method is grounded in image physics and uses a staged learning process to remove the effects of the contrast enhancement from the image. This approach can reduce the need for repeat scans, saving time, cost, and patient discomfort. It also helps improve the accuracy of tools that rely on both image types. We tested our method across many different body parts, diseases, and scanners, and found it to be accurate, reliable, and fast, making it a valuable addition to real-world medical imaging workflows."
