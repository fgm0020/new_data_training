type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation,https://ICML.cc//virtual/2025/poster/46696,"Jaeho Kim, Seulki Lee","Unsupervised domain adaptation (UDA) for time series data remains a critical challenge in deep learning, with traditional pseudo-labeling strategies failing to capture temporal patterns and channel-wise shifts between domains, producing sub-optimal pseudo labels. As such, we introduce TransPL, a novel approach that addresses these limitations by modeling the joint distribution $P(X,y)$ of the source domain through code transition matrices, where the codes are derived from vector quantization (VQ) of time series patches. Our method constructs class- and channel-wise code transition matrices from the source domain and employs Bayes' rule for target domain adaptation, generating pseudo-labels based on channel-wise weighted class-conditional likelihoods. TransPL offers three key advantages: explicit modeling of temporal transitions and channel-wise shifts between different domains, versatility towards different UDA scenarios (e.g., weakly-supervised UDA), and explainable pseudo-label generation. We validate TransPL's effectiveness through extensive analysis on four time series UDA benchmarks and confirm that it consistently outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1\% accuracy improvement, 4.9\% F1 improvement), while providing interpretable insights into the domain adaptation process through its learned code transition matrices.","We utilized a method to convert complex time series data into simple discrete tokens (like letters in an alphabet) using a technique called VQVAE. This transformation allows us to study how these tokens transition from one to another over time, creating unique transition patterns for each time series. By aggregating all the token patterns from a user, we can construct a transition matrix to represent a user.By calculating these transition probabilities between tokens, we created a mathematical ""fingerprint"" for each user's data. When we compared these fingerprints between different users, we discovered that some properties are shared while others are unique. This insight allowed us to develop a pseudo-labeling approach where we can use one user's fingerprint to classify another user's time series data.Simply put, when two fingerprints share significant similarities, we can reasonably conclude that the underlying time series belong to the same class. Conversely, when fingerprints differ substantially, we can confidently determine they represent different classes. This approach provides a powerful method for constructing pseudo labels when traditional labeled data is limited or unavailable."
Poster,TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree,https://ICML.cc//virtual/2025/poster/44546,"Yu-Yang Qian, Yuan-Ze Xu, Zhen-Yu Zhang, Peng Zhao, Zhi-Hua Zhou","Many real-world applications collect data in a streaming environment, where learning tasks are encountered sequentially. This necessitates *continual learning* (CL) to update models online, enabling adaptation to new tasks while preserving past knowledge to prevent catastrophic forgetting. Nowadays, with the flourish of *large pre-trained models* (LPMs), *efficiency* has become increasingly critical for CL, due to their substantial computational demands and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of Low-Rank Adapters), a novel approach that constructs *layer-wise* adapters by leveraging hierarchical gradient similarity to enable efficient CL, particularly for LPMs. To reduce the computational burden of task similarity estimation, we employ *bandit* techniques to develop an algorithm based on lower confidence bounds to efficiently explore the task structure. Furthermore, we use sparse gradient updates to facilitate parameter optimization, making the approach better suited for LPMs. Theoretical analysis is provided to justify the rationale behind our approach, and experiments on both *vision transformers* (ViTs) and *large language models* (LLMs) demonstrate the effectiveness and efficiency of our approach across various domains, including vision and natural language processing tasks.","Modern AI systems often need to learn new tasks continuously, but when they learn something new, they tend to forget previous knowledge - a problem called ""catastrophic forgetting."" We developed TreeLoRA, a method that organizes past learning experiences in a tree-like structure and uses smart adaptive search techniques to help large AI models efficiently learn new tasks while remembering old ones. Our approach achieves up to 3.2 times faster training compared to existing methods while maintaining good performance, making it more practical to deploy continuously learning AI systems in real-world applications that need to continuously adapt without forgetting."
Poster,Tree-Sliced Wasserstein Distance: A Geometric Perspective,https://ICML.cc//virtual/2025/poster/45198,"Viet Hoang Tran, Trang Pham, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Chu, Tam Le, Tan Nguyen","Many variants of Optimal Transport (OT) have been developed to address its heavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used for application domains by projecting the OT problem onto one-dimensional lines, and leveraging the closed-form expression of the univariate OT to reduce the computational burden. However, projecting measures onto low-dimensional spaces can lead to a loss of topological information. To mitigate this issue, in this work, we propose to replace one-dimensional lines with a more intricate structure, called \emph{tree systems}. This structure is metrizable by a tree metric, which yields a closed-form expression for OT problems on tree systems. We provide an extensive theoretical analysis to formally define tree systems with their topological properties, introduce the concept of splitting maps, which operate as the projection mechanism onto these structures, then finally propose a novel variant of Radon transform for tree systems and verify its injectivity. This framework leads to an efficient metric between measures, termed Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a variety of experiments on gradient flows, image style transfer, and generative models, we illustrate that our proposed approach performs favorably compared to SW and its variants.","Optimal Transport (OT) is a powerful tool but often too slow to compute. A popular faster version, called Sliced Wasserstein (SW), simplifies the problem by projecting it onto many lines, which can miss important shape information. In this work, we propose a new method that uses a more complex structure called tree systems instead of simple lines. This lets us keep more of the original data’s structure while still computing OT efficiently. We introduce a new way to project onto these trees and show that it works well. Our method, called Tree-Sliced Wasserstein on Systems of Lines (TSW-SL), runs efficiently and outperforms SW in experiments like image style transfer and generative modeling."
Poster,Tree-Sliced Wasserstein Distance with Nonlinear Projection,https://ICML.cc//virtual/2025/poster/45193,"Thanh Tran, Viet Hoang Tran, Thanh Chu, Trang Pham, Laurent Ghaoui, Tam Le, Tan Nguyen","Tree-Sliced methods have recently emerged as an alternative to the traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines with tree-based metric spaces and incorporating a splitting mechanism for projecting measures. This approach enhances the ability to capture the topological structures of integration domains in Sliced Optimal Transport while maintaining low computational costs. Building on this foundation, we propose a novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, substituting the linear projections in earlier versions with general projections, while ensuring the injectivity of the associated Radon Transform and preserving the well-definedness of the resulting metric. By designing appropriate projections, we construct efficient metrics for measures on both Euclidean spaces and spheres. Finally, we validate our proposed metric through extensive numerical experiments for Euclidean and spherical datasets. Applications include gradient flows, self-supervised learning, and generative models, where our methods demonstrate significant improvements over recent SW and TSW variants.","Tree-Sliced methods are a new way to compare probability distributions by using trees instead of lines, making it easier to capture complex shapes in the data while keeping computations fast. Building on this idea, we introduce a new version that uses more flexible (nonlinear) projections but still keeps the math valid and efficient. We design these projections for both flat (Euclidean) and curved (spherical) data, and test them in various tasks like learning representations and generating data. Our method shows better results than previous approaches."
Poster,Triple-Optimistic Learning for Stochastic Contextual Bandits with General Constraints,https://ICML.cc//virtual/2025/poster/45489,"Hengquan Guo, Lingkai Zu, Xin Liu","We study contextual bandits with general constraints, where a learner observes contexts and aims to maximize cumulative rewards while satisfying a wide range of general constraints.We introduce the Optimistic$^3$ framework, a novel learning and decision-making approach that integrates optimistic design into parameter learning, primal decision, and dual violation adaptation (i.e., triple-optimism), combined with an efficient primal-dual architecture. Optimistic$^3$ achieves $\tilde{O}(\sqrt{T})$ regret and constraint violation for contextual bandits with general constraints. This framework not only outperforms the state-of-the-art results that achieve $\tilde{O}(T^{\frac{3}{4}})$ guarantees when Slater's condition does not hold but also improves on previous results that achieve $\tilde{O}(\sqrt{T}/\delta)$ when Slater's condition holds ($\delta$ denotes the Slater's condition parameter), offering a $O(1/\delta)$ improvement. Note this improvement is significant because $\delta$ can be arbitrarily small when constraints are particularly challenging.Moreover, we show that Optimistic$^3$ can be extended to classical multi-armed bandits with both stochastic and adversarial constraints, recovering the best-of-both-worlds guarantee established in the state-of-the-art works, but with significantly less computational overhead.","Online decision-making systems, such as recommendation engines and online platforms, often need to make choices that maximize rewards while satisfying important real-world constraints. However, efficiently designing learning algorithms that respect such general constraints remains a major challenge, particularly when the feasibility conditions are difficult to verify or entirely unknown. To address this, we developed Optimistic$^3$, a novel algorithmic framework for contextual bandits with general constraints. Our approach achieves optimal and improved theoretical guarantees even without relying on strong feasibility assumptions, substantially outperforming previous methods that either required such assumptions or exhibited degraded performance under tight constraints."
Poster,Trusted Multi-View Classification  with Expert Knowledge Constraints,https://ICML.cc//virtual/2025/poster/45140,"Xinyan Liang, Shijie Wang, Yuhua Qian, Qian Guo, Liang Du, Bingbing Jiang, Tingjin Luo, Feijiang Li","Multi-view classification (MVC) based on the Dempster-Shafer theory has gained significant recognition for its reliability in safety-critical applications. However, existing methods predominantly focus on providing confidence levels for decision outcomes without explaining the reasoning behind these decisions. Moreover, the reliance on first-order statistical magnitudes of belief masses often inadequately capture the intrinsic uncertainty within the evidence.  To address these limitations, we propose a novel framework termed Trusted Multi-view Classification Constrained with Expert Knowledge (TMCEK). TMCEK integrates expert knowledge to enhance feature-level interpretability and introduces a distribution-aware subjective opinion mechanism to derive more reliable and realistic confidence estimates. The theoretical superiority of the proposed uncertainty measure over conventional approaches is rigorously established. Extensive experiments conducted on three multi-view datasets for sleep stage classification demonstrate that TMCEK achieves state-of-the-art performance while offering interpretability at both the feature and decision levels. These results position TMCEK as a robust and interpretable solution for MVC in safety-critical domains. The code is available at https://github.com/jie019/TMCEK_ICML2025.","Artificial intelligence (AI) is increasingly used in high-stakes areas like healthcare, where making reliable and understandable decisions is critical. One useful AI approach, called trusted multi-view classification, combines different types of data—such as signals from multiple brain sensors—to make reliable decisions like identifying a person’s sleep stage. However, current methods mainly focus on showing how confident they are in their decisions, without explaining why they made them. This makes it difficult for users, like doctors or patients, to understand or trust the results. Additionally, these methods measure uncertainty only based on the amount of evidence, which may lead to inaccurate uncertainty estimates. To address these problems, we developed a new framework called TMCEK. It has two key innovations: By guiding the model to focus on meaningful patterns—such as certain shapes in the signals, it becomes easier to understand why the system made a particular decision. It improves how uncertainty is measured by considering not just how much evidence is present, but also how that evidence is distributed—resulting in more trustworthy confidence scores. We tested TMCEK on three sleep-related datasets, and it outperformed existing methods both in accuracy and interpretability."
Poster,Trust-Region Twisted Policy Improvement,https://ICML.cc//virtual/2025/poster/45522,"Joery de Vries, Jinke He, Yaniv Oren, Matthijs T. J. Spaan","Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep reinforcement learning (RL).However, scaling MCTS to parallel compute has proven challenging in practice which has motivated alternative planners like sequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters for smoothing through a reformulation of RL as a policy inference problem.Yet, persisting design choices of these particle filters often conflict with the aim of online planning in RL, which is to obtain a policy improvement at the start of planning.Drawing inspiration from MCTS, we tailor SMC planners specifically to RL by improving data generation within the planner through constrained action sampling and explicit terminal state handling, as well as improving policy and value target estimation.This leads to our *Trust-Region Twisted* SMC (TRT-SMC), which shows improved runtime and sample-efficiency over baseline MCTS and SMC methods in both discrete and continuous domains.","An important part of sequential decision making is to effectively plan ahead the consequences of actions. Planning over time, however, is often an expensive task in terms of computational and time resources. One powerful approach used in recent years includes learning to steer the planner's search budget. This learning method is able to iterate on itself to improve the guidance quality further when given more data and learning time.Our paper combines ideas from two succesful approximate planning algorithms within this area to develop a new method.  Our contributions ensure that 1) we can exploit specialized hardware more effectively, 2) we improve the quality of the data generated by the planning algorithm, and 3) improve how and where the planner should spend its budget.In our results we find that our method can greatly reduce the real time and data needed to learn a sequential decision making agent compared to previous approaches. This improves the applicability and accessibility of these types of algorithms by reducing their practical costs."
Poster,TRUST-VLM: Thorough Red-Teaming for Uncovering Safety Threats in Vision-Language Models,https://ICML.cc//virtual/2025/poster/44631,"Kangjie Chen, Muyang Li, Guanlin Li, Shudong Zhang, Shangwei Guo, Tianwei Zhang","Vision-Language Models (VLMs) have become a cornerstone in multi-modal artificial intelligence, enabling seamless integration of visual and textual information for tasks such as image captioning, visual question answering, and cross-modal retrieval. Despite their impressive capabilities, these models often exhibit inherent vulnerabilities that can lead to safety failures in critical applications. Red-teaming is an important approach to identify and test system's vulnerabilities, but how to conduct red-teaming for contemporary VLMs is an unexplored area. In this paper, we propose a novel multi-modal red-teaming approach, TRUST-VLM, to enhance both the attack success rate and the diversity of successful test cases for VLMs. Specifically, TRUST-VLM is built upon the in-context learning to adversarially test a VLM on both image and text inputs. Furthermore, we involve feedback from the target VLM to improve the efficiency of test case generation. Extensive experiments show that TRUST-VLM not only outperforms traditional red-teaming techniques in generating diverse and effective adversarial cases but also provides actionable insights for model improvement. These findings highlight the importance of advanced red-teaming strategies in ensuring the reliability of VLMs.","Many modern AI systems can look at images, read accompanying text, and then answer questions or describe what they see. While these Vision-Language Models are powerful, they can still be tricked into producing harmful or biased outputs by carefully crafted inputs, raising safety and reliability concerns in real-world applications. In this work, we introduce TRUST-VLM, an automated “red-teaming” method that mimics adversarial attacks by generating and refining realistic image-and-text test cases. By feeding back model responses into the testing loop, TRUST-VLM uncovers a wider range of hidden failures than previous techniques. Our extensive experiments on multiple leading models show that this approach not only finds more vulnerabilities but also suggests concrete ways to make these AI systems safer before they are deployed to users."
Poster,Trustworthy Machine Learning through Data-Specific Indistinguishability,https://ICML.cc//virtual/2025/poster/45694,"Hanshen Xiao, Zhen Yang, Edward Suh","This paper studies a range of AI/ML trust concepts, including memorization, data poisoning, and copyright, which can be modeled as constraints on the influence of data on a (trained) model, characterized by the outcome difference from a processing function (training algorithm). In this realm, we show that provable trust guarantees can be efficiently provided through a new framework termed Data-Specific Indistinguishability (DSI) to select trust-preserving randomization tightly aligning with targeted outcome differences, as a relaxation of the classic Input-Independent Indistinguishability (III). We establish both the theoretical and algorithmic foundations of DSI with the optimal multivariate Gaussian mechanism. We further show its applications to develop trustworthy deep learning with black-box optimizers. The experimental results on memorization mitigation, backdoor defense, and copyright protection show both the efficiency and effectiveness of the DSI noise mechanism.","This paper addresses a fundamental problem: how to provably ensure proper data usage and trustworthy behavior of a trained machine learning model. We introduce a new methodology, termed Data-Specific Indistinguishability (DSI), to provide high-probability guarantees regarding data usage and data influence. The central idea is to ensure that the output of a (potentially black-box) machine learning algorithm is statistically indistinguishable from that of a set of safe reference models.For instance, in the context of memorization mitigation, these reference models may be trained without access to specific sensitive data. In the case of copyright protection, they could be models trained on datasets excluding particular artworks from a given artist.To operationalize DSI, we propose an optimal noise mechanism, adding a minimal amount of Gaussian noise to reduce divergence between the target output and the safe references. Extensive experiments demonstrate the effectiveness of our approach in several applications, including memorization mitigation in large language models, defense against poisoned backdoor attacks, and copyright protection."
Poster,TruthFlow: Truthful LLM Generation via Representation Flow Correction,https://ICML.cc//virtual/2025/poster/46324,"Hanyu Wang, Bochuan Cao, Yuanpu Cao, Jinghui Chen","Large language models (LLMs) are known to struggle with consistently generating truthful responses. While various representation intervention techniques have been proposed, these methods typically apply a universal representation correction vector to all input queries, limiting their effectiveness against diverse queries in practice. In this study, we introduce TruthFlow, a novel method that leverages the Flow Matching technique for query-specific truthful representation correction. Specifically, TruthFlow first uses a flow model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. Then, during inference, the trained flow model generates these correction vectors to enhance the truthfulness of LLM outputs. Experimental results demonstrate that TruthFlow significantly improves performance on open-ended generation tasks across various advanced LLMs evaluated on TruthfulQA. Moreover, the trained TruthFlow model exhibits strong transferability, performing effectively on other unseen hallucination benchmarks.","Hallucination, which refers to seemingly plausible but factually inaccurate generation, is a challenging problem for LLMs. We developed an effective mitigation method to accommodate the diversity of input queries better. This will help correct the potential mistakes resulting from hallucinations based on different user inputs, making LLMs more trustworthy."
