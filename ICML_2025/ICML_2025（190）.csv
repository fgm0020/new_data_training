type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking,https://ICML.cc//virtual/2025/poster/44493,"Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah","Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step ""reward hacks"") even if humans are not able to detect that the behavior is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering.","As AI agents are increasingly tasked to solve complex multi-step problems, a key risk is ""reward hacking""—where AI finds loopholes to get rewards in unintended ways. For instance, an AI coding assistant tasked with solving a problem by first writing test cases and then a solution for the problem might write overly simple tests in its first step, just so it can easily ""pass"" them without actually solving the problem well.Our method, MONA, addresses this. MONA trains the AI to focus on the quality of its immediate action as assessed by a supervisor that considers the long-term impact of the AI's actions. This change requires the AI to make plans the supervisor understands and approves of which avoids many “reward hacking” plans.Our experiments show MONA reduces reward hacking in different environments, including a coding task and a loan application decision making task. The results suggest that MONA can help to make AI more reliable and safer by reducing many particularly difficult-to-detect instances of reward hacking."
Poster,Monte Carlo Tree Diffusion for System 2 Planning,https://ICML.cc//virtual/2025/poster/44944,"Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, Sungjin Ahn","Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)—whose performance naturally improves with inference-time computation scaling—standard diffusion‐based planners offer only limited avenues for the scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree‐structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long‐horizon tasks show that MCTD outperforms diffusion baselines, yielding higher‐quality solutions as inference-time computation increases.","We introduce Monte Carlo Tree Diffusion (MCTD), a novel AI planning method that combines diffusion models (used for generating images and text) with Monte Carlo Tree Search (the algorithm behind AlphaGo).Unlike traditional diffusion approaches that generate entire action sequences at once, MCTD builds plans incrementally through tree-structured search, exploring different possibilities, evaluating outcomes, and refining promising paths. This enables more thoughtful, strategic planning.Testing on challenging tasks like robot navigation and object manipulation, MCTD significantly outperforms existing methods. Importantly, it scales well: given more computational time, it consistently finds better solutions, making it ideal for complex problems where quality matters.This work bridges generative AI with strategic reasoning, advancing how AI systems plan long-term actions."
Poster,Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design,https://ICML.cc//virtual/2025/poster/45984,"Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, Bryan Hooi","Handcrafting heuristics for solving complex optimization tasks (e.g., route planning and task allocation) is a common practice but requires extensive domain knowledge. Recently, Large Language Model (LLM)-based automatic heuristic design (AHD) methods have shown promise in generating high-quality heuristics without manual interventions. Existing LLM-based AHD methods employ a population to maintain a fixed number of top-performing LLM-generated heuristics and introduce evolutionary computation (EC) to iteratively enhance the population. However, these population-based procedures cannot fully develop the potential of each heuristic and are prone to converge into local optima. To more comprehensively explore the space of heuristics, this paper proposes to use Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The proposed MCTS-AHD method organizes all LLM-generated heuristics in a tree structure and can better develop the potential of temporarily underperforming heuristics. In experiments, MCTS-AHD delivers significantly higher-quality heuristics on various complex tasks. Our code is available.","(1) Optimization problems (e.g., combinatorial optimization) play an essential role in real life but are NP-hard. (2) We extend the existing LLM-based Automatic Heuristic Design method and propose a more effective MCTS-AHD method, which contributes to a comprehensive exploration of LLM-based Automatic Heuristic Design. (3) It has a significant improvement in effectiveness, being beneficial for optimizing algorithms in practical application scenarios."
Poster,Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport,https://ICML.cc//virtual/2025/poster/46010,"Tuan Dam, Pascal Stenger, Lukas Schneider, Joni Pajarinen, Carlo D&#x27;Eramo, Odalric-Ambrym Maillard","This paper introduces a novel backup strategy for Monte-Carlo Tree Search (MCTS) tailored for highly stochastic and partially observable Markov decision processes. We adopt a probabilistic approach, modeling both value and action-value nodes as Gaussian distributions, to introduce a novel backup operator that computes value nodes as the Wasserstein barycenter of their action-value children nodes; thus, propagating the uncertainty of the estimate across the tree to the root node. We study our novel backup operator when using a novel combination of $L^1$-Wasserstein barycenter with $\alpha$-divergence, by drawing a crucial connection to the generalized mean backup operator. We complement our probabilistic backup operator with two sampling strategies, based on optimistic selection and Thompson sampling, obtaining our Wasserstein MCTS algorithm. We provide theoretical guarantees of asymptotic convergence of $\mathcal{O}(n^{-1/2})$, with $n$ as the number of visited trajectories, to the optimal policy and an empirical evaluation on several stochastic and partially observable environments, where our approach outperforms well-known related baselines.","Imagine you're playing a complex game where the rules keep changing randomly, and you can't see all the information you need to make good decisions. Traditional computer algorithms that plan moves ahead (like those used in chess or Go) struggle in these uncertain situations because they assume the world is predictable and all information is available. Our research introduces a new planning algorithm called ""Wasserstein MCTS"" that's specifically designed to handle uncertainty and randomness. Instead of just tracking single ""best guess"" values for each possible move, our algorithm keeps track of entire probability distributions - essentially maintaining a range of possible outcomes and their likelihoods. The key innovation is how we combine information from different possible moves. We use a mathematical technique called ""Wasserstein barycenters"" (think of it as a sophisticated way of averaging probability distributions) that allows the algorithm to properly account for uncertainty when deciding which moves to explore. This is like having a chess player who not only considers the most likely outcome of each move, but also weighs the full range of possible results and their uncertainties. We tested our algorithm on various challenging scenarios - from navigating slippery frozen lakes where movements are unpredictable, to complex maze-like environments where important information is hidden. In these tests, our approach consistently outperformed existing methods, often by substantial margins (20-80% improvement in many cases). This research has practical implications for real-world applications like robot navigation in unpredictable environments, autonomous vehicle planning in uncertain traffic conditions, and resource management where outcomes depend on many random factors. By better handling uncertainty, our algorithm makes more robust decisions in situations where traditional planning methods might fail."
Poster,MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles,https://ICML.cc//virtual/2025/poster/43879,"Jing Han, Binwei Yan, Tianyu Guo, Zheyuan Bai, Mengyu Zheng, Hanting Chen, Ying Nie","Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant \textit{Reason+Action} paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification.We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io","In this paper, we propose a novel parameter-efficient fine-tuning method for LLM-based agent tasks. 1) We first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized LoRA groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the overall agent task. 3) We also develop a multi-role data generation pipeline based on publicly available datasets to effectively fine-tune the framework. We conduct extensive experiments on various LLMs and agent benchmarks, demonstrating the effectiveness of our method."
Poster,More Than Meets the Eye: Enhancing Multi-Object Tracking Even with Prolonged Occlusions,https://ICML.cc//virtual/2025/poster/46062,"Bishoy Galoaa, Somaieh Amraee, Sarah Ostadabbas","This paper introduces MOTE (MOre Than meets the Eye), a novel multi-object tracking (MOT) algorithm designed to address the challenges of tracking occluded objects. By integrating deformable detection transformers with a custom disocclusion matrix, MOTE significantly enhances the ability to track objects even when they are temporarily hidden from view. The algorithm leverages optical flow to generate features that are processed through a softmax splatting layer, which aids in the creation of a disocclusion matrix. This matrix plays a crucial role in maintaining track consistency by estimating the motion of occluded objects. MOTE's architecture includes modifications to the enhanced track embedding module (ETEM), which allows it to incorporate these advanced features into the track query layer embeddings. This integration ensures that the model not only tracks visible objects but also accurately predicts the trajectories of occluded ones, much like the human visual system. The proposed method is evaluated on multiple datasets, including MOT17, MOT20, and DanceTrack, where it achieves impressive tracking metrics--82.0 MOTA and 66.3 HOTA on the MOT17 dataset, 81.7 MOTA and 65.8 HOTA on the MOT20 dataset, and 93.2 MOTA and 74.2 HOTA on the DanceTrack dataset. Notably, MOTE excels in reducing identity switches and maintaining consistent tracking in complex real-world scenarios with frequent occlusions, outperforming existing state-of-the-art methods across all tested benchmarks.","Imagine watching security footage of a crowded mall where you need to track specific people, but they keep disappearing behind pillars or other shoppers. Current computer vision systems often lose track and confuse identities when people reappear, like mixing up two people after they cross paths. This is a critical problem for applications from autonomous vehicles to elderly care monitoring.We developed MOTE, which mimics how humans naturally predict where hidden objects will reappear. Just as you can guess where someone will emerge after walking behind a tree based on their walking speed and direction, MOTE combines three techniques: analyzing motion patterns, creating ""depth maps"" to understand who's in front, and using AI memory to maintain identities. Think of it as giving computers the ability to mentally ""fill in the gaps"" when objects are temporarily hidden.Our tests show MOTE reduces identity confusion by 25% compared to existing methods while running fast enough for real-time applications. This breakthrough could make self-driving cars safer by better tracking pedestrians who step behind parked vehicles, improve sports analytics by following players through pile-ups, and enhance security systems in crowded spaces where reliable tracking is crucial for public safety."
Poster,Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models,https://ICML.cc//virtual/2025/poster/45820,"Chao Li, Jiawei Fan, Anbang Yao","In this paper, we present $Morse$, a simple dual-sampling framework for accelerating diffusion models losslessly. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called $Dash$ and $Dot$ that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78$\times$ to 3.31$\times$ on average over a wide range of sampling step budgets relative to 9 baseline diffusion models on 6 image generation tasks. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code and models are available at https://github.com/deep-optimization/Morse.","A diffusion model can generate high quality images by converting noise to image iteratively. The full generation process consists of $T$ sampling steps maximally, where $T$ is decided according to the sampling method. The generation process is mostly very time-consuming. Therefore, most diffusion methods adopt jump sampling for acceleration. By using the number of steps $t$ (mostly much lower than $T$), namely jumping over $T-t$ sampling steps, we can achieve faster generation but lower image quality. We want to compensate the quality degradation, in order to achieve a better tradeoff between quality and latency.In this paper, we present a simple framework $Morse$. For a pre-trained diffusion model, Morse can be used to accelerate it. Given a desired image quality, Morse helps a diffusion model achieve the quality using lower latency. For a diffusion process which jumps over an amount of steps, Morse adds several extra steps between each pair of adjacent steps. In the extra steps, we don't use the pre-trained diffusion model but introduce an extra faster model. We name the pre-trained diffusion model as Dash and the extra model as Dot. For a sampling step, Dot is multiple times faster than Dash in latency. While Dash only takes the information about current state during the generation process from noise to image, we additionally provide the trajectory information about the previous state to Dot. Therefore, Dot can perform as well as Dash. With our proposed strategy, the Dot model can be trained efficiently.Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78$\times$ to 3.31$\times$ on average over different numbers of sampling steps to 9 baseline diffusion models on 6 image generation tasks."
Poster,MP-Nav: Enhancing Data Poisoning Attacks against Multimodal Learning,https://ICML.cc//virtual/2025/poster/43444,"Jingfeng Zhang, Prashanth Krishnamurthy, Naman Patel, Anthony Tzes, Farshad Khorrami","Despite the success of current multimodal learning at scale, its susceptibility to data poisoning attacks poses security concerns in critical applications. Attacker can manipulate model behavior by injecting maliciously crafted yet minute instances into the training set, stealthily mismatching distinct concepts. Recent studies have manifested the vulnerability by poisoning multimodal tasks such as Text-Image Retrieval (TIR) and Visual Question Answering (VQA). However, the current attacking method only rely on random choice of concepts for misassociation and random instance selections for injecting the poisoning noise, which often achieves the suboptimal effect and even risks failure due to the dilution of poisons by the large number of benign instances. This study introduces MP-Nav (Multimodal Poison Navigator), a plug-and-play module designed to evaluate and even enhance data poisoning attacks against multimodal models. MP-Nav operates at both the concept and instance levels, identifying semantically similar concept pairs and selecting robust instances to maximize the attack efficacy. The experiments corroborate MP-Nav can significantly improve the efficacy of state-of-the-art data poisoning attacks such as AtoB and ShadowCast in multimodal tasks, and maintain model utility across diverse datasets. Notably, this study underscores the vulnerabilities of multimodal models and calls for the counterpart defenses.","Multimodal artificial intelligence (AI) systems, which combine information from images and text, are becoming increasingly popular and powerful. These systems help with tasks like finding images that match a description or answering questions about pictures. However, as they grow in use, especially in sensitive areas like healthcare or autonomous vehicles, it's important to understand and prevent possible vulnerabilities. One such vulnerability is called a data poisoning attack. In this type of attack, a bad actor sneaks a few carefully chosen and slightly altered examples into the training data used to teach the AI. These hidden changes can cause the model to learn incorrect associations—such as confusing a medicine type A for type B—without noticeably affecting its general performance. This makes the attack hard to detect but potentially dangerous. This paper introduces a new tool called MP-Nav that comprehensively uncover this vulnerability of data poisons in Multimodal AI systems."
Poster,MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment,https://ICML.cc//virtual/2025/poster/46277,"Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang","Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.","Today's AI tools, such as ChatGPT, are usually trained to follow what people want - but most rely on just one idea of what 'good' looks like. That’s a problem because people care about different things: some want answers to be helpful, others want them to be funny, and some want them to be safe.Training a new AI for all kinds of preference takes a lot of time and computing power. Instead, we created a method called Mixing Preference Optimization (MPO). It takes a few existing AIs, each tuned to a different goal, and combines their decisions into one balanced model with provable guarantees.Rather than picking a single answer or switching between models, MPO blends their outputs in a stable and thoughtful way — like averaging expert advice to reach a fair decision. This creates an AI that better reflects diverse human preferences, while using less time and resources than existing methods."
Poster,MTL-UE: Learning to Learn Nothing for Multi-Task Learning,https://ICML.cc//virtual/2025/poster/43643,"Yi Yu, Song Xia, SIYUAN YANG, Chenqi KONG, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex Kot","Most existing unlearnable strategies focus on preventing unauthorized users from training single-task learning (STL) models with personal data. Nevertheless, the paradigm has recently shifted towards multi-task data and multi-task learning (MTL), targeting generalist and foundation models that can handle multiple tasks simultaneously. Despite their growing importance, MTL data and models have been largely neglected while pursuing unlearnable strategies. This paper presents MTL-UE, the first unified framework for generating unlearnable examples for multi-task data and MTL models. Instead of optimizing perturbations for each sample, we design a generator-based structure that introduces label priors and class-wise feature embeddings which leads to much better attacking performance. In addition, MTL-UE incorporates intra-task and inter-task embedding regularization to increase inter-class separation and suppress intra-class variance which enhances the attack robustness greatly. Furthermore, MTL-UE is versatile with good supports for dense prediction tasks in MTL. It is also plug-and-play allowing integrating existing surrogate-dependent unlearnable methods with little adaptation. Extensive experiments show that MTL-UE achieves superior attacking performance consistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5 MTL task-weighting strategies. Code is available at https://github.com/yuyi-sd/MTL-UE.","Modern machine learning models are often trained to handle many tasks at once — like recognizing faces, detecting diseases, or understanding scenes — using a technique called multi-task learning (MTL). However, as more personal or proprietary data is used in training, concerns have grown about data privacy and misuse.Our research introduces MTL-UE, a method to protect multi-task data from being exploited by machine learning models. The idea is simple but powerful: we add tiny, invisible changes (called unlearnable examples) to each training image so that any model trained on this data performs poorly — effectively ""learning nothing"" useful. Unlike earlier methods that focused only on single tasks, MTL-UE is the first method specifically designed to work on multi-task setups, where multiple outputs are predicted from the same input.We designed a new generator that adds these hidden changes more intelligently using shared representations across tasks, which makes the attack more effective. We also built in controls to make these changes harder to undo.This technique is important for safeguarding sensitive datasets in fields like healthcare, surveillance, and finance, where preventing unauthorized AI training is as crucial as enabling it."
