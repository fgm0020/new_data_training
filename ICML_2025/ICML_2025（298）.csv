type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,The Diffusion Duality,https://ICML.cc//virtual/2025/poster/46226,"Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, Volodymyr Kuleshov","Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models.  In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, **doubling training speed** by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks.Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm **unlocks few-step generation in diffusion language models** by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo","Today’s language models typically generate text one word at a time using an autoregressive (AR) approach, which lacks the ability to revise earlier predictions. In contrast, a newer class—diffusion language models—can predict multiple words simultaneously and revise their predictions, offering both self-correction and the potential for faster generation. However, they often lag behind AR methods in overall performance.In this work, we uncover a surprising connection: these discrete diffusion models are fundamentally linked to a more powerful class of models based on continuous Gaussian diffusion, which has achieved remarkable success in image generation. Building on this insight, we introduce Duo—a new training and sampling framework that transfers advanced techniques from the continuous domain to the discrete setting. This results in both faster training and improved model quality, with **Duo outperforming AR models on several benchmarks**.To enable rapid text generation, we also propose a novel algorithm called Discrete Consistency Distillation. For instance, while traditional AR models require roughly 1,000 steps to generate 1,000 words, Duo can achieve the same result in as few as 10 steps—**a 100× speedup**.Together, these advances bring us closer to real-time, high-performance language models for applications such as smarter and faster chatbots."
Poster,The Disparate Benefits of Deep Ensembles,https://ICML.cc//virtual/2025/poster/43767,"Kajetan Schweighofer, Adrián Arnaiz-Rodríguez, Sepp Hochreiter, Nuria Oliver","Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a simple way to boost predictive performance. However, their impact on algorithmic fairness is not well understood yet. Algorithmic fairness examines how a model's performance varies across socially relevant groups defined by protected attributes such as age, gender, or race. In this work, we explore the interplay between the performance gains from Deep Ensembles and fairness. Our analysis reveals that they unevenly favor different groups, a phenomenon that we term the disparate benefits effect. We empirically investigate this effect using popular facial analysis and medical imaging datasets with protected group attributes and find that it affects multiple established group fairness metrics, including statistical parity and equal opportunity. Furthermore, we identify that the per-group differences in predictive diversity of ensemble members can explain this effect. Finally, we demonstrate that the classical Hardt post-processing method is particularly effective at mitigating the disparate benefits effect of Deep Ensembles by leveraging their better-calibrated predictive distributions.","Deep Ensembles (multiple deep learning models combined) are widely used to improve accuracy. However, in this paper we show that combining models (ensembling) leads to different groups, e.g. race or gender, being treated more unfairly. This is, because ensembling helps some groups more than others, a phenomenon we call the disparate benefits effect of Deep Ensembles. We investigate this effect for multiple datasets with face and medical image data, demonstrating increased unfairness under standard fairness metrics. Furthermore, we investigate the reason for this effect and find that different levels of disagreement between individual models may explain it. Finally, we investigate ways to mitigate this issue, finding that a known method from the literature is particularly effective in this setting. This allows to use the improved accuracy of Deep Ensembles without a decrease in fairness."
Poster,The Double-Ellipsoid Geometry of CLIP,https://ICML.cc//virtual/2025/poster/45348,"Meir Yossef Levi, Guy Gilboa","Contrastive Language-Image Pre-Training (CLIP) is highly instrumental in machine learning applications within a large variety of domains. We investigate the geometry of this embedding, which is still not well understood, and show that text and image reside on linearly separable ellipsoid shells, not centered at the origin. We explain the benefits of having this structure, allowing to better embed instances according to their uncertainty during contrastive training.Frequent concepts in the dataset yield more false negatives, inducing  greater uncertainty.A new notion of conformity is introduced, which measures the average cosine similarity of an instance to any other instance within a representative data set. We show this measure can be accurately estimated by simply computing the cosine similarity to the modality mean vector. Furthermore, we find that CLIP's modality gap optimizes the matching of the conformity distributions of image and text.","Modern AI models like CLIP can understand both images and text by mapping them into a shared mathematical space. This shared space allows the model to match an image to a caption or vice versa. Our paper uncovers a surprising geometric structure inside this space. Rather than forming a neat, uniform cloud of points, we found that CLIP’s image and text representations each lie on separate, off-center ellipsoid shapes. In simple terms, the way images and text are stored is uneven and misaligned. This misalignment can lead to inefficiencies when the model tries to compare different types of data.We introduce a simple yet powerful measure called conformity, which reveals how “common” or typical an image is in CLIP’s eyes. Images of everyday concepts—like a dog playing in a yard—tend to be embedded near the center of the ellipsoid, while rare or unusual ones—like a specific person or abstract artwork—are placed further away. This insight helps us better understand how CLIP organizes information and could be valuable for improving tasks that involve rare or hard-to-edit cases, such as personalized image generation or targeted editing."
Poster,The Elicitation Game: Evaluating Capability Elicitation Techniques,https://ICML.cc//virtual/2025/poster/44250,"Felix Hofstätter, Teun van der Weij, Jayden Teoh, Rada Djoneva, Henning Bartsch, Francis Rhys Ward","Capability evaluations are required to understand and regulate AI systems that maybe deployed or further developed. Therefore, it is important that evaluations providean accurate estimation of an AI system’s capabilities. However, in numerous cases,previously latent capabilities have been elicited from models, sometimes longafter initial release. Accordingly, substantial efforts have been made to developmethods for eliciting latent capabilities from models. In this paper, we evaluate theeffectiveness of capability elicitation techniques by intentionally training modelorganisms – language models with hidden capabilities that are revealed by apassword. We introduce a novel method for training model organisms, basedon circuit-breaking, which is more robust to elicitation techniques than standardpassword-locked models. We focus on elicitation techniques based on promptingand activation steering, and compare these to fine-tuning methods. Promptingtechniques can elicit the actual capability of both password-locked and circuit-broken model organisms in an MCQA setting, while steering fails to do so. Fora code-generation task, only fine-tuning can elicit the hidden capabilities of ournovel model organism. Additionally, our results suggest that combining techniquesimproves elicitation. Still, if possible, fine-tuning should be the method of choiceto improve the trustworthiness of capability evaluations.","# Testing Hidden AI Capabilities: Can We Find What AI Systems Are Really Capable Of?When we build AI systems, we need to understand what they can actually do – both for safety and to make good decisions about how to use them. However, AI systems sometimes have hidden abilities that only show up later, which can be a problem if we've underestimated what they're capable of.To address this challenge, researchers have developed techniques to try to uncover these hidden capabilities. But how well do these techniques actually work? This study tested different methods by creating special AI models with intentionally hidden abilities – like having a secret password that unlocks extra skills.We created two types of ""model organisms"" (test AI systems): simple password-protected models and more sophisticated ones using a technique called ""circuit-breaking"" that makes the hidden abilities harder to find. We then tried various methods to uncover these hidden capabilities, including different prompting strategies, a technique called activation steering, and retraining the models.The results showed that simple prompting could reveal hidden abilities in multiple-choice question tasks, but steering techniques failed. For programming tasks, only retraining the AI model could uncover the hidden capabilities of our more sophisticated test models. Combining multiple techniques generally worked better than using just one.These findings suggest that if we want to be confident about what an AI system can do, we should use multiple evaluation methods, with retraining being the most reliable approach. This research helps make AI capability testing more trustworthy and thorough."
Poster,The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination,https://ICML.cc//virtual/2025/poster/45153,"Yifan Sun, Han Wang, Dongbai Li, Gang Wang, Huan Zhang","Benchmark Data Contamination (BDC)—the inclusion of benchmark testing samples in the training set—has raised increasing concerns in Large Language Model (LLM) evaluation, leading to falsely inflated performance estimates and undermining evaluation reliability. To address this, researchers have proposed various mitigation strategies to update existing benchmarks, including modifying original questions or generating new ones based on them. However, a rigorous examination of the effectiveness of these mitigation strategies remains lacking. In this paper, we design a systematic and controlled pipeline along with two novel metrics—*fidelity* and *contamination resistance*—to provide a fine-grained and comprehensive assessment of existing BDC mitigation strategies. Previous assessment methods, such as accuracy drop and accuracy matching, focus solely on aggregate accuracy, often leading to incomplete or misleading conclusions. Our metrics address this limitation by emphasizing *question-level* evaluation result matching. Extensive experiments with 10 LLMs, 5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios reveal that no existing strategy effectively balances fidelity and contamination resistance. No semantic-preserving strategy yields a significant improvement in resistance over the vanilla case (*i.e.*, no benchmark update) across *all* benchmarks, while semantic-altering strategies sacrifice fidelity for resistance. These findings underscore the urgent need for designing more effective BDC mitigation strategies. Our code repository is available at https://github.com/ASTRAL-Group/BDC_mitigation_assessment.","Modern language models often appear to perform impressively on evaluation tests. However, many of these test questions are unintentionally included in the models’ training data—a problem known as benchmark data contamination. This leads to overly optimistic scores and unreliable assessments of what the models truly understand. To address this, researchers have proposed ways to revise test questions or create new ones. But are these fixes actually effective?In this paper, we conduct a thorough study of 20 different contamination-mitigation methods, testing them on five evaluation sets and ten language models. We introduce two new ways to measure how well these methods work—whether they preserve the original test’s intent and whether they successfully reduce the contamination. Our findings show that most current approaches fail to strike a good balance: some make the test cleaner but change its meaning, while others preserve meaning but do little to reduce contamination. This highlights the need for better solutions to ensure fair and trustworthy evaluation of language models."
Poster,The Empirical Mean is Minimax Optimal for Local Glivenko-Cantelli,https://ICML.cc//virtual/2025/poster/46552,"Doron Cohen, Aryeh Kontorovich, Roi Weiss","We revisit the recently introduced Local Glivenko-Cantelli setting, which studies distribution-dependent uniform convergence rates of the Empirical Mean Estimator (EME). In this work, we investigate generalizations of this setting where arbitrary estimators are allowed rather than just the EME. Can a strictly larger class of measures be learned? Can better risk decay rates be obtained? We provide exhaustive answers to these questions—which are both negative, provided the learner is barred from exploiting some infinite-dimensional pathologies. On the other hand, allowing such exploits does lead to a strictly larger class of learnable measures.","Picture an endless line of coins, each with its own (unknown) probability $p_j$ of landing heads. You flip **every** coin only $n$ times—one global round after another—and must then guess every $p_j$.The obvious move is to use the *empirical mean*: for each coin, count the heads and divide by $n$. We prove this simple maximum-likelihood rule is *minimax-optimal* when performance is judged by the **expected maximum absolute error**—that is, the average (over the randomness of the flips) of the worst miss $\max_j |\hat p_j-p_j|$. No alternative procedure can guarantee a smaller worst-case error without extra data.We also locate the exact “phase boundary’’: if the true biases fail a certain *local Glivenko–Cantelli* regularity, then—irrespective of ingenuity—no method can consistently estimate all $p_j$ from finitely many rounds. Thus, even when facing infinitely many parameters at once, the humble empirical mean already reaches the fundamental statistical limit."
Poster,The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking,https://ICML.cc//virtual/2025/poster/46294,"Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao","This work identifies the *Energy Loss Phenomenon* in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically,  energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an *excessive* increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an *Energy loss-aware PPO algorithm (EPPO)* which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of EPPO in mitigating reward hacking and improving RLHF performance.","Large language models (LLMs) like ChatGPT are trained to respond helpfully and safely to human instructions. One popular method for improving them is called Reinforcement Learning from Human Feedback (RLHF), where the model learns from examples ranked or rated by humans. However, during this training process, models sometimes learn to ""game the system""—producing responses that look good to the reward model but lack genuine understanding or relevance. This issue is known as reward hacking.In this work, we uncover a new phenomenon linked to reward hacking: as training progresses, the internal signals (called “energy loss”) in the final layer of the model steadily increase in a harmful way. When this increase becomes too large, it often signals that the model is overfitting to shallow patterns favored by the reward model rather than producing truly meaningful responses.To fix this, we introduce a new method called EPPO, which helps the model maintain healthy internal behavior during training. EPPO carefully controls the energy loss levels in the model to avoid dangerous drift. We show that this method not only reduces reward hacking but also makes the model more reliable overall. Our experiments on different tasks and models confirm that this energy-related problem is widespread—and that our solution works."
Poster,The Four Color Theorem for Cell Instance Segmentation,https://ICML.cc//virtual/2025/poster/45071,"Ye Zhang, Yu Zhou, Yifeng Wang, Jun Xiao, Ziyue Wang, Yongbing Zhang, Jianxu Chen","Cell instance segmentation is critical to analyzing biomedical images, yet accurately distinguishing tightly touching cells remains a persistent challenge. Existing instance segmentation frameworks, including detection-based, contour-based, and distance mapping-based approaches, have made significant progress, but balancing model performance with computational efficiency remains an open problem. In this paper, we propose a novel cell instance segmentation method inspired by the four-color theorem. By conceptualizing cells as countries and tissues as oceans, we introduce a four-color encoding scheme that ensures adjacent instances receive distinct labels. This reformulation transforms instance segmentation into a constrained semantic segmentation problem with only four predicted classes, substantially simplifying the instance differentiation process. To solve the training instability caused by the non-uniqueness of four-color encoding, we design an asymptotic training strategy and encoding transformation method. Extensive experiments on various modes demonstrate our approach achieves state-of-the-art performance. The code is available at https://github.com/zhangye-zoe/FCIS.","Imagine trying to pick out every single grape in a tightly packed bunch, especially when they're squished together. That's a bit like what scientists face when trying to identify individual cells in biomedical images. It's incredibly important for understanding diseases, but current computer programs struggle to tell apart cells that are touching or overlapping, and they can be slow.We came up with a new way to solve this problem, inspired by a map-making rule: you only need four colors to color any map so that no two bordering countries have the same color. We thought of cells as tiny countries and the surrounding tissue as the ocean.Our method colors each cell with one of four colors. This helps the computer easily distinguish between adjacent cells. This might sound simple, but it turns the complex task of finding individual cells into a simpler one of just identifying four different patterns.To make sure this coloring system works reliably, we developed special training techniques. Our experiments show that this new approach is very effective, achieving top performance in identifying individual cells in various types of images. We've also made our code publicly available so other researchers can use and build upon our work."
Poster,The Generalized Skew Spectrum of Graphs,https://ICML.cc//virtual/2025/poster/45119,"Armando Bellante, Martin Plávala, Alessandro Luongo","This paper proposes a family of permutation-invariant graph embeddings, generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded in group theory and harmonic analysis, our method introduces a new class of graph invariants that are isomorphism-invariant and capable of embedding richer graph structures - including attributed graphs, multilayer graphs, and hypergraphs - which the Skew Spectrum could not handle. Our generalization further defines a family of functions that enables a trade-off between computational complexity and expressivity. By applying generalization-preserving heuristics to this family, we improve the Skew Spectrum's expressivity at the same computational cost. We formally prove the invariance of our generalization, demonstrate its improved expressiveness through experiments, and discuss its efficient computation.","Graphs are a powerful way to represent complex systems, from molecules and social networks to financial connections. But teaching computers to compare graphs is surprisingly tricky. Imagine two subway maps for the same city: the stations and routes are identical, but laid out differently. To a person, it's clearly the same network. To a computer, they can look completely different. The challenge is finding a machine representation of a graph that expresses the connections unambiguously, regardless of how the routes are laid out.We address this problem using abstract mathematical tools for symmetry and signal processing - specifically group theory, representation theory, and Fourier analysis. Building on a classical method called the Skew Spectrum, we create a new family of graph ""fingerprints"" that are invariant to graph reordering. Unlike earlier techniques, ours can also handle more complex systems - like graphs with attributes, multiple layers, or richer types of connections. Our approach is flexible: it lets users trade off between computational cost and representational power.This result makes it easier for machines to compare complex networks in the real world, potentially enhancing graph neural networks and applications in drug discovery, fraud detection, and social science."
Poster,The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence,https://ICML.cc//virtual/2025/poster/46298,"Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger","The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a *single* refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional *concept cones* that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of *representational independence* that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.","Large language models (LLMs) are designed with safety measures, but specially crafted inputs can bypass these safeguards. The precise ways these ""attacks"" succeed are not well understood. Previous research suggested that an LLM's decision to refuse a problematic request might be controlled by a single, specific signal or ""direction"" within its internal representational space. This study introduces a novel method for investigating these internal decision-making processes. Contrary to earlier findings, our research reveals that LLM refusal is not governed by a single factor. Instead, we identified multiple distinct, independent directions and even more complex multi-dimensional structures that trigger refusal. Importantly, we also demonstrate that simply because these internal signals are mathematically different (orthogonal) does not guarantee they operate independently when the model processes information. We therefore propose the concept of ""representational independence,"" which considers both linear and non-linear interactions, to more accurately identify truly distinct mechanisms. Using this framework, we successfully identified mechanistically independent refusal pathways. This confirms that LLM refusal behavior is driven by a complex interplay of multiple, functionally distinct internal mechanisms, rather than a monolithic one. Our approach not only uncovers these complex structures but also provides a valuable tool for future research aimed at a deeper understanding of LLM operations."
