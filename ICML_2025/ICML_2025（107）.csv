type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,FedBEns: One-Shot Federated Learning based on Bayesian Ensemble,https://ICML.cc//virtual/2025/poster/44060,"Jacopo Talpini, Marco Savi, Giovanni Neglia","One-Shot Federated Learning (FL) is a recent paradigm that enables multiple clients to cooperatively learn a global model in a single round of communication with a central server. In this paper, we analyze the One-Shot FL problem through the lens of Bayesian inference and propose FedBEns, an algorithm that leverages the inherent multimodality of local loss functions to find better global models.Our algorithm leverages a mixture of Laplace approximations for the clients' local posteriors, which the server then aggregates to infer the global model. We conduct extensive experiments on various datasets, demonstrating that the proposed method outperforms competing baselines that typically rely on unimodal approximations of the local losses.","In traditional machine learning, data is sent to a central server to train a model. A different approach is Federated Learning, where multiple clients (like different institutions or different devices) work together to train a model collaboratively without sharing their data, ensuring user privacy. This study introduces FedBEns, an improved method for these clients to combine their knowledge and train a global model in a single communication round with the central server. FedBEns uses a technique based on Bayesian inference to better capture the unique learning patterns of each client. Tests on various datasets show that FedBEns produces more accurate models than existing methods."
Poster,FedClean: A General Robust Label Noise Correction for Federated Learning,https://ICML.cc//virtual/2025/poster/46469,"Xiaoqian Jiang, Jing Zhang","Many federated learning scenarios encounter label noises in the client-side datasets. The resulting degradation in global model performance raises the urgent need to address label noise. This paper proposes FedClean -- a novel general robust label noise correction for federated learning. FedClean first uses the local centralized noisy label learning to select clean samples to train a global model. Then, it employs a two-stage correction scheme to correct the noisy labels from two distinct perspectives of local noisy label learning and the global model. FedClean also proposes a novel model aggregation method, further reducing the impact of label noises. FedClean neither assumes the existence of clean clients nor the specific noise distributions, showing the maximum versatility. Extensive experimental results show that FedClean effectively identifies and rectifies label noises even if all clients exhibit label noises,  which outperforms the state-of-the-art noise-label learning methods for federated learning.","When we train machine learning models using data from many different users — such as phones, hospitals, or sensors — we often rely on federated learning, which keeps data private by training the model directly on each device. However, these local datasets may contain mistakes, like mislabeled images or incorrect diagnoses, which can seriously hurt the model’s performance.Our research tackles this problem by introducing FedClean, a new method that automatically detects and fixes these labeling errors. First, FedClean looks at each user’s data to identify reliable examples, then builds a stronger shared model using only those. Next, it corrects the remaining mistakes using a two-step process: one based on local insights and the other using the shared global model’s predictions. Finally, it combines all models in a smarter way that reduces the influence of noisy data.FedClean works even when all users have some incorrect data and doesn't assume we know where the errors come from. This makes it a practical solution for real-world federated learning systems in healthcare, education, and mobile apps."
Poster,FedECADO: A Dynamical System Model of Federated Learning,https://ICML.cc//virtual/2025/poster/44445,"Aayushya Agarwal, Gauri Joshi, Lawrence Pileggi","Federated learning harnesses the power of distributed optimization to train a unified machine learning model across separate clients. However, heterogeneous data distributions and computational workloads can lead to inconsistent updates and limit model performance. This work tackles these challenges by proposing FedECADO, a new algorithm inspired by a dynamical system representation of the federated learning process. FedECADO addresses non-IID data distribution through an aggregate sensitivity model that reflects the amount of data processed by each client. To tackle heterogeneous computing, we design a multi-rate integration method with adaptive step-size selections that synchronizes active client updates in continuous time. Compared to prominent techniques, including FedProx, FedExp, and FedNova, FedECADO achieves higher classification accuracies in numerous heterogeneous scenarios.","As machine learning models and their training datasets get bigger, it is no longer practical to train them on just one computer. Federated learning helps by dividing the training process across many computers, each working with its own slice of data. However, this setup brings new challenges where not all computers have the same computational abilities and the data they use can vary widely. To address this, we created a new method to train models across multiple machines while accounting for these computational differences. Our method draws on principles from electrical circuits, enabling us to design faster and more efficient ways to train machine learning models at scale."
Poster,Federated Causal Structure Learning with Non-identical Variable Sets,https://ICML.cc//virtual/2025/poster/45325,"Yunxia Wang, Fuyuan CAO, Kui Yu, Jiye Liang","Federated causal structure learning aims to infer causal relationships from data stored on individual clients, with privacy concerns. Most existing methods assume identical variable sets across clients and present federated strategies for aggregating local updates. However, in practice, clients often observe overlapping but non-identical variable sets, and non-overlapping variables may introduce spurious dependencies. Moreover, existing strategies typically reflect only the overall quality of local graphs, ignoring the varying importance of relationships within each graph. In this paper, we study federated causal structure learning with non-identical variable sets, aiming to design an effective strategy for aggregating “correct” and “good” (non-)causal relationships across distributed datasets. Specifically, we first develop theories for detecting spurious dependencies, examining whether the learned relationships are “correct” or not. Furthermore, we define stable relationships as those that are both “correct” and “good” across multiple graphs, and finally design a two-level priority selection strategy for aggregating local updates, obtaining a global causal graph over the integrated variables. Experimental results on synthetic, benchmark and real-world data demonstrate the effectiveness of our method.","We focus on discovering causal relationships from distributed data stored across individual clients (such as different hospitals), while preserving data privacy.In many real-world scenarios (e.g., healthcare), clients observe overlapping but non-identical sets of variables. And each client's data may be particularly suitable for uncovering certain causal relationships, due to differences in expertise, available resources, or measurement practices. For example, hospitals often measure non-identical clinical indicators due to practical concerns, and are better suited to uncover causal relationships most relevant to their specialties.Our paper presents an effective method for learning causal relationships over the integrated set of variables, without requiring clients to share their raw data. The method involves two considerations. One is to detect potentially “incorrect” causal and non-causal relationships, arising from non-overlapping variable pairs—those that are not observed together by any client. The other is to evaluate the varying importance of learned relationships, enhancing the reliability of causal discovery.Our method can be applied to healthcare, finance, and other domains where privacy-sensitive, heterogeneous data is distributed across multiple institutions."
Poster,Federated Disentangled Tuning with Textual Prior Decoupling and Visual Dynamic Adaptation,https://ICML.cc//virtual/2025/poster/46665,"Yihao Yang, Wenke Huang, Guancheng Wan, Bin Yang, Mang Ye","Federated Parameter-Efficient Fine-Tuning aims to adapt Vision-Language Models for downstream tasks in distributed environments. However, data heterogeneity across participants hinders collaborative effectiveness, necessitating personalized adaptation to cover distinct data distributions. Current personalized methods suffer from two limitations. 1) Textual Property Loss: Existing methods facilitate the collaboration between decoupled prompts at the feature level, which potentially undermines the textual properties of the prompts. 2) Visual Feature Diversity: The diversity of visual features makes it challenging to leverage naive image features directly for image-text alignment in downstream tasks. In this work, we propose Federated Disentangled Tuning with Textual Prior Decoupling and Visual Dynamic Adaptation (FedDDA) to overcome the above limitations. Specifically, we encourage decoupling prompts in a way that maximizes the efficacy of prior knowledge, which is essential for maintaining a coherent linguistic context. Furthermore, we design a visual adaption model to reshape visual space to optimally align with the textual space. Extensive experiments on various image classification tasks show the effectiveness of our work in addressing data heterogeneity. The codes are released at https://github.com/MoratalYang/FedDDA.","Federated Parameter-Efficient Fine-Tuning facilitates collaborative and privacy-preserving fine-tuning of Vision-Language Models by updating only a limited number of parameters. However, the presence of non-independent and identically distributed (non-IID) data across clients often leads to suboptimal performance. To address this challenge, we propose FedDDA, a simple yet effective method that leverages both textual and visual modalities. Specifically, our approach integrates global and local prompts to maintain semantic consistency during the shared learning process. Furthermore, it dynamically adapts visual representations from images to ensure better alignment with their associated textual descriptions. These mechanisms enable our approach to improve how models learn from distributed and heterogeneous data significantly."
Poster,Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework,https://ICML.cc//virtual/2025/poster/45557,"Terje Mildner, Oliver Hamelijnck, Paris Giampouras, Theodoros Damoulas","We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.","Our paper tackles the following questions: (1) Can we train a probabilistic machine learning model on different, private data sets from individuals without sharing them with other individuals or a server, (2) can we be sure that potential misspecification, such as outliers in any individual's data, does not negatively impact the model, and (3) can we train our model efficiently? These questions have been answered in isolation but not in combination. Our approach, FedGVI, allows us to answer (1), (2), and (3) simultaneously. The main result is that FedGVI successfully disregards misspecification during training that can harmfully impact the model, and that this model can be found faster than models found through existing methods that only solve (1). We demonstrate this by training a Neural Network for classifying images when some of the training data has the wrong label showing that FedGVI leads to better results than other methods. These findings have important practical implications, for instance in healthcare where patient data is sensitive and models need to be reliable."
Poster,Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance,https://ICML.cc//virtual/2025/poster/46305,"Guoqing Chao, Zhenghao Zhang, Lei Meng, Jie Wen, Dianhui Chu","Federated multi-view clustering has been proposed to mine the valuable information within multi-view data distributed across different devices and has achieved impressive results while preserving the privacy. Despite great progress,  most federated multi-view clustering methods only used global pseudo-labels to guide the downstream clustering process and failed to exploit the global information when extracting features. In addition, missing data problem in federated multi-view clustering task is less explored. To address these problems, we propose a novel Federated Incomplete Multi-view Clustering method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a dual-head graph convolutional encoder at each client to extract two kinds of underlying features containing global and view-specific information. Subsequently, under the guidance of the fused graph, the two underlying features are fused into high-level features, based on which clustering is conducted under the supervision of pseudo-labeling. Finally, the high-level features are uploaded to the server to refine the graph fusion and pseudo-labeling computation. Extensive experimental results demonstrate the effectiveness and superiority of FIMCFG.  Our code ispublicly available at https://github.com/PaddiHunter/FIMCFG.","When data is spread across different devices, it’s hard to analyze it all together without risking privacy. When some of the data is missing, the existing methods struggle to work well.We designed a smart way to deal with the above problems with a federated learning paradigm, which can cluster the missing data across different devices well.Our method outperforms the existing techniques and ensure the stronger privacy, more accurate clustering performance and missing data handling well. This method is a reliable tool for real-world applications like healthcare or smart devices."
Poster,Federated In-Context Learning: Iterative Refinement for Improved Answer Quality,https://ICML.cc//virtual/2025/poster/45173,"Ruhan Wang, Zhiyong Wang, Chengkai Huang, Rui Wang, Tong Yu, Lina Yao, John C. S. Lui, Dongruo Zhou","For question-answering (QA) tasks, in-context learning (ICL) enables language models (LMs) to generate responses without modifying their parameters by leveraging examples provided in the input. However, the effectiveness of ICL heavily depends on the availability of high-quality examples, which are often scarce due to data privacy constraints, annotation costs, and distribution disparities. A natural solution is to utilize examples stored on client devices, but existing approaches either require transmitting model parameters—incurring significant communication overhead—or fail to fully exploit local datasets, limiting their effectiveness. To address these challenges, we propose Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL through an iterative, collaborative process. Fed-ICL progressively refines responses by leveraging multi-round interactions between clients and a central server, improving answer quality without the need to transmit model parameters. We establish theoretical guarantees for the convergence of Fed-ICL and conduct extensive experiments on standard QA benchmarks, demonstrating that our proposed approach achieves strong performance while maintaining low communication costs.","Language models can answer questions by learning from example inputs, a process called in-context learning (ICL). While powerful, ICL depends on access to high-quality examples, which are often scarce due to privacy concerns, labeling costs, and data distribution differences. A natural solution is to use examples stored locally on users’ devices. However, existing approaches either require transmitting large model files—leading to high communication costs—or fail to fully utilize local data. We propose Federated In-Context Learning (Fed-ICL), a new framework that enables user devices to collaborate with a central server through multiple rounds of interaction. Without sharing raw data or model parameters, each round helps refine the model’s responses using locally available examples. This design preserves privacy and keeps communication efficient. Fed-ICL is backed by theoretical convergence guarantees and shows strong empirical performance on widely used question-answering benchmarks. It demonstrates a practical path toward more effective and privacy-preserving language model applications, especially in decentralized environments."
Poster,Federated Learning for Feature Generalization with Convex Constraints,https://ICML.cc//virtual/2025/poster/44018,"Dongwon Kim, Donghee Kim, Sung Kuk Shyn, Kwangsu Kim","Federated learning (FL) often struggles with generalization due to heterogeneous client data. Local models are prone to overfitting their local data distributions, and even transferable features can be distorted during aggregation. To address these challenges, we propose FedCONST, an approach that adaptively modulates update magnitudes based on the global model’s parameter strength. This prevents over-emphasizing well-learned parameters while reinforcing underdeveloped ones. Specifically, FedCONST employs linear convex constraints to ensure training stability and preserve locally learned generalization capabilities during aggregation. A Gradient Signal-to-Noise Ratio (GSNR) analysis further validates FedCONST's effectiveness in enhancing feature transferability and robustness. As a result, FedCONST effectively aligns local and global objectives, mitigating overfitting and promoting stronger generalization across diverse FL environments, achieving state-of-the-art performance.","Federated learning (FL) allows multiple devices to collaboratively train machine learning models without sharing their data. However, when data is distributed unevenly across devices, models often overfit to local patterns and fail to generalize well. Our method, FedCONST, addresses this challenge by adjusting how much each part of the model is updated, based on how strongly that part has already learned. This helps prevent over-updating well-trained parts while encouraging weaker parts to catch up. By adding lightweight constraints during training, FedCONST improves stability and helps preserve useful features. As a result, it achieves stronger generalization and state-of-the-art performance across a variety of FL scenarios."
Poster,Federated Node-Level Clustering Network with Cross-Subgraph Link Mending,https://ICML.cc//virtual/2025/poster/46550,"Jingxin Liu, Renda Han, Wenxuan Tu, Haotian Wang, Junlong Wu, Jieren Cheng","Subgraphs of a complete graph are usually distributed across multiple devices and can only be accessed locally because the raw data cannot be directly shared. However, existing node-level federated graph learning suffers from at least one of the following issues: 1) heavily relying on labeled graph samples that are difficult to obtain in real-world applications, and 2) partitioning a complete graph into several subgraphs inevitably causes missing links, leading to sub-optimal sample representations. To solve these issues, we propose a novel $\underline{\text{Fed}}$erated $\underline{\text{N}}$ode-level $\underline{\text{C}}$lustering $\underline{\text{N}}$etwork (FedNCN), which mends the destroyed cross-subgraph links using clustering prior knowledge. Specifically, within each client, we first design an MLP-based projector to implicitly preserve key clustering properties of a subgraph in a denoising learning-like manner, and then upload the resultant clustering signals that are hard to reconstruct for subsequent cross-subgraph links restoration. In the server, we maximize the potential affinity between subgraphs stemming from clustering signals by graph similarity estimation and minimize redundant links via the N-Cut criterion. Moreover, we employ a GNN-based generator to learn consensus prototypes from this mended graph, enabling the MLP-GNN joint-optimized learner to enhance data privacy during data transmission and further promote the local model for better clustering. Extensive experiments demonstrate the superiority of FedNCN.","Subgraphs of a complete graph are usually distributed across multiple devices and can only be accessed locally because the raw data cannot be directly shared. However, existing node-level federated graph learning suffers from at least one of the following issues: 1) heavily relying on labeled graph samples that are difficult to obtain in real-world applications, and 2) partitioning a complete graph into several subgraphs inevitably causes missing links, leading to sub-optimal sample representations. To solve these issues, we propose a novel $\underline{\text{Fed}}$erated $\underline{\text{N}}$ode-level $\underline{\text{C}}$lustering $\underline{\text{N}}$etwork (FedNCN), which mends the destroyed cross-subgraph links using clustering prior knowledge."
