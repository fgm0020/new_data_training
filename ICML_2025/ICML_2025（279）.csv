type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo,https://ICML.cc//virtual/2025/poster/45342,"Filip Ekström Kelvinius, Zheng Zhao, Fredrik Lindsten","A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion"", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic as well as protein and image data. Further, we demonstrate how the approach can be extended to discrete data.","If using an AI tool for generating an image, this tool is likely to be based on something called a diffusion model. A diffusion model generates an image (or some other kind of data) in steps, starting from something that is complete noise, and gradually makes this into an image.In this work, we develop a method that can guide such model so that it generates something based (or conditioned) on some ”partial” data, also known as an inverse problem. For example, if you have an image where some parts are missing, and you have a diffusion model which can generate images, a method like the one developed in this work would enabling using this model to “fill in” the missing parts. Our method is based on a method from statistics called sequential Monte Carlo. The developed method is a way of bridging modern and powerful AI methods with classical statistical methods, reaping the benefits from both."
Poster,Solving Probabilistic Verification Problems of Neural Networks using Branch and Bound,https://ICML.cc//virtual/2025/poster/43808,"David Boetius, Stefan Leue, Tobias Sutter","Probabilistic verification problems of neural networks are concerned with formally analysing the output distribution of a neural network under a probability distribution of the inputs. Examples of probabilistic verification problems include verifying the demographic parity fairness notion or quantifying the safety of a neural network. We present a new algorithm for solving probabilistic verification problems of neural networks based on an algorithm for computing and iteratively refining lower and upper bounds on probabilities over the outputs of a neural network. By applying state-of-the-art bound propagation and branch and bound techniques from non-probabilistic neural network verification, our algorithm significantly outpaces existing probabilistic verification algorithms, reducing solving times for various benchmarks from the literature from tens of minutes to tens of seconds. Furthermore, our algorithm compares favourably even to dedicated algorithms for restricted probabilistic verification problems. We complement our empirical evaluation with a theoretical analysis, proving that our algorithm is sound and, under mildly restrictive conditions, also complete when using a suitable set of heuristics.","An artificial neural network that suggests hiring someone for a job should be impartial to gender.This could mean that applicants of all genders have an equal chance of being hired.Artificial neural networks are also used in autonomous cars.In this use case, the network should not brake sharply too often.Questions like these are difficult to answer because artificial neural networks are very complex internally.We created a computer program to answer such questions.What makes our program special is that it always gives the correct answer.However, our program may take some time to come up with an answer.Still, it is much faster than similar programs."
Poster,Solving Satisfiability Modulo Counting Exactly with Probabilistic Circuits,https://ICML.cc//virtual/2025/poster/44923,"Jinzhao Li, Nan Jiang, Yexiang Xue","Satisfiability Modulo Counting (SMC) is a recently proposed general language to reason about problems integrating statistical and symbolic Artificial Intelligence. An SMC problem is an extended SAT problem in which the truth values of a few Boolean variables are determined by probabilistic inference. Approximate solvers may return solutions that violate constraints. Directly integrating available SAT solvers and probabilistic inference solvers gives exact solutions but results in slow performance because of many back-and-forth invocations of both solvers. We propose KOCO-SMC, an integrated exact SMC solver that efficiently tracks lower and upper bounds in the probabilistic inference process. It enhances computational efficiency by enabling early estimation of probabilistic inference using only partial variable assignments, whereas existing methods require full variable assignments. In the experiment, we compare KOCO-SMC with currently available approximate and exact SMC solvers on large-scale datasets and real-world applications. The proposed KOCO-SMC finds exact solutions with much less time.","Many important AI tasks today involve both logic (like solving a puzzle) and probability (like making guesses). But solving problems that combine both is extremely challenging. A recently proposed framework for describing such problems, called Satisfiability Modulo Counting (SMC), helps bridge these two areas, but solving SMC problems exactly can be very slow.Current approaches to solving SMC problems exactly rely on gluing together two separate tools: one that handles logic and another that handles probability. These tools must repeatedly interact, which consumes significant time and computing power.In our work, we introduce KOCO-SMC, a new algorithm that unifies logic and probability solving into a single tool. KOCO-SMC can make in-time decisions about whether to skip unpromising attempts early, using only partial information.We tested KOCO-SMC on large-scale synthetic datasets and real-world problems, and found that it solves them both exactly and much faster than existing methods. This demonstrates that it’s possible to achieve both accuracy and efficiency when tackling some of the most complex AI challenges."
Poster,Solving Zero-Sum Convex Markov Games,https://ICML.cc//virtual/2025/poster/44636,"Fivos Kalogiannis, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Ian Gemp, Georgios Piliouras","We contribute the first provable guarantees of global convergence to Nash equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using independent policy gradient methods. Convex Markov games, recently defined by Gemp et al.(2024), extend Markov decision processes to multi-agent settings with preferences that are convex over occupancy measures, offering a broad framework for modeling generic strategic interactions. However, even the fundamental min-max case of cMGs presents significant challenges, including inherent nonconvexity, the absence of Bellman consistency, and the complexity of the infinite horizon.Our results follow a two-step approach. First, leveraging properties of hidden-convex–hidden-concave functions, we show that a simple nonconvex regularization transforms the min-max optimization problem into a nonconvex–proximal Polyak-Łojasiewicz (NC-pPL) objective. Crucially, this regularization can stabilize the iterates of independent policy gradient methods and ultimately lead them to converge to equilibria. Second, building on this reduction, we address the general constrained min-max problems under NC-pPL and two-sided pPL conditions, providing the first global convergence guarantees for stochastic nested and alternating gradient descent-ascent methods, which we believe may be of independent interest.","Convex Markov games model a number of applications spanning multi-agent robotic exploration, improving creativity in machinic Chess playing, language model alignment and more. We propose the first algorithm to solve two-player zero-sum games of this kind with a simple twist of conventional RL algorithms, namely, policy gradient. The convergence is guaranteed by regularization and alternating gradient updates for the minimizing and maximizing variables."
Poster,SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation,https://ICML.cc//virtual/2025/poster/44792,"Zihan Liu, Shuangrui Ding, Zhixiong Zhang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang","Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, leading to cumbersome training and inference pipelines, as well as suboptimal overall generation quality due to error accumulation across stages.In this paper, we propose **SongGen**, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: **mixed mode**, which generates a mixture of vocals and accompaniment directly, and **dual-track mode**, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline.The code is available at https://github.com/LiuZH-19/SongGen.","Generating songs from text — including both vocals and accompaniment — is a complex task. Existing methods use multi-stage processes that are slow and often reduce the final output quality. We introduce SongGen, a single-stage AI model that creates songs from lyrics, musical descriptions, and short voice samples, delivering better quality and efficiency than multi-stage approaches. SongGen is fully open-source, aiming to make high-quality AI song generation more accessible and controllable."
Poster,Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language Model,https://ICML.cc//virtual/2025/poster/46416,"Kaiwen Tang, Zhanglu Yan, Weng-Fai Wong","For reasons such as privacy, there are use cases for language models at the edge. This has given rise to small language models targeted for deployment in resource-constrained devices where energy efficiency is critical. Spiking neural networks (SNNs) offer a promising solution due to their energy efficiency, and there are already works on realizing transformer-based models on SNNs. However, key operations like softmax and layer normalization (LN) are difficult to implement on neuromorphic hardware, and many of these early works sidestepped them. To address these challenges, we introduce Sorbet, a transformer-based spiking language model that is more neuromorphic hardware-compatible. Sorbet incorporates a novel shifting-based softmax called PTsoftmax and a BitShifting-based PowerNorm (BSPN), both designed to replace the respective energy-intensive operations. By leveraging knowledge distillation and model quantization, Sorbet achieved a highly compressed binary weight model that maintains competitive performance while achieving $27.16\times$ energy savings compared to BERT. We validate Sorbet through extensive testing on the GLUE benchmark and a series of ablation studies, demonstrating its potential as an energy-efficient solution for language model inference.  Our code is publicly available at [https://github.com/Kaiwen-Tang/Sorbet](https://github.com/Kaiwen-Tang/Sorbet)","To protect user privacy, we aim to run language models directly on small devices like phones, which have limited computing power and need to save energy. Many key steps in standard language models are hard to run efficiently on such low-power hardware. To solve this, we developed Sorbet, a new language model based on spiking neural networks (SNN) that is designed specifically for energy-efficient hardware. Sorbet replaces traditional power-hungry operations with new methods, making the model much more energy-saving. Using advanced techniques, we also compressed Sorbet into a very small model while keeping strong accuracy. Our tests show that Sorbet uses over 27 times less energy than typical models. This work opens the door to smarter, more energy-efficient devices that better protect user privacy."
Poster,Sort Before You Prune: Improved Worst-Case Guarantees of the DiskANN Family of Graphs,https://ICML.cc//virtual/2025/poster/45675,"Siddharth Gollapudi, Ravishankar Krishnaswamy, Kirankumar Shiragur, Harsh Wardhan","Graph-based data structures have become powerful and ubiquitous tools for scalable approximate nearest-neighbor (ANN) search over the past decade. In spite of their apparent practical performance, there has only recently been progress on the **worst-case** performance of these data structures. Indeed, the influential work of Indyx and Xu (2023) introduced the key concept of $\alpha$-reachable graphs, showing that graphs constructed by the DiskANN algorithm (Subramanya, et. al. 2023) produce an $\left(\frac{\alpha+1}{\alpha-1}\right)$-approximate solution with a simple best-first search that runs in poly-logarithmic query time. In our work, we improve and generalize this analysis as follows:    -  We introduce **sorted** $\alpha$-reachable graphs, and use this notion to obtain a stronger approximation factor of $\frac{\alpha}{\alpha-1}$ for the DiskANN algorithm on Euclidean metrics.    -  We present the **first** worst-case theoretical analysis for the popular **beam-search** algorithm, which is used in practice to search these graphs for $k > 1$ candidate nearest neighbors.We also present empirical results validating the significance of sorted $\alpha$-reachable graphs, which aligns with our theoretical findings.","In recent years, graph-based data structures have become a popular and performant option for vector search. However, theoretical understanding of these structures has remained far behind their practical performance. Prior work has provided strong theory for a simplified version of these data structures.Our findings bring the theoretical understanding of these data structures closer to the practical versions: by adding some additional structure (which is motivated by commonly used modifications in-practice) to the problem, we are able to show better theoretical guarantees. In addition, we prove results for when a query requires more than one vector."
Poster,Sortformer: A Novel Approach for Permutation-Resolved Speaker Supervision in Speech-to-Text Systems,https://ICML.cc//virtual/2025/poster/46140,"Taejin Park, Ivan Medennikov, Kunal Dhawan, Weiqing Wang, He Huang, Nithin Koluguri, Krishna Puvvada, Jagadeesh Balam, Boris Ginsburg","Sortformer is an encoder-based speaker diarization model designed for supervising speaker tagging in speech-to-text models. Instead of relying solely on permutation invariant loss (PIL), Sortformer introduces Sort Loss to resolve the permutation problem, either independently or in tandem with PIL. In addition, we propose a streamlined multi-speaker speech-to-text architecture that leverages Sortformer for speaker supervision, embedding speaker labels into the encoder using sinusoidal kernel functions. This design addresses the speaker permutation problem through sorted objectives, effectively bridging timestamps and tokens to supervise speaker labels in the output transcriptions. Experiments demonstrate that Sort Loss can boost speaker diarization performance, and incorporating the speaker supervision from Sortformer improves multi-speaker transcription accuracy. We anticipate that the proposed Sortformer and multi-speaker architecture will enable the seamless integration of speaker tagging capabilities into foundational speech-to-text systems and multimodal large language models (LLMs), offering an easily adoptable and user-friendly mechanism to enhance their versatility and performance in speaker-aware tasks. The code and trained models are made publicly available through the NVIDIA NeMo Framework.","Transcribing multiple speakers with speaker tagging involves a problem of mapping which speaker belongs to which prediction the model generated. This issue, known as the ""permutation problem,"" often requires a complicated system to resolve predicted speakers and real-life speakers. To address this, we developed Sortformer, a Transformer-encoder-based model that sorts speech segments by the order in which speakers first appear, effectively resolving permutation problems. Sortformer introduces ""Sort Loss,"" which trains the model to order speech by arrival time.Our approach seamlessly integrates into existing speech recognition systems, requiring minimal adjustments while significantly enhancing their accuracy. Sortformer and arrival-time-ordered multi-speaker transcription make multi-speaker ASR model training much easier, since it only requires exactly the same training framework as that used in monoaural ASR models, without needing specialized permutation-oriented loss calculations. By reducing complexity, Sortformer helps speech-to-text technologies become more robust and user-friendly. Ultimately, this work allows everyday applications—like virtual meeting transcriptions or smart assistants—to better understand group conversations, paving the way for clearer communication and richer interaction experiences."
Poster,Sounding that Object: Interactive Object-Aware Image to Audio Generation,https://ICML.cc//virtual/2025/poster/46382,"Tingle Li, Baihe Huang, Xiaobin Zhuang, Dongya Jia, Jiawei Chen, Yuping Wang, Zhuo Chen, Gopala Anumanchipalli, Yuxuan Wang","Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an interactive object-aware audio generation model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the object level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds.","Real-world scenes often include multiple objects that each make their own sounds—like cars, footsteps and chatter—yet current AI systems can’t isolate these sounds from still images. We present an interactive model that links sounds to objects a user selects in a picture, using segmentation masks that let you click on a visual object and generate its specific sound. We build on a latent diffusion framework and integrate object-centric learning so the system learns which image regions correspond to which sounds. At test time, segmentation masks guide generation, ensuring engine noises come from cars and crowd hum from people. Our theoretical analysis shows that replacing attention with segmentation masks yields equivalent grounding. We evaluate our model with objective measures and human studies: it outperforms existing methods in accuracy, audio quality and user satisfaction. Users can mix sounds from multiple objects to create a coherent soundscape. It also captures interactions like sticks splashing water rather than generic water sounds. This work opens the door to intuitive audio-visual tools for filmmakers, virtual reality and accessible media, making it easy to bring images to life with sound."
Poster,Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging,https://ICML.cc//virtual/2025/poster/45553,"Pierre Ablin, Angelos Katharopoulos, Skyler Seto, David Grangier","Machine learning models are routinely trained on a mixture of different data domains. Different domain weights yield very different downstream performances.We propose the Soup-of-Experts, a novel architecture that can instantiate a model at test time for any domain weights with minimal computational cost and without re-training the model. Our architecture consists of a bank of expert parameters, which are linearly combined to instantiate one model. We learn the linear combination coefficients as a function of the input domain weights.To train this architecture, we sample random domain weights, instantiate the corresponding model, and backprop through one batch of data sampled with these domain weights.We demonstrate how our approach obtains small specialized models on several language modeling tasks quickly.Soup-of-Experts are particularly appealing when one needs to ship many different specialist models quickly under a size constraint.","We propose a new neural network architecture that holds many parameters that are trained jointly. Unlike standard architecture, when we want to use the model to address a new task, we first select a relevant small subset of the parameters of the model, and then use only these parameters to address the new task. Since each task requires a small number of parameters, the models are very efficient."
