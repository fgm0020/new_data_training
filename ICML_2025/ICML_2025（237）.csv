type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models,https://ICML.cc//virtual/2025/poster/44046,"Xuelin Shen, Jiayin Xu, Kangsheng Yin, Wenhan Yang","The improved semantic understanding of vision-language pretrained (VLP) models has made it increasingly difficult to protect publicly posted images from being exploited by search engines and other similar tools. In this context, this paper seeks to protect users' privacy by implementing defenses at the image compression stage to prevent exploitation. Specifically, we propose a flexible coding method, termed Privacy-Shielded Image Compression (PSIC), that can produce bitstreams with multiple decoding options. By default, the bitstream is decoded to preserve satisfactory perceptual quality while preventing interpretation by VLP models. Our method also retains the original image compression functionality. With a customizable input condition, the proposed scheme can reconstruct the image that preserves its full semantic information. A Conditional Latent Trigger Generation (CLTG) module is proposed to produce bias information based on customizable conditions to guide the decoding process into different reconstructed versions, and an Uncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed to leverage the soft labels inferred from the target VLP model's uncertainty on the training data. This paper further incorporates an adaptive multi-objective optimization strategy to obtain improved encrypting performance and perceptual quality simultaneously within a unified training process. The proposed scheme is plug-and-play and can be seamlessly integrated into most existing Learned Image Compression (LIC) models. Extensive experiments across multiple downstream tasks have demonstrated the effectiveness of our design.","In today's digital landscape, images are widely shared and processed by AI systems capable of extracting detailed information. While this enables many valuable applications, it also raises significant privacy concerns, as individuals may unintentionally disclose sensitive content.Our research proposes a novel image compression framework to tackle this challenge. The framework compresses images to preserve visual quality for human observers while making them less accessible to AI models, thereby enhancing privacy protection.By embedding privacy considerations into the image compression pipeline, our approach provides a practical solution for individuals and organizations seeking to protect sensitive visual information in an increasingly AI-driven world."
Poster,Private Federated Learning using Preference-Optimized Synthetic Data,https://ICML.cc//virtual/2025/poster/44827,"Charlie Hou, Mei-Yu Wang, Yige Zhu, Daniel Lazar, Giulia Fanti","In practical settings, differentially private federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data  (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024)  can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri closes the gap in next-token prediction accuracy between the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at https://github.com/meiyuw/POPri.","Smartphones and other personal devices hold valuable text that could teach predictive keyboards and voice assistants to work better for everyone. But collecting this text from personal devices to central data warehouses may violate user privacy. The current solution for this, called federated learning, trains a small copy of a model on each device and shares only scrambled updates, but it so far has not been able to scale to today's large models.Our research shows a different path: let a large language model (LLM) guess private-style text on the server, then train the small on-device model on that synthetic data. To make those guesses accurate without peeking at the real data, each phone simply scores the guessed samples for their closeness to the real data and sends back a noise-blurred version of those scores. We turn these crowd-sourced scores into a powerful fine-tuning recipe called POPri, which teaches the LLM to write ever-better synthetic text. Across several real-world datasets, POPri nearly erases the accuracy gap between fully private training and a no-privacy baseline while keeping users’ original data safe and sound."
Poster,Private Lossless Multiple Release,https://ICML.cc//virtual/2025/poster/44189,"Joel Daniel Andersson, Lukas Retschmeier, Boel Nelson, Rasmus Pagh","Koufogiannis et al. (2016) showed a $\textit{gradual release}$ result for Laplace noise-based differentially private mechanisms: given an $\varepsilon$-DP release, a new release with privacy parameter $\varepsilon' > \varepsilon$ can be computed such that the combined privacy loss of both releases is at most $\varepsilon'$ and the distribution of the latter is the same as a single release with parameter $\varepsilon'$.They also showed gradual release techniques for Gaussian noise, later also explored by Whitehouse et al. (2022).In this paper, we consider a more general $\textit{multiple release}$ setting in which analysts hold private releases with different privacy parameters corresponding to different access/trust levels.These releases are determined one by one, with privacy parameters in arbitrary order.A multiple release is $\textit{lossless}$ if having access to a subset $S$ of the releases has the same privacy guarantee as the least private release in $S$, and each release has the same distribution as a single release with the same privacy parameter.Our main result is that lossless multiple release is possible for a large class of additive noise mechanisms.For the Gaussian mechanism we give a simple method for lossless multiple release with a short, self-contained analysis that does not require knowledge of the mathematics of Brownian motion.We also present lossless multiple release for the Laplace and Poisson mechanisms.Finally, we consider how to efficiently do gradual release of sparse histograms, and present a mechanism with running time independent of the number of dimensions.","Organizations often want to share insights from sensitive data, like health records or user behavior, without revealing too much about individuals. Differential privacy offers a way to do this by adding noise, creating a balance between accuracy and privacy. But what happens when different people or groups need different levels of access — for example, internal staff, external consultants, or the public?Today, releasing multiple versions of the same data often increases privacy risk. Our research introduces a method to make multiple data releases with different privacy levels that are “lossless” — meaning they don’t leak more information when combined than the least private one would alone.This technique works for common types of noise used in privacy protection, including Gaussian and Laplace. It also supports releasing information in any order of privacy level — crucial in real-world scenarios like evolving trust or data markets.We show how to apply this to complex cases like high-dimensional statistics and sparse histograms, all while keeping computations efficient. The result: organizations can offer flexible data access without sacrificing privacy or accuracy."
Poster,Private Model Personalization Revisited,https://ICML.cc//virtual/2025/poster/44387,"Conor Snedeker, Xinyu Zhou, Raef Bassily","We study model personalization  under user-level differential privacy (DP) in the shared representation framework. In this problem, there are $n$ users whose data is statistically heterogeneous, and their optimal parameters share an unknown embedding $U^* \in\mathbb{R}^{d\times k}$ that maps the user parameters in $\mathbb{R}^d$ to low-dimensional representations in $\mathbb{R}^k$, where $k\ll d$. Our goal is to privately recover the shared embedding and the local low-dimensional representations with small excess risk in the federated setting.  We propose a private, efficient federated learning algorithm to learn the shared embedding based on the FedRep algorithm in  (Collins et al.,2021). Unlike  (Collins et al., 2021), our algorithm satisfies differential privacy, and our results hold for the case of noisy labels. In contrast to prior work on private model personalization (Jain et al., 2021), our utility guarantees hold under a larger class of users' distributions (sub-Gaussian instead of Gaussian distributions). Additionally, in natural parameter regimes, we improve the privacy error term in (Jainet al., 2021) by a factor of $\widetilde{O}(dk)$. Next, we consider the binary classification setting. We  present an information-theoretic construction to privately learn the shared embedding and derive a  margin-based accuracy guarantee that is independent of $d$. Our method utilizes the Johnson-Lindenstrauss transform to reduce the effective dimensions of the shared embedding and the users' data. This result shows that dimension-independent risk bounds are possible in this setting under a margin loss.","We develop new methods for training personalized machine learning models in a way that rigorously protects users’ privacy. In many real-world applications, users have different kinds of data—so personalized models tailored to each user’s data are more accurate than a single shared model. However, training such models while safeguarding sensitive user information remains a major challenge.Our work tackles this challenge by proposing private algorithms that allow users to collaboratively learn a shared low-dimensional representation of their data without revealing individual information. Compared to earlier approaches, our method works under more realistic assumptions about user data and offers improved theoretical guarantees on both privacy and accuracy.We also extend our results to binary classification tasks, where we show that privacy-preserving personalization can be achieved with guarantees that do not depend on the data’s complexity (i.e., its dimensionality), thanks to dimensionality reduction techniques."
Poster,Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty,https://ICML.cc//virtual/2025/poster/44553,"Meera Hahn, Wenjun Zeng, Nithish Kannen, Rich Galt, Kartikeya Badola, Been Kim, Zi Wang","User prompts for generative AI models are often underspecified, leading to a misalignment between the user intent and models' understanding. As a result, users commonly have to painstakingly refine their prompts. We study this alignment problem in text-to-image (T2I) generation and propose a prototype for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their uncertainty about user intent as an understandable and editable belief graph. We build simple prototypes for such agents and propose a new scalable and automated evaluation approach using two agents, one with a ground truth intent (an image) while the other tries to ask as few questions as possible to align with the ground truth. We experiment over three image-text datasets: ImageInWords (Garg et al., 2024) , COCO  (Lin et al., 2014) and DesignBench, a benchmark we curated with strong artistic and design elements. Experiments over the three datasets demonstrate the proposed T2I agents' ability to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024)  than the standard T2I generation. Moreover, we conducted human studies and observed that at least 90\% of human subjects found these agents and their belief graphs helpful for their T2I workflow, highlighting the effectiveness of our approach. Code and DesignBench can be found at https://github.com/google-deepmind/proactive_t2i_agents.","When you ask an AI to create an image, it often gets it wrong because your description isn't specific enough. This means you have to keep trying different prompts, which can be frustrating. For example lets say you ask it for “an image of a dog in a park” the AI has to guess things like the type of dog you want, or what time of day the picture should be. Instead of rewriting the prompt over and over, our paper proposes an AI that proactively asks questions to clarify aspects of the image before generating it. For example the AI would ask about which type of dog you want to be shown.In addition to asking the questions, the AI we propose also shows you a simple and editable flow chart of the image. We call this image chart a “belief graph,” and it has the elements of the image so the user can directly change the AI’s understanding of the image. Tests show that this new method is twice as effective at creating the image the user actually wants. Furthermore, when real people tried it, over 90% said the AI's questions and the ""belief graph"" were very helpful for getting the image they imagined. In short, it's like having a helpful conversation with an AI artist instead of just giving it commands and hoping for the best."
Poster,Probabilistic Factorial Experimental Design for Combinatorial Interventions,https://ICML.cc//virtual/2025/poster/45285,"Divya Shyamal, Jiaqi Zhang, Caroline Uhler","A _combinatorial intervention_, consisting of multiple treatments applied to a single unit with potential interactive effects, has substantial applications in fields such as biomedicine, engineering, and beyond. Given $p$ possible treatments, conducting all possible $2^p$ combinatorial interventions can be laborious and quickly becomes infeasible as $p$ increases. Here we introduce the _probabilistic factorial experimental design_, formalized from how scientists perform lab experiments. In this framework, the experimenter selects a dosage for each possible treatment and applies it to a group of units. Each unit independently receives a random combination of treatments, sampled from a product Bernoulli distribution determined by the dosages. Additionally, the experimenter can carry out such experiments over multiple rounds, adapting the design in an active manner. We address the optimal experimental design problem within a novel intervention model that imposes bounded-degree interactions between treatments. In the passive setting, we provide a closed-form solution for the near-optimal design. Our results prove that a dosage of $\frac{1}{2}$ for each treatment is optimal up to a factor of $1+O(\frac{\ln(n)}{n})$ for estimating any $k$-way interaction model, regardless of $k$, and imply that $O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate this model. For the multi-round setting, we provide a near-optimal acquisition function that can be numerically optimized. We also explore several extensions of the design problem and finally validate our findings through simulations.","Many scientific and engineering problems require testing multiple treatments in combination, but trying every possible combination quickly becomes impractical as the number of treatments grows. We introduce a probabilistic approach to experimental design where each unit receives a random combination of treatments, guided by dosage levels set by the experimenter, which can be refined over multiple rounds. This method dramatically reduces the burden of manually selecting numerous combinatorial treatments while still allowing researchers to accurately estimate complex interactions between treatments, and we provide theoretical guarantees and practical algorithms to support its use."
Poster,Probabilistic Group Mask Guided Discrete Optimization for Incremental Learning,https://ICML.cc//virtual/2025/poster/45798,"Fengqiang Wan, Yang Yang","Incremental learning (IL) aims to sequentially learn new tasks while mitigating catastrophic forgetting. Among various IL strategies, parameter-isolation methods stand out by using mask techniques to allocate distinct parameters to each task, explicitly addressing forgetting. However, existing approaches often disregard parameter dependencies, resulting in an over-reliance on newly allocated parameters. To address this issue, we propose Probabilistic Group Mask selection (PGM), a group-wise approach that captures parameter dependencies by exploring candidate masks within each group. Specifically, PGM partitions parameters into groups with multiple candidate masks, assigning probabilities to these masks and leveraging Gumbel-Softmax for differentiable sampling, enabling efficient optimization of the discrete mask selection process. Our theoretical analysis demonstrates that incorporating parameter dependencies enhances sub-network selection. Experiments conducted on standard benchmarks confirm its superior effectiveness compared to existing IL approaches. The source code is available at: \url{https://github.com/njustkmg/ICML25-PGM}.","When computers learn a series of tasks, they often forget earlier ones — a problem known as “catastrophic forgetting.” One common strategy to avoid this is to assign different parts of the model to different tasks, like giving each task its own space in the model’s memory. However, most of these approaches ignore the relationships between different parts of the model, leading to inefficient use of its capacity. To solve this, we propose a new method called Probabilistic Group Masking (PGM). It groups model parameters and explores multiple options within each group, using a technique that allows the computer to gradually learn which combination works best. This way, the model can better coordinate how it shares and separates knowledge across tasks. Our theoretical analysis shows that modeling these parameter relationships leads to better task-specific configurations."
Poster,Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes,https://ICML.cc//virtual/2025/poster/46357,"Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-jakob Sonke, Efstratios Gavves","Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentations and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose \emph{NPISeg3D}, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model’s ability to capture object-aware context and quantify predictive uncertainty. Experiments on four  3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.","Teaching a computer to recognize 3D objects — like chairs, tables, or people in a cluttered room — is challenging, especially when only a few user clicks are provided as guidance. Our research explores how to make this process both smarter and more trustworthy.We developed a method called NPISeg3D that learns how to segment objects in 3D environments with just a small number of clicks from the user. It works by building a kind of ""mental model"" of each scene and object, helping the system understand both the big picture and the details. This allows it to make better guesses, even with little input.But we didn’t stop there — we also taught the system to know when it might be wrong. This helps users spot and fix mistakes easily. Our method shows strong results across multiple 3D datasets, providing more accurate segmentations with fewer clicks — and more confidence in the results."
Poster,Probably Approximately Global Robustness Certification,https://ICML.cc//virtual/2025/poster/45128,"Peter Blohm, Patrick Indri, Thomas Gärtner, SAGAR MALHOTRA","We propose and investigate probabilistic guarantees for the adversarial robustness of classification algorithms.While traditional formal verification approaches for robustness are intractable and sampling-based approaches do not provide formal guarantees, our approach is able to efficiently certify a probabilistic relaxation of robustness.The key idea is to sample an $\epsilon$-net and invoke a local robustness oracle on the sample.Remarkably, the size of the sample needed to achieve probably approximately global robustness guarantees is independent of the input dimensionality, the number of classes, and the learning algorithm itself.Our approach can, therefore, be applied even to large neural networks that are beyond the scope of traditional formal verification.Experiments empirically confirm that it characterizes robustness better thanstate-of-the-art sampling-based approaches and scales better than formal methods.","We introduce a new way to check how well a classification system, like a neural network, handles small, ill-intended changes to its input, i.e., its *robustness*.Neural networks can commonly be fooled by tiny changes to the input in specific ways, which opens them up to manipulation that is not easily detected by humans.Methods that give absolute formal robustness guarantees are too slow for large neural networks, while faster methods just test the network on a set of samples but do not offer certainty in its robustness for other data points. Our method strikes a balance by providing guarantees while being efficient.We formalize how many examples are enough to achieve a desired level of certainty for robustness on new data.Importantly, the number of tests needed does not grow with the size or complexity of the system, making our method feasible even for large models."
Poster,Probing Visual Language Priors in VLMs,https://ICML.cc//virtual/2025/poster/44730,"Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, Honglak Lee","Vision-Language Models (VLMs) may over-rely on visual language priors from their training data rather than true visual reasoning. To investigate this, we introduce ViLP, a benchmark featuring deliberately out-of-distribution images synthesized via image generation models and out-of-distribution Q\&A pairs. Each question in ViLP is coupled with three potential answers and three corresponding images: one that can be resolved by text priors alone and two that demand visual reasoning. Although humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4o achieves only 66.17\% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA data and then apply pixel-level and semantic corruptions to form ``good-bad"" image pairs for self-training. Our proposed training objective, Image-DPO, compels VLMs to focus more on the actual visual inputs, and we demonstrate its effectiveness in LLaVA-v1.5 and Cambrian. Project Page: \href{https://vilp-team.github.io/}{ViLP}.","Modern AI systems that look at pictures and read text—called vision-language models—sometimes answer by guessing from familiar patterns in their training data instead of truly “seeing” the image. We built a new test, ViLP, that shows them unusual, computer-generated pictures with tricky questions. Each question has three possible answers: one you could guess from general knowledge and two that require careful visual inspection. People get almost everything right, but even the powerful GPT-4o is correct only 66 percent of the time, revealing a heavy reliance on shortcuts.To help these models improve, we created a self-training recipe called Image-DPO. The model first invents its own image–question pairs, then practices on slightly “corrupted” versions of those images and learns to change its answer whenever the picture changes. This teaches the model to pay closer attention to what it actually sees. Two open-source models, LLaVA-v1.5 and Cambrian, showed clear gains after this training. All code, data, and a demo are freely available at the ViLP website: https://vilp-team.github.io/."
