type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Do NOT Think That Much for 2+3=? On the Overthinking of Long Reasoning Models,https://ICML.cc//virtual/2025/poster/45540,"Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu","The remarkable performance of long reasoning models can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where long reasoning models generate redundant solutions that contribute minimally to accuracy and diversity, thereby wasting computational resources on simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by long reasoning models. Using a self-training paradigm, we propose strategies to mitigate overthinking, simplifying reasoning processes without compromising accuracy.  Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME. Our code is open-source and available at https://github.com/galaxyChen/overthinking.","Is the long reasoning model efficient when thinking? We found that the long reasoning model has a serious overthinking problem: the model will repeatedly generate redundant homogeneous solutions, which have little impact on the accuracy but greatly increase the cost of inference. We comprehensively analyzed this phenomenon and proposed a training strategy based on the preference optimization algorithm. Experiments show that our method can greatly reduce the number of tokens in generation while maintaining the mathematical reasoning ability."
Poster,"Don't Restart, Just Reuse: Reoptimizing MILPs with Dynamic Parameters",https://ICML.cc//virtual/2025/poster/44856,"Sijia Zhang, Shuli Zeng, Shaoang Li, Feng Wu, Shaojie Tang, Xiangyang Li","Many real-world applications, such as logistics, routing, scheduling, and production planning, involve dynamic systems that require continuous updates to solutions for new Mixed Integer Linear Programming (MILP) problems. These systems often require rapid updates to their solutions to accommodate slight modifications in constraints or objectives introduced by evolving conditions.While reoptimization techniques have been explored for Linear Programming (LP) and certain specific MILP problems, their effectiveness in addressing general MILP is limited. In this work, we propose a two-stage reoptimization framework for efficiently identifying high-quality feasible solutions. Specifically, we first utilize the historical solving process information to predict a high confidence solution space for modified MILPs, which is likely to contain high-quality solutions. Building on the prediction results, we fix a part of variables within the predicted intervals and apply the Thompson Sampling algorithm to determine which variables to fix. This is done by updating the Beta distributions based on the solutions obtained from the solver. Extensive experiments across nine reoptimization datasets show that our VP-OR outperforms the state-of-the-art methods, achieving higher-quality solutions under strict time limits.","Dynamic planning systems (e.g., delivery routing or factory scheduling) often require immediate updates when conditions change. Traditional methods respond too slowly to disruptions like traffic jams or new orders. Our approach has two key steps: First, it predicts promising solution areas by learning from historical decision patterns. Second, it intelligently tests critical choices through adaptive trial-and-error. Validated across logistics and manufacturing scenarios, our method generates high-quality plans faster than state-of-the-art techniques. This enables minute-level production rescheduling, and faster emergency response routing."
Poster,Double-Filter: Efficient Fine-tuning of Pre-trained Vision-Language Models via Patch&Layer Filtering,https://ICML.cc//virtual/2025/poster/43782,"Yaoqin He, Junchen Fu, Kaiwen Zheng, Songpei Xu, Fuhai Chen, Jie Li, Joemon Jose, Xuri Ge","In this paper, we present a novel approach, termed Double-Filter,to “slim down” the fine-tuning process of vision-language pre-trained (VLP) models via filtering redundancies in feature inputs and architectural components. We enhance the fine-tuning process using two approaches. First, we develop a new patch selection method incorporating image patch filtering through background and foreground separation, followed by a refined patch selection process. Second, we design a genetic algorithm to eliminate redundant fine-grained architecture layers, improving the efficiency and effectiveness of the model. The former makes patch selection semantics more comprehensive, improving inference efficiency while ensuring semantic representation. The latter’s fine-grained layer filter removes architectural redundancy to the extent possible and mitigates the impact on performance. Experimental results demonstrate that the proposed Double-Filter achieves superior efficiency of model fine-tuning and maintains competitive performance compared with the advanced efficient fine-tuning methods on three downstream tasks, VQA, NLVR and Retrieval. In addition, it has been proven to be effective under METER and ViLT VLP models.","Modern AI models pre-trained on both images and text (vision-language models) are powerful but often inefficient to adapt for specific tasks. Our new method, Double-Filter, streamlines this process by cutting unnecessary computations in two ways:Adaptive Patch Selection: Unlike methods that focus only on foreground objects, our approach dynamically evaluates both foreground and background regions, retaining only the most semantically meaningful patches. This ensures comprehensive image understanding while avoiding redundant processing of unimportant details.Optimized Architecture: Using an automated search inspired by evolution (genetic algorithm), we identify and remove redundant layers in the model, making it leaner while preserving accuracy.Tests on three key tasks—visual question answering (VQA), visual reasoning (NLVR), and image-text retrieval—show that that Double-Filter makes fine-tuning faster and more efficient, with little to no drop in performance. It works across different popular models, offering a practical way to reduce costs and energy use in AI applications.Why it matters: By intelligently filtering data and architecture, our method helps deploy adaptable AI systems more sustainably—balancing speed, cost, and accuracy."
Poster,Double Machine Learning for Causal Inference under Shared-State Interference,https://ICML.cc//virtual/2025/poster/44507,"Chris Hays, Manish Raghavan","Researchers and practitioners often wish to measure treatment effects in settings where units interact via markets and recommendation systems.  In these settings, units are affected by certain *shared states*, like prices, algorithmic recommendations or social signals. We formalize this  structure, calling it shared-state interference, and argue that our formulation captures many relevant applied settings. Our key modeling assumption is that individuals' potential outcomes are independent conditional on the shared state. We then prove an extension of a double machine learning (DML) theorem providing conditions for achieving efficient inference under shared-state interference. We also instantiate our general theorem in several models of interest where it is possible to efficiently estimate the average direct effect (ADE) or global average treatment effect (GATE).","Researchers and practitioners often wish to measure causal effects in recommendation systems or markets, where units interact via centralized information, prices or other *shared states*. We develop theory allowing for estimation of causal effects in these settings without imposing strict assumptions on the data generating process while preserving the asymptotic efficiency of our method. This allows for valid and efficient inference in important, socially salient settings that was not possible before."
Poster,Doubly Protected Estimation for Survival Outcomes Utilizing External Controls for Randomized Clinical Trials,https://ICML.cc//virtual/2025/poster/43681,"Chenyin Gao, Shu Yang, Mingyang Shan, Wenyu Ye, Ilya Lipkovich, Douglas Faries","Censored survival data are common in clinical trials, but small control groups can pose challenges, particularly in rare diseases or where balanced randomization is impractical. Recent approaches leverage external controls from historical studies or real-world data to strengthen treatment evaluation for survival outcomes. However, using external controls directly may introduce biases due to data heterogeneity. We propose a doubly protected estimator for the treatment-specific restricted mean survival time difference that is more efficient than trial-only estimators and mitigates biases from external data. Our method adjusts for covariate shifts via doubly robust estimation and addresses outcome drift using the DR-Learner for selective borrowing. The approach can incorporate machine learning to approximate survival curves and detect outcome drifts without strict parametric assumptions, borrowing only comparable external controls. Extensive simulation studies and a real-data application evaluating the efficacy of Galcanezumab in mitigating migraine headaches have been conducted to illustrate the effectiveness of our proposed framework.","Clinical trials can take a long time and often don’t include enough patients to show if a drug helps people live longer, especially for rare diseases. To solve this problem, we developed a data-adaptive integrative approach that gathers information from previous studies or real-world data to strengthen the placebo (or control) groups. This means more patients can be assigned to the new drug, making it easier and quicker to see if the treatment is effective, and ultimately speeding up the process of getting important new medicines to people who need them."
Poster,Doubly Robust Conformalized Survival Analysis with Right-Censored Data,https://ICML.cc//virtual/2025/poster/46585,"Matteo Sesia, vladimir svetnik","We present a conformal inference method for constructing lower prediction bounds for survival times from right-censored data, extending recent approaches designed for more restrictive type-I censoring scenarios. The proposed method imputes unobserved censoring times using a machine learning model, and then analyzes the imputed data using a survival model calibrated via weighted conformal inference. This approach is theoretically supported by an asymptotic double robustness property. Empirical studies on simulated and real data demonstrate that our method leads to relatively informative predictive inferences and is especially robust in challenging settings where the survival model may be inaccurate.","Survival analysis is a subfield of data science focused on modeling and predicting when an event will occur, such as a patient’s death or a device’s failure. These problems are common in fields like medicine and engineering. A major challenge in survival analysis is that the data are usually incomplete: we often don’t observe the true event time for every individual. For example, in a clinical study, some patients may still be alive when the study ends or may leave early. In such cases, we only know that the event hasn’t happened yet, not when it eventually will. This kind of missing information makes survival analysis more difficult than many other prediction tasks.Nowadays, researchers increasingly use complex machine learning models in many areas of data science, including survival analysis. While these models can be accurate, they can also fail in unpredictable ways as they still often struggle to provide reliable insight into how confident we should be in their predictions. This lack of transparency tends to be a serious limitation in applications where the model may inform high-stakes decisions, such as treatment prioritization in healthcare.Our work introduces a new method, compatible with any machine learning model, to obtain uncertainty-aware predictions for survival times based on censored data. This method builds on a statistical framework called conformal inference, and it seeks to adjust a model’s output to generate a likely lower bound (as opposed to a single estimate) for the survival time of each individual, accompanied by a formal guarantee that the true survival time falls above this bound with a specified level of confidence.Unlike existing methods, which rely on overly restrictive assumptions about how the data are censored, our approach is more practical because it is designed to handle the kinds of incomplete data commonly seen in real-world applications of survival analysis."
Poster,Doubly Robust Fusion of Many Treatments for Policy Learning,https://ICML.cc//virtual/2025/poster/46608,"Ke Zhu, Jianing Chu, Ilya Lipkovich, Wenyu Ye, Shu Yang","Individualized treatment rules/recommendations (ITRs) aim to improve patient outcomes by tailoring treatments to the characteristics of each individual. However, in high-dimensional treatment settings, existing methods face significant challenges due to data sparsity within treatment groups and highly unbalanced covariate distributions across groups. To address these challenges, we propose a novel calibration-weighted treatment fusion procedure that robustly balances covariates across treatment groups and fuses similar treatments using a penalized working model. The fusion procedure ensures the recovery of latent treatment group structures when either the calibration model or the outcome model is correctly specified. In the fused treatment space, practitioners can seamlessly apply state-of-the-art ITR learning methods with the flexibility to utilize a subset of covariates, thereby achieving robustness while addressing practical concerns such as fairness. We establish theoretical guarantees, including consistency, the oracle property of treatment fusion, and regret bounds when integrated with multi-armed ITR learning methods such as policy trees. Simulation studies show superior group recovery and policy value compared to existing approaches. We illustrate the practical utility of our method using EHR-derived data from patients with Chronic Lymphocytic Leukemia and Small Lymphocytic Lymphoma.","Personalized medicine aims to recommend treatments that are tailored to the specific characteristics of each patient. However, when faced with numerous treatment options, deriving individualized treatment rules (ITRs) from limited data can be challenging. Small sample sizes in some treatment groups, coupled with variations across patient populations, often result in unreliable decisions.We introduce a novel statistical method that tackles both challenges by adjusting for baseline differences and combining treatments that yield similar outcomes. This is achieved in two stages: first, by calibrating patient characteristics across treatment groups, and then by grouping together similar treatments. A major benefit is its double robustness—our method remains valid as long as either the treatment model or the outcome model is correct, which provides reliability even amid messy real-world healthcare data.This strategy assists physicians in making optimal use of the data at hand, particularly in intricate clinical contexts with many treatment options. It leads to more consistent and interpretable treatment recommendations, particularly in fields like oncology, where diverse treatment options exist but patient information may be limited. In both simulations and real-world leukemia data, our method enhances the identification of treatment groups and promotes more informed, patient-centered care."
Poster,Do Vision-Language Models Really Understand Visual Language?,https://ICML.cc//virtual/2025/poster/44858,"Yifan Hou, Buse Giledereli, Yilei Tu, Mrinmaya Sachan","Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an image. The symbolic nature of diagrams presents significant challenges for building models capable of understanding them. Yet, recent studies seem to suggest that Large Vision-Language Models (LVLMs) can even tackle complex reasoning tasks involving diagrams. In this paper, we investigate this phenomenon by developing a comprehensive test suite to evaluate the diagram comprehension capability of LVLMs. Our test suite uses a variety of questions focused on concept entities and their relationships over a set of synthetic as well as real diagrams across several domains to evaluate the recognition and reasoning abilities of models. Our evaluation of six LVLMs shows that while these models can accurately identify and reason about entities, their ability to understand relationships is notably limited. Further testing reveals that the decent performance on diagram understanding largely stems from leveraging their background knowledge as shortcuts to identify and reason about the relational information. Thus, we conclude that LVLMs have a limited capability for genuine diagram understanding, and their impressive performance in diagram reasoning is an illusion emanating from other confounding factors, such as the background knowledge in the models.","Diagrams help us understand complex ideas by using symbols and spatial layouts instead of words. While recent AI systems, called Vision-Language Models (VLMs), appear to solve diagram-related tasks impressively, our research finds this success may be misleading. We built a broad evaluation suite to test whether these models truly understand diagrams. Our results show that while VLMs are good at spotting objects in diagrams, they often fail to grasp how those objects relate to each other. Instead, they rely on memorized knowledge rather than genuinely interpreting the diagrams. This raises concerns about how well these models really ""understand"" visual information, and calls for more rigorous testing in future AI systems."
Poster,Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective,https://ICML.cc//virtual/2025/poster/46497,"Zeyu Jia, Alexander Rakhlin, Tengyang Xie","Process and outcome supervision represent two fundamental approaches to   reinforcement learning, especially for complex reasoning tasks in large language models. While process supervision offers intuitive advantages for long-term credit assignment, the precise relationship between these paradigms has remained an open question. Conventional wisdom suggests that outcome supervision is fundamentally more challenging due to the trajectory-level coverage problem, leading to significant investment in collecting fine-grained process supervision data.In this paper, we provide a possible theoretical resolution to this debate. Perhaps surprisingly, our main theorem shows that: *under standard data coverage assumptions, reinforcement learning through outcome supervision is no more statistically difficult than through process supervision*. At the core of this result lies the novel *Change of Trajectory Measure Lemma*---a powerful technical tool that bridges return-based trajectory measure and step-level distribution shift. Furthermore, for settings with access to a verifier or a rollout capability, we prove that any policy's advantage function can serve as an optimal process reward model, providing a simple yet powerful connection between outcome and process supervision. These findings suggest that the empirically observed performance gap between outcome and process supervision likely stems from algorithmic limitations rather than inherent statistical difficulties, potentially transforming how we approach data and algorithm design for reinforcement learning.",Process supervision and outcome supervision are two key types of data used in reasoning tasks. Our findings show that the statistical difficulty of learning from these two data types is not significantly different. This suggests that models trained using process supervision data—often easier to obtain—can perform just as well as those trained with outcome supervision data.
Poster,Do We Really Need Message Passing in Brain Network Modeling?,https://ICML.cc//virtual/2025/poster/45642,"Liang Yang, Yuwei Liu, Jiaming Zhuo, Di Jin, Chuan Wang, Zhen Wang, Xiaochun Cao","Brain network analysis plays a critical role in brain disease prediction and diagnosis. Graph mining tools have made remarkable progress. Graph neural networks (GNNs) and Transformers, which rely on the message-passing scheme, recently dominated this field due to their powerful expressive ability on graph data. Unfortunately, by considering brain network construction using pairwise Pearson’s coefficients between any pairs of ROIs, model analysis and experimental verification reveal that *the message-passing under both GNNs and Transformers can NOT be fully explored and exploited*. Surprisingly, this paper observes the significant performance and efficiency enhancements of the Hadamard product compared to the matrix product, which is the matrix form of message passing, in processing the brain network. Inspired by this finding, a novel Brain Quadratic Network (BQN) is proposed by incorporating quadratic networks, which possess better universal approximation properties. Moreover, theoretical analysis demonstrates that BQN implicitly performs community detection along with representation learning. Extensive evaluations verify the superiority of the proposed BQN compared to the message-passing-based brain network modeling. Source code is available at [https://github.com/LYWJUN/BQN-demo](https://github.com/LYWJUN/BQN-demo).","Researchers have been using advanced models like Graph Neural Networks (GNNs) and Graph Transformers (GTs) to study how brain regions communicate. These models act like ""brain detectives,"" tracking how different areas share information—imagine students (regions) passing notes in the classroom (brain)! However, most brain network data are built using simple statistical measures—such as how similar activity levels are between different regions of the brain—which already capture communication between brain regions. In this work, we revisit these models and reveal that their core mechanisms might not be essential: all of them over-rely on statistical measures. Therefore, we propose a simple Brain Quadratic Network (BQN) directly based on statistical measures for brain modeling. In theory, it performs even better as it automatically learns how brain areas naturally collaborate (like how friend groups form at school). Our experiments show that BQN outperforms complex models in accuracy and speed."
