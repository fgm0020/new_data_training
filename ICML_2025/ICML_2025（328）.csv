type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for Molecular Ground-State Conformation Prediction,https://ICML.cc//virtual/2025/poster/46561,"Fanmeng Wang, Minjie Cheng, Hongteng Xu","Predicting molecular ground-state conformation (i.e., energy-minimized conformation) is crucial for many chemical applications such as molecular docking and property prediction. Classic energy-based simulation is time-consuming when solving this problem, while existing learning-based methods have advantages in computational efficiency but sacrifice accuracy and interpretability.In this work, we propose a novel and effective method to bridge the energy-based simulation and the learning-based strategy, which designs and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called WGFormer, for ground-state conformation prediction. Specifically, our method tackles this task within an auto-encoding framework, which encodes low-quality conformations by the proposed WGFormer and decodes corresponding ground-state conformations by an MLP.The architecture of WGFormer corresponds to Wasserstein gradient flows --- it optimizes conformations by minimizing an energy function defined on the latent mixture models of atoms, thereby significantly improving performance and interpretability.Extensive experiments demonstrate that our method consistently outperforms state-of-the-art competitors, providing a new and insightful paradigm to predict ground-state conformation.The code is available at https://github.com/FanmengWang/WGFormer.","How can the molecular ground-state conformation, which represents the most stable 3D structure corresponding to the energy-minimized state on the potential energy surface, be accurately and efficiently obtained?In this work, we propose WGFormer, a Wasserstein gradient flow-driven SE(3)-Transformer, to tackle this task.Specifically, it can be interpreted as Wasserstein gradient flows, which optimizes molecular conformation by minimizing a physically reasonable energy function defined on the latent mixture models of atoms, thereby significantly improving performance and interpretability. Extensive experiments and analyses comprehensively validate its rationality and superiority, thus providing a new and insightful paradigm for molecular ground-state conformation prediction."
Poster,What can large language models do for sustainable food?,https://ICML.cc//virtual/2025/poster/44547,"Anna Thomas, Adam Yee, Andrew Mayne, Maya Mathur, Dan Jurafsky, Kristina Gligoric","Food systems are responsible for a third of human-caused greenhouse gas emissions. We investigate what Large Language Models (LLMs) can contribute to reducing the environmental impacts of food production. We define a typology of design and prediction tasks based on the sustainable food literature and collaboration with domain experts, and evaluate six LLMs on four tasks in our typology. For example, for a sustainable protein design task, food science experts estimated that collaboration with an LLM can reduce time spent by 45% on average, compared to 22% for collaboration with another expert human food scientist. However, for a sustainable menu design task, LLMs produce suboptimal solutions when instructed to consider both human satisfaction and climate impacts. We propose a general framework for integrating LLMs with combinatorial optimization to improve reasoning capabilities. Our approach decreases emissions of food choices by 79% in a hypothetical restaurant while maintaining participants' satisfaction with their set of choices. Our results demonstrate LLMs' potential, supported by optimization techniques, to accelerate sustainable food development and adoption.","Food systems are responsible for a third of human-caused greenhouse gas emissions. We investigate what Large Language Models (LLMs) can contribute to reducing the environmental impacts of food production. We define a typology of design and prediction tasks based on the sustainable food literature and collaboration with domain experts, and evaluate six LLMs on four tasks in our typology. For example, for a sustainable protein design task, food science experts estimated that collaboration with an LLM can reduce time spent by 45% on average, compared to 22% for collaboration with another expert human food scientist. However, for a sustainable menu design task, LLMs produce suboptimal solutions when instructed to consider both human satisfaction and climate impacts. We propose a general framework for integrating LLMs with optimization methods to improve reasoning capabilities. Our approach decreases emissions of food choices by 79% in a hypothetical restaurant while maintaining participants' satisfaction with their set of choices. Our results demonstrate LLMs' potential, supported by optimization techniques, to accelerate sustainable food development and adoption. Our code is available at https://github.com/thomasat/llms-sustainable-food."
Poster,What Do Learning Dynamics Reveal About Generalization in LLM Mathematical Reasoning?,https://ICML.cc//virtual/2025/poster/44724,"Katie Kang, Amrith Setlur, Dibya Ghosh, Jacob Steinhardt, Claire Tomlin, Sergey Levine, Aviral Kumar","Modern large language models (LLMs) excel at fitting finetuning data, but often struggle on unseen examples. In order to teach models genuine reasoning abilities rather than superficial pattern matching, our work aims to better understand how the learning dynamics of LLM finetuning shapes downstream generalization. Our analysis focuses on reasoning tasks, whose problem structure allows us to distinguish between memorization (the exact replication of reasoning steps from the training data) and performance (the correctness of the final solution). We find that a model's performance on test prompts can be effectively characterized by a training metric we call pre-memorization train accuracy: the accuracy of model samples on training queries before they begin to copy the exact reasoning steps from the training set. On the dataset level, this metric is able to almost perfectly predict test accuracy, achieving $R^2$ of $\geq 0.9$ across various models (Llama3 8B, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On a per-example level, this metric is also indicative of whether individual model predictions are robust to perturbations in the training query. By connecting a model's learning dynamics to test performance, pre-memorization train accuracy can inform training decisions, such as the makeup of the training data. Our experiments on data curation show that prioritizing examples with low pre-memorization accuracy leads to 1.5-2x improvements in data efficiency compared to i.i.d. data scaling and other data scaling techniques.","Large Language Models often excel at mimicking training data rather than genuinely learning problem-solving skills, leading to poor performance on new problems. Our research introduces ""pre-memorization train accuracy""—a measure of how well models solve problems before they start copying training solutions. This metric robustly predicts how well models will generalize to unseen reasoning tasks across various LLMs and datasets. By focusing training on examples with low pre-memorization accuracy, we can significantly improve data efficiency (1.5-2x), leading to LLMs with more genuine problem-solving abilities."
Poster,What Has a Foundation Model Found? Inductive Bias Reveals World Models,https://ICML.cc//virtual/2025/poster/44374,"Keyon Vafa, Peter Chang, Ashesh Rambachan, Sendhil Mullainathan","Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize.","Scientists have often made discoveries by first making predictions about the world around them. For example, astronomers like Kepler noticed geometric patterns that could be used to pinpoint the future locations of planets in the night sky. Newton would later expand on these results to develop Newtonian mechanics, fundamental laws that could not only predict the movement of planets but also explain physical properties across the universe. Similarly, modern AI systems known as foundation models can predict the next item in a sequence -- whether that sequence is words in a sentence or positions of planets. But does that predictive skill mean the model truly understands the deeper rules that govern the world? This paper develops a test for assessing these ""world models"" of foundation models. We find that most often, foundation models that excel at making predictions have not uncovered the laws that govern them. We illustrate this by training a foundation model on planetary orbits, and show that it uncovers a warped understanding of the world with no resemblance to Newtonian mechanics. So far, foundation models are not capable of making the transition humans are capable of making: from good predictions to accurate world models."
Poster,What If We Recaption Billions of Web Images with LLaMA-3?,https://ICML.cc//virtual/2025/poster/45764,"Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, Cihang Xie","Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and $\textit{open-sourced}$ LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption ~1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe an average of 3.1% enhanced zero-shot performance cross four cross-modal retrieval tasks using a mixed set of the original and our captions. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. Our project page is https://www.haqtu.me/Recap-Datacomp-1B/.","To understand images usually requires a lot of text describing what’s in those images — captions. But collecting billions of content rich image-caption pairs to train models is expensive and time-consuming. To address this, we created Recap-DataComp-1B, the first publicly available dataset with one billion synthetic captions generated by a powerful large language model, LLaMA-3.While generating captions isn’t new, doing it at this scale is. This massive dataset helps researchers train and evaluate systems that connect images and language, like those that generate pictures from text or find images that match a written description. Our experiments show that models trained on Recap-DataComp-1B perform better at understanding long and complex image-text relationships.By releasing this dataset to the public, we hope to accelerate progress in multimodal systems that learn from both images and text — and make cutting-edge tools more accessible to everyone. We believe this work sets a new standard for building large-scale, high-quality datasets with synthetic data."
Poster,What Limits Bidirectional Model's Generative Capabilities? A Uni-Bi-Directional Mixture-of-Expert Method For Bidirectional Fine-tuning,https://ICML.cc//virtual/2025/poster/44255,"Zuchao Li, Yonghua Hei, Qiwei Li, Lefei Zhang, Ping Wang, hai zhao, qi baoyuan, Liu Guoming","Large Language Models (LLMs) excel in generation tasks, yet their causal attention mechanisms limit performance in embedding tasks. While bidirectional modeling may enhance embeddings, naively fine-tuning unidirectional models bidirectionally severely degrades generative performance.To investigate this trade-off, we analyze attention weights as dependence indicators and find that bidirectional fine-tuning increases subsequent dependence, impairing unidirectional generation. Through systematic Transformer module evaluations, we discover the FFN layer is least affected by such dependence.  Leveraging this discovery, we propose UBMoE-LLM, a novel Uni-Bi-directional Mixture-of-Experts LLM, which integrates the original unidirectional FFN with a bidirectionally fine-tuned FFN via unsupervised contrastive learning.  This MoE-based approach enhances embedding performance while preserving robust generation.Extensive experiments across diverse datasets and model scales validate our attention dependence metric and demonstrate UBMoE-LLM’s superior generative quality and reduced hallucination. Code is available at: https://github.com/heiyonghua/ubmoe_llm.","Large Language Models (LLMs) excel in generation tasks, yet their causal attention mechanisms limit performance in embedding tasks. While bidirectional modeling may enhance embeddings, naively fine-tuning unidirectional models bidirectionally severely degrades generative performance. To investigate this trade-off, we analyze attention weights as dependence indicators and find that bidirectional fine-tuning increases subsequent dependence, impairing unidirectional generation. Through systematic Transformer module evaluations, we discover the FFN layer is least affected by such dependence. Leveraging this discovery, we propose UBMoE-LLM, a novel Uni-Bi-directional Mixture-of-Experts LLM, which integrates the original unidirectional FFN with a bidirectionally fine-tuned FFN via unsupervised contrastive learning. This MoE-based approach enhances embedding performance while preserving robust generation. Extensive experiments across diverse datasets and model scales validate our attention dependence metric and demonstrate UBMoE-LLM’s superior generative quality and reduced hallucination."
Poster,What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities,https://ICML.cc//virtual/2025/poster/46463,"Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, liyunfei, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang","As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it improves generalization across environments. We conduct multidimensional evaluations for virtual agents, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at https://omni-bench.github.io.","Virtual agents are like digital assistants that can perform various tasks. As these agents become more advanced, we need better ways to test and improve them. Current testing methods have issues: task complexity is hard to control, they require lots of manual work, and they don't properly check different abilities.To fix this, we've created a new testing system called OmniBench. It's like a game-level designer that can automatically create different tasks with controlled difficulty levels, kind of like how a video game creates different challenges for players. Our system has 36k tasks of different kinds, like editing pictures or videos. These tasks are more like real-world problems and can be used to test virtual agents better. We've found that training agents on our tasks helps them do better in different situations. By testing these agents in many ways, we can find out their strengths and weaknesses and give direction on how to make them better in the future."
Poster,What Makes a Good Feedforward Computational Graph?,https://ICML.cc//virtual/2025/poster/43635,"Alex Vitvitskyi, João Madeira Araujo, Marc Lackenby, Petar Veličković","As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.","AI systems frequently have to process information that arrives _sequentially_ -- whether it's dealing with forecasting the dynamic structure of friendship connections of a social network, or future words in a sentence, or anything in between. A key defining property of contemporary efficient approaches to such tasks is processing this data in a _feedforward_ manner: explicitly disallowing information from future snapshots of the data from influencing the past. But even with this constraint in place, we do not need to allow _all_ previous snapshots to directly interact with the current one: in fact, this might even be problematic when data is collected over long horizons, given several previous theoretical results. Naturally, we are interested in discovering a ""good"" set of allowed connections between data points under such a feedforward regime -- a _good feedforward computational graph_. In this work, we define two interesting but complementary metrics that quantify the properties of information transfer within a given feedforward graph. We make several theoretical connections between these metrics and known structural quantities of graphs, as well as analysing several well-known graph distributions from this perspective, both theoretically and empirically. To our surprise, we find that this issue was relatively understudied even in mathematics (in spite of its importance), and that many existing concepts helping us determine good graphs only apply to graphs with allowed backwards connections. As our research makes some of the first steps towards answering this question, we hope it meaningfully paves the way for works that come after."
Poster,What makes an Ensemble (Un) Interpretable?,https://ICML.cc//virtual/2025/poster/44253,"Shahaf Bassan, Guy Amir, Meirav Zehavi, Guy Katz","Ensemble models are widely recognized in the ML community for their limited interpretability. For instance, while a single decision tree is considered interpretable, ensembles of trees (e.g., boosted trees) are often treated as black-boxes. Despite this folklore recognition, there remains a lack of rigorous mathematical understanding of what particularly makes an ensemble (un)-interpretable, including how fundamental factors like the (1) *number*, (2) *size*, and (3) *type* of base models influence its interpretability. In this work, we seek to bridge this gap by applying concepts from computational complexity theory to study the challenges of generating explanations for various ensemble configurations. Our analysis uncovers nuanced complexity patterns influenced by various factors. For example, we demonstrate that under standard complexity assumptions like P$\neq$NP, interpreting ensembles remains intractable even when base models are of constant size. Surprisingly, the complexity changes drastically with the number of base models: small ensembles of decision trees are efficiently interpretable, whereas ensembles of linear models remain intractable, even with a constant number of models. We believe that our findings provide a more robust foundation for understanding the interpretability of ensembles, emphasizing the benefits of examining it through a computational complexity lens.","Ensemble models - where multiple smaller base-models are combined to make predictions - are commonly used in ML and are known for being hard to interpret. For example, a single decision tree is fairly easy to understand, but when many trees are combined (like in boosted trees), the model is usually treated as a ""black box."" While this idea is widely accepted, we still lack a clear mathematical understanding of *why* these models are difficult to interpret.In this work, we take a step toward answering that question. Using tools from *computational complexity* theory, we study how hard it is to explain the predictions of different types of ensemble models. We look at how factors like the **number**, **size**, and **type** of the individual models within the ensemble affect how difficult it is to generate explanations.Our findings show a high versatility of results. For instance, even if each base-model in the ensemble is very small, explaining the whole ensemble can still be highly difficult. Interestingly, the *type* of base-model within the ensemble matters a lot - for example, ensembles with just a few decision trees can be interpreted efficiently, but an ensemble with even a significantly small number of linear models is hard to explain.By analyzing these challenges through the lens of computational complexity, our work helps lay a more solid foundation for understanding when and why ensemble models are (un) interpretable."
Poster,What Makes In-context Learning Effective for Mathematical Reasoning,https://ICML.cc//virtual/2025/poster/43714,"Jiayu Liu, Zhenya Huang, Chaokun Wang, Xunpeng Huang, Chengxiang Zhai, Enhong Chen","Owing to the capability of in-context learning, large language models (LLMs) have shown impressive performance across diverse mathematical reasoning benchmarks. However, we find that few-shot demonstrations can sometimes bring negative performance and their effectiveness on LLMs' reasoning abilities remains unreliable. To this end, in this paper, we aim to theoretically analyze the impact of in-context demonstrations on LLMs' reasoning performance. We prove that the reasoning efficacy (measured by empirical prediction loss) can be bounded by an \emph{LLM-oriented semantic similarity} and an \emph{inference stability of demonstrations}, which is general for both one-shot and few-shot scenarios. Based on this finding, we propose a straightforward, generalizable, and low-complexity demonstration selection method named LMS3. It facilitates to select the most pertinent samples for different LLMs and includes a novel demonstration rejection mechanism to automatically filter out samples that are unsuitable for few-shot learning. Through experiments on three representative benchmarks, two LLM backbones, and multiple few-shot settings, we verify that our LMS3 has superiority and achieves consistent improvements on all datasets, which existing methods have been unable to accomplish. Our code is available at \url{https://github.com/Ljyustc/LMS3}.","Large language models (LLMs), like GPT-4, can solve math problems just by seeing a few examples — a skill called in-context learning. But surprisingly, giving these models these examples doesn't always help — sometimes it even hurts performance. Why?Our research provides a theoretical explanation. We show that the effectiveness of examples depends on two key factors: how semantically similar they are to the new problem, and how stable the model's reasoning is on the examples themselves. This helps us understand when and why examples help or hurt — moving beyond trial-and-error toward a more principled understanding. Based on this insight, we introduce LMS3, a simple and general method that selects the most helpful examples and filters out harmful ones.Our work provides deeper insights into how LLMs actually reason with examples — shedding light on the inner workings of these powerful models. This makes LLMs more trustworthy for solving math problems and could benefit broader automated reasoning tasks."
