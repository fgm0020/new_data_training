type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Global Optimization with a Power-Transformed Objective and Gaussian Smoothing,https://ICML.cc//virtual/2025/poster/46360,Chen Xu,"We propose a novel method, namely Gaussian Smoothing with a Power-Transformed Objective (GS-PowerOpt), that solves global optimization problems in two steps: (1) perform a (exponential) power-$N$ transformation to the not necessarily differentiable objective $f:\mathbb{R}^d\rightarrow \mathbb{R}$ and get $f_N$, and (2) optimize the Gaussian-smoothed $f_N$ with stochastic approximations. Under mild conditions on $f$, for any $\delta>0$, we prove that with a sufficiently large power $N_\delta$, this method converges to a solution in the $\delta$-neighborhood of $f$'s global optimum point, at the iteration complexity of $O(d^4\varepsilon^{-2})$. If we require that $f$ is differentiable and further assume the Lipschitz condition on $f$ and its gradient, the iteration complexity reduces to $O(d^2\varepsilon^{-2})$, which is significantly faster than the standard homotopy method. In most of the experiments performed, our method produces better solutions than other algorithms that also apply the smoothing technique.","We propose a novel method, namely GS-PowerOpt, for finding the global maximum point $x^*$ of a given function $f(x)$ with multiple maxima. It has important applications in machine learning problems, such as model training.GS-PowerOpt constructs a new objective function $F_N(\mu)$, and then looks for its maximum point $\mu^\*$ by small and random adjustments (i.e., stochastic gradient ascent). The construction of $F_N(\mu)$ consists of two transforms from the original objective $f$. One is the power-$N$ transform, which is for decreasing the distance between $\mu^\*$ and $x^\*$. The other one is Gaussian smoothing, which is for removing the local maximum point of $F_N(\mu)$ that is far from $x^\*$ (so that the search will not be trapped in a local maximum point far from $x^\*$). Under mild conditions on $f$, we have shown with rigorous proofs that, for any small neighborhood $U$ of $x^*$, there exists a sufficiently large $N$ such that all the maximum points of $F_N(\mu)$ lie in $U$.In theory, GS-PowerOpt is significantly faster than the standard method (standard homotopy) in this area. Our experiments also show than it outperforms the compared methods that also apply the smoothing transform."
Poster,GMAIL: Generative Modality Alignment for generated Image Learning,https://ICML.cc//virtual/2025/poster/43745,"Shentong Mo, Sukmin Yun","Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined \textit{GMAIL}, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.","Generative models have made it possible to synthesize highly realistic data, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, integrating generated data into training pipelines as real data often leads to performance drops due to mismatches between real and synthetic domains.In this paper, we propose a novel framework for discriminative use of generated images that explicitly treats generated images as a distinct data type from real images.Instead of indiscriminately replacing real images with generated ones in pixel space, our approach aims to align these two different types of images in latent space.Specifically, we first fine-tune a model exclusively on generated images using a alignment loss, and then use this aligned model to further train various vision-language models with generated images.By aligning the generated data type in the latent space, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks.Our framework can be easily incorporated into recent large-scale vision-language models, and we demonstrate its efficacy in extensive experiments."
Poster,Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44839,"Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng","Offline multi-task reinforcement learning aims to learn a unified policy capable of solving multiple tasks using only pre-collected task-mixed datasets, without requiring any online interaction with the environment. However, it faces significant challenges in effectively sharing knowledge across tasks. Inspired by the efficient knowledge abstraction observed in human learning, we propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed to extract and utilize reusable skills to enhance knowledge transfer and task performance. Our approach uncovers reusable skills through a goal-oriented skill extraction process and leverages vector quantization to construct a discrete skill library. To mitigate class imbalances between broadly applicable and task-specific skills, we introduce a skill enhancement phase to refine the extracted skills. Furthermore, we integrate these skills using hierarchical policy learning, enabling the construction of a high-level policy that dynamically orchestrates discrete skills to accomplish specific tasks. Extensive experiments on diverse robotic manipulation tasks within the MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.","Learning to solve multiple tasks simultaneously is a major challenge in artificial intelligence, especially when relying only on pre-collected data without live trial and error. This process, known as offline multi-task reinforcement learning, struggles to share useful knowledge across tasks. Inspired by the way humans build reusable skills, we developed a new approach called Goal-Oriented Skill Abstraction (GO-Skill). It breaks down complex problems into smaller, reusable skills that can be applied across different situations. These skills are organized into a library, making it easier for the system to choose the right skills for each task. GO-Skill then combines these skills into high-level decision-making strategies, allowing artificial systems to handle varied challenges more effectively. In our tests with robotic systems, this approach showed promising results, helping machines learn more efficiently and adapt to a variety of challenges."
Poster,Goal-Space Planning with Subgoal Models,https://ICML.cc//virtual/2025/poster/46711,"Chunlok Lo, Kevin Roice, Parham Mohammad Panahi, Scott Jordan, Adam White, Gabor Mihucz, Farzane Aminmansour, Martha White","This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a given set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning, and avoids learning the transition dynamics entirely. We show that our GSP algorithm can propagate value from an abstract space in a manner that helps a variety of base learners learn significantly faster in different domains.",
Poster,Going Deeper into Locally Differentially Private Graph Neural Networks,https://ICML.cc//virtual/2025/poster/46579,"Longzhu He, Chaozhuo Li, Peng Tang, Sen Su","Graph Neural Networks (GNNs) have demonstrated superior performance in a variety of graph mining and learning tasks. However, when node representations involve sensitive personal information or variables related to individuals, learning from graph data can raise significant privacy concerns. Although recent studies have explored local differential privacy (LDP) to address these concerns, they often introduce significant distortions to graph data, severely degrading private learning utility (e.g., node classification accuracy). In this paper, we present UPGNET, an LDP-based privacy-preserving graph learning framework that enhances utility while protecting user data privacy. Specifically, we propose a three-stage pipeline that generalizes the LDP protocols for node features, targeting privacy-sensitive scenarios. Our analysis identifies two key factors that affect the utility of privacy-preserving graph learning: *feature dimension* and *neighborhood size*. Based on the above analysis, UPGNET enhances utility by introducing two core layers: High-Order Aggregator (HOA) layer and the Node Feature Regularization (NFR) layer. Extensive experiments on real-world datasets indicate that UPGNET significantly outperforms existing methods in terms of both privacy protection and learning utility.","Graph-based machine learning models are increasingly used to analyze social networks, biological systems, and other connected data. But in doing so, they often process sensitive personal information, raising serious privacy concerns. Existing solutions that try to protect user privacy by adding noise to the data often make these models much less accurate. Our research addresses this trade-off between privacy and performance. We developed a new framework called UPGNET that protects users’ data while keeping the model effective. It works by identifying two key factors—the dimensionality of each node’s features and the size of its local neighborhood—and improves graph learning utility by addressing both. As a result, UPGNET significantly improves the accuracy of graph learning while preserving strong privacy guarantees. This makes it a promising step toward safer, more trustworthy AI systems that can learn from sensitive data without exposing it."
Poster,GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction,https://ICML.cc//virtual/2025/poster/43591,"Muleilan Pei, Shaoshuai Shi, Lu Zhang, Peiliang Li, Shaojie Shen","Trajectory prediction for surrounding agents is a challenging task in autonomous driving due to its inherent uncertainty and underlying multimodality. Unlike prevailing data-driven methods that primarily rely on supervised learning, in this paper, we introduce a novel **G**raph-**o**riented **I**nverse **R**einforcement **L**earning (GoIRL) framework, which is an IRL-based predictor equipped with vectorized context representations. We develop a feature adaptor to effectively aggregate lane-graph features into grid space, enabling seamless integration with the maximum entropy IRL paradigm to infer the reward distribution and obtain the policy that can be sampled to induce multiple plausible plans. Furthermore, conditioned on the sampled plans, we implement a hierarchical parameterized trajectory generator with a refinement module to enhance prediction accuracy and a probability fusion strategy to boost prediction confidence. Extensive experimental results showcase our approach not only achieves state-of-the-art performance on the large-scale Argoverse & nuScenes motion forecasting benchmarks but also exhibits superior generalization abilities compared to existing supervised models.","Accurately predicting how surrounding traffic participants, such as cars, cyclists, and pedestrians, will move is essential for safe autonomous driving, yet remains a major challenge due to the inherently multi-modal nature of human behavior. Most existing approaches rely on supervised learning, which tends to overfit to seen behaviors and struggles to generalize to new or unseen driving scenarios. To address this, we propose GoIRL, a novel trajectory prediction framework built on Inverse Reinforcement Learning (IRL), a technique that infers the goals and intentions behind observed movements, rather than merely imitating them. GoIRL leverages a vectorized lane-graph representation to capture rich road context and introduces a feature adaptor that bridges this information into a grid-based format compatible with the IRL paradigm. This enables the model to learn reward functions that guide realistic and diverse future motion plans. We further develop a hierarchical trajectory generator with refinement modules that boost both prediction accuracy and confidence. GoIRL achieves state-of-the-art performance on large-scale motion forecasting benchmarks and demonstrates superior generalization to changes in drivable areas than current supervised models. Our research represents a significant advancement towards enhanced and generalizable foresight, which is critical for the safe deployment of autonomous vehicles in the real world."
Poster,GPEN: Global Position Encoding Network for Enhanced Subgraph Representation Learning,https://ICML.cc//virtual/2025/poster/46329,"Nannan Wu, Yuming Huang, Yiming Zhao, Jie Chen, Wenjun Wang","Subgraph representation learning has attracted growing interest due to its wide applications in various domains. However, existing methods primarily focus on local neighborhood structures while overlooking the significant impact of global structural information, in particular the influence of multi-hop neighbors beyond immediate neighborhoods. This presents two key challenges: how to effectively capture the structural relationships between distant nodes, and how to prevent excessive aggregation of global structural information from weakening the discriminative ability of subgraph representations.To address these challenges, we propose GPEN (Global Position Encoding Network). GPEN leverages a hierarchical tree structure to encode each node's global position based on its path distance to the root node, enabling a systematic way to capture relationships between distant nodes. Furthermore, we introduce a boundary-aware convolution module that selectively integrates global structural information while maintaining the unique structural patterns of each subgraph. Extensive experiments on eight public datasets identify that GPEN significantly outperforms state-of-the-art methods in subgraph representation learning.","Imagine you're a detective trying to identify suspicious groups of people by only looking at who talks to whom within each group, but ignoring how these groups connect to the rest of the city—you'd miss important clues like whether they're getting money from known criminals several steps away. This is exactly the problem computers face when analyzing small sections of interconnected data, like social networks or financial transactions: they focus on local patterns but miss the bigger picture. We built a system called GPEN that gives each person a ""global address"" based on their position in the entire network, like noting whether someone lives in the downtown business district versus a remote suspicious area. Our approach helps computers spot the difference between groups that look similar up close but are actually very different when you consider their connections to the wider world. This breakthrough significantly improves how computers detect fraud, understand biological systems, and analyze any situation where small groups are embedded within larger networks."
Poster,GPTAQ: Efficient Finetuning-Free Quantization for Asymmetric Calibration,https://ICML.cc//virtual/2025/poster/45331,"Yuhang Li, Ruokai Yin, Donghyun Lee, Shiting Xiao, Priyadarshini Panda","We introduce GPTAQ, a novel finetuning-free quantization method for compressing large-scale transformer architectures.Unlike the previous GPTQ method, which independently calibrates each layer, we always match the quantized layer's output to the exact output in the full-precision model, resulting in a scheme that we call *asymmetric calibration*. Such a scheme can effectively reduce the quantization error accumulated in previous layers. We analyze this problem using optimal brain compression to derive a close-formed solution. The new solution explicitly minimizes the quantization error as well as the accumulated asymmetry error. Furthermore, we utilize various techniques to parallelize the solution calculation, including channel parallelization, neuron decomposition, and Cholesky reformulation for matrix fusion. As a result, GPTAQ is easy to implement, simply using 20 more lines of code than GPTQ but improving its performance under low-bit quantization. Remarkably, on a single GPU, we quantize a 405B language transformer as well as EVA-02—the rank first vision transformer that achieves 90% pretraining Imagenet accuracy. Code is available at [Github](https://github.com/Intelligent-Computing-Lab-Yale/GPTAQ).","We provide an approach to compress the existing large language models and other vision foundation models. We wondered if the existing method could follow the original model behavior when compressing them, especially in the middle of the process, where we have to match the compressed model output with the original model output. We give a closed-form solution to this problem, and manage to execute the algorithm in a very efficient way. Moreover, our solution can be integrated into the widely supported GPTQ APIs, using only 20 more lines of code, and improves their performance."
Poster,GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning,https://ICML.cc//virtual/2025/poster/46264,"Zhun Mou, Bin Xia, Zhengchao Huang, Wenming Yang, Jiaya Jia","Recent great advances in video generation models have demonstrated their potential to produce high-quality videos, bringing challenges to effective evaluation. Unlike human evaluation, existing automated evaluation metrics lack high-level semantic understanding and reasoning capabilities for video, thus making them infeasible and unexplainable. To fill this gap, we curate **GRADEO-Instruct**, a multi-dimensional T2V evaluation instruction tuning dataset, including 3.3k videos from over 10 existing video generation models and multi-step reasoning assessments converted by 16k human annotations. We then introduce **GRADEO**, one of the first specifically designed video evaluation models, which **grades** AI-generated **videos** for explainable scores and assessments through multi-step reasoning. Experiments show that our method aligns better with human evaluations than existing methods. Furthermore, our benchmarking reveals that current video generation models struggle to produce content that aligns with human reasoning and complex real-world scenarios. The models, datasets, and codes will be released soon.","Recent AI tools can create impressive videos, but it’s hard to tell how good these videos really are. Current automatic methods to judge video quality don’t understand the story or details in the video like humans do, so their ratings aren’t very reliable or easy to explain.To fix this, we gathered thousands of AI-made videos and asked many people to give detailed feedback on them, focusing on different aspects of video quality.Using this feedback, we built a new AI system called GRADEO that can score videos in a way that matches human opinions and explains why it gave that score.Our tests show that GRADEO judges videos better than other automatic methods. We also found that today’s AI video makers still have trouble creating videos that make sense in real life."
Poster,Gradient Aligned Regression via Pairwise Losses,https://ICML.cc//virtual/2025/poster/46628,"Dixian Zhu, Tianbao Yang, Livnat Jerby","Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample. Recent research endeavors have introduced novel perspectives by incorporating label similarity into regression through the imposition of additional pairwise regularization or contrastive learning on the latent feature space, demonstrating their effectiveness. However, there are two drawbacks to these approaches: (i) their pairwise operations in the latent feature space are computationally more expensive than conventional regression losses; (ii) they lack theoretical insights behind these methods. In this work, we propose GAR (Gradient Aligned Regression) as a competitive alternative method in label space, which is constituted by a conventional regression loss and two pairwise label difference losses for gradient alignment including magnitude and direction. GAR enjoys: i) the same level efficiency as conventional regression loss because the quadratic complexity for the proposed pairwise losses can be reduced to linear complexity; ii) theoretical insights from learning the pairwise label difference to learning the gradient of the ground truth function. We limit our current scope as regression on the clean data setting without noises, outliers or distributional shifts, etc. We demonstrate the effectiveness of the proposed method practically on two synthetic datasets and on eight extensive real-world tasks from six benchmark datasets with other eight competitive baselines. Running time experiments demonstrate the superior efficiency of the proposed GAR compared to existing methods with pairwise regularization or contrastive learning in the latent feature space. Additionally, ablation studies confirm the effectiveness of each component of GAR. The code is open sourced at https://github.com/DixianZhu/GAR.","In many real-world problems, like predicting a patient’s disease risk or estimating housing prices, machines are trained to make predictions from data — a process called regression. Traditional regression methods look at each example one by one, trying to predict each output as accurately as possible. However, they often miss important relationships between examples — like how two similar houses should have similar prices.This paper introduces a new way to teach machines to better understand relationships between data points. Instead of looking at each example separately, the proposed method compares pairs of examples to see how their differences in input features relate to differences in their outputs. Think of it like this: if one student studies more than another, and also scores higher, the model learns to align that pattern — it ""learns from comparisons.""The method, called Gradient-Aligned Regression (GAR) via Pairwise Losses, allows the computer to capture these kinds of trends more effectively. It does this efficiently, even with large datasets, and is theoretically related to capturing the gradients of the ground truth function."
