type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,The Global Convergence Time of Stochastic Gradient Descent in Non-Convex Landscapes: Sharp Estimates via Large Deviations,https://ICML.cc//virtual/2025/poster/46133,"Waïss Azizian, Franck Iutzeler, Jérôme Malick, Panayotis Mertikopoulos","In this paper, we examine the time it takes for stochastic gradient descent (SGD) to reach the global minimum of a general, non-convex loss function. We approach this question through the lens of large deviations theory and randomly perturbed dynamical systems, and we provide a tight characterization of the associated hitting times of SGD with matching upper and lower bounds. Our analysis reveals that the global convergence time of SGD is dominated by the most ""costly"" set of obstacles that the algorithm may need to overcome in order to reach a global minimizer, coupling in this way the geometry of the underlying loss landscape with the statistics of the noise entering the process. Finally, motivated by applications to the training of deep neural networks, we provide a series of refinements and extensions of our analysis to, among others, loss functions with no spurious local minima or ones with bounded depths.","The stochastic gradient algorithm (SGD) is widely used in the training of neural networks. However, despite its practical success, its behavior is still elusive due to the non-convexity of the objective. We propose to tackle the question of how fast does SGD attain a global optimum by relying on tools from the theory of large deviations. Our analysis builds on a careful estimation of transition times between critical points and enables matching upper- and lower-bounds.This work thus enables a better understanding of how SGD behaves for deep neural networks and opens the way towards practical improvements."
Poster,The Harder Path: Last Iterate Convergence for Uncoupled Learning in Zero-Sum Games with Bandit Feedback,https://ICML.cc//virtual/2025/poster/45419,"Côme Fiegel, Pierre Menard, Tadashi Kozuno, Michal Valko, Vianney Perchet","We study the problem of learning in zero-sum matrix games with repeated play and bandit feedback. Specifically, we focus on developing uncoupled algorithms that guarantee, without communication between players, convergence of the last-iterate to a Nash equilibrium. Although the non-bandit case has been studied extensively, this setting has only been explored recently, with a bound of $\mathcal{O}(T^{-1/8})$ on the exploitability gap. We show that, for uncoupled algorithms, guaranteeing convergence of the policy profiles to a Nash equilibrium is detrimental to the performances, with the best attainable rate being $\mathcal{O}(T^{-1/4})$ in contrast to the usual $\mathcal{O}(T^{-1/2})$ rate for convergence of the average iterates. We then propose two algorithms that achieve this optimal rate. The first algorithm leverages a straightforward tradeoff between exploration and exploitation, while the second employs a regularization technique based on a two-step mirror descent approach.","We study the design of efficient algorithms that learn optimal strategies for playing games. We focus on a setting where two independent algorithms learn by repeatedly playing against each other, without any communication.Most existing works assume that the algorithms can freely play against each other and only output a final good strategy. In this article, we add a constraint: the strategies must improve over time. Especially, the algorithms are not allowed to submit a potentially ""dumb"" strategy at a given time as a test. We show that this makes learning harder and propose two methods that are mathematically near-optimal.This research can be applied to learning actual games such as poker, but not only, as many important machine learning problems can be formulated this way. For example, when two language models propose a response, the winner can be the one chosen by the user."
Poster,The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions,https://ICML.cc//virtual/2025/poster/43629,"Wenbo Pan, Zhichao Liu, Qiguang Chen, Xiangyang Zhou, Yu Haining, Xiaohua Jia","Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective.","(1) Large language models (LLMs) like ChatGPT are trained to avoid producing harmful content by refusing problematic questions—a process known as “safety alignment.” However, it’s still unclear exactly how these models internally learn to refuse harmful prompts. (2) In our study, we looked inside the AI’s internal representations—its “mental space”—to understand how it decides when to reject harmful instructions. Using mathematical tools, we discovered that instead of using just one simple criterion, the model uses several independent factors simultaneously. One main factor strongly influences whether the model refuses dangerous requests, while smaller, secondary factors handle nuances like fictional storytelling or role-playing scenarios. (3) Understanding these hidden dimensions helps researchers improve safety training methods. Additionally, we found that certain trigger words in prompts can bypass the safety system by affecting these secondary dimensions. Our findings can inform future strategies to strengthen LLMs against attempts to circumvent their safety measures, ultimately making AI safer and more reliable for real-world use."
Poster,The Hidden Joules: Evaluating the Energy Consumption of Vision Backbones for Progress Towards More Efficient Model Inference,https://ICML.cc//virtual/2025/poster/45063,"Zeyu Yang, Wesley Armour","Deep learning has achieved significant success but poses increasing concerns about energy consumption and sustainability. Despite these concerns, there is a lack of understanding of their energy efficiency during inference. In this study, we conduct a comprehensive analysis of the inference energy consumption of 1,200 ImageNet classification models—the largest evaluation of its kind to date. Our findings reveal a steep decline in accuracy gains relative to the increase in energy usage, highlighting sustainability concerns in the pursuit of marginal improvements. We identify key factors contributing to energy consumption and demonstrate methods to improve energy efficiency. To promote more sustainable AI practices, we introduce an energy efficiency scoring system and develop an interactive web application that allows users to compare models based on accuracy and energy consumption. By providing extensive empirical data and practical tools, we aim to facilitate informed decision-making and encourage collaborative efforts in the development of energy-efficient AI technologies.","Deep learning powers many of the smart technologies we use today—from voice assistants to image recognition—but running these models can use a lot of energy. As artificial intelligence (AI) becomes more widely used, concerns are growing about its environmental impact, especially during inference (the phase when trained models are actually used to make predictions).In our research, we studied the energy use of 1,200 image classification models, making this the largest study of its kind so far. We found that while newer models often achieve slightly better accuracy, they tend to consume much more energy, raising questions about whether these small improvements are worth the environmental cost.We also identified what makes some models more energy-hungry than others and showed how certain design choices can improve energy efficiency. To help the AI community make more sustainable choices, we created an energy efficiency score and built a web tool that lets users compare models based on both their accuracy and energy use.Our goal is to provide practical tools and clear data to help researchers and developers build AI systems that are not only powerful but also environmentally responsible."
Poster,The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models Via Visual Information Steering,https://ICML.cc//virtual/2025/poster/46338,"Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, Dimitris Metaxas","Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) *gradual visual information loss* -- visually grounded tokens gradually become less favored throughout generation, and (2) *early excitation* -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer.(3) *hidden genuine information* -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference.Based on these insights, we propose **VISTA** (**V**isual **I**nformation **S**teering with **T**oken-logit **A**ugmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by about 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies. Code is available at https://github.com/LzVv123456/VISTA.","AI systems that can understand both images and text are becoming increasingly powerful, but they have a serious flaw: they often ""hallucinate"" by describing things that aren't actually in the images they're looking at. For example, when shown a photo of a baseball game, these systems might confidently describe objects or people that simply aren't there, making them unreliable for real-world applications. We discovered that as these AI models generate descriptions, they gradually lose track of visual information and instead rely too heavily on language patterns they learned during training. To fix this, we developed VISTA, a method that works like visual ""steering"" – it continuously reinforces what the AI actually sees in the image throughout the text generation process, while also using information from earlier processing layers where visual understanding is stronger. Our approach reduces hallucinations by about 40% across different AI models and tasks, without requiring any retraining of the systems. This makes vision-language AI more trustworthy and reliable for applications like medical image analysis, autonomous vehicles, and accessibility tools, helping ensure these powerful systems describe what they actually see rather than what they think they should see."
Poster,The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them),https://ICML.cc//virtual/2025/poster/46075,"Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang","Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role—a concept we call *role separation*—is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine *role-separation learning*: the process of teaching LLMs to robustly distinguish system and user tokens. Through a *simple, controlled experimental framework*, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing *invariant signals* that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, modifying position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.","Modern AI systems like ChatGPT handle multiple types of instructions simultaneously: system rules that define their behavior, user questions, and information from external tools. For these systems to work safely and reliably, they must clearly distinguish between trusted instructions (from the system) and potentially untrusted input (from users)—much like how a bank teller must distinguish between their manager's policies and customer requests.While existing training methods achieve impressive performance on standard tests, we discovered they succeed for the wrong reasons. Instead of truly learning to identify different roles, AI models rely on unreliable shortcuts: they assume certain types of tasks are always instructions, or they simply follow whatever text appears first in their input. This creates serious security risks—imagine if that bank teller started following customer instructions to override bank policies just because the customer mentioned ""account management.""We tested this by training AI models on safe examples, then evaluating them on tricky situations they hadn't seen before. The models consistently failed, revealing they hadn't learned genuine role separation but had simply memorized patterns from their training.Traditional solutions like showing the AI more varied examples only create temporary fixes—new shortcuts inevitably emerge. Instead, we developed a technique that strengthens the fundamental signals that should distinguish different roles. By modifying how the AI processes the position of different text segments, we help it learn clearer distinctions between trusted and untrusted inputs.This approach significantly improves AI reliability and security without compromising performance on normal tasks—a crucial advancement as AI systems are deployed in sensitive applications like healthcare and finance."
Poster,The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks,https://ICML.cc//virtual/2025/poster/44665,"Walter Mayor, Johan Obando-Ceron, Aaron Courville, Pablo Samuel Castro","The use of parallel actors for data collection has been an effective technique used in reinforcement learning (RL) algorithms. The manner in which data is collected in these algorithms, controlled via the number of parallel environments and the rollout length, induces a form of bias-variance trade-off; the number of training passes over the collected data, on the other hand, must strike a balance between sample efficiency and overfitting. We conduct an empirical analysis of these trade-offs on PPO, one of the most popular RL algorithms that uses parallel actors, and establish connections to network plasticity and, more generally, optimization stability. We examine its impact on network architectures, as well as the hyper-parameter sensitivity when scaling data. Our analyses indicate that larger dataset sizes can increase final performance across a variety of settings, and that scaling parallel environments is more effective than increasing rollout lengths. These findings highlight the critical role of data collection strategies in improving agent performance.","The use of parallel actors for data collection has been an effective technique used in reinforcement learning (RL) algorithms. The manner in which data is collected in these algorithms, controlled via the number of parallel environments and the rollout length, induces a form of bias-variance trade-off; the number of training passes over the collected data, on the other hand, must strike a balance between sample efficiency and overfitting. We conduct an empirical analysis of these trade-offs on PPO, one of the most popular RL algorithms that uses parallel actors, and establish connections to network plasticity and, more generally, optimization stability. We examine its impact on network architectures, as well as the hyper-parameter sensitivity when scaling data. Our analyses indicate that larger dataset sizes can increase final performance across a variety of settings, and that scaling parallel environments is more effective than increasing rollout lengths. These findings highlight the critical role of data collection strategies in improving agent performance."
Poster,The impact of uncertainty on regularized learning in games,https://ICML.cc//virtual/2025/poster/44383,"Pierre-Louis Cauvin, Davide Legacci, Panayotis Mertikopoulos","In this paper, we investigate how randomness and uncertainty influence learning in games. Specifically, we examine a perturbed variant of the dynamics of “follow-the-regularized-leader” (FTRL), where the players’ payoff observations and strategy updates are continually impacted by random shocks. Our findings reveal that, in a fairly precise sense, “uncertainty favors extremes”: in any game, regardless of the noise level, every player’s trajectory of play reaches an arbitrarily small neighborhood of a pure strategy in finite time (which we estimate). Moreover, even if the player does not ultimately settle at this strategy, they return arbitrarily close to some(possibly different) pure strategy infinitely often. This prompts the question of which sets of pure strategies emerge as robust predictions of learning under uncertainty. We show that (a) the only possible limits of the FTRL dynamics under uncertainty are pure Nash equilibria; and (b) a span of pure strategies is stable and attracting if and only if it is closed under better replies. Finally, we turn to games where the deterministic dynamics are recurrent—such as zero-sum games with interior equilibria—and show that randomness disrupts this behavior, causing the stochastic dynamics to drift toward the boundary on average.","We examine what happens when decision-makers are involved in a repeated decision process that repeats over time—players interact, compete, and make decisions, each aiming to maximize their individual rewards. The specific question we're asking is what happens in the presence of noise, randomness and uncertainty, that is, when players have a very hazy vision of their environment, and/or the behavior and objectives of other players in the game.In this case, we find that uncertainty favors extremes: after some time, players act in rigid, uncompromising ways. Yet, even in the face of uncertainty, not all rationality is lost to chance, and some patterns can still be anticipated: one can predict which extreme behaviors are more likely to emerge from the range of possible ones. In particular, players still gravitate toward behaviors that, while extreme, reflect the best possible compromise given the circumstances. However, such a compromise isn't always possible. In some scenarios, no stable agreement exists, and the players alternate between extremes, without ever getting close to a stable, equilibrium state."
Poster,The Importance of Being Lazy: Scaling Limits of Continual Learning,https://ICML.cc//virtual/2025/poster/44575,"Jacopo Graldi, Alessandro Breccia, Giulia Lanzillotta, Thomas Hofmann, Lorenzo Noci","Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete.In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between *lazy* and *rich* training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of *feature learning*, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and *transfers across model scales*. This work provides a unified perspective on the role of scale and feature learning in continual learning.","Neural networks are powerful learners when their tasks stay consistent, but they struggle when tasks change, often losing previously learned information—a problem known as catastrophic forgetting. Although catastrophic forgetting has been studied for a long time, it’s still unclear how best to prevent it. For instance, we don’t know if simply making networks larger helps them remember better, as research findings have been mixed.We systematically studied how network size and their level of adaptability—called “feature learning”—affect their ability to continuously remember changing tasks. Networks can be “lazy,” meaning they minimally adjust their internal settings, or “rich,” meaning they extensively modify themselves. Our experiments show that making networks larger only helps if they remain relatively “lazy.” Networks that extensively adapt their internal settings rapidly forget previously learned tasks, especially if the tasks are very different, and making these networks larger doesn’t prevent this forgetting.Our results demonstrate that maintaining some degree of laziness is essential for neural networks to retain knowledge over time. Practically, this insight helps researchers and engineers design better neural networks for real-world applications where tasks frequently change, such as adaptive robotics or lifelong learning systems."
Poster,The Jailbreak Tax: How Useful are Your Jailbreak Outputs?,https://ICML.cc//virtual/2025/poster/44418,"Kristina Nikolić, Luze Sun, Jie Zhang, Florian Tramer","Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs.In this paper, we ask whether the model outputs produced by existing jailbreaks are actually *useful*. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions?Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math).Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the *jailbreak tax*. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy.Overall, our work proposes jailbreak utility as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks.  We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax","Large language models (like ChatGPT) are designed to refuse certain questions, such as instructions for making a bomb, because they follow built-in safety rules. However, people have found ways to trick the model into giving such dangerous answers. This kind of attack is called a jailbreak.In this work, we ask an important question: even if a jailbreak gets around safety filters, is the answer it produces actually useful? For example, are produced bomb making instructions helpful and correct?To find out, we made the model refuse to answer safe questions like math or science. Then we used jailbreaks to force it to respond. Since we already knew the right answers, we could check how useful the replies were.We discovered that jailbreaks often make the model give much worse answers — sometimes almost completely wrong. We call this the jailbreak tax — the price you pay in quality when you break the rules.So even if an attack makes the chatbot talk, the answer might not be very helpful."
