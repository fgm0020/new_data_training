type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,RE-Bench: Evaluating Frontier AI R&D Capabilities of Language Model Agents against Human Experts,https://ICML.cc//virtual/2025/poster/46519,"Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Joshua Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Jun Koba Sato, William Saunders, Maksym Taran, Ben West, Elizabeth Barnes","Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, V1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-$k$ with varying time budgets and agent designs, and find that the best AI agents achieve a score 4× higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2× the score of the top AI agent when both are given 32 total hours (across different attempts).","We think that AI models will soon be extremely transformative in a very short amount of time. A key capability AIs will have is the ability to do the kind of research that goes into developing better AIs. We wanted to create tasks for AIs that measure how well they can do specific parts of such research. In order to understand how well they end up doing, we compare them to human experts, who are asked to do these tasks in the exact same environment and setup. We analyze the results comparing humans to AIs and discuss what factors might contribute or affect these results."
Poster,Recommendations with Sparse Comparison Data: Provably Fast Convergence for Nonconvex Matrix Factorization,https://ICML.cc//virtual/2025/poster/43663,"Suryanarayana Sankagiri, Jalal Etesami, Matthias Grossglauser","In this paper, we consider a recommender system that elicits user feedback through pairwise comparisons instead of ratings. We study the problem of learning personalised preferences from such comparison data via collaborative filtering. Similar to the classical matrix completion setting, we assume that users and items are endowed with low-dimensional latent features. These features give rise to user-item utilities, and the comparison outcomes are governed by a discrete choice model over these utilities. The task of learning these features is then formulated as a maximum likelihood problem over the comparison dataset. Despite the resulting optimization problem being nonconvex, we show that gradient-based methods converge exponentially to the latent features, given a warm start. Importantly, this result holds in a sparse data regime, where each user compares only a few pairs of items. Our main technical contribution is to extend key concentration results commonly used in matrix completion to our model. Simulations reveal that the empirical performance of the method exceeds theoretical predictions, even when some assumptions are relaxed. Our work demonstrates that learning personalised recommendations from comparison data is both computationally and statistically efficient.","Most recommendation systems, such as those used by Netflix or Amazon, ask users to rate items with stars or numbers. In this work, we explore an alternative approach where users express their preferences by choosing between two options, such as “Do you prefer A or B?” This type of feedback is often more natural and easier for users to provide. As with any recommendation system, the main challenge is to offer personalised suggestions for items a user has not yet seen.We adopt a well-established framework where the system builds hidden profiles for each user and item, which it uses to predict preferences. These profiles are learned by fitting model parameters to the observed data. Although this method works well in practice, it is difficult to analyse theoretically. The key challenges are that both user and item features are unknown (leading to nonconvexity), and each data point contains only binary comparison information (introducing nonlinearity).Our work is the first to provide theoretical guarantees that a simple and practical algorithm—gradient descent—can reliably learn personalised recommendations from comparison data. The main innovation lies in the careful use of classical probabilistic tools, in particular matrix concentration inequalities. Experiments support our findings, showing strong performance even beyond the idealised conditions assumed in the theory. These results suggest that learning from comparisons can be a practical and effective alternative to traditional rating-based recommendation systems."
Poster,Reconstructing Cell Lineage Trees from Phenotypic Features with Metric Learning,https://ICML.cc//virtual/2025/poster/45677,"Da Kuang, GuanWen Qiu, Junhyong Kim","How a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology. The cells replicate to generate cell lineages and acquire differentiated characteristics through poorly understood molecular processes. A key approach to studying developmental processes is to infer the tree graph of cell lineage histories, which provides an analytical framework for dissecting individual cells' molecular decisions during replication and differentiation (i.e., acquisition of specialized traits). Although genetically engineered lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in many organisms. By contrast, modern single-cell technologies can measure high-content molecular profiles (*e.g.*, transcriptomes) in a wide range of biological systems. Here, we introduce *CellTreeQM*, a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for tree-graph inference. By formulating the lineage reconstruction problem as tree-metric learning, we systematically explore weakly supervised training settings at different levels of information and present the *Cell Lineage Reconstruction Benchmark* to facilitate comprehensive evaluation. This benchmark includes (1) synthetic data modeled via Brownian motion with independent noise and spurious signals; (2) lineage-resolved single-cell RNA sequencing datasets. Experimental results show that *CellTreeQM* recovers lineage structures with minimal supervision and limited data, offering a scalable framework for uncovering cell lineage relationships. To our knowledge, this is the first method to cast cell lineage inference explicitly as a metric learning task, paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage. Code and benchmarks are available at: https://kuang-da.github.io/CellTreeQM-page","How does a single fertilized cell become a whole organism? To understand this, scientists study the ""family tree"" of cells—how each cell gives rise to others during development. While genetic tools can track these relationships, they are often limited by technical or ethical constraints.Our paper introduces a new AI-based method called CellTreeQM that reconstructs these cell family trees using molecular data from individual cells, such as gene activity (RNA levels), which can now be measured at scale thanks to modern biotechnology. Instead of relying on direct lineage labels, CellTreeQM learns how to position cells in a geometric space where distances reflect how closely related they are in the developmental process. The method uses a transformer-based deep learning model and learns from partial or indirect clues—like having only some parts of the cell tree or knowing which cells are grouped together. This level of supervision is often available in practice and relatively easy to extract from the data.To rigorously test our method, we created a new benchmark with both simulated and real biological data. Our results show that CellTreeQM can uncover meaningful lineage relationships even with limited supervision, making it a practical tool for studying development in systems where traditional lineage tracing isn’t feasible. This work reframes lineage reconstruction as a metric learning problem and opens up new directions for using AI to study how cells develop and specialize."
Poster,Rectifying Conformity Scores for Better Conditional Coverage,https://ICML.cc//virtual/2025/poster/45223,"Vincent Plassier, Alexander Fishkov, Victor Dheur, Mohsen Guizani, Souhaib Ben Taieb, Maxim Panov, Eric Moulines","We present a new method for generating confidence sets within the split conformal prediction framework. Our method performs a trainable transformation of any given conformity score to improve conditional coverage while ensuring exact marginal coverage. The transformation is based on an estimate of the conditional quantile of conformity scores. The resulting method is particularly beneficial for constructing adaptive confidence sets in multi-output problems where standard conformal quantile regression approaches have limited applicability. We develop a theoretical bound that captures the influence of the accuracy of the quantile estimate on the approximate conditional validity, unlike classical bounds for conformal prediction methods that only offer marginal coverage. We experimentally show that our method is highly adaptive to the local data structure and outperforms existing methods in terms of conditional coverage, improving the reliability of statistical inference in various applications.","When artificial intelligence (AI) models make predictions—such as forecasting the weather, diagnosing diseases, or making financial decisions—it’s important not just to get a single answer, but to know how confident the model is in that answer. This is where “prediction intervals” come in: they provide a range of possible outcomes that are likely to contain the true answer.A popular technique called conformal prediction already does this. It can guarantee that, on average, the true answer falls inside the prediction interval a desired percentage of the time (say, 90%). However, these guarantees are “marginal”—they only hold when averaged across all situations. They don't adjust well for different types of inputs. For instance, the model might be very accurate on common cases but less reliable on unusual ones, yet treat them the same.This paper introduces a new method that improves how these confidence intervals adapt to different situations. The authors developed a technique that tweaks the model’s internal uncertainty scores in a smart, data-driven way. By doing this, their method keeps the original guarantees while making the intervals better suited to the specific conditions of each case—like giving tighter estimates when the model is more certain and wider ones when it’s less sure.The authors show that this approach is especially helpful for complex tasks where the model has to predict multiple values at once. In tests on both simulated and real-world datasets, their method provided more reliable predictions than existing tools."
Poster,Reducing Confounding Bias without Data Splitting for Causal Inference via Optimal Transport,https://ICML.cc//virtual/2025/poster/44515,"Yuguang Yan, Zongyu Li, Haolin Yang, Zeqin Yang, Hao Zhou, Ruichu Cai, Zhifeng Hao","Causal inference seeks to estimate the effect given a treatment such as a medicine or the dosage of a medication. To reduce the confounding bias caused by the non-randomized treatment assignment, most existing methods reduce the shift between subpopulations receiving different treatments. However, these methods split limited training samples into smaller groups, which cuts down the number of samples in each group, while precise distribution estimation and alignment highly rely on a sufficient number of training samples. In this paper, we propose a distribution alignment paradigm without data splitting, which can be naturally applied in the settings of binary and continuous treatments. To this end, we characterize the confounding bias by considering different probability measures of the same set including all the training samples, and exploit the optimal transport theory to analyze the confounding bias and outcome estimation error. Based on this, we propose to learn balanced representations by reducing the bias between the marginal distribution and the conditional distribution of a treatment. As a result, data reduction caused by splitting is avoided, and the outcome prediction model trained on one treatment group can be generalized to the entire population. The experiments on both binary and continuous treatment settings demonstrate the effectiveness of our method.","Causal inference aims to evaluate the effect of a treatment, such as a medicine or the dosage of a medication.Ideally, the treatment effect can be estimated by randomized controlled trials,in which individuals are assigned to the treated group and the control group randomly,and the effect can be calculated by comparing the results of the two groups.However, the individuals do not receive treatments randomly.For example, sicker patients might be more likely to receive a particular medication, making it difficult to estimate the treatment effect accurately since the treated and control groups follow different distributions.Existing methods usually reduce the distribution discrepancy between the groups,in which training samples are split into different subsets,which cuts down the number of samples in each group and hampers distribution estimation and alignment.To address this, we propose a distribution alignment paradigm without data splitting.Specifically, we analyze the distribution discrepancy and the effect estimation error,and align different distributions based on optimal transport between probability measures,which is built on all the samples instead of subsets.As a result, data splitting is avoided, and data efficiency is improved for accurate distribution modeling.We conduct experiments in different settings, including the binary treatment setting (receiving a treatment or not) and the continuous treatment setting (receiving a dosage of the treatment).The experimental results demonstrate that our method can estimate the treatment effect accurately.Existing methods often mitigate this bias by splitting data into subgroups, but this reduces the number of training samples per group, which undermines reliable distribution estimation and outcome prediction.We tackled the problem by proposing a novel distribution alignment framework that avoids data splitting. They model confounding bias using different probability measures over the same dataset and employ optimal transport theory to analyze and reduce both confounding bias and prediction error. By learning balanced representations that align marginal and conditional treatment distributions, the method generalizes across the population without reducing the sample size.This research provides a more data-efficient and theoretically grounded approach to causal inference for both binary and continuous treatments. By avoiding data splitting and leveraging optimal transport, our method works for both simple yes-or-no treatments and more complex cases like varying dosage levels. It improves accuracy and helps build more reliable models to guide medical decisions, benefiting both researchers and patients."
Poster,Reducing Tool Hallucination via Reliability Alignment,https://ICML.cc//virtual/2025/poster/45001,"Hongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, Kai Yu","Large Language Models (LLMs) have expanded their capabilities beyond language generation to interact with external tools, enabling automation and real-world applications. However, tool hallucinations—where models either select inappropriate tools or misuse them—pose significant challenges, leading to erroneous task execution, increased computational costs, and reduced system reliability. To systematically address this issue, we define and categorize tool hallucinations into two main types: tool selection hallucination and tool usage hallucination. To evaluate and mitigate these issues, we introduce RelyToolBench, which integrates specialized test cases and novel metrics to assess hallucination-aware task success and efficiency. Finally, we propose Relign, a reliability alignment framework that expands the tool-use action space to include indecisive actions, allowing LLMs to defer tool use, seek clarification, or adjust tool selection dynamically. Through extensive experiments, we demonstrate that Relign significantly reduces tool hallucinations, improves task reliability, and enhances the efficiency of LLM tool interactions. The code and data will be publicly available.","Large language models (LLMs), like the ones behind AI chatbots, are becoming smarter — they can now use external tools such as calculators or search engines to solve real-world tasks. But sometimes, they make mistakes when using these tools. For example, they might pick the wrong tool or use it in the wrong way, leading to wrong answers, wasted resources, or unreliable systems. To better understand and fix this issue, we first define two types of tool mistakes: picking the wrong tool (selection errors) and using the right tool incorrectly (usage errors). We then introduce a new evaluation set called RelyToolBench that tests how well models use tools and how often they make these mistakes. Building on this, we propose a new method called Relign, which teaches models to be more cautious. Instead of rushing to use a tool, the model can now say “I’m not sure,” ask for more information, or try a different tool. Our experiments show that this approach makes tool use more accurate, reliable, and efficient, helping AI systems become more trustworthy in practical applications."
Poster,Reducing Variance of Stochastic Optimization for Approximating Nash Equilibria in Normal-Form Games,https://ICML.cc//virtual/2025/poster/45762,"Linjian Meng, Wubing Chen, Wenbin Li, Tianpei Yang, Youzhi Zhang, Yang Gao","Nash equilibrium (NE) plays an important role in game theory. How to efficiently compute an NE in NFGs is challenging due to its complexity and non-convex optimization property. Machine Learning (ML), the cornerstone of modern artificial intelligence, has demonstrated remarkable empirical performance across various applications including non-convex optimization. To leverage non-convex stochastic optimization techniques from ML for approximating an NE, various loss functions have been proposed. Among these, only one loss function is unbiased, allowing for unbiased estimation under the sampled play. Unfortunately, this loss function suffers from high variance, which degrades the convergence rate. To improve the convergence rate by mitigating the high variance associated with the existing unbiased loss function, we propose a novel surrogate loss function named Nash Advantage Loss (NAL). NAL is theoretically proved unbiased and exhibits significantly lower variance than the existing unbiased loss function. Experimental results demonstrate that the algorithm minimizing NAL achieves a significantly faster empirical convergence rates compared to other algorithms, while also reducing the variance of estimated loss value by several orders of magnitude.","(1) Leveraging non-convex stochastic optimization techniques from machine learning for Nash equilibria computation remains largely unexplored, primarily due to the lack of an appropriate loss function. (2) We propose a novel surrogate unbiased loss function with low variance. (3) This will help to improve the convergence rate in learning Nash equilibria with non-convex stochastic optimization techniques from machine learning."
Poster,Redundancy Undermines the Trustworthiness of Self-Interpretable GNNs,https://ICML.cc//virtual/2025/poster/44426,"Wenxin Tai, Ting Zhong, Goce Trajcevski, Fan Zhou","This work presents a systematic investigation into the trustworthiness of explanations generated by self-interpretable graph neural networks (GNNs), revealing why models trained with different random seeds yield inconsistent explanations. We identify redundancy—resulting from weak conciseness constraints—as the root cause of both explanation inconsistency and its associated inaccuracy, ultimately hindering user trust and limiting GNN deployment in high-stakes applications. Our analysis demonstrates that redundancy is difficult to eliminate; however, a simple ensemble strategy can mitigate its detrimental effects. We validate our findings through extensive experiments across diverse datasets, model architectures, and self-interpretable GNN frameworks, providing a benchmark to guide future research on addressing redundancy and advancing GNN deployment in critical domains. Our code is available at \url{https://github.com/ICDM-UESTC/TrustworthyExplanation}.","Graph neural networks (GNNs) are powerful tools used in areas like healthcare, finance, and scientific discovery. In many of these high-stakes applications, it’s important not only that a model makes accurate predictions, but also that it can explain why it made those predictions. Some GNNs are designed to be self-explaining—that is, they provide their own reasons for each decision. However, our study finds that these explanations can vary significantly when the model is trained multiple times, even with the same data. This inconsistency can confuse users and reduce trust in the system. We find that the root cause of this issue is redundancy in the explanations—extra or unnecessary parts that don’t help with understanding the decision. While it’s hard to eliminate this redundancy completely, we show that using a simple ensemble (combining multiple models) can help improve explanation consistency and its associated accuracy. Our findings help pave the way for safer and more trustworthy GNN use in real-world, high-impact domains."
Poster,ReferSplat: Referring Segmentation in 3D Gaussian Splatting,https://ICML.cc//virtual/2025/poster/43877,"Shuting He, Guangquan Jie, Changshuo Wang, Yun Zhou, Shuming Hu, Guanbin Li, Henghui Ding","We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view,posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI.To support research in this area, we construct the first R3DGS dataset, Ref-LERF.Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS.To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks.Dataset and code are available at https://github.com/heshuting555/ReferSplat.","Imagine describing a scene with words like “the red chair next to the table” and having a computer automatically find that object in 3D Gaussian scenes. In our work, we explore how to teach AI systems to understand such descriptions and locate the right objects — even if they're partially hidden or viewed from new angles.We introduce a new task called Referring 3D Gaussian Splatting Segmentation (R3DGS), which asks AI to identify target objects in a 3D Gaussian scene using natural language. To support this, we built the first dedicated dataset, called Ref-LERF.Understanding spatial relationships in 3D is hard, so we created a method called ReferSplat that links 3D scene points to language in a way that’s aware of spatial layouts. Our approach not only handles complex language cues but also achieves state-of-the-art performance in multiple 3D understanding tasks.We’ve released both our dataset and code to help others explore this new challenge: https://github.com/heshuting555/ReferSplat."
Poster,R*: Efficient Reward Design via Reward Structure Evolution and Parameter Alignment Optimization with Large Language Models,https://ICML.cc//virtual/2025/poster/43934,"Pengyi Li, Jianye Hao, Hongyao Tang, Yifu Yuan, Jinbin Qiao, Zibin Dong, Yan Zheng","Reward functions are crucial for policy learning. Large Language Models (LLMs), with strong coding capabilities and valuable domain knowledge, provide an automated solution for high-quality reward design. However, code-based reward functions require precise guiding logic and parameter configurations within a vast design space, leading to low optimization efficiency.To address the challenges,we propose an efficient automated reward design framework, called R*,which decomposes reward design into two parts: reward structure evolution and parameter alignment optimization. To design high-quality reward structures, R* maintains a reward function population and modularizes the functional components. LLMs are employed as the mutation operator, and module-level crossover is proposed to facilitate efficient exploration and exploitation.To design more efficient reward parameters, R* first leverages LLMs to generate multiple critic functions for trajectory comparison and annotation. Based on these critics, a voting mechanism is employed to collect the trajectory segments with high-confidence labels.These labeled segments are then used to refine the reward function parameters through preference learning.Experiments on diverse robotic control tasks demonstrate that R* outperforms strong baselines in both reward design efficiency and quality, surpassing human-designed reward functions.","High-quality reward functions are a prerequisite for stable and efficient reinforcement learning, yet crafting them manually is labor-intensive and error-prone.  Recent attempts to let large language models (LLMs) write rewards still struggle, because naïve searches over the vast design space converge slowly and often miss good parameter choices. We introduce R*, an automated framework that separates reward design into two coordinated steps: reward-structure evolution and parameter-alignment optimisation. First, a population of modular reward functions is evolved with LLM-driven mutation and module-level crossover, reusing useful code blocks while encouraging diversity. Second, multiple LLM-generated critic functions compare short trajectory segments; a voting scheme retains only high-confidence labels, making parameter tuning data-efficient and fully automatic. Alignment operates on segments where at least three of five critics agree, ensuring reliable supervision without human intervention. Across eight robotic-manipulation benchmarks from Isaac Gym and Dexterity, the rewards produced by R* let agents learn faster and achieve higher final success than Eureka, the previous state-of-the-art. Because the entire loop—from structure search to parameter optimisation and critic labelling—runs automatically, R* turns reward shaping from an expert art into a repeatable pipeline.These advances could shorten the path from research code to reliable factory or household robots that learn new tasks safely and quickly."
