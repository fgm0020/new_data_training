type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,On the Generalization Ability of Next-Token-Prediction Pretraining,https://ICML.cc//virtual/2025/poster/44423,"Zhihao Li, Xue JIANG, Liyuan Liu, xuelin zhang, Hong Chen, Feng Zheng","Large language models (LLMs) have demonstrated remarkable potential in handling natural language processing (NLP) tasks and beyond. LLMs usually can be categorized as transformer decoder-only models (DOMs), utilizing Next-Token-Prediction (NTP) as their pre-training methodology. Despite their tremendous empirical successes, the theoretical understanding of how NTP pre-training affects the model's generalization behavior is lacking. To fill this gap, we establish the fine-grained generalization analysis for NTP pre-training based on Rademacher complexity, where the dependence between tokens is also addressed. Technically, a novel decomposition of Rademacher complexity is developed to study DOMs from the representation learner and the token predictor, respectively. Furthermore, the upper bounds of covering number are established for multi-layer and multi-head transformer-decoder models under the Frobenius norm, which theoretically pioneers the incorporation of mask matrix within the self-attention mechanism. Our results reveal that the generalization ability of NTP pre-training is affected quantitively by the number of token sequences $N$, the maximum length of sequence $m$, and the count of parameters in the transformer model $\Theta$. Additionally, experiments on public datasets verify our theoretical findings.","Large language models (LLMs) like ChatGPT excel by predicting the next word, but we lack a theoretical understanding of why this simple training method gives them such powerful generalization abilities. This knowledge gap prevents us from fundamentally grasping how these models work or reliably improving them.We developed a novel mathematical framework to quantify how three key factors—training data volume ($N$), text sequence length ($m$), and model size ($\Theta$)—collectively shape generalization. Our approach analyzed the model’s learning process while accounting for complex dependencies between words. We validated this theory through experiments on real-world language datasets.This work is to mathematically explain how next-word prediction training enables generalization in LLMs, effectively decoding their ""learning mechanism."" These insights allow developers to build more efficient, reliable models with less trial-and-error, and lay the groundwork for safer, more interpretable AI systems in the future."
Poster,On the Guidance of Flow Matching,https://ICML.cc//virtual/2025/poster/44016,"Ruiqi Feng, Chenglei Yu, Wenhao Deng, Peiyan Hu, Tailin Wu","Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.","We have developed a new method to help computers create things like images and make decisions more effectively. Our work focuses on improving how these computer programs follow instructions or ""guidance"" to get better results. While earlier methods used one type of approach, our method is more flexible and can handle a wider range of tasks. We created new ways for these computer programs to follow ""guidance"", and we carefully studied when and why to use each method. We tested our ideas in different situations, like creating images and controlling robots. Our experiments show our new methods work well, and we provide code for others to try them."
Poster,On the Impact of Hard Adversarial Instances on Overfitting in Adversarial Training,https://ICML.cc//virtual/2025/poster/46715,"Chen Liu, Zhichao Huang, Mathieu Salzmann, Tong Zhang, Sabine Süsstrunk","Adversarial training is a popular method to robustify models against adversarial attacks. However, it exhibits much more severe overfitting than training on clean inputs. In this work, we investigate this phenomenon from the perspective of training instances, i.e., training input-target pairs. Based on a quantitative metric measuring the relative difficulty of an instance in the training set, we analyze the model's behavior on training instances of different difficulty levels. This lets us demonstrate that the decay in generalization performance of adversarial training is a result of fitting hard adversarial instances. We theoretically verify our observations for both linear and general nonlinear models, proving that models trained on hard instances have worse generalization performance than ones trained on easy instances, and that this generalization gap increases with the size of the adversarial budget. Finally, we investigate solutions to mitigate adversarial overfitting in several scenarios, including fast adversarial training and fine-tuning a pretrained model with additional data. Our results demonstrate that using training data adaptively improves the model's robustness.",
Poster,On the Impact of Performative Risk Minimization for Binary Random Variables,https://ICML.cc//virtual/2025/poster/43544,"Nikita Tsoy, Ivan Kirev, Negin Rahimiyazdi, Nikola Konstantinov","Performativity, the phenomenon where outcomes are influenced by predictions, is particularly prevalent in social contexts where individuals strategically respond to a deployed model. In order to preserve the high accuracy of machine learning models under distribution shifts caused by performativity, Perdomo et al. (2020) introduced the concept of performative risk minimization (PRM). While this framework ensures model accuracy, it overlooks the impact of the PRM on the underlying distributions and the predictions of the model. In this paper, we initiate the analysis of the impact of PRM, by studying performativity for a sequential performative risk minimization problem with binary random variables and linear performative shifts. We formulate two natural measures of impact. In the case of full information, where the distribution dynamics are known, we derive explicit formulas for the PRM solution and our impact measures. In the case of partialinformation, we provide performative-aware statistical estimators, as well as simulations. Our analysis contrasts PRM to alternatives that do not model data shift and indicates that PRM can have amplified side effects compared to such methods.","Predictions made by machine learning models often impact their environment (a phenomenon known as performativity); for example, drug efficacy estimates influence the drug's effectiveness due to the placebo effect. Our work theoretically studies how different approaches to model training impact the surrounding environment in the presence of performativity. In particular, we find that model training methods that explicitly account for performativity often lead to a larger shift in the distribution and bias of the decisions, compared to standard training alternatives. We hope that our work will help practitioners to better understand and control potential negative side effects of performative ML training."
Poster,On the Importance of Embedding Norms in Self-Supervised Learning,https://ICML.cc//virtual/2025/poster/45610,"Andrew Draganov, Sharvaree Vadgama, Sebastian Damrich, Jan Böhm, Lucas Maes, Dmitry Kobak, Erik Bekkers","Self-supervised learning (SSL) allows training data representations without a supervised signal and has become an important paradigm in machine learning. Most SSL methods employ the cosine similarity between embedding vectors and hence effectively embed data on a hypersphere. While this seemingly implies that embedding norms cannot play any role in SSL, a few recent works have suggested that embedding norms have properties related to network convergence and confidence. In this paper, we resolve this apparent contradiction and systematically establish the embedding norm's role in SSL training. Using theoretical analysis, simulations, and experiments, we show that embedding norms (i) govern SSL convergence rates and (ii) encode network confidence, with smaller norms corresponding to unexpected samples. Additionally, we show that manipulating embedding norms can have large effects on convergence speed.Our findings demonstrate that SSL embedding norms are integral to understanding and optimizing network behavior.","Machine learning models process information using embeddings -- high-dimensional points which encode what the model extracted from the input. Many machine learning training objectives treat these embeddings as having a fixed size and, consequently, most analysis of these embeddings ignores their size. In this paper, we show that embedding sizes (norms) *both* contain valuable information *and* control how well the model learns. Specifically, we show that the embedding norm represents the model's certainty in the corresponding input and that, if the embedding norm is large, then the model has a difficult time updating this representation."
Poster,On the Importance of Gaussianizing Representations,https://ICML.cc//virtual/2025/poster/43501,"Daniel Eftekhari, Vardan Papyan","The normal distribution plays a central role in information theory – it is at the same time the best-case signal and worst-case noise distribution, has the greatest representational capacity of any distribution, and offers an equivalence between uncorrelatedness and independence for joint distributions. Accounting for the mean and variance of activations throughout the layers of deep neural networks has had a significant effect on facilitating their effective training, but seldom has a prescription for precisely what distribution these activations should take, and how this might be achieved, been offered. Motivated by the information-theoretic properties of the normal distribution, we address this question and concurrently present normality normalization: a novel normalization layer which encourages normality in the feature representations of neural networks using the power transform and employs additive Gaussian noise during training. Our experiments comprehensively demonstrate the effectiveness of normality normalization, in regards to its generalization performance on an array of widely used model and dataset combinations, its strong performance across various common factors of variation such as model width, depth, and training minibatch size, its suitability for usage wherever existing normalization layers are conventionally used, and as a means to improving model robustness to random perturbations.","Successfully training deep neural networks depends greatly on how data is represented, as it is processed through the layers of a network. Up until now, controlling the average and spread of these representations was the main approach used to help neural networks train effectively. In this work, we furthermore motivated a specific distribution that neural network representations should follow, and materialized this choice of distribution using a new layer we developed. Our experiments and analysis comprehensively demonstrated the effectiveness of this new layer."
Poster,On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks,https://ICML.cc//virtual/2025/poster/45447,"Junwei Su, Chuan Wu","This paper studies the interplay between learning algorithms and graph structure for graph neural networks (GNNs). Existing theoretical studies on the learning dynamics of GNNs primarily focus on the convergence rates of learning algorithms under the interpolation regime (noise-free) and offer only a crude connection between these dynamics and the actual graph structure (e.g., maximum degree). This paper aims to bridge this gap by investigating the excessive risk (generalization performance) of learning algorithms in GNNs within the generalization regime (with noise). Specifically, we extend the conventional settings from the learning theory literature to the context of GNNs and examine how graph structure influences the performance of learning algorithms such as stochastic gradient descent (SGD) and Ridge regression. Our study makes several key contributions toward understanding the interplay between graph structure and learning in GNNs. First, we derive the excess risk profiles of SGD and Ridge regression in GNNs and connect these profiles to the graph structure through spectral graph theory. With this established framework, we further explore how different graph structures (regular vs. power-law) impact the performance of these algorithms through comparative analysis. Additionally, we extend our analysis to multi-layer linear GNNs, revealing an increasing non-isotropic effect on the excess risk profile, thereby offering new insights into the over-smoothing issue in GNNs from the perspective of learning algorithms. Our empirical results align with our theoretical predictions, \emph{collectively showcasing a coupling relation among graph structure, GNNs and learning algorithms, and providing insights on GNN algorithm design and selection in practice.}","Graph Neural Networks (GNNs) are widely used for analyzing data structured as graphs, such as social networks, molecular structures, or interconnected web pages. However, their performance heavily depends on the graph’s structure and the chosen training methods. This paper explores how different network configurations—such as evenly connected networks versus those dominated by a few highly connected nodes—influence the success of common training algorithms. Additionally, the authors investigate deeper GNNs, identifying conditions that explain why deeper networks often face challenges, known as ""over-smoothing."" Understanding these factors helps in selecting or designing GNNs that perform better in practical applications."
Poster,On the Learnability of Distribution Classes with Adaptive Adversaries,https://ICML.cc//virtual/2025/poster/44440,"Tosca Lechner, Alex Bie, Gautam Kamath","We consider the question of learnability of distribution classes in the presence of adaptive adversaries -- that is, adversaries capable of intercepting the samples requested by a learner and applying manipulations with full knowledge of the samples before passing it on to the learner. This stands in contrast to oblivious adversaries, who can only modify the underlying distribution the samples come from but not their i.i.d.\ nature. We formulate a general notion of learnability with respect to adaptive adversaries, taking into account the budget of the adversary. We show that learnability with respect to additive adaptive adversaries is a strictly stronger condition than learnability with respect to additive oblivious adversaries.","Generalizing from training data underlies most machine learning processes. Often this training data is assumed to be generated directly from the phenomena one wants to learn. In our work we study the situation, where an adversary gets to manipulate the training data, before the learner gets to see it. We study adaptive adversaries, who have access to the whole training data and can therefore manipulate with this full knowledge. We contrast them with oblivious adversaries, who only are aware of the data generating process, but not of the training data itself. We show that adaptive adversaries can be strictly stronger than oblivious adversaries.In particular, we study additive adversaries, who can add data points and subtractive adversaries, who can delete data points. We show a separation between adaptive additive and oblivious adaptive adversaries. Thus, we show that in some situations adding data points when knowing a sample can gravely hurt the learning process, while similar additive manipulations on the data-generating process will not hurt the learning process too much."
Poster,On the Local Complexity of Linear Regions in Deep ReLU Networks,https://ICML.cc//virtual/2025/poster/44346,"Niket Patel, Guido Montufar","We define the *local complexity* of a neural network with continuous piecewise linear activations as a measure of the density of linear regions over an input data distribution. We show theoretically that ReLU networks that learn low-dimensional feature representations have a lower local complexity. This allows us to connect recent empirical observations on feature learning at the level of the weight matrices with concrete properties of the learned functions. In particular, we show that the local complexity serves as an upper bound on the total variation of the function over the input data distribution and thus that feature learning can be related to adversarial robustness. Lastly, we consider how optimization drives ReLU networks towards solutions with lower local complexity. Overall, this work contributes a theoretical framework towards relating geometric properties of ReLU networks to different aspects of learning such as feature learning and representation cost.","The input-output relationships implemented by many modern neural networks can be regarded as large ``folded maps'' with many sharp bends. These bends are determined by the activation patterns of the different neurons in the network and are thought to affect how well the network is able to learn from data and resist adversarial attacks. We introduce a notion of local complexity that measures the distribution of such bends near a given dataset. We prove that this measure is tied to several important properties, such as the dimension of the data representations learned by the network, the variability of the computations that it implements over different inputs, as well as phenomena observed late in training. Local complexity can give researchers and practitioners a new diagnostic for building smoother and more robust neural networks."
Poster,On the Out-of-Distribution Generalization of Self-Supervised Learning,https://ICML.cc//virtual/2025/poster/44835,"Wenwen Qiang, Jingyao Wang, Zeen Song, Jiangmeng Li, Changwen Zheng","In this paper, we focus on the out-of-distribution (OOD) generalization of self-supervised learning (SSL). By analyzing the mini-batch construction during the SSL training phase, we first give one plausible explanation for SSL having OOD generalization. Then, from the perspective of data generation and causal inference, we analyze and conclude that SSL learns spurious correlations during the training process, which leads to a reduction in OOD generalization. To address this issue, we propose a post-intervention distribution (PID) grounded in the Structural Causal Model. PID offers a scenario where the spurious variable and label variable is mutually independent. Besides, we demonstrate that if each mini-batch during SSL training satisfies PID, the resulting SSL model can achieve optimal worst-case OOD performance. This motivates us to develop a batch sampling strategy that enforces PID constraints through the learning of a latent variable model. Through theoretical analysis, we demonstrate the identifiability of the latent variable model and validate the effectiveness of the proposed sampling strategy. Experiments conducted on various downstream OOD tasks demonstrate the effectiveness of the proposed sampling strategy.","In machine learning, models often struggle when faced with data different from their training examples, a challenge known as out-of-distribution (OOD) generalization. We explored how self-supervised learning (SSL)—a method where models learn from unlabeled data—handles this challenge. We first investigated why SSL models sometimes perform well on OOD tasks and found that the way training examples are grouped (or batched) might explain this ability. However, we also identified a key issue: SSL can inadvertently learn irrelevant relationships (called spurious correlations) from the training data, making models less reliable on new, unseen examples. To solve this, we introduced a novel technique called post-intervention distribution (PID), based on causal modeling, which ensures the training data batches don't include misleading correlations. We then created a practical method to select training batches that satisfy this PID condition. Our theoretical and experimental results confirm that this method significantly improves SSL’s performance."
