type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Floating-Point Neural Networks Can Represent Almost All Floating-Point Functions,https://ICML.cc//virtual/2025/poster/45517,"Geonho Hwang, Yeachan Park, Wonyeol Lee, Sejun Park","Existing works on the expressive power of neural networks typically assume real-valued parameters and exact mathematical operations during the evaluation of networks. However, neural networks run on actual computers can take parameters only from a small subset of the reals and perform inexact mathematical operations with round-off errors and overflows. In this work, we study the expressive power of floating-point neural networks, i.e., networks with floating-point parameters and operations. We first observe that for floating-point neural networks to represent all functions from floating-point vectors to floating-point vectors, it is necessary to distinguish different inputs: the first layer of a network should be able to generate different outputs for different inputs. We also prove that such distinguishability is sufficient, along with mild conditions on activation functions. Our result shows that with practical activation functions, floating-point neural networks can represent floating-point functions from a wide domain to all finite or infinite floats. For example, the domain is all finite floats for Sigmoid and tanh, and it is all finite floats of magnitude less than 1/8 times the largest float for ReLU, ELU, SeLU, GELU, Swish, Mish, and sin.","Neural networks are a key part of modern AI, that has been used for face recognition, language translation, and medical image analysis. Most research about how neural networks work assumes they can do perfect math. But in the real world, computers cannot; they use rounded numbers that can introduce tiny errors.We investigate the following question: Can neural networks still work well using these imperfect numbers? We prove that the answer is yes. Despite the rounding and limitations of computer-based math, these networks can still perform nearly any task they need to.This is important because it shows that the AI tools we use in the real world are just as powerful as the ones described in theory assuming perfect math. The results help confirm that computers can run neural networks effectively, even with the small errors that come from working with real hardware. In short, this research builds confidence that practical AI systems are as capable as researchers expect them to be."
Poster,FloE: On-the-Fly MoE Inference on Memory-constrained GPU,https://ICML.cc//virtual/2025/poster/44378,"Yuxin Zhou, Zheng Li, Jun Zhang, Jue Wang, Yiping Wang, Zhongle Xie, Ke Chen, Lidan Shou","With the widespread adoption of Mixture-of-Experts (MoE) models, there is a growing demand for efficient inference on memory-constrained devices.While offloading expert parameters to CPU memory and loading activated experts on demand has emerged as a potential solution, the large size of activated experts overburdens the limited PCIe bandwidth, hindering the effectiveness in latency-sensitive scenarios.To mitigate this, we propose FloE, an on-the-fly MoE inference system on memory-constrained GPUs.FloE  is built on the insight that there exists substantial untapped redundancy within sparsely activated experts.It employs various compression techniques on the expert's internal parameter matrices to reduce the data movement load, combined with low-cost sparse prediction, achieving perceptible inference acceleration in wall-clock time on resource-constrained devices.Empirically, FloE  achieves a 9.3$\times$ compression of parameters per expert in Mixtral-8$\times$7B; enables deployment on a GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5$\times$; and delivers a 48.7$\times$ inference speedup compared to DeepSpeed-MII on a single GeForce RTX 3090—all with only a 4.4\% $\sim$ 7.6\% average performance degradation.","Language models known as ""Mixture-of-Experts"" (MoE) are powerful tools, but their huge size makes it difficult to run them quickly on devices with limited memory, such as consumer-grade GPUs. To manage this, some systems temporarily store model parts on slower memory (e.g., CPU main memory) and load them only when needed—but this method is slow, especially when quick responses are crucial.We developed a new approach called FloE, which cleverly compresses parts of these models to significantly speed up the process. FloE finds hidden redundancies within the model's expert components—essentially, unnecessary details that can be trimmed without significantly harming accuracy. By reducing the size of the experts’ internal data, FloE lets these large models fit comfortably into small memory spaces.Our tests show that FloE makes these models almost 49 times faster on common GPUs and reduces the memory requirement dramatically, all while maintaining excellent performance. This advancement makes powerful machine learning tools accessible for more users, even with limited hardware resources."
Poster,Flopping for FLOPs: Leveraging Equivariance for Computational Efficiency,https://ICML.cc//virtual/2025/poster/45974,"Georg Bökman, David Nordström, Fredrik Kahl","Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs.This paper introduces new equivariant neural networksthat preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks.The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group.This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs.Our approach reduces both FLOPs and wall-clock time,providing a practical solution for efficient, scalable symmetry-aware architectures.","We introduce new and improved neural networks for image processing. A neural network is a program with many free parameters that can be tuned on training data. In our case, the training data consists of images and corresponding labels, and we aim to train networks that can accurately classify images (image classification is a useful prototype task). Since most images are equally likely to appear in mirrored form, we enforce mirror-invariant classification by putting restrictions on the neural network parameters. This contradicts the trend of allowing neural networks to learn as much as possible from data; however, we provide new arguments for enforcing invariances rather than learning them from data. In particular, enforcing mirror-invariance yields faster neural networks."
Poster,FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching,https://ICML.cc//virtual/2025/poster/45682,"Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen","Autoregressive (AR) modeling has achieved remarkable success in natural language processing by enabling models to generate text with coherence and contextual understanding through next token prediction. Recently, in image generation, VAR proposes scale-wise autoregressive modeling, which extends the next token prediction to the next scale prediction, preserving the 2D structure of images. However, VAR encounters two primary challenges: (1) its complex and rigid scale design limits generalization in next scale prediction, and (2) the generator’s dependence on a discrete tokenizer with the same complex scale structure restricts modularity and flexibility in updating the tokenizer. To address these limitations, we introduce FlowAR, a general next scale prediction method featuring a streamlined scale design, where each subsequent scale is simply double the previous one. This eliminates the need for VAR’s intricate multi-scale residual tokenizer and enables the use of any off-the-shelf Variational AutoEncoder (VAE). Our simplified design enhances generalization in next scale prediction and facilitates the integration of Flow Matching for high-quality image synthesis. We validate the effectiveness of FlowAR on the challenging ImageNet-256 benchmark, demonstrating superior generation performance compared to previous methods. Codes is available at \href{https://github.com/OliverRensu/FlowAR}{https://github.com/OliverRensu/FlowAR}.","Modern image-generation methods often build pictures in stages—first sketching a rough outline at low resolution, then filling in details at finer scales. However, existing approaches rely on complex, custom tokenizers that are difficult to update and don’t always generalize well to new resolutions. In this work, we introduce FlowAR, a streamlined technique that doubles the image size at each step (e.g., from 64×64 to 128×128) and plugs into any standard image encoder–decoder system.By replacing elaborate, scale-specific components with a simple “next-scale” predictor, FlowAR becomes more flexible: you can swap in improved encoders without redesigning the whole pipeline. We also integrate a modern “flow matching” strategy to enhance image quality, yielding sharper, more realistic results. On a challenging benchmark of 256×256 photographs, FlowAR outperforms previous multi-scale models in both fidelity and diversity. Our code is publicly available, paving the way for easier adoption and future improvements in scalable image synthesis."
Poster,Flow-based Domain Randomization for Learning and Sequencing Robotic Skills,https://ICML.cc//virtual/2025/poster/46239,"Aidan Curtis, Eric Li, Michael S Noseworthy, Nishad Gothoskar, Sachin Chitta, Hui Li, Leslie Kaelbling, Nicole Carey","Domain randomization in reinforcement learning is an established technique for increasing the robustness of control policies learned in simulation. By randomizing properties of the environment during training, the learned policy can be robust to uncertainty along the randomized dimensions. While the environment distribution is typically specified by hand, in this paper we investigate the problem of automatically discovering this sampling distribution via entropy-regularized reward maximization of a neural sampling distribution in the form of a normalizing flow. We show that this architecture is more flexible and results in better robustness than existing approaches to learning simple parameterized sampling distributions. We demonstrate that these policies can be used to learn robust policies for contact-rich assembly tasks. Additionally, we explore how these sampling distributions, in combination with a privileged value function, can be used for out-of-distribution detection in the context of an uncertainty-aware multi-step manipulation planner.","Robots often learn new skills in computer simulations, but what they learn does not always work well in the real world. One way to fix this is by making the training environments more varied, so the robot gets used to different situations. However, these variations are usually chosen by people, which can be time-consuming and not always effective. In this work, we introduce GoFlow, a new method that teaches robots in a smarter way by automatically creating a wide range of helpful training situations. This leads to better learning and makes the robot more prepared for the real world. We tested GoFlow in both virtual and real tasks and found that it helped robots succeed more often. GoFlow can also help the robot know when it needs more information before making a move, which makes it safer and more reliable."
Poster,FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields,https://ICML.cc//virtual/2025/poster/43848,"Gwanhyeong Koo, Sunjae Yoon, Younghwan Lee, Ji Woo Hong, Chang Yoo","Drag-based editing allows precise object manipulation through point-based control, offering user convenience. However, current methods often suffer from a geometric inconsistency problem by focusing exclusively on matching user-defined points, neglecting the broader geometry and leading to artifacts or unstable edits. We propose FlowDrag, which leverages geometric information for more accurate and coherent transformations. Our approach constructs a 3D mesh from the image, using an energy function to guide mesh deformation based on user-defined drag points. The resulting mesh displacements are projected into 2D and incorporated into a UNet denoising process, enabling precise handle-to-target point alignment while preserving structural integrity. Additionally, existing drag-editing benchmarks provide no ground truth, making it difficult to assess how accurately the edits match the intended transformations. To address this, we present VFD (VidFrameDrag) benchmark dataset, which provides ground-truth frames using consecutive shots in a video dataset. FlowDrag outperforms existing drag-based editing methods on both VFD Bench and DragBench.","Editing images by simply dragging points—like moving a person’s arm or rotating an animal’s head—can often lead to unrealistic or distorted results. Current methods frequently focus only on the specific points users move, ignoring the object’s overall shape and structure. This leads to what we call the “geometric inconsistency problem,” where edits become unnatural and incoherent.To solve this, we introduce FlowDrag, a method that incorporates structured 3D mesh deformation into drag-based editing. Specifically, FlowDrag first constructs a 3D mesh representation of the object, then uses carefully calculated mesh deformations to guide image edits. This ensures that object transformations maintain realism and geometric consistency, significantly reducing unnatural distortions and instability.Additionally, we created a new benchmark dataset called VFD (VidFrameDrag) from real video datasets. VFD provides clearly defined ground-truth transformations between consecutive video frames, enabling more accurate and reliable evaluation and comparison of drag-based editing methods."
Poster,Flow-field inference from neural data using deep recurrent networks,https://ICML.cc//virtual/2025/poster/44862,"Timothy Doyeon Kim, Thomas Luo, Tankut Can, Kamesh Krishnamurthy, Jonathan Pillow, Carlos Brody","Neural computations underlying processes such as decision-making, working memory, and motor control are thought to emerge from neural population dynamics. But estimating these dynamics remains a significant challenge. Here we introduce Flow-field Inference from Neural Data using deep Recurrent networks (FINDR), an unsupervised deep learning method for inferring low-dimensional, nonlinear, stochastic dynamics underlying neural population activity. Using spike train data from frontal brain regions of rats performing an auditory decision-making task, we demonstrate that FINDR performs competitively with existing methods in capturing the heterogeneous responses of individual neurons. When trained to disentangle task-relevant and irrelevant activity, FINDR uncovers interpretable low-dimensional dynamics. These dynamics can be visualized as flow fields and attractors, enabling direct tests of attractor-based theories of neural computation. We suggest FINDR as a powerful method for revealing the low-dimensional task-relevant dynamics of neural populations and their associated computations.","Neurons work together in large groups to solve tasks---like deciding whether to buy a laptop or not based on online reviews. A central premise in neuroscience is that the brain's algorithm for doing such tasks can be succinctly represented as a differential equation describing how this group activity changes over time. In this work, we present a method called **FINDR** that aims to discover what this differential equation is, using real brain activity data from animals doing specific tasks. The method does this in two main steps:1) It separates the brain activity that is relevant to the task from activity that isn't.2) Then it learns the most likely differential equation that is consistent with the task-relevant brain activity.We show that this approach performs competitively with existing methods in predicting neural activity, while being better at discovering differential equations that neuroscientists can interpret."
Poster,Flowing Datasets with Wasserstein over Wasserstein Gradient Flows,https://ICML.cc//virtual/2025/poster/45752,"Clément Bonet, Christophe Vauthier, Anna Korba","Many applications in machine learning involve data represented as probability distributions. The emergence of such data requires radically novel techniques to design tractable gradient flows on probability distributions over this type of (infinite-dimensional) objects. For instance, being able to flow labeled datasets is a core task for applications ranging from domain adaptation to transfer learning or dataset distillation. In this setting, we propose to represent each class by the associated conditional distribution of features, and to model the dataset as a mixture distribution supported on these classes (which are themselves probability distributions), meaning that labeled datasets can be seen as probability distributions over probability distributions. We endow this space with a metric structure from optimal transport, namely the Wasserstein over Wasserstein (WoW) distance, derive a differential structure on this space, and define WoW gradient flows. The latter enables to design dynamics over this space that decrease a given objective functional. We apply our framework to transfer learning and dataset distillation tasks, leveraging our gradient flow construction as well as novel tractable functionals that take the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels between probability distributions.","Many modern machine learning tasks involve data that is best understood not as individual points, but as probability distributions. For example, in labeled datasets, each class (like ""cat"" or ""dog"") can be seen as its own distribution of examples, making the entire dataset a collection of distributions, or a “distribution of distributions”. This layered structure is powerful, but traditional learning tools aren’t designed to handle it.We introduce a new mathematical framework that allows to smoothly transform one such dataset into another, by treating them as objects in a geometric space and defining meaningful ways to move through that space. Our approach builds on ideas from optimal transport, a field focused on comparing and moving distributions in the most efficient way.This new perspective enables models to better adapt to new tasks and to compress datasets into smaller sets that still capture the key information needed for learning. More broadly, it offers a foundation for working directly with richer, more structured data in a principled and scalable way."
Poster,Flow Matching for Denoised Social Recommendation,https://ICML.cc//virtual/2025/poster/44451,"Yinxuan Huang, KE LIANG, Zhuofan Dong, Xiaodong Qu, Wang Tianxiang, Yue Han, Jingao Xu, Bin Zhou, Ye Wang","Graph-based social recommendation (SR) models suffer from various noises of the social graphs, hindering their recommendation performances. Either graph-level redundancy or graph-level missing will indeed influence the social graph structures, further influencing the message propagation procedure of graph neural networks (GNNs). Generative models, especially diffusion-based models, are usually used to reconstruct and recover the data in better quality from original data with noises. Motivated by it, a few works take attempts on it for social recommendation. However, they can only handle isotropic Gaussian noises but fail to leverage the anisotropic ones. Meanwhile the anisotropic relational structures in social graphs are commonly seen, so that existing models cannot sufficiently utilize the graph structures, which constraints the capacity of noise removal and recommendation performances. Compared to the diffusion strategy, the flow matching strategy shows better ability to handle the data with anisotropic noises since they can better preserve the data structures during the learning procedure. Inspired by it, we propose RecFlow which is the first flow-matching based SR model. Concretely, RecFlow performs flow matching on the structure representations of social graphs. Then, a conditional learning procedure is designed for optimization. Extensive performances prove the promising performances of our RecFlow from six aspects, including superiority, effectiveness, robustnesses, sensitivity, convergence and visualization.","This paper introduces RecFlow, a flow-based social recommendation model that captures anisotropic and directed noise in user interactions. By leveraging flow matching, RecFlow enhances representation learning and denoising efficiency. This work advances understanding of continuous-time generative models in graph-structured data and emphasizes the practical benefits for personalized recommendation. We also acknowledge potential societal risks, such as bias amplification, and highlight the need for fairness and robustness in future flow-based recommendation systems."
Poster,Flow Matching for Few-Trial Neural Adaptation with Stable Latent Dynamics,https://ICML.cc//virtual/2025/poster/44109,"Puli Wang, Yu Qi, Yueming Wang, Gang Pan","The primary goal of brain-computer interfaces (BCIs) is to establish a direct linkage between neural activities and behavioral actions via neural decoders. Due to the nonstationary property of neural signals, BCIs trained on one day usually obtain degraded performance on other days, hindering the user experience. Existing studies attempted to address this problem by aligning neural signals across different days. However, these neural adaptation methods may exhibit  instability and poor performance when only a few trials are available for alignment, limiting their practicality in real-world BCI deployment. To achieve efficient and stable neural adaptation with few trials, we propose Flow-Based Distribution Alignment (FDA), a novel framework that utilizes flow matching to learn flexible neural representations with stable latent dynamics, thereby facilitating source-free domain alignment through likelihood maximization. The latent dynamics of FDA framework is theoretically proven to be stable using Lyapunov exponents, allowing for robust adaptation. Further experiments across multiple motor cortex datasets demonstrate the superior performance of FDA, achieving reliable results with fewer than five trials. Our FDA approach offers a novel and efficient solution for few-trial neural data adaptation, offering significant potential for improving the long-term viability of real-world BCI applications.","Brain-computer interfaces (BCIs) aim to translate neural activity into intended behavior, but their performance often degrades over time due to the nonstationary nature of neural signals. This makes it difficult to maintain reliable decoding across days, especially when only limited target data is available.We introduce Flow-Based Distribution Alignment (FDA), a novel method that leverages flow matching to align neural activity across sessions with minimal data. FDA does not require access to source data during adaptation and performs well with as few as five target trials.Evaluations on multiple motor cortex datasets show that FDA consistently delivers more stable and accurate decoding than existing approaches. Our method addresses a core challenge in real-world BCI applications and provides a practical solution for efficient neural alignment."
