type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,https://ICML.cc//virtual/2025/poster/43699,"Hui Dai, Ryan Teehan, Mengye Ren","Many existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to the emergence of new models and training data. These benchmarks also fall short in assessing how LLM performance changes over time, as they consist of a static set of questions without a temporal dimension. To address these limitations, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict ""future"" event outcomes. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists, highlighting the need for continuous model updates. Code and data are available at https://agenticlearning.ai/daily-oracle.","AI language models stop learning after their last training update. They may answer past questions well, but do they stay sharp as the world keeps changing? To study this, we tracked how well AI models maintain their prediction ability of current events over time. We built Daily Oracle, a benchmark that posts brand-new true/false and multiple-choice forecasting questions every day, based on current news articles. We tested popular models and saw their performance decline from 2020 to 2024—by about 22% on true/false questions and 11% on multiple-choice. Even when models were given recent news articles to help them answer, the decline continued. More surprisingly, this drop happened even in reading tasks where the exact answer was right there in the text. This shows that giving models new information on the fly isn’t enough—they may need fresh training to keep up.Daily Oracle shows how quickly AI models can fall behind and offers a way to measure that drift. It helps evaluate how well models forecast future events and encourages research into strategies like continuous pretraining to help models keep pace with the world."
Poster,A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations,https://ICML.cc//virtual/2025/poster/45188,"Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, Rémi Gribonval","Robustness with respect to weight perturbations underpins guarantees for generalization, pruning and quantization. Existingguarantees rely on *Lipschitz bounds in parameter space*, cover only plain feed-forward MLPs, and break under the ubiquitous neuron-wise rescaling symmetry of ReLU networks. We prove a new Lipschitz inequality expressed through the $\ell^{1}$-*path-metric* of the weights.  The bound is (i) *rescaling-invariant* by construction and (ii) applies to any ReLU-DAG architecture with any combination of convolutions, skip connections, pooling, and frozen (inference-time) batch-normalization —thus encompassing ResNets, U-Nets, VGG-style CNNs, and more. By respecting the network’s natural symmetries, the new bound strictly sharpens prior parameter-space bounds and can be computed in two forward passes. To illustrate its utility, we derive from it a symmetry-aware pruning criterion andshow—through a proof-of-concept experiment on a ResNet-18 trained on ImageNet—that its pruning performance matches that of classical magnitude pruning, while becoming totally immune to arbitrary neuron-wise rescalings.","Imagine adjusting the settings on a complex sound system with countless knobs and switches. In the world of artificial intelligence, these ""knobs"" are the internal settings of neural networks. People often tweak them to make AI models more efficient. (e.g., faster and/or cheaper to use). However, even small adjustments can sometimes lead to unexpected changes in how the AI behaves. Traditional methods to check this stability often don’t apply to modern, complex AI models, and can even predict highly pessimistic instability—even for changes that are known to be harmless.Our research introduces a new method that extends stability checks to modern AI architectures and can significantly reduce these misleading warnings. In particular, it ensures that harmless knob changes can’t trigger overly negative predictions about the model’s behavior. We also show that this method can help simplify AI models by safely removing unneeded parts, without sacrificing performance. This opens the door to more reliable and efficient AI systems in real-world use."
Poster,Are Sparse Autoencoders Useful? A Case Study in Sparse Probing,https://ICML.cc//virtual/2025/poster/43895,"Subhash Kantamneni, Josh Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda","Sparse autoencoders (SAEs) are a popular method for interpreting concepts represented in large language model (LLM) activations. However, there is a lack of evidence regarding the validity of their interpretations due to the lack of a ground truth for the concepts used by an LLM, and a growing number of works have presented problems with current SAEs. One alternative source of evidence would be demonstrating that SAEs improve performance on downstream tasks beyond existing baselines. We test this by applying SAEs to the real-world task of LLM activation probing in four regimes: data scarcity, class imbalance, label noise, and covariate shift. Due to the difficulty of detecting concepts in these challenging settings, we hypothesize that SAEs’ basis of interpretable, concept-level latents should provide a useful inductive bias. However, although SAEs occasionally perform better than baselines on individual datasets, we are unable to design ensemble methods combining SAEs with baselines that consistently outperform ensemble methods solely using baselines. Additionally, although SAEs initially appear promising for identifying spurious correlations, detecting poor dataset quality, and training multi-token probes, we are able to achieve similar results with simple non-SAE baselines as well. Though we cannot discount SAEs’ utility on other tasks, our findings highlight the shortcomings of current SAEs and the need to rigorously evaluate interpretability methods on downstream tasks with strong baselines.","Large language models (LLMs) can do impressive things, but even though we have created them, we really have no idea how they work. Sparse autoencoders (SAEs) are a recent tool that tries to understand LLMs by turning the LLM's internal state (an uninterpretable list of numbers) into a human-interpretable set of ""thoughts"" that the LLM is currently thinking about. For example, when the LLM is given input that says ""the quick brown fox jumps over a lazy dog"", the SAE might return that the LLM is thinking about animals, colors, movements, pangrams, poetry, and whimsey.However, it's not clear whether SAEs are actually useful. Maybe they're just telling us things we already know, or could have guessed from the original internal state. To test this, we looked at probing, a task where we want to predict something (""is the LLM telling the truth,"" or ""does the input contain a dangerous instruction"") using the LLM's internal state. We find that even in difficult probing settings (e.g. where we don't have many LLM internal states available, or the labels of our probe training are very noisy), the SAE doesn't seem to help. Even in cases where you might expect an SAE to shine, like where a probe on the internal states might latch on to something spurious, we find ways to train probes without an SAE.Our results highlight the shortcomings of SAEs and imply that we should better test our interpretability methods."
Poster,Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster,https://ICML.cc//virtual/2025/poster/45597,"Sharan Vaswani, Reza Babanezhad","Armijo line-search (Armijo-LS) is a standard method to set the step-size for gradient descent (GD). For smooth functions, Armijo-LS alleviates the need to know the global smoothness constant $L$ and adapts to the ``local'' smoothness, enabling GD to converge faster. Existing theoretical analyses show that GD with Armijo-LS ($\texttt{GD-LS}$) can result in constant factor improvements over GD with a $1/L$ step-size (denoted as $\texttt{GD(1/L)}$). We strengthen these results and show that if the objective function satisfies a certain non-uniform smoothness condition, $\texttt{GD-LS}$ can result in a faster convergence rate than $\texttt{GD(1/L)}$. In particular, we prove that for convex objectives corresponding to logistic regression and multi-class classification, $\texttt{GD-LS}$ can converge to the optimum at a linear rate, and hence improves over the sublinear convergence of $\texttt{GD(1/L)}$. Furthermore, for non-convex objectives satisfying gradient domination (e.g., those corresponding to the softmax policy gradient in RL or generalized linear models with a logistic link function), $\texttt{GD-LS}$ can match the fast convergence of algorithms tailored for these specific settings. Finally, we prove that under the interpolation assumption, for convex losses, stochastic GD with a stochastic line-search can match the fast convergence of $\texttt{GD-LS}$.","Gradient descent (GD) is the standard optimization method for training machine learning (ML) models. The performance of GD is sensitive to the choice of its step-size parameter. Armijo line-search is a common technique to ""search"" for a good step-size in each GD step. Armijo line-search does not only make GD more robust, but also makes the method faster in practice. In this paper, we theoretically characterize how fast can it go? We show that for common ML objectives such as logistic regression, GD with Armijo line-search (GDLS) can be exponentially faster than using GD with a fixed, pre-determined step-size. Moreover, for specific problems in supervised learning and reinforcement learning, we prove that GDLS can theoretically match or outperform algorithms explicitly designed for these problems. Our results thus demonstrate the universal effectiveness of GDLS, and show that this classic algorithm is all you need!"
Poster,ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior,https://ICML.cc//virtual/2025/poster/46648,"Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury","Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixturesrecorded by a microphone array. The problem ischallenging because it is a blind inverse problem,i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, areall unknown. We propose ArrayDPS to solve theBSS problem in an unsupervised, array-agnostic,and generative manner. The core idea builds ondiffusion posterior sampling (DPS), but unlikeDPS where the likelihood is tractable, ArrayDPSmust approximate the likelihood by formulatinga separate optimization problem. The solution to the optimization approximates room acousticsand the relative transfer functions between microphones. These approximations, along with thediffusion priors, iterate through the ArrayDPSsampling process and ultimately yield separatedvoice sources. We only need a simple single-speaker speech diffusion model as a prior, alongwith the mixtures recorded at the microphones; nomicrophone array information is necessary. Evaluation results show that ArrayDPS outperformsall baseline unsupervised methods while beingcomparable to supervised methods in terms ofSDR. Audio demos and codes are provided at:https://arraydps.github.io/ArrayDPSDemo/ andhttps://github.com/ArrayDPS/ArrayDPS.","With multiple voice sources mixed and recorded by microphones, how to separate different sources? Intuitively, if we know what a single speaker's speech sounds like, how would that help with separation? Is it possible to design an algorithm that would allow source separation for any microphone array, without needing any extra model training?To solve this problem, we use a diffusion model that models the pattern of single-speaker speech. Then, with this prior information about single-speaker speech, we design a novel posterior sampling algorithm for multi-microphone source separation. We enforce the separation result to follow the single-speaker speech pattern modeled by the diffusion model.Our finding shows that without any supervision, our method can achieve superior source separation, only using a speech diffusion prior model. The model can easily generalize to any microphone array and is generative."
Poster,Arrow: Accelerator for Time Series Causal Discovery with Time Weaving,https://ICML.cc//virtual/2025/poster/46084,"Yuanyuan Yao, Yuan Dong, Lu Chen, Kun Kuang, Ziquan Fang, Cheng Long, Yunjun Gao, TIANYI LI","Current causal discovery methods for time series data can effectively address a variety of scenarios; however, they remain constrained by inefficiencies. The significant inefficiencies arise primarily from the high computational costs associated with binning, the uncertainty in selecting appropriate time lags, and the extensive sets of candidate variables. To achieve both high efficiency and effectiveness of causal discovery, we introduce an accelerator termed ARROW. It incorporates an innovative concept termed “Time Weaving” that efficiently encodes time series data to well capture the dynamic trends, thereby mitigating computational complexity. We also propose a novel time lag discovery strategy utilizing XOR operations, which derives a theorem to obtain the optimal time lag and significantly enhances the efficiency using XOR operations. To optimize the search space for causal relationships, we design an efficient pruning strategy that intelligently identifies the most relevant candidate variables, enhancing the efficiency and accuracy of causal discovery. We applied ARROW to four different types of time series causal discovery algorithms and evaluated it on 25 synthetic and real-world datasets. The results demonstrate that, compared to the original algorithms, ARROW achieves up to 153x speedup while achieving higher accuracy in most cases.","We aim to enable efficient causal discovery in time series data. However, existing methods often suffer from low efficiency when dealing with complex time lags, high-dimensional variable combinations, and intensive computation.Our paper introduces a simple yet highly effective idea: to discover causal relationships in time series, we can start from the overall trends and use a lightweight approach to quickly capture temporal dependencies between variables. This might seem surprising, as traditional methods often rely on complex modeling and heavy computation to handle challenges like time lags and non-linearity. Our method builds on two key ideas: Time Weaving, which captures dynamic temporal trend changes with low overhead, and a XOR-based strategy for fast time-lag discovery, while intelligent variable selection minimizes redundant computation.Our research results provide new insights into accelerating causal discovery algorithms and demonstrate that, through ARROW, we can not only significantly speed up the causal discovery process but also achieve better results in most scenarios."
Poster,ARS: Adaptive Reward Scaling for Multi-Task Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45144,"MYUNG-SIK CHO, Jong Eui Park, Jeonghye Kim, Youngchul Sung","Multi-task reinforcement learning (RL) encounters significant challenges due to varying task complexities and their reward distributions from the environment. To address these issues, in this paper, we propose Adaptive Reward Scaling (ARS), a novel framework that dynamically adjusts reward magnitudes and leverages a periodic network reset mechanism. ARS introduces a history-based reward scaling strategy that ensures balanced reward distributions across tasks, enabling stable and efficient training. The reset mechanism complements this approach by mitigating overfitting and ensuring robust convergence. Empirical evaluations on the Meta-World benchmark demonstrate that ARS significantly outperforms baseline methods, achieving superior performance on challenging tasks while maintaining overall learning efficiency. These results validate ARS's effectiveness in tackling diverse multi-task RL problems, paving the way for scalable solutions in complex real-world applications.","How can we train a single agent to tackle many tasks that each give very different rewards? We introduce Adaptive Reward Scaling (ARS), a simple method that watches past rewards and automatically rescales them so no task dominates learning. At the same time, ARS periodically “restarts” the agent’s network weights to prevent overfitting to easy tasks. In experiments on the Meta-World robot benchmark, this combination lets a single policy learn faster, solve harder tasks, and stay stable, showing that balancing reward scales and occasional resets can make multi-task reinforcement learning both more efficient and more reliable."
Poster,A Sample Efficient Conditional Independence Test in the Presence of Discretization,https://ICML.cc//virtual/2025/poster/43894,"Boyang Sun, Yu Yao, Xinshuai Dong, Zongfang Liu, Tongliang Liu, Yumou Qiu, Kun Zhang","Conditional independence (CI) test is a fundamental concept in statistics. In many real-world scenarios, some variables may be difficult to measure accurately, often leading to data being represented as discretized values. Applying CI tests directly to discretized data, however, can lead to incorrect conclusions about the independence of latent variables. To address this, recent advancements have sought to infer the correct CI relationship between the latent variables by binarizing the observed data. However, this process results in a loss of information, which degrades the test's performance, particularly with small sample sizes. Motivated by this, this paper introduces a new sample-efficient CI test that does not rely on the binarization process. We find that the relationship can be established by addressing an \textit{over-identifying} restriction problem with \textit{Generalized Method of Moments} (GMM). Based on this finding, we have designed a new test statistic, and its asymptotic distribution has been derived. Empirical results across various datasets show that our method consistently outperforms existing ones.","We propose DCT-GMM, a sample-efficient conditional independence (CI) test tailored for scenarios where inherently continuous variables are discretized due to measurement limitations. Unlike the original DCT method, which estimates the CI-related parameter by solving a single equation despite the availability of multiple moment conditions, DCT-GMM addresses this overidentification problem using the Generalized Method of Moments (GMM). This allows for more efficient estimation and valid statistical inference. We demonstrate both theoretically and empirically that DCT-GMM outperforms DCT in terms of accuracy and robustness."
Poster,A Selective Learning Method for Temporal Graph Continual Learning,https://ICML.cc//virtual/2025/poster/46118,"Hanmo Liu, Shimin Di, Haoyang LI, Xun Jian, Yue Wang, Lei Chen","Node classification is a key task in temporal graph learning (TGL). Real-life temporal graphs often introduce new node classes over time, but existing TGL methods assume a fixed set of classes. This assumption brings limitations, as updating models with full data is costly, while focusing only on new classes results in forgetting old ones. Graph continual learning (GCL) methods mitigate forgetting using old-class subsets but fail to account for their evolution. We define this novel problem as temporal graph continual learning (TGCL), which focuses on efficiently maintaining up-to-date knowledge of old classes. To tackle TGCL, we propose a selective learning framework that substitutes the old-class data with its subsets, Learning Towards the Future (LTF). We derive an upper bound on the error caused by such replacement and transform it into objectives for selecting and learning subsets that minimize classification error while preserving the distribution of the full old-class data. Experiments on three real-world datasets show that LTF effectively addresses the TGCL challenge.","Many real-world networks—like social media or online marketplaces—change over time. Our research was motivated by a challenge faced in these dynamic systems: traditional models assume a fixed set of categories, but over time, new categories arise while old ones evolve. To address this, we developed a method called Learning Towards the Future (LTF) that smartly selects and updates a small, representative subset of old data instead of retraining with the full dataset. This selective approach reduces costly computations, avoids losing important information on older categories, and maintains an up-to-date model in environments where data is continuously changing. Our experiments using real-world datasets show that LTF effectively preserves accuracy and adapts over time, ensuring better performance in dynamic settings."
Poster,A Sharper Global Convergence Analysis for Average Reward Reinforcement Learning via an Actor-Critic Approach,https://ICML.cc//virtual/2025/poster/43810,"Swetha Ganesh, Washim Mondal, Vaneet Aggarwal","This work examines average-reward reinforcement learning with general policy parametrization. Existing state-of-the-art (SOTA) guarantees for this problem are either suboptimal or hindered by several challenges, including poor scalability with respect to the size of the state-action space, high iteration complexity, and a significant dependence on knowledge of mixing times and hitting times. To address these limitations, we propose a Multi-level Monte Carlo-based Natural Actor-Critic (MLMC-NAC) algorithm. Our work is the first to achieve a global convergence rate of $\tilde{\mathcal{O}}(1/\sqrt{T})$ for average-reward Markov Decision Processes (MDPs) (where $T$ is the horizon length), using an Actor-Critic approach. Moreover, the convergence rate does not scale with the size of the state space, therefore even being applicable to infinite state spaces.","This work focuses on training decision-making systems that aim to maximize long-term rewards without any discounting, a problem known as average-reward reinforcement learning. Existing theoretical results are either suboptimal or struggle to scale when faced with large state and action spaces. To address these limitations, we introduce a new method called MLMC-NAC, short for Multi-level Monte Carlo-based Natural Actor-Critic. The use of Multi-level Monte Carlo (MLMC) helps to efficiently reduce the bias from Markovian sampling."
