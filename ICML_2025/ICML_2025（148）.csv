type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Info-Coevolution: An Efficient Framework for Data Model Coevolution,https://ICML.cc//virtual/2025/poster/45070,"Ziheng Qin, Hailun Xu, Wei Yew, Qi Jia, Yang Luo, Kanchan Sarkar, Danhui Guan, Kai Wang, Yang You","Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costsby 32% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.","Machine learning relies on data, and in many cases data (annotation) cost could be higher than the training cost. In this work, we propose an efficient and scalable algorithm, Info-Coevolution, to enable building a dataset at a lower cost. It can save both annotation and training costs, and make the model coevolve with data."
Poster,InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory,https://ICML.cc//virtual/2025/poster/44659,"Feifei Li, Mi Zhang, Zhaoxiang Wang, Min Yang","Interpretability of point cloud (PC) models becomes imperative given their deployment in safety-critical scenarios such as autonomous vehicles. We focus on attributing PC model outputs to interpretable critical concepts, defined as meaningful subsets of the input point cloud.To enable human-understandable diagnostics of model failures, an ideal critical subset should be *faithful* (preserving points that causally influence predictions) and *conceptually coherent* (forming semantically meaningful structures that align with human perception).We propose InfoCons, an explanation framework that applies information-theoretic principles to decompose the point cloud into 3D concepts, enabling the examination of their causal effect on model predictions with learnable priors.We evaluate InfoCons on synthetic datasets for classification, comparing it qualitatively and quantitatively with four baselines. We further demonstrate its scalability and flexibility on two real-world datasets and in two applications that utilize critical scores of PC.","3D point clouds are like images of the world made up of millions of points, captured by sensors such as LiDAR. Neural networks are trained to percept these point clouds—for example, to help self-driving cars or robots understand their surroundings—but they sometimes make mistakes. Why do they fail? Our method, InfoCons, uses ideas from information theory to break point clouds into meaningful 3D parts, so we can see which parts of the data were critical to the model’s decision. This helps us understand failures—for example, when a model overlooks a car on the road, or confuses a flower pot with a plant, or mixes up a table with a desk. These mistakes can have serious consequences in real-world settings. InfoCons offers visual explanations for model decisions and works across different datasets and tasks, helping researchers and developers diagnose and improve 3D perception systems."
Poster,Information Bottleneck-guided MLPs for Robust Spatial-temporal Forecasting,https://ICML.cc//virtual/2025/poster/43981,"Min Chen, Guansong Pang, Wenjun Wang, Cheng Yan","Spatial-temporal forecasting (STF) plays a pivotal role in urban planning and computing. Spatial-Temporal Graph Neural Networks (STGNNs) excel at modeling spatial-temporal dynamics, thus being robust against noise perturbations. However, they often suffer from relatively poor computational efficiency. Simplifying the architectures can improve efficiency but also weakens robustness with respect to noise interference. In this study, we investigate the problem: *can simple neural networks such as Multi-Layer Perceptrons (MLPs) achieve robust spatial-temporal forecasting while remaining efficient?* To this end, we first reveal the *dual noise effect* in spatial-temporal data and propose a theoretically grounded principle termed *Robust Spatial-Temporal Information Bottleneck* (RSTIB), which holds strong potential for improving model robustness. We then design an implementation named *RSTIB-MLP*, together with a new training regime incorporating a knowledge distillation module, to enhance the robustness of MLPs for STF while maintaining their efficiency. Comprehensive experiments demonstrate that *RSTIB-MLP* achieves an excellent trade-off between robustness and efficiency, outperforming state-of-the-art STGNNs and MLP-based models. Our code is publicly available at: [https://github.com/mchen644/RSTIB](https://github.com/mchen644/RSTIB).","Forecasting how urban environments change over space and time helps city planners make better decisions, such as managing traffic or predicting weather. Currently, advanced forecasting methods known as Spatial-Temporal Graph Neural Networks (STGNNs) can handle the complexity of such tasks effectively, especially when data contains noise. However, these methods tend to be slow and computationally expensive. Simpler models like Multi-Layer Perceptrons (MLPs) are faster but typically struggle when data is noisy. In this study, we aim to investigate: can simpler forecasting models like MLPs become both efficient and robust against noisy data? We first reveal the _dual noise effect_ which characterizes how noise harms the forecasting model. Based on this understanding, we develop a new principle termed the Robust Spatial-Temporal Information Bottleneck (RSTIB). We then design an MLP-based model using RSTIB principle and special training regime. Our experiments show this new model successfully achieve a better balance between efficiency and robustness."
Poster,InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective,https://ICML.cc//virtual/2025/poster/45048,"Yuanhong Zhang, Muyao Yuan, Weizhan Zhang, Tieliang Gong, Wen Wen, Jiangyong Ying, Weijie Shi","The Segment Anything Model (SAM), a vision foundation model, exhibits impressive zero-shot capabilities in general tasks but struggles in specialized domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to unleash the potential of SAM in novel scenarios. However, existing PEFT methods for SAM neglect the domain-invariant relations encoded in the pre-trained model. To bridge this gap, we propose InfoSAM, an information-theoretic approach that enhances SAM fine-tuning by distilling and preserving its pre-trained segmentation knowledge. Specifically, we formulate the knowledge transfer process as two novel mutual information-based objectives: (i) to compress the domain-invariant relation extracted from pre-trained SAM, excluding pseudo-invariant information as possible, and (ii) to maximize mutual information between the relational knowledge learned by the teacher (pre-trained SAM) and the student (fine-tuned model).  The proposed InfoSAM establishes a robust distillation framework for PEFT of SAM. Extensive experiments across diverse benchmarks validate InfoSAM's effectiveness in improving SAM family's performance on real-world tasks, demonstrating its adaptability and superiority in handling specialized scenarios. The code and models are available at https://muyaoyuan.github.io/InfoSAM_Page.","The pretrained AI models like the Segment Anything Model (SAM) promise to recognize anything in an image — but when moved to new domains, like medical or industrial settings, they often fall short. Why? Because these models weren’t trained for those situations.Instead of rebuilding the model from the ground up, researchers often fine-tune just a few parts. This is efficient, but it risks making the model forget the domain-invariant visual patterns it learned from large-scale pretraining. That’s where InfoSAM comes in. Think of it as a translator between the old model and the new task. Drawing from information theory, InfoSAM identifies and preserves the universal visual patterns that remain useful, while adapting only what truly needs to change.This approach makes SAM more effective in specialized settings, with minimal effort and greater accuracy. From natural-scene segmentation to healthcare, InfoSAM helps bring advanced pretrained AI tools into the real world, where reliable performance matters most."
Poster,InfoSEM: A Deep Generative Model with Informative Priors for Gene Regulatory Network Inference,https://ICML.cc//virtual/2025/poster/44742,"Tianyu Cui, Song-Jun Xu, Artem Moskalev, Shuwei Li, Tommaso Mansi, Mangal Prakash, Rui Liao","Inferring Gene Regulatory Networks (GRNs) from gene expression data is crucial for understanding biological processes. While supervised models are reported to achieve high performance for this task, they rely on costly ground truth (GT) labels and risk learning gene-specific biases—such as class imbalances of GT interactions—rather than true regulatory mechanisms. To address these issues, we introduce InfoSEM, an unsupervised generative model that leverages textual gene embeddings as informative priors, improving GRN inference without GT labels. InfoSEM can also integrate GT labels as an additional prior when available, avoiding biases and further enhancing performance. Additionally, we propose a biologically motivated benchmarking framework that better reflects real-world applications such as biomarker discovery and reveals learned biases of existing supervised methods. InfoSEM outperforms existing models by 38.5% across four datasets using textual embeddings prior and further boosts performance by 11.1% when integrating labeled data as priors.","To understand how our cells work, scientists study how genes turn each other on or off. New biotechnologies let us look at gene activity in individual cells, but the data is messy and hard to analyze. Current computational models that do this well often depend on expensive lab results and may give misleading answers by relying too much on patterns in the data that don’t reflect real biology. We created a new AI tool called InfoSEM that learns how genes interact by combining generative models with existing knowledge about what genes do—without needing costly lab results. When available, InfoSEM can still use lab data to improve its accuracy, but in a way that avoids common pitfalls. We also designed a new way to test these tools that better matches real scientific problems. InfoSEM gives more accurate results than other tools, even those that use expensive lab data. It helps scientists find real gene interactions more reliably, which makes it a powerful and practical tool for studying health, disease, and medicine."
Poster,INRFlow: Flow Matching for INRs in Ambient Space,https://ICML.cc//virtual/2025/poster/43948,"Yuyang Wang, Anurag Ranjan, Joshua M Susskind, Miguel Angel Bautista Martin","Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on irregular or unstructured data like 3D point clouds or even protein structures. These models are commonly trained in two stages: first, a data compressor is trained, and in a subsequent training stage a flow matching generative model is trained in the latent space of the data compressor. This two-stage paradigm sets obstacles for unifying models across data domains, as hand-crafted compressors architectures are used for different data modalities. To this end, we introduce INRFlow, a domain-agnostic approach to learn flow matching transformers directly in ambient space. Drawing inspiration from INRs, we introduce a conditionally independent point-wise training objective that enables INRFlow to make predictions continuously in coordinate space. Our empirical results demonstrate that INRFlow effectively handles different data modalities such as images, 3D point clouds and protein structure data, achieving strong performance in different domains and outperforming comparable approaches. INRFlow is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.","We present a new generative model called INRFlow, which is designed to generate different types of data—like images, 3D objects, or protein structures—in a flexible and unified way. Currently popular generative models involve two separate steps: one model to compress the data, and another model to learn to generate the compressed data. However, this approach often requires handcrafting different models for different types of data which makes it hard to use the same method across domains. INRFlow avoids this issue by skipping the compression step and working directly with the original data. Inspired by a concept called ""implicit neural representations"" (INRs), it learns to understand and generate data continuously across space, treating data points (like pixels) as coordinate-value pairs. We show that INRFlow works well across different kinds of data and doesn't need custom tweaks for each domain. This makes it a promising step toward building a single, general-purpose model that can generate diverse types of data without specialized design."
Poster,Instance Correlation Graph-based Naive Bayes,https://ICML.cc//virtual/2025/poster/44386,"Chengyuan Li, Liangxiao Jiang, Wenjun Zhang, Liangjun Yu, Huan Zhang","Due to its simplicity, effectiveness and robustness, naive Bayes (NB) has continued to be one of the top 10 data mining algorithms. To improve its performance, a large number of improved algorithms have been proposed in the last few decades. However, in addition to Gaussian naive Bayes (GNB), there is little work on numerical attributes. At the same time,  none of them takes into account the correlations among instances. To fill this gap, we propose a novel algorithm called instance correlation graph-based naive Bayes (ICGNB). Specifically, it first uses original attributes to construct an instance correlation graph (ICG) to represent the correlations among instances. Then, it employs a variational graph auto-encoder (VGAE) to generate new attributes from the constructed ICG and uses them to augment original attributes.Finally, it weights each augmented attribute to alleviate the attribute redundancy and builds GNB on the weighted attributes. The experimental results on tens of datasets show that ICGNB significantly outperforms its deserved competitors.Our codes and datasets are available at https://github.com/jiangliangxiao/ICGNB.","Naive Bayes (NB) is a simple and widely-used method that predicts the classes of unknown items according to some existing items whose classes are known. The original information of existing items is usually limited, and we wanted to obtain more information by representing and using the correlations among items.Specifically, we construct a graph to represent these correlations and then use a powerful information generation program to mine new information from the constructed graph. Then, we group together the original information and the generated new information, and then alleviate redundant information. Surprisingly, we found that this grouped information works really well and improves NB’s predictive ability, thereby overcoming existing correlated methods in most cases.Our method has implications for how to predict the classes of unknown items by leveraging correlations among items. To help other researchers explore this idea, we have released our method called ICGNB, along with the method’s settings."
Poster,Instance-Optimal Pure Exploration for Linear Bandits on Continuous Arms,https://ICML.cc//virtual/2025/poster/44570,"Sho Takemori, Yuhei Umeda, Aditya Gopalan","This paper studies a pure exploration problem with linear bandit feedback on continuous arm sets, aiming to identify an $\epsilon$-optimal arm with high probability. Previous approaches for continuous arm sets have employed instance-independent methods due to technical challenges such as the infinite dimensionality of the space of probability measures and the non-smoothness of the objective function. This paper proposes a novel, tractable algorithm that addresses these challenges by leveraging a reparametrization of the sampling distribution and projected subgradient descent. However, this approach introduces new challenges related to the projection and reconstruction of the distribution from the reparametrization. We address these by focusing on the connection to the approximate Carath\'eodory  problem. Compared to the original optimization problem on the infinite-dimensional space, our method is tractable, requiring only the solution of quadratic and fractional quadratic problems on the arm set. We establish an instance-dependent optimality for our method, and empirical results on synthetic data demonstrate its superiority over existing instance-independent baselines.","The linear bandit problem is an online optimization problem involving an unknown, linear reward function. The learner gains information about this function solely through noisy evaluations. Recognizing its practical relevance, we focus on the case of a continuous arm set (action space), which can be viewed as a specialized instance of Bayesian optimization. Furthermore, we address a pure exploration setting, where the learner's primary goal is to identify a high-performing arm as quickly as possible. Devising an (asymptotically) optimal learning strategy is a significant hurdle, as it necessitates optimization over the probability space defined on the arm set, a potentially infinite-dimensional space. We present a tractable algorithm, requiring a manageable number of oracle calls, and formally demonstrate its asymptotic optimality."
Poster,Instruct2See: Learning to Remove Any Obstructions Across Distributions,https://ICML.cc//virtual/2025/poster/45263,"Junhang Li, Yu Guo, Xian, Shengfeng He","Images are often obstructed by various obstacles due to capture limitations, hindering the observation of objects of interest. Most existing methods address occlusions from specific elements like fences or raindrops, but are constrained by the wide range of real-world obstructions, making comprehensive data collection impractical. To overcome these challenges, we propose Instruct2See, a novel zero-shot framework capable of handling both seen and unseen obstacles. The core idea of our approach is to unify obstruction removal by treating it as a soft-hard mask restoration problem, where any obstruction can be represented using multi-modal prompts, such as visual semantics and textual instructions, processed through a cross-attention unit to enhance contextual understanding and improve mode control. Additionally, a tunable mask adapter allows for dynamic soft masking, enabling real-time adjustment of inaccurate masks. Extensive experiments on both in-distribution and out-of-distribution obstacles show that Instruct2See consistently achieves strong performance and generalization in obstruction removal, regardless of whether the obstacles were present during the training phase. Code and dataset are available at https://jhscut.github.io/Instruct2See.","Images are often obstructed by various obstacles due to capture limitations, but existing methods can't handle all types of obstruction. We create Instruct2See, which adopts multi-modal prompts to automatically remove unwanted obstructions. Unlike traditional methods, it removes any obstruction without extra training required for specific types, making image restoration universal and effortless."
Poster,Instruction-Following Pruning for Large Language Models,https://ICML.cc//virtual/2025/poster/44283,"Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei","With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed ""instruction-following pruning'', introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model.","Modern LLMs are powerful but often too large to run efficiently on devices like laptops or phones. One common way to make them smaller is model pruning - removing parts of the model that aren’t essential. But these pruned models are usually fixed. However, the fixed nature of the pruned model poses challenges in real-world inference scenarios, where tasks can vary significantly.We propose a smarter approach: dynamically decide which parts of the LLM should be used based on the user’s request. For example, if the prompt is about programming, the model activates the parts that are good at code. If it’s a math problem, it chooses different parts. This technique, which we call Instruction-Following Pruning (IFPruning), saves memory and speeds up the model without sacrificing much performance.IFPruning achieves strong performance, often beating models of the same reduced size, and stays close to the original full-size model. It also leads to interpretable patterns, where similar tasks activate similar parts of the model — helping us better understand what the model uses to think. Finally, it brings significant speedups, cutting generation time by over 40% while keeping performance high."
