type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Learning Vision and Language Concepts for Controllable Image Generation,https://ICML.cc//virtual/2025/poster/44412,"Shaoan Xie, Lingjing Kong, Yujia Zheng, Zeyu Tang, Eric Xing, Guangyi Chen, Kun Zhang","Concept learning seeks to extract semantic and interpretable representations of atomic concepts from high-dimensional data such as images and text, which can be instrumental to a variety of downstream tasks (e.g., image generation/editing). Despite its importance, the theoretical foundations for learning atomic concepts and their interactions, especially from multimodal distributions, remain underexplored.In this work, we establish fundamental conditions for learning atomic multimodal concepts and their underlying interactions With identfiability guarantees. We formulate concept learning as a latent variable identification problem, representing atomic concepts in each modality as latent variables, with a graphical model to specify their interactions across modalities. Our theoretical contribution is to provide component-wise identifiability of atomic concepts under flexible, nonparametric conditions that accommodate both continuous and discrete modalities.  Building on these theoretical insights, we demonstrate the practical utility of our theory in a downstream task text-to-image (T2I) generation. We develop a principled T2I model that explicitly learns atomic textual and visual concepts with sparse connections between them, allowing us to achieve image generation and editing at the atomic concept level. Empirical evaluations show that our model outperforms existing methods in T2I generation tasks, offering superior controllability and interpretability.","Consider a common use case in text-to-image generation: a user provides a text prompt to generate an image, then wishes to make minor edits, such as changing only the color of the clothing. Controllable generation enables the user to modify the prompt accordingly, prompting the model to adjust the specified feature while preserving all other aspects of the image. This ability to make targeted changes without unintended alterations underscores the importance of controllable text-to-image generation.In this study, the authors provide a solid theoretical foundation for how to learn atomic vision and language concepts and understand how they relate to each other. They treat concept learning as a hidden structure problem, using a mathematical model where each concept is unknown. Their theory shows that it's possible to identify each concept—even when the data types are varied and complex—without making strict assumptions.They then apply their theory to improve a practical task: generating images from text descriptions. They introduce a new model  that learns clear, sparse connections between text and image concepts. Tests show that this model produces better, more controllable results than existing methods."
Poster,Learning with Exact Invariances in Polynomial Time,https://ICML.cc//virtual/2025/poster/44601,"Ashkan Soleymani, Behrooz Tahmasebi, Stefanie Jegelka, Patrick Jaillet","We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with \emph{exact} invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem.  To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (as opposed to approximate) invariances in this setting, partially addressing a question posed by Diaz (2025) regarding the avoidance of prohibitively large and computationally intensive group averaging methods in kernel regression with exact invariances.  Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest.","We study how to efficiently learn models that are exactly invariant to symmetries in the data—such as rotations or permutations—using kernel methods. Common strategies like data augmentation or group averaging can be computationally expensive or hard to apply in kernel settings. In contrast, we propose a polynomial-time algorithm that enforces exact invariance, assuming access to some geometric structure of the input space. Our method matches the generalization performance of standard kernel regression, while guaranteeing exact symmetry. To our knowledge, this is the first such efficient approach in this setting, addressing a question recently raised in the literature. Our solution is built on a novel optimization reformulation that may also be of independent interest."
Poster,Learning with Expected Signatures: Theory and Applications,https://ICML.cc//virtual/2025/poster/43548,"Lorenzo Lucchese, Mikko S. Pakkanen, Almut E. D. Veraart","The expected signature maps a collection of data streams to a lower dimensional representation, with a remarkable property: the resulting feature tensor can fully characterize the data generating distribution. This ""model-free""' embedding has been successfully leveraged to build multiple domain-agnostic machine learning (ML) algorithms for time series and sequential data. The convergence results proved in this paper bridge the gap between the expected signature's empirical discrete-time estimator and its theoretical continuous-time value, allowing for a more complete probabilistic interpretation of expected signature-based ML methods. Moreover, when the data generating process is a martingale, we suggest a simple modification of the expected signature estimator with significantly lower mean squared error and empirically demonstrate how it can be effectively applied to improve predictive performance.","Stream data—also known as time series, path data, data sequences, or by countless other names—are a common way to represent quantities that evolve over time. These representations are often long and may include redundant information, which can make them challenging for standard machine learning models to handle effectively. The expected signature offers a way to summarize such data into a lower-dimensional representation (an embedding) that is easier to work with. This transformation does not rely on additional assumptions about how the data were generated and does not require setting extra parameters, making it broadly applicable.In the ideal case of continuous data sampled at infinite frequency, this embedding enjoys strong theoretical guarantees that also cover processes with highly irregular realizations, such as those observed in financial markets or complex physical systems. In practice, however, we only observe finitely many data at discrete time points. This paper provides general conditions under which the estimator we compute from such data reliably converges to the ideal expected signature. We also propose a new estimator that improves on the standard one—both in terms of theoretical guarantees and empirical performance across a wide range of scenarios."
Poster,Learning With Multi-Group Guarantees For Clusterable Subpopulations,https://ICML.cc//virtual/2025/poster/44264,"Jessica Dai, Nika Haghtalab, Eric Zhao","A canonical desideratum for prediction problems is that performance guarantees should hold not just on average over the population, but also for meaningful subpopulations within the overall population. But what constitutes a meaningful subpopulation? In this work, we take the perspective that relevant subpopulations should be defined with respect to the clusters that naturally emerge from the distribution of individuals for which predictions are being made. In this view, a population refers to a mixture model whose components constitute the relevant subpopulations. We suggest two formalisms for capturing per-subgroup guarantees: first, by attributing each individual to the component from which they were most likely drawn, given their features; and second, by attributing each individual to all components in proportion to their relative likelihood of having been drawn from each component. Using online calibration as a case study, we study a multi-objective algorithm that provides guarantees for each of these formalisms by handling all plausible underlying subpopulation structures simultaneously, and achieve an $O(T^{1/2})$ rate even when the subpopulations are not well-separated. In comparison, the more natural cluster-then-predict approach that first recovers the structure of the subpopulations and then makes predictions suffers from a $O(T^{2/3})$ rate and requires the subpopulations to be separable. Along the way, we prove that providing per-subgroup calibration guarantees for underlying clusters can be easier than learning the clusters: separation between median subgroup features is required for the latter but not the former.","One common goal for designing algorithms that make predictions is for those algorithms to perform well not just on average over the whole population, but also for meaningful subsets of that population. For example, the algorithm should be equally accurate across gender, race, and their intersections. In this work, we explore an alternative way to define subgroups from a population. Rather than using static, pre-defined identity categories (like gender and race), can we provide performance guarantees on subgroups that we learn directly from the relevant data? We study this question from a theoretical perspective and answer in the affirmative, showing an algorithmic strategy that, surprisingly, can perform almost as well as an algorithm that had pre-defined the groups ahead of time."
Poster,Learning without Isolation: Pathway Protection for Continual Learning,https://ICML.cc//virtual/2025/poster/46536,"Zhikang Chen, Abudukelimu Wuerkaixi, Sen Cui, Haoxuan Li, Ding Li, Jingfeng Zhang, Bo Han, Gang Niu, Houfang Liu, Yi Yang, Sifan YANG, Changshui Zhang, Tianling Ren","Deep networks are prone to catastrophic forgetting during sequential task learning, i.e., losing the knowledge about old tasks upon learning new tasks. To this end, continual learning (CL) has emerged, whose existing methods focus mostly on regulating or protecting the parameters associated with the previous tasks. However, parameter protection is often impractical, since the size of parameters for storing the old-task knowledge increases linearly with the number of tasks, otherwise it is hard to preserve the parameters related to the old-task knowledge. In this work, we bring a dual opinion from neuroscience and physics to CL: in the whole networks, the pathways matter more than the parameters when concerning the knowledge acquired from the old tasks. Following this opinion, we propose a novel CL framework, learning without isolation (LwI), where model fusion is formulated as graph matching and the pathways occupied by the old tasks are protected without being isolated. Thanks to the sparsity of activation channels in a deep network, LwI can adaptively allocate available pathways for a new task, realizing pathway protection and addressing catastrophic forgetting in a parameter-effcient manner. Experiments on popular benchmark datasets demonstrate the superiority of the proposed LwI.","Deep learning models often struggle with ""catastrophic forgetting""—when learning new tasks, they abruptly lose knowledge of previous ones. Inspired by neuroscience and physics, where knowledge relies more on pathways (how neurons connect) than individual neurons, we propose Learning without Isolation (LwI). Instead of isolating or freezing old parameters, LwI treats the model like a dynamic road network: it identifies and protects critical ""pathways"" for old tasks while flexibly assigning unused ""routes"" to new tasks. This sparse, adaptive approach avoids forgetting without wasting resources. Tested on standard benchmarks, LwI outperforms existing methods while using fewer parameters—like a brain efficiently organizing knowledge. This work bridges neuroscience, physics and AI, offering a scalable solution for lifelong learning systems."
Poster,Learning with Selectively Labeled Data from Multiple Decision-makers,https://ICML.cc//virtual/2025/poster/43975,"Jian Chen, Zhehao Li, Xiaojie Mao","We study the problem of classification with selectively labeled data, whose distribution may differ from the full population due to historical decision-making. We exploit the fact that in many applications historical decisions were made by multiple decision-makers, each with different decision rules. We analyze this setup under a principled instrumental variable (IV) framework and rigorously study the identification of classification risk. We establish conditions for the exact identification of classification risk and derive tight partial identification bounds when exact identification fails. We further propose a unified cost-sensitive learning (UCL) approach to learn classifiers robust to selection bias in both identification settings. Finally, we theoretically and numerically validate the efficacy of our proposed method.","We study the problem of classification with selectively labeled data, whose distribution may differ from the full population due to historical decision-making. We exploit the fact that in many applications historical decisions were made by multiple decision-makers, each with different decision rules. We analyze this setup under a principled instrumental variable (IV) framework and rigorously study the identification of classification risk. We establish conditions for the exact identification of classification risk and derive tight partial identification bounds when exact identification fails. We further propose a unified cost-sensitive learning (UCL) approach to learn classifiers robust to selection bias in both identification settings. Finally, we theoretically and numerically validate the efficacy of our proposed method."
Poster,Learn Singularly Perturbed Solutions via Homotopy Dynamics,https://ICML.cc//virtual/2025/poster/45414,"Chuqi CHEN, Yahong Yang, Yang Xiang, Wenrui Hao","Solving partial differential equations (PDEs) using neural networks has become a central focus in scientific machine learning. Training neural networks for singularly perturbed problems is particularly challenging due to certain parameters in the PDEs that introduce near-singularities in the loss function. In this study, we overcome this challenge by introducing a novel method based on homotopy dynamics to effectively manipulate these parameters.  From a theoretical perspective, we analyze the effects of these parameters on training difficulty in these singularly perturbed problems and establish the convergence of the proposed homotopy dynamics method. Experimentally, we demonstrate that our approach significantly accelerates convergence and improves the accuracy of these singularly perturbed problems. These findings present an efficient optimization strategy leveraging homotopy dynamics, offering a robust framework to extend the applicability of neural networks for solving singularly perturbed differential equations.","Solving partial differential equations (PDEs) is fundamental to modeling physical systems, yet traditional numerical methods can be computationally expensive and often break down on complex tasks—especially high-dimensional PDEs or large families of similar problems. Neural networks offer an attractive alternative; however, they frequently fail to converge on singularly perturbed PDEs, where small parameters induce sharp layers or rapid oscillations in the solution. Such behavior produces highly nonconvex loss landscapes, making it difficult for gradient-based optimizers to drive the loss down and easy to become trapped in poor local minima.To address this, we introduce a novel training strategy called Homotopy Dynamics. Our method gradually transforms an easier problem into the target problem by smoothly changing the PDE parameter during training. This helps the neural network stay close to the correct solution path and avoid poor local minima.We analyze this approach theoretically and show that it improves convergence. Empirically, we test it on several difficult problems, including the Allen–Cahn and Helmholtz equations, as well as in an operator learning setting with DeepONet. In all cases, our method significantly improves training stability and solution quality.This work not only deepens our understanding of training dynamics for neural PDE solvers, but also provides a practical tool for accelerating scientific computing with machine learning."
Poster,Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control,https://ICML.cc//virtual/2025/poster/46404,"Sepehr Elahi, Paula Mürmann, Patrick Thiran","The Susceptible-Infected-Susceptible (SIS) model is a widely used model for the spread of information and infectious diseases, particularly non-immunizing ones, on a graph. Given a highly contagious disease, a natural question is how to best vaccinate individuals to minimize the disease's extinction time. While previous works showed that the problem of optimal vaccination is closely linked to the NP-hard *Spectral Radius Minimization* (SRM) problem, they assumed that the graph is known, which is often not the case in practice. In this work, we consider the problem of minimizing the extinction time of an outbreak modeled by an SIS model where the graph on which the disease spreads is unknown and only the infection states of the vertices are observed. To this end, we split the problem into two: learning the graph and determining effective vaccination strategies. We propose a novel inclusion-exclusion-based learning algorithm and, unlike previous approaches, establish its sample complexity for graph recovery. We then detail an optimal algorithm for the SRM problem and prove that its running time is polynomial in the number of vertices for graphs with bounded treewidth. This is complemented by an efficient and effective polynomial-time greedy heuristic for any graph. Finally, we present experiments on synthetic and real-world data that numerically validate our learning and vaccination algorithms.","Imagine a contagious disease spreading through a population, but the contact network—who is infecting whom—remains completely hidden. Only who gets infected and when is observed, and a limited number of vaccines must be allocated to stop the outbreak as quickly as possible. Our work tackles exactly this: how to choose effective vaccinations when the contact network is hidden. We first develop an algorithm that reconstructs the hidden contact network using only observations of who gets sick and when. With the learned network in hand, we propose two vaccination strategies that determine who to vaccinate: one that’s mathematically optimal but slow to compute on big networks, and another that’s much quicker and almost as good. We show that our learn-to-vaccinate approach effectively controls simulated outbreaks across a range of settings, including real-world contact networks from flu epidemics. This early work paves the way for smarter, data-efficient vaccination strategies that can support faster, more effective outbreak response—even when the underlying contact network is unknown."
Poster,Learnware Specification via Dual Alignment,https://ICML.cc//virtual/2025/poster/43500,"Wei Chen, Jun-Xiang Mao, Xiaozheng Wang, Min-Ling Zhang","The learnware paradigm aims to establish a learnware dock system that contains numerous leanwares, each consisting of a well-trained model and a specification, enabling users to reuse high-performing models for their tasks instead of training from scratch. The specification, as a unique characterization of the model's specialties, dominates the effectiveness of model reuse. Existing specification methods mainly employ distribution alignment to generate specifications. However, this approach overlooks the model's discriminative performance, hindering an adequate specialty characterization. In this paper, we claim that it is beneficial to incorporate such discriminative performance for high-quality specification generation. Accordingly, a novel specification approach named Dali, i.e., Learnware Specification via Dual ALIgnment, is proposed. In Dali, the characterization of the model's discriminative performance is modeled as discriminative alignment, which is considered along with distribution alignment in the specification generation process. Theoretical and empirical analyses clearly demonstrate that the proposed approach is capable of facilitating model reuse in the learnware paradigm with high-quality specification generation.","The learnware system lets people reuse existing machine learning models instead of building new ones. Each model has a ""specification"" describing its capability. Existing methods focused on matching data patterns but ignored how well the model works on actual tasks. The new method, Dali, looks at both data patterns and real-world performance to create a better specification. Experiment and theory show this approach works better for finding useful models."
Poster,Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams,https://ICML.cc//virtual/2025/poster/45829,"Yuan Feng, Yukun Cao, Hairu Wang, Xike Xie, S Kevin Zhou","Sketches, probabilistic structures for estimating item frequencies in infinite data streams with limited space, are widely used across various domains. Recent studies have shifted the focus from handcrafted sketches to neural sketches, leveraging memory-augmented neural networks (MANNs) to enhance the streaming compression capabilities and achieve better space-accuracy trade-offs.However, existing neural sketches struggle to scale across different data domains and space budgets due to inflexible MANN configurations. In this paper, we introduce a scalable MANN architecture that brings to life the Lego sketch, a novel sketch with superior scalability and accuracy.Much like assembling creations with modular Lego bricks, the Lego sketch dynamically coordinates multiple memory bricks to adapt to various space budgets and diverse data domains.Theoretical analysis and empirical studies demonstrate its scalability and superior space-accuracy trade-offs, outperforming existing handcrafted and neural sketches.","Traditional methods typically employ hand-crafted algorithms to compress large-scale, high-speed data streams and support subsequent queries. Recent approaches using end-to-end learned memory-augmented neural networks have improved the compression accuracy; however, their limited the scalability hinder real-world applications. We introduce a scalable, end-to-end trained memory-augmented neural network architecture for data stream compression that markedly enhances scalability while further improving accuracy. This work overcomes critical barriers to the practical deployment of such end-to-end learning-based compression techniques, paving the way for more accurate and efficient processing of large-scale, high-speed data streams."
