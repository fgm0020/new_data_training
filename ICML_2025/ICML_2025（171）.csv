type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,LightGTS: A Lightweight General Time Series Forecasting Model,https://ICML.cc//virtual/2025/poster/44879,"Yihang Wang, Yuying Qiu, Peng Chen, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo","Existing works on general time series forecasting build foundation models with heavy model parameters through large-scale multi-source pretraining. These models achieve superior generalization ability across various datasets at the cost of significant computational burdens and limitations in resource-constrained scenarios. This paper introduces LightGTS, a lightweight general time series forecasting model designed from the perspective of consistent periodical modeling. To handle diverse scales and intrinsic periods in multi-source pre-training, we introduce Periodical Tokenization, which extracts consistent periodic patterns across different datasets with varying scales. To better utilize the periodicity in the decoding process, we further introduce Periodical Parallel Decoding, which leverage historical tokens to improve forecasting. Based on the two techniques above which fully leverage the inductive bias of periods inherent in time series, LightGTS uses a lightweight model to achieve outstanding performance on general time series forecasting. It achieves state-of-the-art forecasting performance on 9 real-world benchmarks in both zero-shot and full-shot setting with much better efficiency compared with existing time series foundation models","Existing approaches to general time series forecasting often rely on large-scale data and complex multi-source pre-training, resulting in models with massive parameters and high computational costs—making them impractical in resource-constrained settings. To address this, we introduce LightGTS, a lightweight general time series forecasting model designed around consistent periodical modeling. To handle diverse scales and intrinsic periodicities in multi-source pre-training, we propose Periodical Tokenization, which extracts consistent periodic patterns across datasets of varying sizes. For improved prediction during decoding, we further develop Periodical Parallel Decoding, leveraging historical information to enhance forecasting accuracy. By fully harnessing the inherent periodic inductive bias of time series through these two techniques, LightGTS achieves exceptional performance with a lightweight architecture. It delivers state-of-the-art forecasting results on 9 real-world benchmarks under both zero-shot and full-shot settings, while significantly outperforming existing time series foundation models in efficiency."
Poster,LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos,https://ICML.cc//virtual/2025/poster/45029,"Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent Tan, Jiashi Feng","Accuracy and speed are critical in image editing tasks. Pan et al. introduced a drag-based framework using Generative Adversarial Networks, and subsequent studies have leveraged large-scale diffusion models. However, these methods often require over a minute per edit and exhibit low success rates. We present LightningDrag, which achieves high-quality drag-based editing in about one second on general images. By redefining drag-based editing as a conditional generation task, we eliminate the need for time-consuming latent optimization or gradient-based guidance. Our model is trained on large-scale paired video frames, capturing diverse motion (object translations, pose shifts, zooming, etc.) to significantly improve accuracy and consistency. Despite being trained only on videos, our model generalizes to local deformations beyond the training data (e.g., lengthening hair, twisting rainbows). Extensive evaluations confirm the superiority of our approach, and we will release both code and model.","Editing images by “dragging” parts of them --- like stretching a smile or moving an object --- is powerful but often slow or limited to narrow domains. Our work introduces LightningDrag, a new AI tool that enables fast and precise drag-based editing on a wide variety of images. Instead of relying on manually labeled data, our method learns from videos, using them to understand how parts of objects move and deform. We also design special inference strategies to improve both the realism of the results and the accuracy of the edits. LightningDrag runs in under a second and works on general images, from faces to abstract scenes, without being confined to a single type of object."
Poster,Lightspeed Geometric Dataset Distance via Sliced Optimal Transport,https://ICML.cc//virtual/2025/poster/43901,"Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho","We introduce  sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is  Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-)linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation.","It is challenging to compare datasets when they vary in size, shape, or the number of classes. We developed a novel method called the sliced optimal transport dataset distance (s-OTDD) that can solve such issues. s-OTDD does not need training and can operate even if datasets have totally different labels. The s-OTDD principle involves using a method called Moment Transform Projection (MTP) to change complex data into simple numbers. Projecting data sets onto a single dimension makes it easy to identify similarity. The process is efficient, accurate, and can be easily applied to big data sets. We used s-OTDD and found that it is in excellent agreement with existing methods which are much slower and also can predict how well a model trained on one dataset will perform on another. This puts s-OTDD as a great tool for machine learning researchers who need to compare various data in a convenient way."
Poster,Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty,https://ICML.cc//virtual/2025/poster/46202,"Yeseul Cho, Baekrok Shin, Changmin Kang, Chulhee Yun","Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs. Dataset pruning aims to alleviate this demand by discarding redundant examples. However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset. To overcome this limitation, we introduce the **Difficulty and Uncertainty-Aware Lightweight (DUAL)** score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning ratio, we further propose a pruning ratio-adaptive sampling using Beta distribution.Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66\% compared to previous methods while achieving a SOTA 60\% test accuracy at a 90\% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15\% while maintaining SOTA performance.","Training modern AI models requires massive datasets and days of computation. Data pruning—removing less important data—can cut training costs while preserving performance. However, most recent pruning methods overemphasize accuracy, often consuming more resources than training on the full dataset.We introduce a faster, cost-effective pruning technique called ""DUAL score"", which evaluates both difficulty and uncertainty of each data point early in training. By combining these signals, DUAL score identifies less informative examples with a decrement in overhead. Data is adaptively pruned based on a prespecified pruning ratio: the higher the ratio, the more likely easier data points are selected for the training. Our method revives the original goal of data pruning—to reduce training cost—without sacrificing accuracy. Experiments on CIFAR and ImageNet-1k demonstrate that DUAL score achieves state-of-the-art accuracy even when pruning 90% of the data while reducing training time by 33% on ImageNet-1k and 85% on CIFAR."
Poster,Lightweight-Mark: Rethinking Deep Learning-Based Watermarking,https://ICML.cc//virtual/2025/poster/44782,"Yupeng Qiu, Han Fang, Ee-Chien Chang","Deep learning-based watermarking models play a crucial role in copyright protection across various applications. However, many high-performance models are limited in practical deployment due to their large number of parameters. Meanwhile, the robustness and invisibility performance of existing lightweight models are unsatisfactory. This presents a pressing need for a watermarking model that combines lightweight capacity with satisfactory performance. Our research identifies a key reason that limits the performance of existing watermarking frameworks: a mismatch between commonly used decoding losses (e.g., mean squared error and binary cross-entropy loss) and the actual decoding goal, leading to parameter redundancy. We propose two innovative solutions: (1) Decoding-oriented surrogate loss (DO), which redesigns the loss function to mitigate the influence of decoding-irrelevant optimization directions; and (2) Detachable projection head (PH), which incorporates a detachable redundant module during training to handle these irrelevant directions and is discarded during inference. Additionally, we propose a novel watermarking framework comprising five submodules, allowing for independent parameter reduction in each component. Our proposed model achieves better efficiency, invisibility, and robustness while utilizing only 2.2\% of the parameters compared to the state-of-the-art frameworks. By improving efficiency while maintaining robust copyright protection, our model is well suited for practical applications in resource-constrained environments. The DO and PH methods are designed to be plug-and-play, facilitating seamless integration into future lightweight models.","Digital watermarks are hidden signatures embedded in media like images or videos to help prove ownership and protect against unauthorized copying. However, most high-performing watermarking tools rely on large models that are too big to run on devices with limited resources.We propose a new watermarking technique that performs well with compact AI models. Instead of directly adapting existing methods, we identified key training challenges for small models and introduced two solutions: one redefines the training objective, and the other introduces a lightweight detachable module used only during training.Our method keeps watermarks invisible and robust, while reducing model size and computational cost. This work makes it easier to apply copyright protection in real-world settings where speed, storage, and power consumption are critical."
Poster,Lightweight Online Adaption for Time Series Foundation Model Forecasts,https://ICML.cc//virtual/2025/poster/44485,"Thomas Lee, William Toner, Rajkarn Singh, Artjom Joosen, Martin Asenov","Foundation models (FMs) have emerged as a promising approach for time series forecasting. While effective, FMs typically remain fixed during deployment due to the high computational costs of learning them online. Consequently, deployed FMs fail to adapt their forecasts to current data characteristics, despite the availability of online feedback from newly arriving data. This raises the question of whether FM performance can be enhanced by the *efficient* usage of this feedback. We propose *ELF* to answer this question. ELF is a lightweight mechanism for the online adaption of FM forecasts in response to online feedback. ELF consists of two parts: **a)** the *ELF-Forecaster* which is used to learn the current data distribution; and **b)** the *ELF-Weighter* which is used to combine the forecasts of the FM and the ELF-Forecaster. We evaluate the performance of ELF in conjunction with several recent FMs across a suite of standard time series datasets. In *all* of our experiments we find that using ELF improves performance. This work demonstrates how efficient usage of online feedback can be used to improve FM forecasts.","One common form of data in the real world are values that change over time. For example, the temperature in a city. Data of this form are called *time series*. One problem of interest is *forecasting* the future of a time series. For instance, predicting future temperatures. In this work we look at a particular type of model to forecast time series: *foundation models* (FMs), large neural networks similar in form to large language models like ChatGPT. While FMs have been shown to forecast time series well, by looking at the new values of time series as we progress through time we should be able to improve a model’s forecasts. We propose a method *ELF* to improve FM forecasts as we see more of the time series. ELF works by learning another small forecaster on the new data of the time series and then adapts the FM forecast by combining it with the forecast of this other forecaster. An important property of the method is that it is fast enough that it can be used between the times where you need to produce a forecast. We found in our experiments that it works well, improving performance of FM forecasts. By improving the performance of forecasts, ELF takes a small step towards improving our ability predict real time series, from future temperature in a city to future demand for a utility, enabling better decision making."
Poster,Lightweight Protocols for Distributed Private Quantile Estimation,https://ICML.cc//virtual/2025/poster/43483,"Anders Aamand, Fabrizio Boninsegna, Abigail Gentle, Jacob Imola, Rasmus Pagh","Distributed data analysis is a large and growing field driven by a massive proliferation of user devices, and by privacy concerns surrounding the centralised storage of data. We consider two \emph{adaptive} algorithms for estimating one quantile (e.g.~the median) when each user holds a single data point lying in a domain $[B]$ that can be queried once through a private mechanism; one under local differential privacy (LDP) and another for shuffle differential privacy (shuffle-DP). In the adaptive setting we present an $\varepsilon$-LDP algorithm which can estimate any quantile within error $\alpha$ only requiring $O(\frac{\log B}{\varepsilon^2\alpha^2})$ users, and an $(\varepsilon,\delta)$-shuffle DP algorithm requiring only $\widetilde{O}((\frac{1}{\varepsilon^2}+\frac{1}{\alpha^2})\log B)$ users. Prior (nonadaptive) algorithms require more users by several logarithmic factors in $B$. We further provide a matching lower bound for adaptive protocols, showing that our LDP algorithm is optimal in the low-$\varepsilon$ regime. Additionally, we establish lower bounds against non-adaptive protocols which paired with our understanding of the adaptive case, proves a fundamental separation between these models.","Finding the middle or “typical” value in a dataset — like the median income — is a common goal in statistical data analysis. But when the data is private, for example users’ personal info on their phones, it becomes tricky: How do we estimate this statistic without revealing any single users personal data.This paper focuses on how to estimate quantiles (such as the median) when data stays on individual devices and is shared in a privacy-preserving way using local differential privacy (LDP). LDP is a method for protecting users by adding noise to their answers before they’re shared. For example, one can imagine asking users yes/no questions about their data, but instead of always answering truthfully, they only tell you the correct answer with probability 60% and otherwise lie with probability 40%. Based on any answer, it is impossible to make a qualified guess about the user's data, but aggregating many such answers from different users one can start to understand statistics of the data set. However, before this work, existing methods have required many such noisy answers from users in order to get accurate results.We propose a smarter method that asks better questions in multiple rounds, adapting the questions based on previous answers — like a game of “warmer/colder” that quickly homes in on the right value. This approach drastically reduces the number of users needed, while still respecting privacy. In fact, we show that our method is optimal: Under the constraints of LDP, no protocol can do better in a strong mathematical sense. We further combine this with a shuffling technique, which mixes up users’ responses, a process which boosts user privacy even further."
Poster,LIMEFLDL: A Local Interpretable Model-Agnostic Explanations Approach for Label Distribution Learning,https://ICML.cc//virtual/2025/poster/44238,"Xiuyi Jia, Jinchi Li, Yunan Lu, Weiwei Li","Label distribution learning (LDL) is a novel machine learning paradigm that can handle label ambiguity. This paper focuses on the interpretability issue of label distribution learning. Existing local interpretability models are mainly designed for single-label learning problems and are difficult to directly interpret label distribution learning models. In response to this situation, we propose an improved local interpretable model-agnostic explanations algorithm that can effectively interpret any black-box model in label distribution learning.To address the label dependency problem, we introduce the feature attribution distribution matrix and derive the solution formula for explanations under the label distribution form. Meanwhile, to enhance the transparency and trustworthiness of the explanation algorithm, we provide an analytical solution and derive the boundary conditions for explanation convergence and stability. In addition, we design a feature selection scoring function and a fidelity metric for the explanation task of label distribution learning. A series of numerical experiments and human experiments were conducted to validate the performance of the proposed algorithm in practical applications. The experimental results demonstrate that the proposed algorithm achieves high fidelity, consistency, and trustworthiness in explaining LDL models.","Most AI models (like image classifiers) work by slapping simple labels on things. For example, calling a picture either ""cat'' or ""dog''. But real life isn’t that clear-cut. Sometimes an image might look 60% like a cat and 40% like a dog, that’s what we call label ambiguity. Traditional AI explanation tools only handle clear labels (like ""100% cat"") and can’t explain these ""fuzzy"" judgments.  So our research tackles a key question: When an AI model says something is ""60% cat, 40% dog'', which features (like ear shape or fur color) lead it to that conclusion?  To solve this, we built an explainer that works with any black-box AI model handling label ambiguity. Specifically, we created a feature contribution matrix that measures how much each detail affects the fuzzy labels (e.g., ""pointy ears boost the cat score by 30%''). We theoretically guaranteed the explanations are reliable and don’t contradict themselves. We designed tools to rank feature importance and check explanation accuracy, keeping things both simple and trustworthy.  After testing with tons of data, our method outperforms existing tools. Our method explains fuzzy-label decisions more clearly and consistently, helping people actually understand and trust the AI’s reasoning."
Poster,Limitations of measure-first protocols in quantum machine learning,https://ICML.cc//virtual/2025/poster/44218,"Casper Gyurik, Riccardo Molteni, Vedran Dunjko","In recent times, there have been major developments in two distinct yet connected domains of quantum information. On the one hand, substantial progress has been made in so-called randomized measurement protocols. Here, a number of properties of unknown quantum states can be deduced from surprisingly few measurement outcomes, using schemes such as classical shadows. On the other hand, significant progress has been made in quantum machine learning. For example, exponential advantages have been proven when the data consists of quantum states and quantum algorithms can coherently measure multiple copies of input states. In this work, we aim to understand the implications and limitations of combining randomized measurement protocols with quantum machine learning, although the implications are broader. Specifically, we investigate quantum machine learning algorithms that, when dealing with quantum data, can either process it entirely using quantum methods or measure the input data through a fixed measurement scheme and utilize the resulting classical information. We prove limitations for the general class of quantum machine learning algorithms that use fixed measurement schemes on the input quantum states.Our results have several implications. From the perspective of randomized measurement procedures, we show limitations of measure-first protocols in the average case, improving on the state-of-the-art which only focuses on worst-case scenarios. Additionally, previous lower bounds were only known for physically unrealizable states. We improve upon this by employing quantum pseudorandom functions to prove that a learning separation also exists when dealing with physically realizable states, which may be encountered in experiments. From a machine learning perspective, our results are crucial for defining a physically meaningful task that shows fully quantum machine learning processing is not only more efficient but also necessary for solving certain problems. The tasks at hand are also realistic, as the algorithms and proven separations hold when working with efficiently preparable states and remain robust in the presence of measurement and preparation errors.","In quantum machine learning, a key question is how to handle input data that come in the form of quantum states. There are two main approaches: one is to apply a fixed measurement scheme to extract classical information from the quantum states, which is then passed to a learning algorithm. The other is to process the input data fully within a quantum computer, allowing the learning algorithm itself to choose and adapt the best measurements during training. We set out to investigate whether these two strategies are equally powerful, or if one has a clear advantage.This paper presents a learning problem that shows a learning separation: when quantum data is processed coherently, the task can be solved using only a polynomial number of samples. In contrast, using fixed measurements followed by classical or quantum processing requires an exponential number of data points. This result is particularly striking given recent advances showing that many properties of quantum states can be estimated efficiently using techniques like classical shadows.Importantly, our task involves quantum states that are realistically preparable in experiments, and the advantage remains even when noise in the state preparation is present. These findings highlight that, in quantum machine learning, quantum computers are essential not just for acquiring data, but also for processing it effectively."
Poster,Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a Bounded Set,https://ICML.cc//virtual/2025/poster/44653,"Xinyu Liu, Zixuan Xie, Shangtong Zhang","$Q$-learning is one of the most fundamental reinforcement learning algorithms.It is widely believed that $Q$-learning with linear function approximation (i.e., linear $Q$-learning) suffers from possible divergence until the recent work Meyn (2024) which establishes the ultimate almost sure boundedness of the iterates of linear $Q$-learning.Building on this success,this paper further establishes the first $L^2$ convergence rate of linear $Q$-learning iterates (to a bounded set).Similar to Meyn (2024),we do not make any modification to the original linear $Q$-learning algorithm, do not make any Bellman completeness assumption,and do not make any near-optimality assumption on the behavior policy.All we need is an $\epsilon$-softmax behavior policy with an adaptive temperature.The key to our analysis is the general result of stochastic approximations under Markovian noise with fast-changing transition functions.As a side product,we also use this general result to establish the $L^2$ convergence rate of tabular $Q$-learning with an $\epsilon$-softmax behavior policy,for which we rely on a novel pseudo-contraction property of the weighted Bellman optimality operator.","Reinforcement learning helps computers learn to make decisions, like choosing moves in games or guiding robots. $Q$-learning is a popular method for finding the best actions. While standard $Q$-learning was shown to settle on good solutions, those proofs needed extra tweaks $Q$-learning, no one had proven it converges to a bounded range—many believed it could spiral out of control. Our research changes that.We’re the first to show linear $Q$-learning stays within a safe range, not exploding uncontrollably. For standard $Q$-learning, we prove it finds the best actions under practical conditions, using fewer restrictions. Our new math approach tracks how both methods update decisions in ever-changing scenarios, like a game with shifting rules.This work makes $Q$-learning more trustworthy for real-world tasks, like self-driving cars or smart assistants, where fast, accurate learning is vital. Our findings help developers create AI that learns reliably and quickly, even in tricky, unpredictable situations, paving the way for more effective and dependable technology."
