type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,SAE-V: Interpreting Multimodal Models for Enhanced Alignment,https://ICML.cc//virtual/2025/poster/45246,"Hantao Lou, Changye Li, Jiaming Ji, Yaodong Yang","With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V’s ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.","Modern AI systems that can understand both text and images (like GPT-4o with vision) are becoming increasingly powerful, but we don't fully understand how they work internally. This lack of transparency makes it difficult to ensure these systems behave safely and align with human values. We developed SAE-V, a tool that acts like an ""X-ray machine"" for these AI models, allowing us to peek inside and see what information they're focusing on.Our tool works by breaking down the AI's internal computations into interpretable ""features"" - like identifying when the model is thinking about specific concepts, such as ""Doberman dogs"" or abstract ideas like ""symmetry"". We discovered that SAE-V can identify which parts of the training data help the AI understand connections between images and text, versus data that confuses it.Using this insight, we created a smart data filtering system. Just like a good teacher selects the best examples to help students learn, our method automatically identifies the highest-quality training examples. Remarkably, by keeping only the best quarter of training data identified by SAE-V, we achieved better performance than using all the data, making AI training both more efficient and more effective. This work helps make AI systems more transparent and easier to improve."
Poster,SafeArena: Evaluating the Safety of Autonomous Web Agents,https://ICML.cc//virtual/2025/poster/46322,"Ada Tur, Nicholas Meade, Xing Han Lù, Alejandra Zambrano, Arkil Patel, Esin Durmus, Spandana Gella, Karolina Stanczak, Siva Reddy","LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, a benchmark focused on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories---misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents.","As AI agents get better at using the web—browsing websites, filling out forms, and posting content—they also introduce new safety risks. What if someone asked them to do something harmful, like spreading misinformation or harassing someone on an online forum?To better understand these safety risks, we created SafeArena, a test suite of 500 web tasks: half harmless, and half intentionally harmful. These harmful tasks span realistic threats like misinformation, online harassment, cybercrime, and more. We used this benchmark to evaluate some of today’s latest AI web agents, including agents backed by GPT-4o and Claude 3.5.We found that even top-performing agents often followed harmful instructions—GPT-4o completed over a third of our malicious tasks. To analyze this behavior systematically, we developed a framework that categorizes web agent behaviour by risk level. Our findings highlight an urgent need for stronger safety alignment, especially as these agents become more capable and widely deployed. By introducing SafeArena, we aim to provide a crucial benchmark to support and accelerate on-going efforts to design safe and aligned web agents."
Poster,SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models,https://ICML.cc//virtual/2025/poster/44108,"Jiawei Zhang, Xuan Yang, Taiqi Wang, Yu Yao, Aleksandr Petiushko, Bo Li","Traditional autonomous driving systems often struggle to connect high-level reasoning with low-level control, leading to suboptimal and sometimes unsafe behaviors. Recent advances in multimodal large language models (MLLMs), which process both visual and textual data, offer an opportunity to unify perception and reasoning. However, effectively embedding precise safety knowledge into MLLMs for autonomous driving remains a significant challenge.To address this, we propose SafeAuto, a framework that enhances MLLM-based autonomous driving by incorporating both unstructured and structured knowledge. First, we introduce a Position-Dependent Cross-Entropy (PDCE) loss to improve low-level control signal predictions when values are represented as text. Second, to explicitly integrate safety knowledge, we develop a reasoning component that translates traffic rules into first-order logic (e.g., ""red light => stop"") and embeds them into a probabilistic graphical model (e.g., Markov Logic Network) to verify predicted actions using recognized environmental attributes.Additionally, our Multimodal Retrieval-Augmented Generation (RAG) model leverages video, control signals, and environmental attributes to learn from past driving experiences. Integrating PDCE, MLN, and Multimodal RAG, SafeAuto outperforms existing baselines across multiple datasets, enabling more accurate, reliable, and safer autonomous driving. The code is available at https://github.com/AI-secure/SafeAuto.","Autonomous driving systems traditionally rely on separate modules for decision-making (e.g., deciding when to stop at a red light) and controlling the vehicle (e.g., adjusting speed or steering). This separation often results in inefficient or unsafe behaviors because high-level decisions and precise control actions are deeply interconnected. Recently, large AI models capable of understanding both visual scenes and text have offered a promising solution by combining these tasks into one unified system. However, teaching these models precise safety rules, like traffic regulations, remains challenging.We introduce a new approach that explicitly integrates safety rules into these models to enhance autonomous driving reliability. Our method includes a specialized loss function (PDCE loss) that improves the accuracy of numerical predictions (like vehicle speed) without sacrificing the AI model’s language-based reasoning capabilities. Additionally, we embed structured safety rules into a logical reasoning framework, enabling the AI to verify its driving decisions explicitly. Lastly, we developed a retrieval system that allows the AI to learn from previous driving scenarios to better handle new situations.Together, these innovations significantly improve autonomous driving safety, accuracy, and reliability compared to existing systems."
Poster,Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets,https://ICML.cc//virtual/2025/poster/45317,"Ning LU, Shengcai Liu, Jiahao Wu, Weiyu CHEN, Zhirui Zhang, Yew Soon ONG, Qi Wang, Ke Tang","Large language models (LLMs) have shown great potential as general-purpose AI assistants across various domains. To fully leverage this potential in specific applications, many companies provide fine-tuning API services, enabling users to upload their own data for LLM customization. However, fine-tuning services introduce a new safety threat: user-uploaded data, whether harmful or benign, can break the model’s alignment, leading to unsafe outputs. Moreover, existing defense methods struggle to address the diversity of fine-tuning datasets (e.g., varying sizes, tasks), often sacrificing utility for safety or vice versa. To address this issue, we propose Safe Delta, a safety-aware post-training defense method that adjusts the delta parameters (i.e., the parameter change before and after fine-tuning). Specifically, Safe Delta estimates the safety degradation, selects delta parameters to maximize utility while limiting overall safety loss, and applies a safety compensation vector to mitigate residual safety loss. Through extensive experiments on four diverse datasets with varying settings, our approach consistently preserves safety while ensuring that the utility gain from benign datasets remains unaffected.","## ProblemWhen companies allow users to customize powerful AI models (like ChatGPT) with their own data, this ""fine-tuning"" process can accidentally or intentionally make the AI unsafe, leading it to produce harmful content. Current safety methods struggle to adapt to different user datasets, often failing to prevent harm or unnecessarily reducing the AI's usefulness.## SolutionWe developed ""Safe Delta,"" a new technique that works after an AI has been customized. It intelligently assesses the changes made, figuring out which ones improve performance and which pose safety risks. Safe Delta then carefully adjusts these modifications to maximize usefulness while ensuring the AI remains safe.## ImpactThis research offers a more reliable way for AI providers to offer customization services. Safe Delta helps ensure that AI models can be effectively tailored for diverse needs without compromising their safety, leading to more trustworthy and beneficial AI applications for everyone."
Poster,Safe-EF: Error Feedback for Non-smooth Constrained Optimization,https://ICML.cc//virtual/2025/poster/46242,"Rustem Islamov, Yarden As, Ilyas Fatkhullin","Federated learning faces severe communication bottlenecks due to the high dimensionality of model updates. Communication compression with contractive compressors (e.g., Top-$K$) is often preferable in practice but can degrade performance without proper handling. Error feedback (EF) mitigates such issues but has been largely restricted for smooth, unconstrained problems, limiting its real-world applicability where non-smooth objectives and safety constraints are critical. We advance our understanding of EF in the canonical non-smooth convex setting by establishing new lower complexity bounds for first-order algorithms with contractive compression. Next, we propose Safe-EF, a novel algorithm that matches our lower bound (up to a constant) while enforcing safety constraints essential for practical applications. Extending our approach to the stochastic setting, we bridge the gap between theory and practical implementation. Extensive experiments in a reinforcement learning setup, simulating distributed humanoid robot training, validate the effectiveness of Safe-EF in ensuring safety and reducing communication complexity.","We consider a problem in which devices such as phones, sensors, or robots collaborate to train a shared AI model without transmitting all their local data to a central server, due to resource constraints or privacy concerns. A key challenge is that each device must upload large model updates at every iteration, which can quickly saturate the communication network. Compressing updates is one approach to mitigating this bottleneck. However, naive compression often disrupts the learning process. A technique known as error feedback can compensate for the error introduced by compression, but to date, it has only proven effective for simpler tasks without constraints. Yet, constraints are critical in practice for enforcing properties such as safety and fairness in the learned model.We introduce a novel distributed learning algorithm, Safe-EF, which incorporates error feedback in a manner that ensures constraint satisfaction and achieves effective optimization. We also analyze the algorithm’s performance in settings where clients use only a small subset of their local data, for example, a finite number of trajectory samples in humanoid robot training, to compute updates. In simulated experiments involving humanoid robot training, Safe-EF not only reduces communication costs by orders of magnitude but also preserves the safety and reliability of the robot’s behavior. This work advances the development of scalable, communication-efficient, and safe distributed AI systems."
Poster,SAFE: Finding Sparse and Flat Minima to Improve Pruning,https://ICML.cc//virtual/2025/poster/46658,"Dongyeop Lee, Kwanhee Lee, Jinseok Chung, Namhoon Lee","Sparsifying neural networks often suffers from seemingly inevitable performance degradation, and it remains challenging to restore the original performance despite much recent progress.Motivated by recent studies in robust optimization, we aim to tackle this problem by finding subnetworks that are both sparse and flat at the same time.Specifically, we formulate pruning as a sparsity-constrained optimization problem where flatness is encouraged as an objective.We solve it explicitly via an augmented Lagrange dual approach and extend it further by proposing a generalized projection operation, resulting in novel pruning methods called SAFE and its extension, SAFE$^+$.Extensive evaluations on standard image classification and language modeling tasks reveal that SAFE consistently yields sparse networks with improved generalization performance, which compares competitively to well-established baselines.In addition, SAFE demonstrates resilience to noisy data, making it well-suited for real-world conditions.","Pruning aims to reduce the memory and computational load of neural networks by zeroing out parameters, but often at the cost of accuracy. The core challenge lies in improving performance during pruning—or better yet, designing methods that can progress towards optimal performance. Such objectives are best studied in optimization, and recent advancements have shed light on new insights and strategies.Notably, recent research has emphasized the benefits of flat minima—regions in the loss landscape with low curvature. Intuitively, it can be understood as finding a solution where the changes in the loss or shifts in the parameters are more tolerable.This has not only shown to improve robustness and generalization in many prior studies, but it can potentially be highly desirable when the parameters are modified through pruning.This motivates us to propose SAFE, a constrained optimization algorithm that aims to find better sparse solutions through enforcing flatness, where we employ various established optimization techniques to enforce flatness while gradually imposing sparsity. We further extend this into SAFE+, which allows for flexible support of diverse pruning scores within its constrained optimization framework.Our results show that SAFE and SAFE+ successfully induce flatter and sparser minima, improving over existing baselines in both image classification and post-training pruning of language models, as well as improving robustness towards label noise, common image noises, and adversarial attacks."
Poster,Safely Learning Optimal Auctions: A Testable Learning Framework for Mechanism Design,https://ICML.cc//virtual/2025/poster/44342,"Vikram Kher, Manolis Zampetakis","When can the distributional assumptions of theorems and learning algorithms be trusted? Inspired by this question, Rubinfeld and Vasilyan (2023) initiated the study of testable learning. In this schema, we always learn one of the following two things: either we have achieved the desired accuracy regardless of whether the distributional assumptions are satisfied, or the input distribution does not satisfy the original distributional assumptions. Motivated by the challenge of relying on strong distributional assumptions in many theorems in mechanism design, we develop a testable learning framework for mechanism design. Traditional models in mechanism design assume that value distributions satisfy some notion of regularity. Unfortunately, testing regularity is not possible in the original testable learning framework as we show. To bypass this impossibility, we propose a regularized version of the testable learning framework. Under this framework, we always learn one of the following two things: either we achieve high revenue compared to the best possible revenue of any regular distribution close to the input distribution, or the input distribution does not satisfy regularity. We then use this framework to provide: 1) a tester-learner pair for revenue optimal mechanisms, 2) a tester for whether the fundamental Bulow-Klemperer Theorem (Bulow and Klemperer 1996) is applicable to a given dataset, and 3) a tester to confirm the existence of an anonymous reserve price that results in the anonymous price auction securing a constant fraction of the optimal revenue.","Many results in auction design assume that you know the exact shape of the bidders’ value distributions. These assumptions are often not met in the real world and cannot be checked in practice. This raises a key question: when can the distributional assumptions of theorems and learning algorithms be trusted? A framework called Testable Learning has been proposed to answer this question by either delivering the promised performance of the algorithm or warning you that the assumptions of the algorithm fail. In our paper, we adapt this framework to auction design, where the critical distributional assumption is called  “regularity”. Under our new framework, we develop an algorithm that takes in an arbitrary distribution and either secures revenue that is close to the best possible for any nearby regular distribution or signals that the dataset is irregular. We also demonstrate how our algorithm can be utilized to verify when other important theorems in auction design, which rely on regularity, can be applied to data."
Poster,SafeMap: Robust HD Map Construction from Incomplete Observations,https://ICML.cc//virtual/2025/poster/45465,"Xiaoshuai Hao, Lingdong Kong, Rong Yin, Pengwei Wang, Jing Zhang, Yunfeng Diao, Shu Zhao","Robust high-definition (HD) map construction is vital for autonomous driving, yet existing methods often struggle with incomplete multi-view camera data. This paper presents SafeMap, a novel framework specifically designed to ensure accuracy even when certain camera views are missing. SafeMap integrates two key components: the Gaussian-based Perspective View Reconstruction (G-PVR) module and the Distillation-based Bird’s-Eye-View (BEV) Correction (D-BEVC) module. G-PVR leverages prior knowledge of view importance to dynamically prioritize the most informative regions based on the relationships among available camera views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV representations derived from incomplete observations. Together, these components facilitate comprehensive data reconstruction and robust HD map generation. SafeMap is easy to implement and integrates seamlessly into existing systems, offering a plug-and-play solution for enhanced robustness. Experimental results demonstrate that SafeMap significantly outperforms previous methods in both complete and incomplete scenarios, highlighting its superior performance and resilience.","SafeMap is an innovative framework designed to create accurate high-definition maps for autonomous driving, even when some camera views are missing. Traditional methods often struggle with incomplete data from multiple cameras, which can lead to errors in map construction. SafeMap addresses this challenge by using two main features:1. G-PVR identifies the most important areas to focus on based on the available camera views, ensuring that the most relevant information is prioritized.2. D-BEVC uses advanced panoramic features to enhance the map representations, correcting any inaccuracies caused by missing views.Together, these features allow for thorough data reconstruction and reliable map generation. SafeMap is user-friendly and can easily fit into existing systems, making it a practical solution for improving the robustness of autonomous driving technologies. Our experiments show that SafeMap performs significantly better than previous methods, proving its effectiveness in both complete and incomplete data scenarios."
Poster,SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes,https://ICML.cc//virtual/2025/poster/46321,"Yishan Shen, Yuyang Ye, Hui Xiong, Yong Chen","Dynamic treatment regimes (DTRs) are critical to precision medicine, optimizing long-term outcomes through personalized, real-time decision-making in evolving clinical contexts, but require careful supervision for unsafe treatment risks. Existing efforts rely primarily on clinician-prescribed gold standards despite the absence of a known optimal strategy, and predominantly using structured EHR data without extracting valuable insights from clinical notes, limiting their reliability for treatment recommendations. In this work, we introduce SAFER, a calibrated risk-aware tabular-language recommendation framework for DTR that integrates both structured EHR and clinical notes, enabling them to learn from each other, and addresses inherent label uncertainty by assuming ambiguous optimal treatment solution for deceased patients. Moreover, SAFER employs conformal prediction to provide statistical guarantees, ensuring safe treatment recommendations while filtering out uncertain predictions. Experiments on two publicly available sepsis datasets demonstrate that SAFER outperforms state-of-the-art baselines across multiple recommendation metrics and counterfactual mortality rate, while offering robust formal assurances. These findings underscore SAFER’s potential as a trustworthy and theoretically grounded solution for high-stakes DTR applications.","In real-world hospitals, doctors often have to make quick decisions using complex medical data, and it’s not always clear which treatment is best—especially when patient conditions are changing rapidly. This paper introduces SAFER, a new system designed to help doctors make safer and more personalized treatment decisions for seriously ill patients, such as those with sepsis. SAFER uses both structured medical records (like lab tests and vitals) and doctors’ notes to learn how different patients respond to treatments. It also knows when it’s uncertain and avoids making risky recommendations. By providing confidence scores and safety checks, SAFER helps reduce the chance of harmful decisions. In tests using real hospital data, SAFER performed better than existing systems and showed it could reduce patient death rates. This work emphasizes the critical responsibility of the research community to ensure safety, ethical standards, andtangible benefits to patient care when advancing such technologies. Therefore, SAFER brings us closer to AI systems that doctors can trust in high-stakes healthcare settings."
Poster,Safety Alignment Can Be Not Superficial With Explicit Safety Signals,https://ICML.cc//virtual/2025/poster/44598,"Jianwei Li, Jung-Eun Kim","Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.","Large language models (LLMs) like ChatGPT are becoming widely used, but they can sometimes respond to harmful or malicious requests — even if they’ve been trained to be “safe.” Current training methods often make the model appear safe on the surface, but these protections can break down when people craft tricky or indirect prompts to bypass safety.Our research introduces a new way to make LLMs more robust by teaching them to recognize unsafe content directly. We add a special signal inside the model that acts like an internal safety monitor, helping it detect and avoid harmful behavior not just at the start of a response, but throughout the entire generation process.This approach is simple to train, easy to apply after standard safety alignment methods, and adds minimal cost. It could help build AI systems that are safer and more trustworthy, even when users try to trick them."
