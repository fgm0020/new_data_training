type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,The Role of Randomness in Stability,https://ICML.cc//virtual/2025/poster/45041,"Max Hopkins, Shay Moran","Stability is a central property in learning and statistics promising the output of an algorithm $\mathcal{A}$ does not change substantially when applied to similar datasets $S$ and $S'$. It is an elementary fact that any sufficiently stable algorithm (e.g.\ one returning the same result with high probability, satisfying privacy guarantees, etc.) must be randomized. This raises a natural question: can we quantify \textit{how much} randomness is needed for algorithmic stability? We study the randomness complexity of two influential notions of stability in learning: \textit{replicability} (which promises $\mathcal{A}$ usually outputs the same result when run over samples from the same distribution), and \textit{differential privacy} (which promises the output distribution of $\mathcal{A}$ remains similar under neighboring datasets). In particular, building on the ideas of (Dixon, Pavan, Vander Woude, and Vinodchandran ICML 2024) and (Cannone, Su, and Vadhan ITCS 2024), we prove a ""weak-to-strong"" boosting theorem for stability in these settings: the randomness complexity of a task $\mathcal{M}$ is tightly controlled by the best replication probability of any \textit{deterministic} algorithm solving $\mathcal{M}$, a parameter known as $\mathcal{M}$'s ""global stability"" (Chase, Moran, Yehudayoff FOCS 2023). Finally, we use this connection to characterize the randomness complexity of PAC Learning: a class has bounded randomness complexity iff it has finite Littlestone dimension, and moreover scales at worst logarithmically in the excess error of the learner. As a corollary, we resolve a question of (Chase, Chornomaz, Moran, and Yehudayoff STOC 2024) about the error-dependent list-replicability of agnostic learning.","*Stability* is a central tenet in algorithm design stating that feeding ""similar inputs"" into an algorithm A should beget ""similar outputs"". It is well known that any strongly stable (e.g. private) algorithm must be randomized, but despite years of work on the cost of stability in machine learning, we have almost no understanding *how much randomness* is needed even in basic settings like binary classification.We characterize the amount of randomness needed for a task by the best weak stability achieved by any deterministic algorithm solving the problem. Using connections between weak stability and combinatorial structure in learning, we use this to give the first randomness-efficient stable algorithm for basic learning tasks, along with corresponding lower bounds.In the era of big data, stability is a critical property both to protect user data and ensure reliability of our algorithms. Our work sheds new light on an understudied resource needed to achieve stability in theory and practice."
Poster,The Role of Sparsity for Length Generalization in LLMs,https://ICML.cc//virtual/2025/poster/45242,"Noah Golowich, Samy Jelassi, David Brandfonbrener, Sham Kakade, Eran Malach","Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of length generalization remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens. We formalize such tasks via a notion we call k-sparse planted correlation distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, our theoretical model allows us to provide justifications for techniques to modify positional embeddings which have been introduced to improve length generalization, such as position coupling.We support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is indeed a ``sparse'' dependency structure of each token on the previous ones. Further, inspired by our theory, we introduce Predictive Position Coupling, a generalization of position coupling which trains the transformer to predict the position IDs used in a positional coupling approach.  Predictive Position Coupling thereby allows us to broaden the array of tasks to which Position Coupling can successfully be applied to achieve length generalization.","Researchers are trying to train language models (like ChatGPT) to make accurate predictions even when working with texts that are much longer than what they saw during training. But we still don’t fully understand why some models can handle these longer texts so well.This paper introduces a new way of thinking about the problem. The key idea is that as long as each word (or token) mostly depends on just a few of the words before it, the model can learn to keep going even past its original training length. We build a mathematical framework to capture this idea and show that a simplified version of a transformer model (the kind used in ChatGPT) can succeed under these conditions.We also explain why certain tricks—like adjusting how the model understands the positions of each word—help with this kind of generalization. Based on our theory, we introduce a new method called Predictive Position Coupling, which teaches the model to guess where in the sequence it is, improving its ability to handle longer inputs across a wider range of tasks.We back up our claims with experiments on both made-up data and real language, showing that the model’s ability to handle longer sequences can be explained in large part by how many previous tokens each word depends on."
Poster,The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability,https://ICML.cc//virtual/2025/poster/44600,"Jiachen Hu, Rui Ai, Han Zhong, Xiaoyu Chen, Liwei Wang, Zhaoran Wang, Zhuoran Yang","Information asymmetry is a pervasive feature of multi-agent systems, especially evident in economics and social sciences. In these settings, agents tailor their actions based on private information to maximize their rewards. These strategic behaviors often introduce complexities due to confounding variables. Simultaneously, knowledge transportability poses another significant challenge, arising from the difficulties of conducting experiments in target environments. It requires transferring knowledge from environments where empirical data is more readily available. Against these backdrops, this paper explores a fundamental question in online learning: Can we employ non-i.i.d. actions to learn about confounders even when requiring knowledge transfer? We present a sample-efficient algorithm designed to accurately identify system dynamics under information asymmetry and to navigate the challenges of knowledge transfer effectively in reinforcement learning, framed within an online strategic interaction model. Our method provably achieves learning of an $\epsilon$-optimal policy with a tight sample complexity of $\tilde{O}(1/\epsilon^2)$.","In many real‑world multi‑agent settings—such as economic markets or social networks—individuals make decisions based on private information, creating “information asymmetry”. In such complicated environments, people often need to transfer knowledge from one domain to another where experiments are hard to run, a challenge known as knowledge transportability.We address these intertwined challenges by designing an online learning algorithm that deliberately uses non‑identically distributed actions to tease apart private factors, while also supporting efficient transfer of what’s learned across different environments.This work provides the first sample‑efficient learning approach in multi‑agent systems under information asymmetry and knowledge transportability. By reducing the experimental burden and improving robustness, our results open the door to better predictive models and decision‑support tools in economics, social science, and beyond."
Poster,The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training,https://ICML.cc//virtual/2025/poster/46001,"Jinbo Wang, Mingze Wang, Zhanpeng Zhou, Junchi Yan, Weinan E, Lei Wu","Transformers have become the cornerstone of modern AI. Unlike traditional architectures, transformers exhibit a distinctive characteristic: diverse types of building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feed-forward networks, work collaboratively. Understanding the disparities and interactions among these blocks is therefore important.In this paper, we uncover a clear **sharpness disparity** across these blocks, which  intriguingly emerges early in training and persists throughout the training process.Building on this insight, we propose a novel **Blockwise Learning Rate (LR)** strategy to accelerate large language model (LLM) pre-training. Specifically, by integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\times$ speedup compared to vanilla AdamW. This improvement is demonstrated across GPT-2 and LLaMA models, with model sizes ranging from 0.12B to 1.1B and datasets including OpenWebText and MiniPile.Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of Adam, achieving a combined $2\times$ speedup and $2\times$ memory savings. These results underscore the potential of leveraging the sharpness disparity principle to improve LLM training.","Modern AI such as ChatGPT are powered by transformer models, which are made up of different types of blocks working together. In this study, we found that these parts exhibit distinct optimization property called *sharpness*, roughly speaking, how quick can they learn. Based on this observation, we propose **Blockwise Learning Rate (LR)**, adjusting how quickly each part learns, rather than treating them all the same. Our method can train popular models like GPT-2 and LLaMA much faster and more efficiently, nearly halves the training time, shedding light on cheaper and more accessible AI development."
Poster,The Sparse-Plus-Low-Rank Quasi-Newton Method for Entropic-Regularized Optimal Transport,https://ICML.cc//virtual/2025/poster/45033,"Chenrui Wang, Yixuan Qiu","The entropic-regularized optimal transport (OT) has gained massive attention in machine learning due to its ability to provide scalable solutions for OT-based tasks. However, most of the existing algorithms, including the Sinkhorn algorithm and its extensions, suffer from relatively slow convergence in many cases. More recently, some second-order methods have been proposed based on the idea of Hessian sparsification. Despite their promising results, they have two major issues: first, there is limited theoretical understanding on the effect of sparsification; second, in cases where the transport plan is dense, Hessian sparsification does not perform well. In this paper, we propose a new quasi-Newton method to address these problems. First, we develop new theoretical analyses to understand the benefits of Hessian sparsification, which lays the foundation for highly flexible sparsification schemes. Then we introduce an additional low-rank term in the approximate Hessian to better handle the dense case. Finally, the convergence properties of the proposed algorithm are rigorously analyzed, and various numerical experiments are conducted to demonstrate its improved performance in solving large-scale OT problems.","Optimal transport is a powerful mathematical tool used to compare data distributions — imagine matching piles of sand to holes with minimal effort. It is used in machine learning to compare images, text, or any data with structure. A modern version called entropic-regularized optimal transport makes this task faster and more stable, but the popular algorithms used for it can still be slow in practice.This paper introduces a new algorithm that speeds things up by improving how the system handles large numerical objects in the computation. The key idea is to approximate a large matrix, called the Hessian matrix, in a smarter way. We use the sum of two simpler structures to approximate the Hessian matrix — one is a sparse matrix that contains many zero elements, and the other is a low-rank matrix that can be represented by simple vectors. In this way, we can capture both the small important details and the overall big picture of the large Hessian matrix.Our new method is faster, more reliable, and better suited for large-scale data tasks. This makes optimal transport more practical for real-world machine learning problems."
Poster,The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training,https://ICML.cc//virtual/2025/poster/44759,"Fabian Schaipp, Alexander Hägele, Adrien Taylor, Umut Simsekli, Francis Bach","We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms.Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.","The problem of training machine learning models is often formulated as a complicated optimization problem, which is generally handled via iterative optimization algorithms.A particularly crucial stage in this procedure is the choice of the size of the steps taken by the optimization algorithm (this is called a ""learning-rate schedule"").We show that many empirical effects of these schedules can be explained by a theoretical model for convex optimization.This is surprising, as it is known that the practical training problems are not convex; however, the theory appears to still match the observed behaviors.It is also surprising, as optimization theory often fails to make accurate predictions about the real-world behavior of optimization algorithms in machine learning.As an application, we can use our theoretical model to design better schedules for practical training scenarios.This is more efficient than a trial-and-error approach and helps to reduce the computational burden of the training procedure."
Poster,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://ICML.cc//virtual/2025/poster/44773,"Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, Jacob Andreas","Language models (LMs) have shown impressive performance on tasks within their training distribution, but often struggle with structurally novel tasks even when given a small number of in-context task examples. We investigate the effectiveness of test-time training (TTT)—temporarily updating model parameters during inference using a loss derived from input data—as a mechanism for improving LMs' reasoning and few-shot learning capabilities. On the Abstraction and Reasoning Corpus (ARC), performing TTT with in-context examples yields up to $6\times$ higher accuracy compared to fine-tuned baselines—reaching $53.0\%$ on the public validation set with an 8B-parameter LM and $61.9\%$ when ensembled with program-synthesis methods, matching average human performance. On BIG-Bench Hard (BBH), TTT on in-context examples surpasses standard few-shot prompting in the $10$-shot setting by $7.3$ percentage points ($50.5\%$ to $57.8\%$). Our findings highlight the limitations of in-context learning for novel tasks and demonstrate the potential of test-time training to enhance language model adaptability.","Language models are good at solving problems they've seen before—but struggle when faced with unfamiliar challenges. Our research shows that we can significantly boost their performance by allowing them to ""update"" themselves a little during the test itself, without needing to retrain them from scratch.We apply this idea—called test-time training (TTT)—to two especially difficult benchmarks: one that uses visual puzzles (ARC) (similar to IQ tests) and another with complex language tasks (BIG-Bench Hard). By letting the model adjust itself slightly using the examples it sees during testing, we achieved up to six times better accuracy on ARC and over 7 percentage points improvement on BIG-Bench Hard compared to traditional approaches.Our method helps models adapt to unfamiliar tasks in real time, making them more useful in situations where they encounter something new or unexpected."
Poster,The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data,https://ICML.cc//virtual/2025/poster/46412,"Thomas Pouplin, Katarzyna Kobalczyk, Hao Sun, Mihaela van der Schaar","Developing autonomous agents capable of performing complex, multi-step decision-making tasks specified in natural language remains a significant challenge, particularly in realistic settings where labeled data is scarce and real-time experimentation is impractical. Existing reinforcement learning (RL) approaches often struggle to generalize to unseen goals and states, limiting their applicability. In this paper, we introduce $\textit{TEDUO}$, a novel training pipeline for offline language-conditioned policy learning in symbolic environments. Unlike conventional methods, $\textit{TEDUO}$ operates on readily available, unlabeled datasets and addresses the challenge of generalization to previously unseen goals and states. Our approach harnesses large language models (LLMs) in a dual capacity: first, as automatization tools augmenting offline datasets with richer annotations, and second, as generalizable instruction-following agents. Empirical results demonstrate that $\textit{TEDUO}$ achieves data-efficient learning of robust language-conditioned policies, accomplishing tasks beyond the reach of conventional RL frameworks or out-of-the-box LLMs alone.","Training AI agents to follow natural language instructions and complete complex tasks remains a major challenge, especially when we can’t rely on lots of labeled data or real-time trial-and-error exploration strategies. Many current reinforcement learning (RL) methods struggle to handle new goals or previously unseen states, making them hard to apply in the real world.In this paper, we present TEDUO, a new training pipeline that teaches agents to follow language instructions using only pre-recorded, unlabelled datasets. TEDUO employs large language models (LLMs) in two ways: first, to help label and enrich offline data, and second, to act as agents that can interpret and carry out instructions. This combination helps the system learn more efficiently and generalise better to new tasks.Our results show that TEDUO can solve tasks that standalone offline RL methods or out-of-the-box LLMs alone cannot handle, offering a more practical path toward building capable, instruction-following agents."
Poster,"The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training",https://ICML.cc//virtual/2025/poster/44452,"Matteo Saponati, Pascal J. Sager, Pau Vilimelis Aceituno, Thilo Stadelmann, Benjamin F. Grewe","Self-attention is essential to Transformer architectures, yet how information is embedded in the self-attention matrices and how different objective functions impact this process remains unclear. We present a mathematical framework to analyze self-attention matrices by deriving the structures governing their weight updates. Using this framework, we demonstrate that bidirectional training induces symmetry in the weight matrices, while autoregressive training results in directionality and column dominance. Our theoretical findings are validated across multiple Transformer models — including ModernBERT, GPT, LLaMA3, and Mistral — and input modalities like text, vision, and audio. Finally, we apply these insights by showing that symmetric initialization improves the performance of encoder-only models on language tasks. This mathematical analysis offers a novel theoretical perspective on how information is embedded through self-attention, thereby improving the interpretability of Transformer models.","Transformer models, which power popular AI tools like ChatGPT and BERT, rely on a mechanism called self-attention to process information. However, researchers still do not fully understand how this mechanism works internally, especially how different training strategies influence it. In this study, we introduce a mathematical approach to examine the structure of self-attention in Transformers. We find that the way the model is trained significantly affects how attention weights are organized. When trained to consider the full context of a sentence, the model develops symmetric attention patterns. In contrast, when trained to predict one word at a time, it forms directional patterns. We confirm these findings across many well-known models used for language, vision, and audio tasks. We also show that starting with symmetric patterns can help language models learn faster and perform better. This research provides new insights into how attention mechanisms shape learning and opens up ways to make AI models more efficient and interpretable."
Poster,The Underlying Universal Statistical Structure of Natural Datasets,https://ICML.cc//virtual/2025/poster/45583,"Noam Levi, Yaron Oz","We study universal properties in real-world complex and synthetically generated datasets. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure.Examining the local and global eigenvalue statistics of feature-feature covariance matrices, we find: (i) bulk eigenvalue power-law scaling vastly differs between uncorrelated Gaussian and real-world data, (ii) this power law behavior is reproducible using Gaussian data with long-range correlations, (iii) all dataset types exhibit chaotic RMT universality, (iv) RMT statistics emerge at smaller dataset sizes than typical training sets, correlating with power-law convergence, (v) Shannon entropy correlates with RMT structure and requires fewer samples in strongly correlated datasets. These results suggest natural image Gram matrices can be approximated by Wishart random matrices with simple covariance structure, enabling rigorous analysis of neural network behavior.","Modern AI models seem to rely on discovering common patterns to learn from large amounts of data, such as images. We've discovered that despite their complexity, these ""natural datasets"" share some surprising and universal statistical ""fingerprints."" By treating data as a physical system, we used tools from physics (particularly Random Matrix Theory) to study the relationships between features in the data (imagine the correlations between pixels in an image). We found that the way these relationships are structured follows predictable mathematical rules, specifically a ""power-law"" pattern in how the data's core components (eigenvalues) are distributed. This pattern is consistent across different types of datasets, from real-world images to specially created ones. Importantly, these datasets behave like ""chaotic"" systems in physics, meaning their statistical properties can be described by well-understood universal theories. This discovery suggests we can create simpler, more understandable models of complex data, which could help us better understand how artificial intelligence learns and how to improve it."
