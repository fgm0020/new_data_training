type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,https://ICML.cc//virtual/2025/poster/44082,"Zhao Yang, jiwei zhu, Bing Su","Inspired by the success of unsupervised pre-training paradigms, researchers have applied these approaches to DNA pre-training. However, we argue that these approaches alone yield suboptimal results because pure DNA sequences lack sufficient information, since their functions are regulated by genomic profiles like chromatin accessibility. Here, we demonstrate that supervised training for genomic profile prediction serves as a more effective alternative to pure sequence pre-training. Furthermore, considering the multi-species and multi-profile nature of genomic profile prediction, we introduce our **S**pecies-**P**rofile **A**daptive **C**ollaborative **E**xperts (SPACE) that leverages Mixture of Experts (MoE) to better capture the relationships between DNA sequences across different species and genomic profiles, thereby learning more effective DNA representations. Through extensive experiments across various tasks, our model achieves state-of-the-art performance, establishing that DNA models trained with supervised genomic profiles serve as powerful DNA representation learners.","Existing methods for training DNA analysis models often rely on learning patterns from raw DNA sequences alone, much like memorizing letters without context. However, these approaches struggle because DNA’s true function depends on dynamic biological factors, such as how tightly packed the DNA is in a cell (chromatin accessibility), which aren’t captured by sequence data alone.We propose a new strategy: instead of analyzing sequences in isolation, we train models to predict these critical biological factors directly. To handle the complexity of diverse species and multiple biological factors, we designed SPACE, a model that uses specialized “expert” modules. Each expert focuses on a specific species or biological feature, then collaborates to build a cohesive understanding of DNA.SPACE outperforms existing methods in tasks like predicting gene activity and disease links, proving that integrating biological context into training produces more accurate DNA models. This breakthrough could accelerate research in genetics, medicine, and biotechnology by providing tools that better decode how DNA orchestrates life."
Poster,SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference,https://ICML.cc//virtual/2025/poster/46341,"Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia wei, Haocheng Xi, Jun Zhu, Jianfei Chen","An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics.","An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics."
Poster,"Sparse Autoencoders, Again?",https://ICML.cc//virtual/2025/poster/43600,"Yin Lu, Xuening Zhu, Tong He, David Wipf","Is there really much more to say about sparse autoencoders (SAEs)?  Autoencoders in general, and SAEs in particular, represent deep architectures that are capable of modeling low-dimensional latent structure in data. Such structure could reflect, among other things, correlation patterns in large language model activations, or complex natural image manifolds. And yet despite the wide-ranging applicability, there have been relatively few changes to SAEs beyond the original recipe from decades ago, namely, standard deep encoder/decoder layers trained with a classical/deterministic sparse regularizer applied within the latent space. One possible exception is the variational autoencoder (VAE), which adopts a stochastic encoder module capable of producing sparse representations when applied to manifold data. In this work we formalize underappreciated weaknesses with both canonical SAEs, as well as analogous VAEs applied to similar tasks, and propose a hybrid alternative model that circumvents these prior limitations.  In terms of theoretical support, we prove that global minima of our proposed model recover certain forms of structured data spread across a union of manifolds.  Meanwhile, empirical evaluations on synthetic and real-world datasets substantiate the efficacy of our approach in accurately estimating underlying manifold dimensions and producing sparser latent representations without compromising reconstruction error.  In general, we are able to exceed the performance of equivalent-capacity SAEs and VAEs, as well as recent diffusion models where applicable, within domains such as images and language model activation patterns.","Sparse autoencoders (SAEs) are a common deep neural network architecture capable of modeling low-dimensional latent structure in data. Such structure could reflect, among other things, correlation patterns in large language model activations, or complex natural image manifolds. And yet despite wide-ranging applicability spanning decades, there have been relatively few changes to the original SAE design predicated on three basic components: a deterministic encoder network that maps data samples to a latent representation, a deterministic decoder network that reconstructs the original data samples, and a training loss with sparsity-based regularization.  The latter penalizes both the reconstruction error and the complexity of the latent representations, pushing many elements towards zero to achieve the eponymous sparsity.  In this work we explore an alternative SAE design, whereby a stochastic encoder network with a novel gating mechanism is introduced with notable benefits, such as the reduction of hyperparameters and the smoothing of local minima that complicate the training of an otherwise complex loss surface.  Both theoretical insights and empirical testing on real-world image and language-model data support the efficacy of this approach."
Poster,Sparse Autoencoders for Hypothesis Generation,https://ICML.cc//virtual/2025/poster/46484,"Rajiv Movva, Kenny Peng, Nikhil Garg, Jon Kleinberg, Emma Pierson","We describe HypotheSAEs, a general method to hypothesize interpretable relationships between text data (e.g., headlines) and a target variable (e.g., clicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text embeddings to produce interpretable features describing the data distribution, (2) select features that predict the target variable, and (3) generate a natural language interpretation of each feature (e.g., *mentions being surprised or shocked*) using an LLM. Each interpretation serves as a hypothesis about what predicts the target variable. Compared to baselines, our method better identifies reference hypotheses on synthetic datasets (at least +0.06 in F1) and produces more predictive hypotheses on real datasets (~twice as many significant findings), despite requiring 1-2 orders of magnitude less compute than recent LLM-based methods. HypotheSAEs also produces novel discoveries on two well-studied tasks: explaining partisan differences in Congressional speeches and identifying drivers of engagement with online headlines.","Discovering relationships between text data and a target variable is an important and fundamental task with diverse applications in economics, political science, sociology, medicine, and business. What features of a restaurant review predict a low rating? What features of a social media post predict whether it will go viral? What features of a patient's clinical notes predict if they will develop cancer?We describe a new method, HypotheSAEs, which extracts interpretable patterns from text datasets using advances in interpretability and language models. For example, consider a computational social scientist who has a large dataset of news headlines and associated engagement levels. HypotheSAEs outputs automatically learns concepts, like ""the headline mentions being surprised or shocked"" or ""the headline mentions a societal issue involving collective action,"" which are positively or negatively correlated with engagement levels. These concepts can be treated by researchers as *hypotheses* for further study and validation. Our method works well in any setting where we have texts as input and some numeric variable of interest as output. On three datasets—news headlines and their click-rates, restaurant reviews and their ratings, and Congressional speeches and their speaker's party—our method generates many hypotheses, more effectively and efficiently than prior methods. We are excited about the possibilities of our method to help computational researchers across many domains. Our method contributes to the growing literature on AI for science, with a specific focus on how AI can help scientists extract more insight from their data."
Poster,Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation,https://ICML.cc//virtual/2025/poster/45274,"Junyu Luo, Yuhao Tang, Yiwei Fu, Xiao Luo, Zhizhuo KOU, Zhiping Xiao, Wei Ju, Wentao Zhang, Ming Zhang","Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain graphs to achieve effective performance in unlabeled target domains despite distribution shifts. However, existing methods often yield suboptimal results due to the entanglement of causal-spurious features and the failure of global alignment strategies. We propose SLOGAN (Sparse Causal Discovery with Generative Intervention), a novel approach that achieves stable graph representation transfer through sparse causal modeling and dynamic intervention mechanisms. Specifically, SLOGAN  first constructs a sparse causal graph structure, leveraging mutual information bottleneck constraints to disentangle sparse, stable causal features while compressing domain-dependent spurious correlations through variational inference. To address residual spurious correlations, we innovatively design a generative intervention mechanism that breaks local spurious couplings through cross-domain feature recombination while maintaining causal feature semantic consistency via covariance constraints. Furthermore, to mitigate error accumulation in target domain pseudo-labels, we introduce a category-adaptive dynamic calibration strategy, ensuring stable discriminative learning. Extensive experiments on multiple real-world datasets demonstrate that SLOGAN significantly outperforms existing baselines.","When AI learns from one set of graph data (like molecular structures) and tries to apply that knowledge to a new, unlabeled set, it often gets confused. It might focus on **superficial differences** between the datasets (spurious factors) instead of the **true underlying reasons** for a property (causal factors) This is a key challenge in Unsupervised Graph Domain Adaptation (UGDA).Our method, **SLOGAN**, helps AI make this jump more effectively:  * First, it **disentangles** the essential *causal information* from misleading, domain-specific *spurious details* using sparse causal modeling.  * Then, through a novel **generative intervention**, SLOGAN trains the AI by swapping these *spurious details* between datasets. This forces the AI to rely only on the stable, *causal features*.  * Finally, it carefully uses self-generated labels on the new data with an **adaptive calibration strategy** to ensure reliable learning.By clearly separating and using *causal patterns* while neutralizing *spurious ones*, SLOGAN significantly boosts the AI's performance and reliability when adapting to new, unlabeled graph data in UGDA. This is a step towards more trustworthy AI for tasks like scientific discovery and network analysis."
Poster,SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity,https://ICML.cc//virtual/2025/poster/43493,"Samir Khaki, Xiuyu Li, Junxian Guo, Ligeng Zhu, Konstantinos N (Kostas) Plataniotis, Amir Yazdanbakhsh, Kurt Keutzer, Song Han, Zhijian Liu","Fine-tuning LLMs is both computationally andmemory-intensive. While parameter-efficient fine-tuning methods, such as QLoRA and DoRA,reduce the number of trainable parameters andlower memory usage, they do not decrease computational cost. In some cases, they may evenslow down fine-tuning. In this paper, we introduceSparseLoRA, a method that accelerates LLM fine-tuning through contextual sparsity. We proposea lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset ofweights for loss and gradient computation. Also,we systematically analyze and address sensitivityacross layers, tokens, and training steps. Our experimental results show that SparseLoRA reducescomputational cost by up to $2.0\times$ and a measuredspeedup of up to $1.5\times$ while maintaining accuracy across various downstream tasks, includingcommonsense and arithmetic reasoning, code generation, and instruction following.","Fine-tuning large language models (LLMs) for new tasks typically requires significant compute and memory. Recent techniques, like QLoRA and DoRA, make fine-tuning more memory-efficient by reducing how many model parameters change during training. However, these methods are often less runtime efficient, slowing down fine-tuning.In our work, we introduce SparseLoRA, a new approach that makes fine-tuning faster by carefully choosing only a small, important subset of parameters to activate at each training step. We use a lightweight, training-free estimator based on singular value decomposition (SVD) to efficiently predict which parts of the model can be skipped during training based on the input activations and weight characteristics. We also thoroughly analyze how this method behaves differently across layers, input tokens, and training phases to ensure stability.Experiments show that SparseLoRA halves computational costs and achieves up to $1.5\times$ faster fine-tuning and $2\times$ computational savings, all without sacrificing model accuracy on various tasks like reasoning, coding, and following instructions. Our work offers a scalable and computationally efficient solution for fine-tuning modern LLMs."
Poster,Sparse-pivot: Dynamic correlation clustering for node insertions,https://ICML.cc//virtual/2025/poster/44798,"Mina Dalirrooyfard, Konstantin Makarychev, Slobodan Mitrovic","We present a new Correlation Clustering algorithm for a dynamic setting where nodes are added one at a time. In this model, proposed by Cohen-Addad, Lattanzi, Maggiori, and Parotsidis (ICML 2024), the algorithm uses database queries to access the input graph and updates the clustering as each new node is added.Our algorithm has the amortized update time of $\log^{O(1)}(n)$. Its approximation factor is $20+\varepsilon$, which is a substantial improvement over the approximation factor of the algorithm by Cohen-Addad et al. We complement our theoretical findings by empirically evaluating the approximation guarantee of our algorithm. The results show that it outperforms the algorithm by Cohen-Addad et al.~in practice.","Suppose we are given items and we know for any two items if they are similar or dissimilar. We want to cluster these items such that similar items are in one cluster and dissimilar items are in different clusters. This is essentially called correlation clustering. In real world, new items might come and we don't want to compute our clustering from scratch. We show that we can maintain a good clustering while spending small amount of time updating it when a new item arrives."
Poster,Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks,https://ICML.cc//virtual/2025/poster/45482,"Jialin Zhao, Yingtao Zhang, Xinghang Li, Huaping Liu, Carlo Cannistraci","The growing demands on GPU memory posed by the increasing number of neural network parameters call for training approaches that are more memory-efficient. Previous memory reduction training techniques, such as Low-Rank Adaptation (LoRA) and ReLoRA, face challenges, with LoRA being constrained by its low-rank structure, particularly during intensive tasks like pre-training, and ReLoRA suffering from saddle point issues. In this paper, we propose **S**parse **S**pectral **T**raining **(SST)** to optimize memory usage for **pre-training**. SST **updates all singular values** and **selectively updates singular vectors** through a multinomial sampling method weighted by the magnitude of the singular values. Furthermore, SST employs **singular value decomposition to initialize and periodically reinitialize** low-rank parameters, reducing distortion relative to full-rank training compared to other low-rank methods. Through comprehensive testing on both Euclidean and hyperbolic neural networks across various tasks, SST demonstrates its ability to outperform existing memory reduction training methods and is comparable to full-rank training in various cases. On LLaMA-1.3B, with only 18.7\% of the parameters trainable compared to full-rank training (using a rank equivalent to 6\% of the embedding dimension), SST reduces the perplexity gap between other low-rank methods and full-rank training by **97.4\%**. This result highlights SST as an effective parameter-efficient technique for model pre-training.","Training large AI models usually requires a huge amount of computer memory and specialized hardware. This creates a high barrier for many researchers who want to contribute to AI development. Our work introduces a new method called **Sparse Spectral Training (SST)** that helps reduce the memory needed for training, without sacrificing performance. Think of it as a way to teach a model more efficiently by focusing only on the most important parts of its internal structure, kind of like learning the main points of a book instead of memorizing every word.While existing methods like LoRA only update a small part of the model, SST updates a broader set of components in a smart, selective way. Our tests show that SST works better than previous methods across many tasks, from language understanding to graph analysis, and it even performs as well as full-scale training in some cases. By lowering the hardware barrier, we hope SST makes AI training more accessible to everyone in the research community."
Poster,Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry,https://ICML.cc//virtual/2025/poster/46094,"Mohammed Adnan, Rohan Jain, Ekansh Sharma, Rahul G. Krishnan, Yani Ioannou","The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask and weights thatachieve the same generalization performance as the dense model while using significantly fewer parameters. However, finding a LTH solution is computationally expensive, and a LTH sparsity mask does not generalize to other random weight initializations. Recent work has suggested that neural networks trained from random initialization find solutions within the same basin modulo permutation, and proposes a method to align trained models within the same loss basin. We hypothesize that misalignment of basins is the reason why LTH masks do not generalize to new random initializations and propose permuting the LTH mask to align with the new optimization basin when performing sparse training from a different random init. We empirically show a significant increase in generalization when sparse training from random initialization with the permuted mask as compared to using the non-permuted LTH mask, on multiple datasets (CIFAR-10/100 & ImageNet) and models (VGG11 & ResNet20/50).","Modern artificial intelligence (AI) systems are incredibly powerful but often require massive amounts of computing power and data to train. This makes them expensive and out of reach for many researchers and developers. To address this, scientists have been exploring “sparser” AI models—systems that use only a small fraction of their potential connections—making them much more efficient to train and run.However, a major hurdle is that a sparse model setup that works well with one starting point for training often fails when training begins from a different starting point. Our research identifies the root cause: misalignment. Think of it like using a key (the sparse setup) on a lock that has been rotated slightly—it just doesn’t fit.To solve this, we developed a method to “re-align” the sparse structure so it matches the patterns of a new starting point. This adjustment dramatically improves the performance of sparse models trained from different starting points, making them nearly as effective as their original versions.Our findings make it easier and more practical to develop leaner, more efficient AI systems, paving the way for broader accessibility and innovation in AI research."
Poster,Sparse Video-Gen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity,https://ICML.cc//virtual/2025/poster/43743,"Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, Jianfei Chen, Ion Stoica, Kurt Keutzer, Song Han","Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs. This inefficiency primarily arises from the quadratic computational complexity of 3D full attention with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (SVG) that leverages the inherent sparsity in 3D full attention to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) Spatial Head, where only spatially-related tokens within each frame dominate the attention output, and (2) Temporal Head, where only temporally-related tokens across different frames dominate. Based on this insight, SVG proposes an online profiling strategy to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor layout transformation and customized kernel implementations, SVG achieves up to 2.28$\times$ and 2.33$\times$ end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality. Our code will be open-sourced upon publication.","AI models can generate high-quality videos, but they’re extremely slow, often taking minutes to produce just a few seconds. We introduce Sparse VideoGen (SVG), a method that speeds up video generation without changing the model or lowering quality. SVG detects when parts of the model focus only on spatial relationships or only on temporal relationships, and skips unnecessary computation. With better data handling and hardware usage, SVG makes leading models 2 times faster while keeping the same video quality."
