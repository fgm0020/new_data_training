type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,General agents need world models,https://ICML.cc//virtual/2025/poster/44620,"Jonathan Richens, Tom Everitt, David Abel","Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient?We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment.We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models.This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.","What are the key ingredients for building truly general AI agents, capable of performing a wide range of tasks? One possibility is that these agents, like humans, need a rich internal model of the world, capable of predicting the consequences of their actions in simulating different possibilities. However, most AI systems are black boxes, and its unclear if they have anything like a coherent understanding of the world.  In this paper we a formal answer to this question, proving that any AI agent that can solve multi-step goal-directed tasks, must have learned a predictive model of its environment, and that we can extract this 'world model' from the agent.  Importantly, we show that to achieve more complex goals with a higher probability of success, agents need to must increasingly accurate and detailed world models.This discovery has several key implications. It implies there is no capability without understanding---agents can't do things unless they learn an accurate model of the task, much like a human would need to learn a mental model of a chess board, its moves, and the opponent, to play chess. This places an upper bound on the capabilities of agents operating in messy real-world environments, where learning an accurate model can be prohibitively difficult. It is also crucial for developing safer and more interpretable agents, as it gives us a way to `back out' a model of the agent and its environment, which we can use to debug the agent's behaviour even in environments we don't fully understand."
Poster,General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization,https://ICML.cc//virtual/2025/poster/44560,"Kwangjun Ahn, Gagik Magakyan, Ashok Cutkosky","This work investigates the effectiveness of schedule-free methods, developed by A. Defazio et al. (NeurIPS 2024), in nonconvex optimization settings, inspired by their remarkable empirical success in training neural networks. Specifically, we show that schedule-free SGD achieves optimal iteration complexity for nonsmooth, non-convex optimization problems. Our proof begins with the development of a general framework for online-to-nonconvex conversion, which converts a given online learning algorithm into an optimization algorithm for nonconvex losses. Our general framework not only recovers existing conversions but also leads to two novel conversion schemes. Notably, one of these new conversions corresponds directly to schedule-free SGD, allowing us to establish its optimality. Additionally, our analysis provides valuable insights into the parameter choices for schedule-free SGD, addressing a theoretical gap that the convex theory cannot explain.","When training neural networks, we adjust the model’s internal settings — called parameters — as we see new data. If we adjust too cautiously, learning is slow. If we adjust too aggressively, learning can become unstable. To avoid this, researchers often tune a value called the ""learning rate"" using hand-crafted schedules, which can be time-consuming and problem-specific.A recent method called “schedule-free” learning avoids this hassle by removing the need for manual scheduling altogether, and yet performs remarkably well in practice.In this work, we explain why schedule-free learning works, even in challenging cases where the learning landscape is highly irregular. We develop a general mathematical tool to turn any online learning algorithm (which learns from data sequentially) into one that can solve difficult optimization problems like those found in neural network training. This also allows us to recover and improve existing methods — including schedule-free learning — and to explain why certain parameter choices, commonly used in practice, are effective even though previous theory couldn’t justify them."
Poster,Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks,https://ICML.cc//virtual/2025/poster/43486,"Angelica Chen, Samuel Stanton, Frances Ding, Robert Alberstein, Andrew Watkins, Richard Bonneau, Vladimir Gligorijevic, Kyunghyun Cho, Nathan Frey","Although large language models (LLMs) have shown promise in biomolecule optimization problems, they incur heavy computational costs and struggle to satisfy precise constraints. On the other hand, specialized solvers like LaMBO-2 offer efficiency and fine-grained control but require more domain expertise. Comparing these approaches is challenging due to expensive laboratory validation and inadequate synthetic benchmarks. We address this by introducing Ehrlich functions, a synthetic test suite that captures the geometric structure of biophysical sequence optimization problems. With prompting alone, off-the-shelf LLMs struggle to optimize Ehrlich functions. In response, we propose LLOME (Language Model Optimization with Margin Expectation), a bilevel optimization routine for online black-box optimization.When combined with a novel preference learning loss, we find LLOME can not only learn to solve some Ehrlich functions, but can even perform as well as or better than LaMBO-2 on moderately difficult Ehrlich variants. However, LLMs also exhibit some likelihood-reward miscalibration and struggle without explicit rewards. Our results indicate LLMs can occasionally provide significant benefits, but specialized solvers are still competitive and incur less overhead.","We explore the application of large language models (LLMs) to biomolecule design, a task traditionally tackled by specialized solvers. While LLMs demonstrate potential on these tasks, their computational cost and difficulty in meeting precise constraints pose challenges. To address the lack of suitable benchmarks for comparing LLMs and specialized tools, we introduce Ehrlich functions, a synthetic test suite that reflects the complexities of real-world biophysical sequence optimization problems but avoids the costs of wet lab experiments. Our initial results show that LLMs struggle at solving these functions through prompting alone. To address this, we propose LLOME, a novel LLM-based optimization algorithm, and demonstrate it can perform competitively with, or even outperform, the specialized tool LaMBO-2 on moderately challenging problems. Despite this, LLMs exhibit some inconsistencies and require high-quality data. Our findings suggest that LLMs can offer advantages in both effectiveness and usability, but specialized solvers remain a strong alternative, especially when considering efficiency."
Poster,Generalizable Multi-Camera 3D Object Detection from a Single Source via Fourier Cross-View Learning,https://ICML.cc//virtual/2025/poster/45298,"Xue Zhao, Qinying Gu, Xinbing Wang, Chenghu Zhou, Nanyang Ye","Improving the generalization of multi-camera 3D object detection is essential for safe autonomous driving in the real world. In this paper, we consider a realistic yet more challenging scenario, which aims to improve the generalization when only single source data available for training, as gathering diverse domains of data and collecting annotations is time-consuming and labor-intensive. To this end, we propose the Fourier Cross-View Learning (FCVL) framework including Fourier Hierarchical Augmentation (FHiAug), an augmentation strategy in the frequency domain to boost domain diversity, and Fourier Cross-View Semantic Consistency Loss to facilitate the model to learn more domain-invariant features from adjacent perspectives. Furthermore, we provide theoretical guarantees via augmentation graph theory. To the best of our knowledge, this is the first study to explore generalizable multi-camera 3D object detection with a single source. Extensive experiments on various testing domains have demonstrated that our approach achieves the best performance across various domain generalization methods.","Enhancing the generalization ability of 3D object detection (adaptive capability in unseen scenarios) is of great significance for the safety of autonomous driving systems, as autonomous driving systems need to operate in dynamic environments, where training data often fails to cover all potential scenarios (such as adverse weather conditions).  In this paper, we consider a realistic yet more challenging scenario, which aims to improve the generalization when only single source data available for training, as gathering diverse domains of data and collecting annotations is time-consuming and labor intensive. To address this challenge, we propose the Fourier Cross-View Learning (FCVL) framework, enhancing model generalization with single-source data.The proposed FCVL consists of two innovative components, including Fourier Hierarchical Augmentation (FHiAug), an augmentation strategy in the frequency domain to boost domain diversity, and Fourier Cross-View Semantic Consistency Loss to facilitate the model to learn more domain-invariant features from adjacent perspectives. Through these two innovations, FCVL has achieved robust multi-camera 3D detection generalization with minimal data. Theoretical analysis and real-world experiments have demonstrated its superiority over traditional methods, enabling reliable object detection in unseen environments. FCVL reduces dependence on large amounts of labeled data, and enhances safety for autonomous driving in challenging conditions."
Poster,Generalization Analysis for Controllable Learning,https://ICML.cc//virtual/2025/poster/46223,"Yi-Fan Zhang, Xiao Zhang, Min-Ling Zhang","Controllability has become a critical issue in trustworthy machine learning, as a controllable learner allows for dynamic model adaptation to task requirements during testing. However, existing research lacks a comprehensive understanding of how to effectively measure and analyze the generalization performance of controllable learning methods. In an attempt to move towards this goal from a generalization perspective, we first establish a unified framework for controllable learning. Then, we develop a novel vector-contraction inequality and derive a tight generalization bound for general controllable learning classes, which is independent of the number of task targets except for logarithmic factors and represents the current best-in-class theoretical result. Furthermore, we derive generalization bounds for two typical controllable learning methods: embedding-based and hypernetwork-based methods. We also upper bound the Rademacher complexities of commonly used control and prediction functions, which serve as modular theoretical components for deriving generalization bounds for specific controllable learning methods in practical applications such as recommender systems. Our theoretical results without strong assumptions provide general theoretical guarantees for controllable learning methods and offer new insights into understanding controllability in machine learning.","Controllability has emerged as a crucial aspect of AI, which ensures that learners can dynamically adapt to evolving task requirements at test time, a concept known as controllable learning. To comprehensive understand the generalization performance of controllable learning methods, we establish an effective unified theoretical framework and derive tight generalization bounds for controllable learning. These state-of-the-art bounds with no dependency on the number of task targets except for logarithmic factors explain the empirical success of existing controllable learning methods. In addition, we also derive bounds for two typical controllable learning methods, i.e., embedding-based and hypernetwork-based methods, and reveal that different manipulation methods based on the input and control function will lead to significant differences in the generalization bounds. The theoretical results we developed can promote a better understanding of the good generalization performance of controllable learning methods."
Poster,Generalization Analysis for Supervised Contrastive Representation Learning under Non-IID Settings,https://ICML.cc//virtual/2025/poster/44246,"Minh Hieu Nong, Antoine Ledent","Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL has remained limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate that the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to that class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks.","Contrastive Representation Learning (CRL) is a powerful machine learning framework that enhances data representation by pulling similar data pairs together while pushing dissimilar pairs apart. This requires each training dataset to take the form of a collection of small groups where each group is composed of two similar objects (referred to as ‘anchors’), together a set of other objects which are known to be very different from the two anchor objects. We study CRL in the context of generalization theory, which is concerned with estimating the amount of data necessary for models to attain a desirable performance (also referred to as the ‘sample complexity’). Previous works have explored CRL settings where the groups are independent of each other. In our work, we explored the setting where the groups are formed from a finite pool of labeled examples, allowing the objects to be recycled across groups, breaking the assumption of statistical independence which is central in classic learning theory. Under some assumptions on the proportion of objects in each class, we show that the sample complexity is not worse than the fully independent settings. Experimentally, we demonstrate that models which reuse objects in different groups can outperform models which do not."
Poster,Generalization  and Robustness of the Tilted Empirical Risk,https://ICML.cc//virtual/2025/poster/44989,"Gholamali Aminian, Amir R. Asadi, Tian Li, Ahmad Beirami, Gesine Reinert, Samuel Cohen","The generalization error (risk) of a supervised statistical learning algorithm quantifies its prediction ability on previously unseen data. Inspired by exponential tilting, Li et al. (2021) proposed the {\it tilted empirical risk} (TER)  as a non-linear risk metric for machine learning applications such as classification and regression problems. In this work, we examine the generalization error of the tilted empirical risk in the robustness regime under \textit{negative tilt}. Our first contribution is to provide uniform and information-theoretic bounds on the {\it tilted generalization error}, defined as the difference between the population risk and the tilted empirical risk, under negative tilt for unbounded loss function under bounded $(1+\epsilon)$-th moment of loss function for some $\epsilon\in(0,1]$ with a convergence rate of $O(n^{-\epsilon/(1+\epsilon)})$ where $n$ is the number of training samples, revealing a novel application for TER under no distribution shift. Secondly, we study the robustness of the tilted empirical risk with respect to noisy outliers at training time and provide theoretical guarantees under distribution shift for the tilted empirical risk. We empirically corroborate our findings in simple experimental setups where we evaluate our bounds to select the value of tilt in a data-driven manner.","When we train a machine-learning model we want to know how well it will perform on new, unseen data—this is called its generalization error. Researchers estimate that error by averaging how wrong the model is on the training set.Recently, a technique called the tilted empirical risk (TER) was proposed. Instead of giving every training example equal weight, TER lets you tilt the calculation so that unusually large errors are emphasized less (negative tilt). In this paper, we answer the following question: *What happens if we use negative tilt with “outliers” ?*We show two main results:1- **Reliable performance guarantees without data shift:** We prove mathematical limits on how far TER can be different from the unknown error even when individual errors (losses) can become very large. As the training set grows, that gap shrinks at a predictable pace, meaning TER remains a reliable method.2- **Robustness when the data are noisy:** We analyze scenarios where the training data are contaminated with noisy outliers or come from a slightly different distribution than the data the model will face later (a “distribution shift”). The theory shows that, under negative tilt, TER still gives dependable guidance.Finally, we run simple experiments to confirm the theory and to demonstrate a practical way to pick the best amount of tilt directly from the data.In short, the study explains why—and by how much—down-weighting extreme training errors (using negative tilt) can make risk estimates both stable and resilient to bad (noisy) data, giving practitioners a solid, data-driven method to tune when their datasets are noisy."
Poster,Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes and Sample Compression Hypernetworks,https://ICML.cc//virtual/2025/poster/45323,"Benjamin Leblanc, Mathieu Bazinet, Nathaniel D&#x27;Amours, Alexandre Drouin, Pascal Germain","Both PAC-Bayesian and Sample Compress learning frameworks have been shown instrumental for deriving tight (non-vacuous) generalization bounds for neural networks. We leverage these results in a meta-learning scheme, relying on a hypernetwork that outputs the parameters of a downstream predictor from a dataset input. The originality of our approach lies in the investigated hypernetwork architectures that encode the dataset before decoding the parameters: (1) a PAC-Bayesian encoder that expresses a posterior distribution over a latent space, (2) a Sample Compress encoder that selects a small sample of the dataset input along with a message from a discrete set, and (3) a hybrid between both approaches motivated by a new Sample Compress theorem handling continuous messages. The latter theorem exploits the pivotal information transiting at the encoder-decoder junction in order to compute generalization guarantees for each downstream predictor obtained by our meta-learning scheme.","Since the beginning of machine learning research, statistical learning theorists have proposed ways of guaranteeing that artificial intelligence systems will ""behave well"" on tasks they haven't seen yet. These kinds of certifications are challenging to obtain for modern deep learning architectures, which we can see as a sophisticated arrangement of many building blocks. When one ""asks"" a neural network to perform a given task, its architecture defines how the information flows in the building blocks before providing an ""answer."" We propose a new way to design deep learning architectures to embody at their core ideas stemming from existing learning theories. This is achieved in a ""meta-learning"" context, where knowledge is gathered on multiple tasks and then leveraged to tackle new problems. In summary, we introduce an original strategy to bridge theoretical guarantees and meta-learning: When asked to solve a new problem, our neural network provides certifications along with the answered solution."
Poster,Generalization in Federated Learning: A Conditional Mutual Information Framework,https://ICML.cc//virtual/2025/poster/44258,"Ziqiao Wang, Cheng Long, Yongyi Mao","Federated learning (FL) is a widely adopted privacy-preserving distributed learning framework, yet its generalization performance remains less explored compared to centralized learning. In FL, the generalization error consists of two components: the out-of-sample gap, which measures the gap between the empirical and true risk for participating clients, and the participation gap, which quantifies the risk difference between participating and non-participating clients. In this work, we apply an information-theoretic analysis via the conditional mutual information (CMI) framework to study FL's two-level generalization. Beyond the traditional supersample-based CMI framework, we introduce a superclient construction to accommodate the two-level generalization setting in FL. We derive multiple CMI-based bounds, including hypothesis-based CMI bounds, illustrating how privacy constraints in FL can imply generalization guarantees. Furthermore, we propose fast-rate evaluated CMI bounds that recover the best-known convergence rate for two-level FL generalization in the small empirical risk regime. For specific FL model aggregation strategies and structured loss functions, we refine our bounds to achieve improved convergence rates with respect to the number of participating clients. Empirical evaluations confirm that our evaluated CMI bounds are non-vacuous and accurately capture the generalization behavior of FL algorithms.","This paper looks at how well federated learning (FL)—a way to train AI models without collecting users' data—can perform on new, unseen data. While FL helps protect privacy, it's less understood than traditional training methods when it comes to generalizing to new users. We break down the sources of error in FL and use tools from information theory to better understand and measure these errors. We also propose new ways to predict how well FL models are likely to perform in practice. Our results provide stronger guarantees about FL performance and are backed up by experiments showing that the predictions match real-world behavior."
Poster,Generalization of noisy SGD in unbounded non-convex settings,https://ICML.cc//virtual/2025/poster/46146,"Leello Dadi, Volkan Cevher","We study generalization of iterative noisy gradient schemes on smooth non-convex losses. Formally, we establish time-independent information theoretic generalization bounds for Stochastic Gradient Langevin Dynamics (SGLD) that do not diverge as the iteration count increases. Our bounds are obtained through a stability argument: we analyze the difference between two SGLD sequences ran in parallel on two datasets sampled from the same distribution. Our result only requires an isoperimetric inequality to hold, which is merely a restriction on the tails of the loss. Our work relaxes the assumptions of prior work to establish that the iterates stay within a bounded KL divergence from each other. Under an additional dissipativity assumption, we show that the stronger Renyi divergence also stays bounded by establishing a uniform log-Sobolev constant of the iterates. Without dissipativity, we sidestep the need for local log-Sobolev inequalities and instead exploit the regularizing properties of Gaussian convolution. These techniques allow us to show that strong convexity is not necessary for finite stability bounds. Our work shows that noisy SGD can have finite, iteration-independent, generalization and differential privacy bounds in unbounded non-convex settings.","Machine learning models are trained by updating the model weights a very large number of times. In principle, having this many steps increases the chances of learning spurious patterns, however in practice, models trained for thousands of iterations appear to still perform well. In this work, we show that this is not only observed in practice but also can be shown theoretically. We show this by modeling a training step as a noisy weight update. We show that this presence of noise ensures that only a limited amount of spurious patterns are picked up. Our result holds for a simplified model of the noise in the weight update and improves previous results in this space."
