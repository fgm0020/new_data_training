type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations,https://ICML.cc//virtual/2025/poster/45293,"Davide Sartor, Alberto Sinigaglia, Gian Antonio Susto","Imposing input-output constraints in multi-layer perceptrons (MLPs) plays a pivotal role in many real world applications. Monotonicity in particular is a common requirement in applications that need transparent and robust machine learning models. Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which poses well known optimization challenges.  In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions.  Additionally, we show an equivalence between saturation side in the activations and sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts.  This results provide theoretical grounding to the empirical effectiveness observed in previous works, while leading to possible architectural simplification.  Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability.  Experimental evaluation reinforce the validity of the theoretical results, showing that our novel approach compares favorably to traditional monotonic architectures.","Previously, it was known that Constrained Monotonic Neural Networks can approximate any monotonic function only when specific activations were used. We show that this is also the case when using more modern activations like ReLU, which was thought to not be possible. We then propose a new architecture that does not use weight constraint but instead switches activation depending on the sign of the parameters. We empirically evaluate our approach and showcase that it achieves state of the arte performance."
Poster,Advancing Personalized Learning with Neural Collapse for Long-Tail Challenge,https://ICML.cc//virtual/2025/poster/45037,"Hanglei Hu, Yingying Guo, Zhikang Chen, Sen Cui, Fei Wu, Kun Kuang, Min Zhang, Bo Jiang","Personalized learning, especially data-based methods, has garnered widespread attention in recent years, aiming to meet individual student needs. However, many works rely on the implicit assumption that benchmarks are high-quality and well-annotated, which limits their practical applicability.  In real-world scenarios, these benchmarks often exhibit long-tail distributions, significantly impacting model performance. To address this challenge, we propose a novel method called **N**eural-**C**ollapse-**A**dvanced personalized **L**earning (NCAL), designed to learn features that conform to the same simplex equiangular tight frame (ETF) structure. NCAL introduces Text-modality Collapse (TC) regularization to optimize the distribution of text embeddings within the large language model (LLM) representation space. Notably, NCAL is model-agnostic, making it compatible with various architectures and approaches, thereby ensuring broad applicability. Extensive experiments demonstrate that NCAL effectively enhances existing works, achieving new state-of-the-art performance. Additionally, NCAL mitigates class imbalance, significantly improving the model’s generalization ability.","Personalized learning has gained significant attention in recent years, yet most existing approaches assume access to well-balanced, high-quality datasets—an assumption that rarely holds in real-world educational settings where data often exhibit long-tail distributions. This imbalance hampers model performance and generalization. To address this challenge, we propose Neural-Collapse-Advanced personalized Learning (NCAL), a model-agnostic method that encourages features to align with a simplex equiangular tight frame (ETF) structure through Text-Modality Collapse (TC) regularization. By optimizing the distribution of text embeddings within the large language model’s representation space, NCAL effectively mitigates the negative effects of class imbalance. Our extensive experiments demonstrate that NCAL not only achieves state-of-the-art results but also significantly enhances the generalization ability of models across diverse architectures, offering a robust and scalable solution for real-world personalized learning tasks."
Poster,Adversarial Combinatorial Semi-bandits with Graph Feedback,https://ICML.cc//virtual/2025/poster/43467,Yuxiao Wen,"In combinatorial semi-bandits, a learner repeatedly selects from a combinatorial decision set of arms, receives the realized sum of rewards, and observes the rewards of the individual selected arms as feedback. In this paper, we extend this framework to include \emph{graph feedback}, where the learner observes the rewards of all neighboring arms of the selected arms in a feedback graph $G$. We establish that the optimal regret over a time horizon $T$ scales as $\widetilde{\Theta}(S\sqrt{T}+\sqrt{\alpha ST})$, where $S$ is the size of the combinatorial decisions and $\alpha$ is the independence number of $G$. This result interpolates between the known regrets $\widetilde\Theta(S\sqrt{T})$ under full information (i.e., $G$ is complete) and $\widetilde\Theta(\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$ has only self-loops), where $K$ is the total number of arms. A key technical ingredient is to realize a convexified action using a random decision vector with negative correlations. We also show that online stochastic mirror descent (OSMD) that only realizes convexified actions in expectation is suboptimal.","In many scenarios, we are making decisions while learning the factors that affect our preferences or gains, which is called online learning. We want to learn about those factors, but we also want to gain as much as possible during this process. For example, we have a few lotteries (numbered 1 to K) with different winning probabilities but same price, and we can draw any one of them every day. Then to get as much money as possible (say, in one year), we need to figure out whose probability is the highest as we go.This type of problem is studied under the name multi-armed bandits. In this work, we study a variant of them, in which we now can draw a few (say S) of them every day. How to maximize our gain? More importantly, if there is additional information, how can we leverage it? For example, imagine whenever you buy lottery k, your rich friend is always buying every lottery with a smaller number, namely 1 to k-1. And she will tell you her outcomes as well. This information structure forms a graph over the candidate actions (in this case, the lotteries we can purchase), and is called graph feedback.In this work, we mathematically the performance guarantees for any policy/strategy you can use to maximize your total gain, in the worst-case scenario. This is the so-called minimax regret bounds. We show how the worst-case performance guarantees relate to the graph structure, and we provide an optimal (in the worst-case sense) algorithm for you to achieve this. For any online decision-making process, such as bidding in advertising, online inventory control, recommendation systems, etc., you may use this algorithm to guarantee your total gain."
Poster,Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets,https://ICML.cc//virtual/2025/poster/44947,"Wei Liu, Zhongyu Niu, Lang Gao, Zhiying Deng, Jun Wang, Haozhao Wang, Ruixuan Li","This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations.Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method significantly outperforms recent rationalization methods.","(2) In the field of interpretability, it is common to identify rationales by maximizing prediction accuracy, which is prone to being affected by spurious correlations. (2)Existing research on spurious correlations has primarily focused on those arising from the original data generation process. However, we find that certain explanation algorithms themselves can introduce additional spurious correlations, even when the original dataset is clean. (3) We introduce an attack-based framework to audit and mitigate these spurious correlations introduced by the algorithms."
Poster,Adversarial Inception Backdoor Attacks against Reinforcement Learning,https://ICML.cc//virtual/2025/poster/43529,"Ethan Rathbun, Alina Oprea, Christopher Amato","Recent works have demonstrated the vulnerability of Deep Reinforcement Learning (DRL) algorithms against training-time, backdoor poisoning attacks. The objectives of these attacks are twofold: induce pre-determined, adversarial behavior in the agent upon observing a fixed trigger during deployment while allowing the agent to solve its intended task during training. Prior attacks assume arbitrary control over the agent's rewards, inducing values far outside the environment's natural constraints. This results in brittle attacks that fail once the proper reward constraints are enforced. Thus, in this work we propose a new class of backdoor attacks against DRL which are the first to achieve state of the art performance under strict reward constraints. These ``inception'' attacks manipulate the agent's training data -- inserting the trigger into prior observations and replacing high return actions with those of the targeted adversarial behavior. We formally define these attacks and prove they achieve both adversarial objectives against arbitrary Markov Decision Processes (MDP). Using this framework we devise an online inception attack which achieves an 100% attack success rate on multiple environments under constrained rewards while minimally impacting the agent's task performance.","Reinforcement Learning systems are often assumed to be trained in environments in which the developer can trust their data source. This isn't always the case, however, as external entities, referred to as the ""adversary"", can potentially modify training data with malicious intent. In this paper we study one such class of attack called a ""backdoor attack"". Here the goal of the adversary is to perturb the training data of a reinforcement learning model, called the ""agent"", such that it exhibits predetermined behavior upon observing some set ""trigger"". In the case of robotic agents with camera sensors, the trigger may take the form of a specific QR code while the predetermined behavior may be to accelerate forward no matter the consequences. Previous studies have demonstrated the possibility of these attacks against reinforcement learning agents with strong assumptions that they can arbitrarily modify the agent's ""reward"" signal. The reward is what reinforcement learning agents use to determine which behavior is good and which is bad. This assumptions is easy to break in practice however, making prior attacks brittle. This may give practitioners a false sense of security that their systems are safe from backdoor attacks, however this is not the case. Therefore we propose a new class of attack which is able to be successful while minimally altering the agent's reward. This motivates further research into stronger defenses against backdoor attacks."
Poster,Adversarial Inputs for Linear Algebra Backends,https://ICML.cc//virtual/2025/poster/45300,"Jonas Möller, Lukas Pirch, Felix Weissberg, Sebastian Baunsgaard, Thorsten Eisenhofer, Konrad Rieck","Linear algebra is a cornerstone of neural network inference. The efficiency of popular frameworks, such as TensorFlow and PyTorch, critically depends on backend libraries providing highly optimized matrix multiplications and convolutions. A diverse range of these backends exists across platforms, including Intel MKL, Nvidia CUDA, and Apple Accelerate. Although these backends provide equivalent functionality, subtle variations in their implementations can lead to seemingly negligible differences during inference. In this paper, we investigate these minor discrepancies and demonstrate how they can be selectively amplified by adversaries. Specifically, we introduce *Chimera examples*, inputs to models that elicit conflicting predictions depending on the employed backend library. These inputs can even be constructed with integer values, creating a vulnerability exploitable from real-world input domains. We analyze the prevalence and extent of the underlying attack surface and propose corresponding defenses to mitigate this threat.","Neural networks rely heavily on math operations, such as multiplying large tables of numbers. To run these calculations efficiently, machine learning systems use specialized software (called linear algebra backends) that are tailored to different types of hardware, like Intel processors, Nvidia graphics cards, or Apple devices. While all these backends perform the same basic computations, they do so in slightly different ways. These tiny differences usually don't matter. In this paper, however, we show that attackers can exploit them. We introduce what we call Chimera examples: carefully crafted inputs that make a neural network produce different results depending on the backend it uses. We examine how often these differences appear, how impactful they can be, and how to defend against this kind of vulnerability."
Poster,Adversarial Perturbations Are Formed by Iteratively Learning Linear Combinations of the Right Singular Vectors of the Adversarial Jacobian,https://ICML.cc//virtual/2025/poster/46521,"Thomas Paniagua, Chinmay Savadikar, Tianfu Wu","White-box targeted adversarial attacks reveal core vulnerabilities in Deep Neural Networks (DNNs), yet two key challenges persist: (i) How many target classes can be attacked simultaneously in a specified order, known as the *ordered top-$K$ attack* problem ($K \geq 1$)? (ii) How to compute the corresponding adversarial perturbations for a given benign image directly in the image space?We address both by showing that *ordered top-$K$ perturbations can be learned via iteratively optimizing linear combinations of the $\underline{ri}ght\text{ } \underline{sing}ular$  vectors of the adversarial Jacobian*  (i.e., the logit-to-image Jacobian constrained by target ranking). These vectors span an orthogonal, informative subspace in the image domain.We introduce **RisingAttacK**, a novel Sequential Quadratic Programming (SQP)-based method that exploits this structure. We propose a holistic figure-of-merits (FoM) metric combining attack success rates (ASRs) and $\ell_p$-norms ($p=1,2,\infty$).Extensive experiments on ImageNet-1k across six ordered top-$K$ levels ($K=1, 5, 10, 15, 20, 25, 30$) and four models (ResNet-50, DenseNet-121, ViT-B, DEiT-B) show RisingAttacK consistently surpasses the state-of-the-art QuadAttacK.","Deep neural networks (DNNs) are highly accurate but remain vulnerable to adversarial attacks—small, often imperceptible changes to input images that cause incorrect outputs. While most attacks focus on altering the top-1 prediction, many real-world systems (e.g., search engines, medical triage) rely on the entire ranked list of outputs. This raises a key question: how can we trick a DNN to produce an ordered set of incorrect predictions?We address this with **RisingAttacK**, a novel method that directly learns adversarial perturbations in image space. Using Sequential Quadratic Programming, it optimizes minimal, interpretable changes that manipulate the model’s top-K ranking. The attack leverages linear combinations of the most sensitive directions—derived from the adversarial Jacobian—to efficiently disrupt the model’s output ordering.RisingAttacK consistently outperforms prior state-of-the-art attacks across four major models and ranking depths (K = 1 to 30), achieving higher success rates and lower perturbation norms.By enabling precise manipulation of ranked outputs, our method delivers the kind of comprehensive stress tests increasingly demanded by regulators and practitioners—tests that top-1-only attacks simply cannot provide."
Poster,Adversarial Reasoning at Jailbreaking Time,https://ICML.cc//virtual/2025/poster/44790,"Mahdi Sabbaghi, Paul Kassianik, George Pappas, Amin Karbasi, Hamed Hassani","As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important.  Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks. In this paper, we apply these advances to the task of model jailbreaking: eliciting harmful responses from aligned LLMs. We develop an adversarial reasoning approach to automatic jailbreaking that leverages a loss signal to guide the test-time compute, achieving SOTA attack success rates against many aligned LLMs, even those that aim to trade inference-time compute for adversarial robustness. Our approach introduces a new paradigm in understanding LLM vulnerabilities, laying the foundation for the development of more robust and trustworthy AI systems.","Large language models (LLMs) can still be tricked into producing disallowed or dangerous content, even after extensive safety‑training. We show that breaking these defences is far more effective when the attacker treats the task as a reasoning problem rather than a trial‑and‑error search. Our framework, Adversarial Reasoning, deploys several reasoning modules that perform specific roles to build the reasoning trajectories and explore the ideas. In just 15 iterations the method reliably outperforms prior jailbreak techniques across several frontier models. These results reveal that giving adversaries more ""thinking time"" at test‑time can break the safety measures. To support more secure future systems, we have released our code."
Poster,Adversarial Robust Generalization of Graph Neural Networks,https://ICML.cc//virtual/2025/poster/46457,"Chang Cao, Han Li, Yulong Wang, Rui Wu, Hong Chen","While Graph Neural Networks (GNNs) have shown outstanding performance in node classification tasks, they are vulnerable to adversarial attacks, which are imperceptible changes to input samples. Adversarial training, as a widely used tool to enhance the adversarial robustness of GNNs, has presented remarkable effectiveness in node classification tasks. However, the generalization properties for explaining their behaviors remain not well understood from the theoretical viewpoint. To fill this gap, we develop a high probability generalization bound of general GNNs in adversarial learning through covering number analysis. We estimate the covering number of the GNN model class based on the entire perturbed feature matrix by constructing a cover for the perturbation set. Our results are generally applicable to a series of GNNs. We demonstrate their applicability by investigating the generalization performance of several popular GNN models under adversarial attacks, which reveal the architecture-related factors influencing the generalization gap. Our experimental results on benchmark datasets provide evidence that supports the established theoretical findings.","Graph Neural Networks (GNNs) are prone to being attacked by some deliberately crafted perturbations. To ensure GNNs' strong ability in processing the graph-structured data, adversarial training is adopted to defend against these attacks. However, we find that GNNs exhibit performance degradation in adversarial training, manifesting as a larger generalization gap. To investigate this problem, we utilize a classical technique, covering number, to measure the generalization properties of GNNs in adversarial training.To be specific, we derive the upper bounds of the generalization gap for several GNN models to explain their generalization behaviors in adversarial training. Our theoretical results provide helpful insights into model construction and algorithm designs to improve the generalization ability of GNNs in adversarial training. We also demonstrate our findings in the experiments."
Poster,Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees,https://ICML.cc//virtual/2025/poster/44436,"Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi","Two-stage Learning-to-Defer (L2D) enables optimal task delegation by assigning each input to either a fixed main model or one of several offline experts, supporting reliable decision-making in complex, multi-agent environments. However, existing L2D frameworks assume clean inputs and are vulnerable to adversarial perturbations that can manipulate query allocation—causing costly misrouting or expert overload. We present the first comprehensive study of adversarial robustness in two-stage L2D systems. We introduce two novel attack strategies—untargeted and targeted—which respectively disrupt optimal allocations or force queries to specific agents. To defend against such threats, we propose SARD, a convex learning algorithm built on a family of surrogate losses that are provably Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent. These guarantees hold across classification, regression, and multi-task settings. Empirical results demonstrate that SARD significantly improves robustness under adversarial attacks while maintaining strong clean performance, marking a critical step toward secure and trustworthy L2D deployment.","Two-Stage Learning-to-Defer systems enable optimal task delegation across multiple agents but assume clean inputs, making them vulnerable to adversarial perturbations. These subtle attacks can misroute queries, overload experts, or bias allocations—compromising both performance and trust in high-stakes applications.We introduce the first comprehensive framework to study and defend two-stage L2D systems against adversarial threats. We design two new attack strategies that reveal systemic vulnerabilities. To defend against these, we propose SARD, a convex algorithm.Our theoretical guarantees and empirical results show that SARD dramatically improves robustness under adversarial conditions while maintaining strong clean performance. This work lays the foundation for secure and trustworthy deployment of L2D systems in safety-critical domains like healthcare, finance, and autonomous systems."
