type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models,https://ICML.cc//virtual/2025/poster/43563,"William Chen, Jinchuan Tian, Yifan Peng, Brian Yan, Chao-Han Yang, Shinji Watanabe","Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. Scaling to larger models can improve ASR performance across the board, in both low and high resource languages, improving the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d for future studies.","Speech-to-text is one of the core technologies that allow computers to perceive and understand our physical world, transforming human speech encoded by sound waves into readable text. Development of AI-powered speech-to-text has traditionally focused on efficient models that can fit on small devices, such as smartphones, in contrast to power-hungry LLMs like ChatGPT that require dedicated data centers. In this paper, we study the effects of scaling speech-to-text to LLM-level compute. We find that as speech-to-text models get larger, their ability to not only transcribe and translate speech improves in a predictable manner. In other words, we can estimate the performance of a larger model from the performance of smaller ones. Furthermore, we find that larger models have ""emergent abilities"" unfound in smaller ones, such as being able to implicitly recognize different dialects and learn to transcribe new languages on the fly.We release all of our models for free, to allow researchers to better understand the capabilities of large AI models."
Poster,OW-VAP: Visual Attribute Parsing for Open World Object Detection,https://ICML.cc//virtual/2025/poster/45439,"Xing Xi, Xing Fu, Weiqiang Wang, Ronghua Luo","Open World Object Detection (OWOD) requires the detector to continuously identify and learn new categories. Existing methods rely on the large language model (LLM) to describe the visual attributes of known categories and use these attributes to mark potential objects. The performance of such methods is influenced by the accuracy of LLM descriptions, and selecting appropriate attributes during incremental learning remains a challenge. In this paper, we propose a novel OWOD framework, termed OW-VAP, which operates independently of LLM and requires only minimal object descriptions to detect unknown objects. Specifically, we propose a Visual Attribute Parser (VAP) that parses the attributes of visual regions and assesses object potential based on the similarity between these attributes and the object descriptions. To enable the VAP to recognize objects in unlabeled areas, we exploit potential objects within background regions.  Finally, we propose Probabilistic Soft Label Assignment (PSLA) to prevent optimization conflicts from misidentifying background as foreground. Comparative results on the OWOD benchmark demonstrate that our approach surpasses existing state-of-the-art methods with a +13 improvement in U-Recall and a +8 increase in U-AP for unknown detection capabilities. Furthermore, OW-VAP approaches the unknown recall upper limit of the detector.","We observed that existing approaches rely heavily on the attribute prediction accuracy of large language models (LLMs). In this paper, we propose an attribute parser that extracts coarse-grained attributes directly from visual regions, rather than relying on fixed, fine-grained attributes. To effectively train the attribute parser, we introduce a probabilistic modeling approach with soft labels. Our evaluation on benchmark demonstrates that the proposed method significantly outperforms previous approaches in performance and approaches or even surpasses the generalization upper bound of attribute detectors."
Poster,PAC-Bayes Analysis for Recalibration in Classification,https://ICML.cc//virtual/2025/poster/44590,"Masahiro Fujisawa, Futoshi Futami","Nonparametric estimation using uniform-width binning is a standard approach for evaluating the calibration performance of machine learning models.However, existing theoretical analyses of the bias induced by binning are limited to binary classification, creating a significant gap with practical applications such as multiclass classification. Additionally, many parametric recalibration algorithms lack theoretical guarantees for their generalization performance.To address these issues, we conduct a generalization analysis of calibration error using the probably approximately correct Bayes framework. This approach enables us to derive the first optimizable upper bound for generalization error in the calibration context. On the basis of our theory, we propose a generalization-aware recalibration algorithm. Numerical experiments show that our algorithm enhances the performance of Gaussian process-based recalibration across various benchmark datasets and models.","When machine learning models make predictions, it’s important to understand how confident they are—and how well that confidence matches reality. This is known as calibration. A common way to evaluate calibration is by dividing predictions into bins, but current theoretical understanding of the errors introduced by this method is limited mostly to simple binary classification problems. Real-world tasks, however, often involve many classes and more complex models. To bridge this gap, we provide a new theoretical framework to analyze how well calibration metrics generalize beyond the training data, even in multiclass settings. Using tools from PAC-Bayes theory, we derive the first generalization bounds that can be optimized directly during recalibration. Building on this theory, we design a new recalibration algorithm that explicitly accounts for generalization performance. Experiments across a variety of datasets and models show that our approach improves calibration quality compared to existing methods. Our work offers both new theoretical insights and practical tools for building more reliable AI systems."
Poster,PAC Learning with Improvements,https://ICML.cc//virtual/2025/poster/46125,"Idan Attias, Avrim Blum, Keziah Naggita, Donya Saless, Dravyansh Sharma, Matthew Walter","One of the most basic lower bounds in machine learning is that in nearly any nontrivial setting, it takes at least $1/\epsilon$ samples to learn to error $\epsilon$ (and more, if the classifier being learned is complex).  However, suppose that data points are agents who have the ability to improve by a small amount if doing so will allow them to receive a (desired) positive classification.  In that case, we may actually be able to achieve zero error by just being ""close enough"".  For example, imagine a hiring test used to measure an agent's skill at some job such that for some threshold $\theta$, agents who score above $\theta$ will be successful and those who score below $\theta$ will not (i.e., learning a threshold on the line). Suppose also that by putting in effort, agents can improve their skill level by some small amount $r$.  In that case, if we learn an approximation $\hat{\theta}$ of $\theta$ such that $\theta \leq \hat{\theta} \leq \theta + r$ and use it for hiring, we can actually achieve error zero, in the sense that (a) any agent classified as positive is truly qualified, and (b) any agent who truly is qualified can be classified as positive by putting in effort.  Thus, the ability for agents to improve has the potential to allow for a goal one could not hope to achieve in standard models, namely zero error.\In this paper, we explore this phenomenon more broadly, giving general results and examining under what conditions the ability of agents to improve can allow for a reduction in the sample complexity of learning, or alternatively, can make learning harder.  We also examine both theoretically and empirically what kinds of improvement-aware algorithms can take into account agents who have the ability to improve to a limited extent when it is in their interest to do so.","In settings where individuals are being judged, such as taking a test to certify competence at some task or admission to a desirable program, individuals often will invest effort to increase the chance of a favorable outcome in response to the published qualification criteria. For example, knowledge of the cutoff score for passing a test would impact the amount that individuals study for it, or a loan applicant may take a money management course if they know it will be used in determining whether they get the loan.This capacity for improvement can impact the design and accuracy of decision-making algorithms. As a simple example, if every individual can truthfully change themselves to match any positive example, then a single positive example is sufficient for an algorithm to achieve zero classification error.Our central question is: How does people's capacity for improvement within a limited amount affect the design of accurate decision-making algorithms? To address this, we examine the conditions under which people's ability to improve can reduce or increase the minimum number of training examples required to learn a function within a desired error margin and confidence level (sample complexity) and theoretically and empirically investigate which algorithms can effectively incorporate this behavior."
Poster,Pairwise Maximum Likelihood For Multi-Class Logistic Regression Model With Multiple Rare Classes,https://ICML.cc//virtual/2025/poster/46236,"Xuetong Li, Danyang Huang, Hansheng Wang","We study in this work the problem of multi-class logistic regression with one major class and multiple rare classes, which is motivated by a real application in TikTok live stream data. The model is inspired by the two-class logistic regression model of Wang (2020) but with surprising theoretical findings, which in turn motivate new estimation methods with excellent statistical and computational efficiency. Specifically, since rigorous theoretical analysis suggests that the resulting maximum likelihood estimators of different rare classes should be asymptotically independent, we consider to solve multiple pairwise two-class logistic regression problems instead of optimizing the joint log-likelihood function with computational challenge in multi-class problem, which are computationally much easier and can be conducted in a fully parallel way. To further reduce the computation cost, a subsample-based pairwise likelihood estimator is developed by down-sampling the major class. We show rigorously that the resulting estimators could be as asymptotically efficient as the global maximum likelihood estimator under appropriate regularity conditions. Extensive simulation studies are presented to support our theoretical findings and a TikTok live stream dataset is analyzed for illustration purpose.","In real-world applications like car plate recognition in TikTok live streams, the multi-class classification task often faces the challenge of class imbalance, where multiple rare classes (e.g., unique license plates) are vastly outnumbered by a major class (background). We propose a new method based on logistic regression. Specifically, the multi-class problem is decomposed into multiple independent binary subproblems, where each rare class is paired separately with the major class. Theoretical analysis shows that this decomposition maintains classification validity while enabling independent optimization for rare classes. To further reduce the computational cost, we reduce the size of the major class through random subsampling. By enabling parallel training, this method offers an efficient and effective solution to imbalanced classification."
Poster,P(all-atom) Is Unlocking New Path For Protein Design,https://ICML.cc//virtual/2025/poster/43525,"Wei Qu, Jiawei Guan, Rui Ma, kezhai, weikun wu, haobo Wang","We introduce Pallatom, an innovative protein generation model capable of producing protein structures with all-atom coordinates. Pallatom directly learns and models the joint distribution $P(\textit{structure}, \textit{seq})$ by focusing on $P(\textit{all-atom})$, effectively addressing the interdependence between sequence and structure in protein generation. To achieve this, we propose a novel network architecture specifically designed for all-atom protein generation. Our model employs a dual-track framework that tokenizes proteins into token-level and atomic-level representations, integrating them through a multi-layer decoding process with ""traversing"" representations and recycling mechanism. We also introduce the $\texttt{atom14}$ representation method, which unifies the description of unknown side-chain coordinates, ensuring high fidelity between the generated all-atom conformation and its physical structure. Experimental results demonstrate that Pallatom excels in key metrics of protein design, including designability, diversity, and novelty, showing significant improvements across the board. Our model not only enhances the accuracy of protein generation but also exhibits excellent sampling efficiency, paving the way for future applications in larger and more complex systems.","Pallatom introduces a novel protein generation model that directly learns the joint distribution of all-atom coordinates P(all-atom), integrating sequence and structure co-design. Key innovations include the atom14 representation, which standardizes side-chain atoms via virtual placements, and a dual-track network with residue/atomic tokenization and recycling mechanisms. Experiments show Pallatom outperforms existing methods in designability, structural diversity, and novelty, while maintaining high sampling efficiency. By modeling atomic coordinates holistically, Pallatom eliminates separate sequence/structure steps, enabling accurate and diverse protein generation."
Poster,"PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling",https://ICML.cc//virtual/2025/poster/43847,"Avery Ma, Yangchen Pan, Amir-massoud Farahmand","Many-shot jailbreaking circumvents the safety alignment of LLMs by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational exchanges between the user and the model. These exchanges are randomly sampled from a pool of unsafe question-answer pairs, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with Positive Affirmations, Negative Demonstrations, and an optimized Adaptive Sampling method tailored to the target prompt's topic. We also introduce ManyHarm, a dataset of harmful question–answer pairs, and demonstrate through extensive experiments that PANDAS significantly outperforms baseline methods in long-context scenarios. Through attention analysis, we provide insights into how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.","Large language models can be tricked into generating harmful output by overloading them with long, fake conversations. These conversations are designed to make it seem like the model has already followed dangerous instructions many times.In this paper, we introduce PANDAS, a technique that improves this type of attack by modifying the fake conversations with positive affirmation phrases, negative demonstrations, and a more targeted selection of content.Results on state-of-the-art open-source models show that PANDAS is more effective at eliciting harmful outputs than previous methods. We also analyze the models' intermediate outputs to understand the effect of PANDAS."
Poster,PaperBench: Evaluating AI’s Ability to Replicate AI Research,https://ICML.cc//virtual/2025/poster/43586,"Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, Tejal Patwardhan","We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We open-source our code (https://github.com/openai/preparedness) to facilitate future research in understanding the AI engineering capabilities of AI agents.","As AI systems grow more capable, we need to understand if they can independently conduct machine learning research - a capability that could accelerate scientific progress but also raise safety concerns. Our work introduces PaperBench, a benchmark that challenges AI systems to replicate 20 cutting-edge machine learning papers completely from scratch, requiring them to understand the research, write code, and successfully run experiments. We developed detailed assessment rubrics with the original paper authors to break down each replication task into hundreds of individually gradable components, turning a complex subjective evaluation into an objective assessment that can be automatically graded by other AI systems. When testing several advanced AI systems, we found that even the best-performing AI agent could only achieve a replication score of 27%, while human machine learning experts scored 41% under similar circumstances.We release PaperBench to provide a valuable tool for measuring how well AI systems can autonomously perform complex machine learning research, helping track progress as these capabilities advance and informing important decisions about AI development and governance."
Poster,ParallelComp: Parallel Long-Context Compressor for Length Extrapolation,https://ICML.cc//virtual/2025/poster/46101,"Jing Xiong, Jianghan Shen, Chuanyang Zheng, Zhongwei Wan, Chenyang Zhao, Chiwun Yang, Fanghua Ye, Hongxia Yang, Lingpeng Kong, Ngai Wong","Extrapolating ultra-long contexts (text length >128K) remains a major challenge for large language models (LLMs), as most training-free extrapolation methods are not only severely limited by memory bottlenecks, but also suffer from the attention sink, which restricts their scalability and effectiveness in practice. In this work, we propose ParallelComp, a parallel long-context compression method that effectively overcomes the memory bottleneck, enabling 8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB GPU in a training-free setting. ParallelComp splits the input into chunks, dynamically evicting redundant chunks and irrelevant tokens, supported by a parallel KV cache eviction mechanism. Importantly, we present a systematic theoretical and empirical analysis of attention biases in parallel attention—including the attention sink, recency bias, and middle bias—and reveal that these biases exhibit distinctive patterns under ultra-long context settings. We further design a KV cache eviction technique to mitigate this phenomenon. Experimental results show that ParallelComp enables an 8B model (trained on 8K context) to achieve 91.17% of GPT-4's performance under ultra-long contexts, outperforming closed-source models such as Claude-2 and Kimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby achieving a 23.50x acceleration in the prefill stage with negligible performance loss and pave the way for scalable and robust ultra-long contexts extrapolation in LLMs. We release the code at https://github.com/menik1126/ParallelComp.","Current large language models (LLMs) still face significant challenges when processing ultra-long texts (over 128,000 tokens), primarily due to computational resource limitations and bias issues within the attention mechanism. We propose a new method called ParallelComp, which significantly reduces memory usage, enabling models to successfully handle texts ranging from 4,000 to 128,000 tokens without retraining. This method divides long texts into smaller chunks and processes them in parallel while automatically removing redundant or irrelevant parts, greatly improving efficiency and performance.We also analyze common biases that models exhibit when processing long texts—such as overemphasizing the beginning or end—and demonstrate that our method mitigates these issues. In experiments, our method enabled a medium-sized model (8 billion parameters) to perform exceptionally well on ultra-long text tasks, reaching performance close to GPT-4 and even surpassing some closed-source models."
Poster,Parallel Simulation for Log-concave Sampling and Score-based Diffusion Models,https://ICML.cc//virtual/2025/poster/43916,"Huanjian Zhou, Masashi Sugiyama","Sampling from high-dimensional probability distributions is fundamental in machine learning and statistics. As datasets grow larger, computational efficiency becomes increasingly important, particularly in reducing *adaptive complexity*, namely the number of sequential rounds required for sampling algorithms. While recent works have introduced several parallelizable techniques, they often exhibit suboptimal convergence rates and remain significantly weaker than the latest lower bounds for log-concave sampling.To address this, we propose a novel parallel sampling method that improves adaptive complexity dependence on dimension $d$ reducing it from $\widetilde{\mathcal{O}}(\log^2 d)$ to $\widetilde{\mathcal{O}}(\log d)$.  Our approach builds on parallel simulation techniques from scientific computing.","Sampling from high-dimensional probability distributions is a fundamental task in machine learning and statistics, essential for applications like Bayesian inference and generative modeling. However, existing sampling methods often require numerous sequential steps, limiting their efficiency and scalability, especially as datasets grow larger.Our research introduces a novel parallel sampling algorithm that significantly reduces the number of sequential steps—known as adaptive complexity—required to generate high-quality samples. By leveraging techniques from scientific computing, our method improves the adaptive complexity from approximately the square of the logarithm of the data's dimensionality to just the logarithm. As a result, our method speeds up sampling for both log-concave distributions and diffusion models.This advancement enables more efficient processing of large-scale data, facilitating faster and more scalable applications in machine learning tasks such as generative modeling and Bayesian inference."
