type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction,https://ICML.cc//virtual/2025/poster/46570,"Zhangyi Hu, Jiemin Wu, Hua XU, Mingqian Liao, Ninghui Feng, Bo Gao, Songning Lai, Yutao Yue","Irregular Multivariate Time Series (IMTS) forecasting is challenging due to the unaligned nature of multi-channel signals and the prevalence of extensive missing data. Existing methods struggle to capture reliable temporal patterns from such data due to significant missing values. While pre-trained foundation models show potential for addressing these challenges, they are typically designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel information and its success in RTS forecasting, we propose **VIMTS**, a framework adapting **V**isual MAE for **IMTS** forecasting. To mitigate the effect of missing values, VIMTS first processes IMTS along the timeline into feature patches at equal intervals. These patches are then complemented using learned cross-channel dependencies. Then it leverages visual MAE's capability in handling sparse multichannel data for patch reconstruction, followed by a coarse-to-fine technique to generate precise predictions from focused contexts. In addition, we integrate self-supervised learning for improved IMTS modeling by adapting the visual MAE to IMTS data. Extensive experiments demonstrate VIMTS's superior performance and few-shot capability, advancing the application of visual foundation models in more general time series tasks. Our code is available at https://github.com/WHU-HZY/VIMTS.","Imagine trying to predict something important using data that's messy – signals from multiple sensors that don't always record at the same time, or have lots of gaps. That's the challenge of Irregular Multivariate Time Series (IMTS) forecasting. Existing methods struggle with this incomplete data, and even powerful pre-trained models are usually built for perfectly clean, regular datasets.We looked to Masked AutoEncoders (MAEs), a type of AI that's great at understanding incomplete images. We developed VIMTS, a new approach that adapts these visual MAEs for IMTS forecasting. VIMTS intelligently organizes messy data into ""patches"" and fills in missing information using relationships between different data channels. Then, it uses the MAE's reconstruction ability to make accurate predictions, refining them from broad to specific details. We also added a self-supervised learning step to help VIMTS learn even better from IMTS data.Our tests show VIMTS performs exceptionally well, even when data is scarce, pushing the boundaries for applying visual AI in diverse time series problems."
Poster,iN2V: Bringing Transductive Node Embeddings to Inductive Graphs,https://ICML.cc//virtual/2025/poster/46107,"Nicolas Lell, Ansgar Scherp","Shallow node embeddings like node2vec (N2V) can be used for nodes without features or to supplement existing features with structure-based information.Embedding methods like N2V are limited in their application on new nodes, which restricts them to the transductive setting where the entire graph, including the test nodes, is available during training.We propose inductive node2vec (iN2V), which combines a post-hoc procedure to compute embeddings for nodes unseen during training and modifications to the original N2V training procedure to prepare the embeddings for this post-hoc procedure.We conduct experiments on several benchmark datasets and demonstrate that iN2V is an effective approach to bringing transductive embeddings to an inductive setting.Using iN2V embeddings improves node classification by 1 point on average, with up to 6 points of improvement depending on the dataset and the number of unseen nodes.Our iN2V is a plug-in approach to create new or enrich existing embeddings. It can also be combined with other embedding methods, making it a versatile approach for inductive node representation learning. Code to reproduce the results is available at https://github.com/Foisunt/iN2V.","In real-world situations, graphs (like social networks) often change over time — for example, when new users create accounts. These changes add new nodes and connections that weren’t available during the original training of the embedding model. Many popular graph embedding methods, such as Node2Vec, can only create embeddings for nodes that were known during training.To handle new, unseen nodes, we introduce a method that can create embeddings for them after the original training is done. Although our method can work with different embedding methods, we focus on Node2Vec and adjust its training so it produces embeddings that are easier to extend later.This approach leads to better performance in real-world tasks where it's unrealistic to assume all nodes are known in advance."
Poster,Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games,https://ICML.cc//virtual/2025/poster/44674,"Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi","Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empiricalestimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.","Multi-agent reinforcement learning (MARL) in unknown environments presents a significant challenge in designing efficient exploration strategies. While common approaches incentivize exploration through explicit bonus terms derived from uncertainty quantification, constructing these bonuses often becomes intractable, particularly with complex function approximation or numerous agents. This paper introduces VMG (Value-incentivized Markov Game solver), a novel model-based algorithm that promotes exploration by biasing the empirical estimate of the game's model parameters. Specifically, VMG favors model parameters associated with higher collective best-response values for all players, thereby encouraging policy deviation from current equilibria without explicit bonus functions. This methodology allows for simultaneous, uncoupled policy updates and is theoretically established to achieve near-optimal regret for finding Nash and Coarse Correlated Equilibria in Markov games, comparable to methods relying on sophisticated uncertainty quantification."
Poster,In-Context Adaptation to Concept Drift for Learned Database Operations,https://ICML.cc//virtual/2025/poster/43752,"Jiaqi Zhu, Shaofeng Cai, Shen, Gang Chen, Fang Deng, Beng Chin Ooi","Machine learning has demonstrated transformative potential for database operations, such as query optimization and in-database data analytics. However, dynamic database environments, characterized by frequent updates and evolving data distributions, introduce concept drift, which leads to performance degradation for learned models and limits their practical applicability. Addressing this challenge requires efficient frameworks capable of adapting to shifting concepts while minimizing the overhead of retraining or fine-tuning.In this paper, we propose FLAIR, an online adaptation framework that introduces a new paradigm called \textit{in-context adaptation} for learned database operations. FLAIR leverages the inherent property of data systems, i.e., immediate availability of execution results for predictions, to enable dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} | \mathcal{C}_t) \to \mathbf{y}$, with $\mathcal{C}_t$ representing a dynamic context memory, FLAIR delivers predictions aligned with the current concept, eliminating the need for runtime parameter optimization. To achieve this, FLAIR integrates two key modules: a Task Featurization Module for encoding task-specific features into standardized representations, and a Dynamic Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly using contextual information at runtime. Extensive experiments across key database tasks demonstrate that FLAIR outperforms state-of-the-art baselines, achieving up to $5.2\times$ faster adaptation and reducing error by 22.5\% for cardinality estimation.","Modern databases often rely on machine learning to speed up tasks like finding and analyzing data. However, these systems struggle to keep up when the data changes over time, which can lead to slower performance and incorrect results.We developed a new framework called FLAIR that helps models adapt more quickly to changes in the data without needing to be retrained every time. FLAIR uses the results of recent database tasks to create a “context” that guides future predictions, much like how humans use recent experiences to make decisions.FLAIR helps databases stay accurate and efficient even as their data evolves, making them more reliable in real-world situations. In our tests, it adapted up to five times faster than current methods and significantly improved accuracy, offering a promising step toward smarter, self-adjusting data systems."
Poster,In-Context Deep Learning via Transformer Models,https://ICML.cc//virtual/2025/poster/44744,"Weimin Wu, Maojiang Su, Jerry Yao-Chieh Hu, Zhao Song, Han Liu","We investigate the transformer's capability for in-context learning (ICL) to simulate the training process of deep models. Our key contribution is providing a positive example of using a transformer to train a deep neural network by gradient descent in an implicit fashion via ICL. Specifically, we provide an explicit construction of a $(2N+4)L$-layer transformer capable of simulating $L$ gradient descent steps of an $N$-layer ReLU network through ICL.We also give the theoretical guarantees for the approximation within any given error and the convergence of the ICL gradient descent.Additionally, we extend our analysis to the more practical setting using Softmax-based transformers. We validate our findings on synthetic datasets for 3-layer, 4-layer, and 6-layer neural networks.The results show that ICL performance matches that of direct training.","Training deep neural networks from scratch is expensive and time-consuming. We asked: can a powerful pretrained transformer model simulate the training of another deep model — without updating its own parameters? This question matters because it could make machine learning far more efficient and accessible.We show that transformers can perform such “in-context learning,” effectively simulating multiple steps of gradient descent by just observing example data. We construct a transformer architecture that replicates the training process of deep neural networks and provide theoretical guarantees on its accuracy and convergence. We also validate our results through experiments.Our results suggest a path where one foundation model can enable the training of many others, reducing redundancy and computational cost."
Poster,In-Context Denoising with One-Layer Transformers: Connections between Attention and Associative Memory Retrieval,https://ICML.cc//virtual/2025/poster/45913,"Matthew Smart, Alberto Bietti, Anirvan Sengupta","We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning.","Large language models can often solve new tasks after seeing only a few examples in their prompt, a skill known as *in-context learning*. However, researchers still don’t fully understand how this process works under the hood.In our work, we studied a version of this problem where a model is asked to recover a clean example from a corrupted one using similar, uncorrupted examples as context. We showed that even a very simple model—a transformer with just one attention layer—can do this remarkably well. In fact, its behavior closely matches how certain mathematical models of memory retrieval work, revealing a surprising connection between in-context learning and associative memory.This discovery helps explain why transformer models are so adaptable. It builds on earlier work linking attention and memory, and shows that the same ideas help explain how pre-trained models can adapt to new tasks using only examples in their input. Our results offer a clearer picture of how these systems generalize, and could inform the design of simpler, more interpretable machine learning systems."
Poster,In-Context Fine-Tuning for Time-Series Foundation Models,https://ICML.cc//virtual/2025/poster/43707,"Matthew Faw, Rajat Sen, Yichen Zhou, Abhimanyu Das","Motivated by the recent success of time-series foundation models for zero-shot forecasting, we present a methodology for _in-context fine-tuning_ of a time-series foundation model. In particular, we design a pretrained foundation model that can be prompted (at inference time) with multiple time-series examples, in order to forecast a target time-series into the future. Our foundation model is specifically trained to utilize examples from multiple related time-series in its context window (in addition to the history of the target time-series) to help it adapt to the specific distribution of the target domain at inference time.  We show that such a foundation model that uses in-context examples at inference time can obtain much better performance on popular forecasting benchmarks compared to supervised deep learning methods, statistical models, and other time series foundation models.  Interestingly, our in-context fine-tuning approach even matches the performance of a foundation model that is explicitly fine-tuned on the target domain.","Traditional time-series forecasting models adhered to the traditional supervised learning framework of training on task-specific data before forecasting for that task. Recently, however, time-series foundation models, which are pretrained on a large set of time-series across multiple domains, have shown strong zero-shot forecasting performance. Despite this success, existing time-series foundation models lack some of the desirable features of LLMs with respect to in-context learning: the zero-shot performance of an LLM can be greatly improved at inference time by using its context window to provide additional instructional prompts with the input. We design a pretrained foundation model that can be prompted (at inference time) with multiple time-series examples to forecast a target time-series into the future. Our foundation model is specifically trained to utilize examples from multiple related time-series in its context window to help it adapt to the specific distribution of the target domain at inference time.We show that in-context examples can boost the performance of a foundation model on a number of popular forecasting benchmarks relative to a large number of baselines. Interestingly, our in-context fine-tuning approach even matches the performance of a foundation model that is explicitly fine-tuned on the target domain."
Poster,In-Context Learning and Occam's Razor,https://ICML.cc//virtual/2025/poster/43798,"Eric Elmoznino, Tom Marty, Tejas Kasetty, Léo Gagnon, Sarthak Mittal, Mahan Fathi, Dhanya Sridhar, Guillaume Lajoie","A central goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best—a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning—an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode.","A major challenge in machine learning (ML) is teaching models to make accurate predictions on data they've never seen before, a capability known as generalization. Typically, ML methods attempt this by not only fitting training data well but also keeping models simple—a principle called Occam's razor. However, most current techniques only indirectly encourage simplicity, often resulting in overly complex models that don't generalize well.Our research uncovers a direct connection between Occam's razor and a popular ML technique known as ""in-context learning,"" where large sequence models like the ones that power modern AI chatbots can quickly adapt to new tasks simply from examples provided in their prompt (without additional training). We show mathematically that the training method used by these models inherently balances fitting the data and keeping models simple. Specifically, this approach resembles a data compression method called ""prequential coding"", where simpler models better predict future data.These insights explain why in-context learning works well and suggest ways to improve it, potentially leading to more reliable and efficient ML models."
Poster,In-Context Learning as Conditioned Associative Memory Retrieval,https://ICML.cc//virtual/2025/poster/44826,"Weimin Wu, Teng-Yun Hsiao, Jerry Yao-Chieh Hu, Wenxin Zhang, Han Liu","We provide an exactly solvable example for interpreting In-Context Learning (ICL) with one-layer attention models as conditional retrieval of dense associative memory models.Our main contribution is to interpret ICL as memory reshaping in the modern Hopfield model from a conditional memory set (in-context examples).Specifically, we show that the in-context sequential examples induce an effective reshaping of the energy landscape of a Hopfield model.We integrate this in-context memory reshaping phenomenon into the existing Bayesian model averaging view of ICL [Zhang et al., AISTATS 2025] via the established equivalence between the modern Hopfield model and transformer attention.Under this unique perspective, we not only characterize how in-context examples shape predictions in the Gaussian linear regression case, but also recover the known $\epsilon$-stability generalization bound of the ICL for the one-layer attention model.We also give explanations for three key behaviors of ICL and validate them through experiments.","Large language models like ChatGPT can solve new problems just by being shown a few examples in a prompt. We are curious about how these models manage to “learn” so quickly without updating their internal parameters, and whether there’s a simple explanation behind this surprising behavior.We found that this process can be understood as a kind of memory retrieval. Specifically, we use a classic brain-inspired model called a Hopfield network to show how each example in the prompt subtly reshapes what the model “remembers.” This reshaping helps the model focus on the most relevant information for making predictions — just like how a person recalls different memories depending on the question they’re asked.To test this idea, we build a simplified version of a language model and run experiments with it. Our results confirm that in-context learning is stronger when the examples are similar to the test case, accurate, and drawn from a familiar setting."
Poster,In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention,https://ICML.cc//virtual/2025/poster/46533,"Jianliang He, Xintian Pan, Siyu Chen, Zhuoran Yang","We study how multi-head softmax attention models are trained to perform in-context learning on linear data. Through extensive empirical experiments and rigorous theoretical analysis, we demystify the emergence of elegant attention patterns: a diagonal and homogeneous pattern in the key-query weights, and a last-entry-only and zero-sum pattern in the output-value weights. Remarkably, these patterns consistently appear from gradient-based training starting from random initialization. Our analysis reveals that such emergent structures enable multi-head attention to approximately implement a debiased gradient descent predictor --- one that outperforms single-head attention and nearly achieves Bayesian optimality up to proportional factor.  We also extend our study to scenarios with anisotropic covariates and multi-task linear regression. Our results reveal that in-context learning ability emerges from the trained transformer as an aggregated effect of its architecture and the underlying data distribution, paving the way for deeper understanding and broader applications of in-context learning.","Many AI models can perform in‐context learning, but it’s been a mystery how complex transformer networks actually do this. We tackled the problem by training a standard multi‐head softmax transformer on a simple linear regression task and studying both its learned parameters and its training process. We find that each attention head learns a neat diagonal pattern in one set of weights and a zero‐sum pattern in another, emerging reliably from random starts. These structures let the transformer perform a one‐step, debiased form of gradient descent, so it predicts almost as well as the best possible (Bayesian) estimator and beats single‐head versions. Our work reveals exactly how architectural choices and data shape in‐context learning, paving the way for more robust, adaptable AI."
