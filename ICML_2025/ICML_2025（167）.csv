type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Learning to (Learn at Test Time): RNNs with Expressive Hidden States,https://ICML.cc//virtual/2025/poster/43617,"Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin","Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.","Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research."
Poster,Learning to Match Unpaired Data with Minimum Entropy Coupling,https://ICML.cc//virtual/2025/poster/44197,"Mustapha Bounoua, Giulio Franzese, Pietro Michiardi","Multimodal data is a precious asset enabling a variety of downstream tasks in machine learning. However, real-world data collected across different modalities is often not paired, which is a significant challenge to learn a joint distribution. A prominent approach to address the modality coupling problem is Minimum Entropy Coupling (MEC), which seeks to minimize the joint Entropy, while satisfying constraints on the marginals. Existing approaches to the MEC problem focus on finite, discrete distributions, limiting their application for cases involving continuous data. In this work, we propose a novel method to solve the continuous MEC problem, using well-known generative diffusion models that learn to approximate and minimize the joint Entropy through a cooperative scheme, while satisfying a relaxed version of the marginal constraints.We empirically demonstrate that our method, DDMEC, is general and can be easily used to address challenging tasks, including unsupervised single-cell multi-omics data alignment and unpaired image translation, outperforming specialized methods.","In the real world, we often gather data from different sources, such as images and text or various biological measurements, but these sources do not always align directly. For example, we might have many images and many captions, but not know which caption corresponds to which image. This makes it difficult for AI systems to learn how the different types of data relate to each other.This research addresses that problem using a method called Minimum Entropy Coupling (MEC), which aims to find the most organized way to connect two datasets while preserving the unique characteristics of each. However, previous versions of MEC only worked with simpler, discrete data.We introduce a new method called DDMEC, which uses a type of generative model known as diffusion models to connect more complex, continuous types of data, such as images or biological signals, even when they are unpaired. The method is both flexible and versatile.We evaluated DDMEC on challenging tasks such as matching different types of biological data and translating images from one domain to another, and it outperformed specialized tools designed specifically for those tasks."
Poster,Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge,https://ICML.cc//virtual/2025/poster/45391,"Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, JASON WESTON, Tianlu Wang","LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-by-step reasoning process that underlies the final evaluation of a response. However, due to the lack of human-annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench and PPE, despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models.","The advancement of AI is hindered by the limitations of evaluation methods, but Large Language Models (LLMs) have emerged as a key solution by serving as effective evaluators. These models generate Chain-of-Thoughts (CoTs) that capture the step-by-step reasoning process underlying the final evaluation of a response. However, due to the lack of human-annotated CoTs for evaluation, training such LLM-as-a-Judge models that can think and reason have proved to be challenging.To that end, we develop a new preference optimization algorithm called EvalPlanner that helps LLMs think before producing judgments. Its thoughts consist of an evaluation plan comprised of all the necessary steps to evaluate responses specific to the given instruction and then an execution of that plan to arrive at the final judgment. EvalPlanner relies entirely on synthetic data, iteratively optimizing the model's plans and executions in a self-training loop.Our method achieved state-of-the-art performance on several benchmarks, outperforming other approaches despite being trained on less and synthetic data. By improving the evaluation process, we can create more reliable and trustworthy AI systems that can have significant implications for applications where accurate evaluation is critical, such as education, hiring, and content moderation."
Poster,Learning to Quantize for Training Vector-Quantized Networks,https://ICML.cc//virtual/2025/poster/43509,"Peijia Qin, Jianguo Zhang","Deep neural networks incorporating discrete latent variables have shown significant potential in sequence modeling.A notable approach is to leverage vector quantization (VQ) to generate discrete representations within a codebook.However, its discrete nature prevents the use of standard backpropagation, which has led to challenges in efficient codebook training.In this work, we introduce **Meta-Quantization (MQ)**, a novel vector quantization training framework inspired by meta-learning.Our method separates the optimization of the codebook and the auto-encoder into two levels.Furthermore, we introduce a hyper-net to replace the embedding-parameterized codebook, enabling the codebook to be dynamically generated based on the feedback from the auto-encoder.Different from previous VQ objectives, our innovation results in a meta-objective that makes the codebook training task-aware.We validate the effectiveness of MQ with VQVAE and VQGAN architecture on image reconstruction and generation tasks.Experimental results showcase the superior generative performance of MQ, underscoring its potential as a robust alternative to existing VQ methods.","When computers learn to create images, they often use a ""dictionary"" of visual patterns, but it's challenging to create an optimal dictionary.  Often, only a few patterns are used, and the dictionary isn't perfectly tuned for the specific task, like generating realistic faces, which can limit the quality of the generated images. Our research introduces ""Meta-Quantization,"" a smarter, two-level approach to build this visual dictionary.  A special ""hyper-network"" first generates the dictionary, and then the main image-processing system learns using it.  Afterwards, this hyper-network refines the dictionary based on how well the main system performed, making the dictionary more effective and specifically tailored to the task. This method results in more efficient visual dictionaries, leading to higher-quality image generation and reconstruction.  Computers can make better use of all learned visual patterns and can learn faster.  Our approach provides a more robust way to train artificial intelligence models for these visual tasks, ultimately improving their overall performance"
Poster,Learning to Reuse Policies in State Evolvable Environments,https://ICML.cc//virtual/2025/poster/46639,"Ziqian Zhang, Bohan Yang, Lihe Li, Yuqi Bian, Ruiqi Xue, Feng Chen, Yi-Chen Li, lei yuan, Yang Yu","The policy trained via reinforcement learning (RL) makes decisions based on sensor-derived state features. It is common for state features to evolve for reasons such as periodic sensor maintenance or the addition of new sensors for performance improvement. The deployed policy fails in new state space when state features are unseen during training. Previous work tackles this challenge by training a sensor-invariant policy or generating multiple policies and selecting the appropriate one with limited samples. However, both directions struggle to guarantee the performance when faced with unpredictable evolutions. In this paper, we formalize this problem as state evolvable reinforcement learning (SERL), where the agent is required to mitigate policy degradation after state evolutions without costly exploration. We propose **Lapse** by reusing policies learned from the old state space in two distinct aspects. On one hand, Lapse directly reuses the *robust* old policy by composing it with a learned state reconstruction model to handle vanishing sensors. On the other hand, the behavioral experience from the old policy is reused by Lapse to train a newly adaptive policy through offline learning, better utilizing new sensors. To leverage advantages of both policies in different scenarios, we further propose *automatic ensemble weight adjustment* to effectively aggregate them. Theoretically, we justify that robust policy reuse helps mitigate uncertainty and error from both evolution and reconstruction. Empirically, Lapse achieves a significant performance improvement, outperforming the strongest baseline by about $2\times$ in benchmark environments.","Many automated systems rely on sensor data to make decisions, but sensors can change over time—for example, due to maintenance or upgrades. When this happens, decision-making policies trained on old sensor data often experience a drop in performance, because they are not prepared for new or missing information.Previous solutions try to make policies ignore sensor differences or train several policies and pick the right one, but these approaches often struggle when facing unexpected sensor changes. To address this, we introduce a new framework called state evolvable reinforcement learning (SERL), which aims to maintain reliable performance even as sensors change, without costly trial-and-error.Our method, called Lapse, reuses knowledge from old sensor setups in two ways: it adapts the old policy to work with missing sensors and uses experience from the old setup to train a new, more adaptive policy for new sensors. Lapse can automatically combine both approaches depending on the situation. In tests, Lapse showed significantly better performance than previous methods, helping automated systems stay reliable as their sensors evolve."
Poster,Learning to Route LLMs with Confidence Tokens,https://ICML.cc//virtual/2025/poster/45145,"Yu-Neng Chuang, Prathusha Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, Xia Hu, Helen Zhou","Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-Reflection with Error-based Feedback (Self-REF), a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.","Large language models (LLMs) often give answers without indicating how confident they are. This can be risky in situations where wrong answers have serious consequences. We introduce a method called Self-REF that helps LLMs signal when their answers might be unreliable by assigning a confidence score. We demonstrate that this confidence score is valuable for learning when to route a query to another more powerful LLM, or alternatively reject the query. Compared to existing approaches on four datasets and two base LLMs, Self-REF performs the best on both LLM routing and LLM rejection learning tasks."
Poster,Learning to Steer Learners in Games,https://ICML.cc//virtual/2025/poster/44476,"Yizhou Zhang, Yian Ma, Eric Mazumdar","We consider the problem of learning to exploit learning algorithms through repeated interactions in games. Specifically, we focus on the case of repeated two player, finite-action games, in which an optimizer aims to steer a no-regret learner to a Stackelberg equilibrium without knowledge of its payoffs. We first show that this is impossible if the optimizer only knows that the learner is using an algorithm from the general class of no-regret algorithms. This suggests that the optimizer requires more information about the learner's objectives or algorithm to successfully exploit them. Building on this intuition, we reduce the problem for the optimizer to that of recovering the learner's payoff structure. We demonstrate the effectiveness of this approach if the learner’s algorithm is drawn from a smaller class by analyzing two examples: one where the learner uses an ascent algorithm, and another where the learner uses stochastic mirror ascent with known regularizer and step sizes.","As AI is increasingly deployed in rapidly changing real-world scenarios such as self-driving cars, online advertising auctions, personalized recommendation platforms, high-frequency trading, autonomous logistics routing, and smart-grid control, these adaptive systems regularly interact with both human users and other algorithms that are simultaneously learning to maximize their own utility. In this paper, we seek to understand how an agent operating in such a competitive, information-limited environment should deliberately deviate from purely myopic learning behavior—without explicit awareness of the internal mechanics of its co-learners—by studying a simplified model in which two players repeatedly play a finite matrix game over many rounds. Our analysis highlights when such strategic deviations succeed and when they provably fail, offering concrete guidance for researchers and engineers who wish to deploy robust, ethical, and transparent AI systems that interact, negotiate, or compete safely and profitably with people and other machine learners in our increasingly interconnected society."
Poster,Learning to Stop: Deep Learning for Mean Field Optimal Stopping,https://ICML.cc//virtual/2025/poster/45598,"Lorenzo Magnino, Yuchen Zhu, Mathieu Lauriere","Optimal stopping is a fundamental problem in optimization with applications in risk management, finance, robotics, and machine learning. We extend the standard framework to a multi-agent setting, named multi-agent optimal stopping (MAOS), where agents cooperate to make optimal stopping decisions in a finite-space, discrete-time environment. Since solving MAOS becomes computationally prohibitive as the number of agents is very large, we study the mean-field optimal stopping (MFOS) problem, obtained as the number of agents tends to infinity. We establish that MFOS provides a good approximation to MAOS and prove a dynamic programming principle (DPP) based on mean-field control theory. We then propose two deep learning approaches: one that learns optimal stopping decisions by simulating full trajectories and another that leverages the DPP to compute the value function and to learn the optimal stopping rule using backward induction. Both methods train neural networks to approximate optimal stopping policies. We demonstrate the effectiveness  and the scalability of our work through numerical experiments on 6 different problems in spatial dimension up to 300. To the best of our knowledge, this is the first work to formalize and computationally solve MFOS in discrete time and finite space, opening new directions for scalable MAOS methods.","Imagine you’re playing a game where you have to decide the perfect moment to stop — whether it's selling a stock, ending a robot’s task, or choosing when to share data. This “when to stop” question is called an optimal stopping problem, and it becomes much more challenging when multiple players (or agents) must make decisions together.Our research addresses this challenge by examining how large groups of decision-makers can collaborate to determine the optimal stopping times. But solving this exactly for many agents quickly becomes too complex. So, we look at what happens when the number of agents becomes very large — and use that insight to simplify the problem.We built two deep learning tools that help these agents learn when to stop by training neural networks. One simulates entire scenarios, and the other breaks the problem down using a mathematical shortcut called dynamic programming.This work opens the door to solving complex, multi-agent problems in fields like finance, robotics, and machine learning — even when the systems are huge and high-dimensional."
Poster,Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL,https://ICML.cc//virtual/2025/poster/44640,"Qin-Wen Luo, Ming-Kun Xie, Ye-Wen Wang, Sheng-Jun Huang","Offline reinforcement learning (RL) aims to learn an effective policy from a static dataset.To alleviate extrapolation errors, existing studies often uniformly regularize the value function or policy updates across all states.However, due to substantial variations in data quality, the fixed regularization strength often leads to a dilemma: Weak regularization strength fails to address extrapolation errors and value overestimation, while strong regularization strength shifts policy learning toward behavior cloning, impeding potential performance enabled by Bellman updates.To address this issue, we propose the selective state-adaptive regularization method for offline RL. Specifically, we introduce state-adaptive regularization coefficients to trust state-level Bellman-driven results, while selectively applying regularization on high-quality actions, aiming to avoid performance degradation caused by tight constraints on low-quality actions.By establishing a connection between the representative value regularization method, CQL, and explicit policy constraint methods, we effectively extend selective state-adaptive regularization to these two mainstream offline RL approaches.Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-the-art approaches in both offline and offline-to-online settings on the D4RL benchmark. The implementation is available at https://github.com/QinwenLuo/SSAR.","AI agents can learn from past experience, but when trained only on existing data—a setting known as offline reinforcement learning—they often make mistakes due to overly optimistic decisions. Previous methods attempted to prevent such errors by enforcing strong constraints to make the agent behave closely to the data. However, applying the same level of constraint everywhere either hinders learning or exposes the agent to unnecessary risks.Our research introduces an adaptive strategy that adjusts the strength of constraints based on how much the agent's behavior deviates from high-quality data. This allows the agent to benefit more from its own learning process. At the same time, we apply constraints only to a carefully selected subset of reliable data, helping the agent take full advantage of the reinforcement learning paradigm.The proposed method can integrate seamlessly with widely used training techniques and enable a smooth transition from offline learning to online interaction."
Poster,Learning Utilities from Demonstrations in Markov Decision Processes,https://ICML.cc//virtual/2025/poster/46034,"Filippo Lazzati, Alberto Maria Metelli","Although it is well-known that humans commonly engage in *risk-sensitive* behaviors in the presence of stochasticity, most Inverse Reinforcement Learning (IRL) models assume a *risk-neutral* agent. As such, beyond $(i)$ introducing model misspecification, $(ii)$ they do not permit direct inference of the risk attitude of the observed agent, which can be useful in many applications. In this paper, we propose a novel model of behavior to cope with these issues. By allowing for risk sensitivity, our model alleviates $(i)$, and by explicitly representing risk attitudes through (learnable) *utility* functions, it solves $(ii)$. Then, we characterize the partial identifiability of an agent’s utility under the new model and note that demonstrations from multiple environments mitigate the problem. We devise two provably-efficient algorithms for learning utilities in a finite-data regime, and we conclude with some proof-of-concept experiments to validate *both* our model and our algorithms.","How can we effectively learn an agent’s risk attitude from demonstrations of their behavior? We address this question by considering a key aspect often overlooked by existing methods: human behavior depends not just on the current state, but on the full history of past experiences.We introduce a new behavioral model that explicitly captures an agent’s risk attitude while allowing decisions to depend on the entire past. We analyze the theoretical properties of this model and develop principled algorithms to infer risk preferences from observed behavior. Our experiments demonstrate that our model outperforms existing approaches.This work lays the foundation for more realistic representations of human behavior in sequential decision-making—models that reflect the complex ways in which past experiences shape present choices."
