type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Mitigating Local Cohesion and Global Sparseness in Graph Contrastive Learning with Fuzzy Boundaries,https://ICML.cc//virtual/2025/poster/46629,"Yuena Lin, Haichun Cai, Jun-Yi Hang, Haobo Wang, Zhen Yang, Gengyu Lyu","Graph contrastive learning (GCL) aims at narrowing positives while dispersing negatives, often causing a minority of samples with great similarities to gather as a small group. It results in two latent shortcomings in GCL: 1) **local cohesion** that a class cluster contains numerous independent small groups, and 2) **global sparseness** that these small groups (or isolated samples) dispersedly distribute among all clusters. These shortcomings make the learned distribution *only focus on local similarities among partial samples, which hinders the ability to capture the ideal global structural properties among real clusters, especially high intra-cluster compactness and inter-cluster separateness*. Considering this, we design a novel fuzzy boundary by extending the original cluster boundary with fuzzy set theory, which involves fuzzy boundary construction and fuzzy boundary contraction to address these shortcomings. The fuzzy boundary construction dilates the original boundaries to bridge the local groups, and the fuzzy boundary contraction forces the dispersed samples or groups within the fuzzy boundary to gather tightly, jointly mitigating local cohesion and global sparseness while forming the ideal global structural distribution. Extensive experiments demonstrate that a graph auto-encoder with the fuzzy boundary significantly outperforms current state-of-the-art GCL models in both downstream tasks and quantitative analysis.","Many AI models learn by looking at how things are connected — like who is friends with whom, or which products are often bought together. One popular method, called graph contrastive learning, helps the model learn by pulling similar samples closer and pushing different ones apart.However, we noticed a problem: when samples are very similar, the model tends to group them into small, tight clusters that end up scattered and far from others. This makes it hard for the model to understand the full picture of how different groups are organized.To fix this, we introduced a new technique that gently expands the boundaries around each group. This allows nearby small clusters to be included together. Then, we bring these items closer within the group, so each group is clearer and more complete. This simple idea helps the model organize information better and leads to improved results in tasks like classification. Our method makes AI models more reliable and better at learning from graph-based data."
Poster,Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance,https://ICML.cc//virtual/2025/poster/43644,"Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu","The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.","Imagine asking an AI to describe a photo—and it confidently tells you there’s a dog on the beach… except there’s no dog at all. These hallucinations, where AI models describe objects that don’t actually exist in the image, are surprisingly common in today’s powerful vision-language models (LVLMs). We introduce MARINE, a lightweight and practical fix to this problem. Instead of relying on expensive retraining or proprietary commercial tools, MARINE takes a smarter approach: it uses open-source vision models to help LVLMs “see” what’s really in the image before responding. Think of it as giving the AI a grounded reality check. MARINE is plug-and-play—it works out of the box with many existing models, requires no retraining, and significantly reduces hallucinations without sacrificing the quality or detail of the answers."
Poster,MITIGATING OVER-EXPLORATION IN LATENT SPACE OPTIMIZATION USING LES,https://ICML.cc//virtual/2025/poster/46670,"Omer Ronen, Ahmed Imtiaz Humayun, Richard Baraniuk, Randall Balestriero, Bin Yu","We develop Latent Exploration Score (LES) to mitigate over-exploration in Latent Space Optimization (LSO), a popular method for solving black-box discrete optimization problems. LSO utilizes continuous optimization within the latent space of a Variational Autoencoder (VAE) and is known to be susceptible to over-exploration, which manifests in unrealistic solutions that reduce its practicality. LES leverages the trained decoder’s approximation of the data distribution, and can be employed with any VAE decoder–including pretrained ones–without additional training, architectural changes or access to the training data. Our evaluation across five LSO benchmark tasks and twenty-two VAE models demonstrates that LES always enhances the quality of the solutions while maintaining high objective values, leading to improvements over existing solutions in most cases. We believe that new avenues to LSO will be opened by LES’ ability to identify out of distribution areas, differentiability, and computational tractability.","The goal of this work is to improve machine learning methods for drug discovery. A widely used strategy for generating new molecules involves performing Bayesian Optimization in the continuous latent space of a Variational Autoencoder (VAE). However, this approach often suffers from a well-known failure mode: it tends to over-explore the latent space, resulting in unrealistic or invalid molecules. To address this issue, we introduce a novel constraint called the Latent Exploration Score (LES), which encourages the optimization process to remain close to the training data—i.e., known, valid molecules. Through extensive experiments, we demonstrate that incorporating LES leads to higher-quality molecular candidates and more efficient optimization."
Poster,Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification,https://ICML.cc//virtual/2025/poster/45487,"Langzhang Liang, Fanchen Bu, Zixing Song, Zenglin Xu, Shirui Pan, Kijung Shin","The message-passing paradigm of Graph Neural Networks often struggles with exchanging information across distant nodes typically due to structural bottlenecks in certain graph regions, a limitation known as over-squashing. To reduce such bottlenecks, graph rewiring, which modifies graph topology, has been widely used. However, existing graph rewiring techniques often overlook the need to preserve critical properties of the original graph, e.g., spectral properties. Moreover, many approaches rely on increasing edge count to improve connectivity, which introduces significant computational overhead and exacerbates the risk of over-smoothing. In this paper, we propose a novel graph-rewiring method that leverages spectral graph sparsification for mitigating over-squashing. Specifically, our method generates graphs with enhanced connectivity while maintaining sparsity and largely preserving the original graph spectrum, effectively balancing structural bottleneck reduction and graph property preservation. Experimental results validate the effectiveness of our approach, demonstrating its superiority over strong baseline methods in classification accuracy and retention of the Laplacian spectrum.","Graph Neural Networks, a type of machine learning from network-like data (e.g., social networks or molecular structures), often struggle when information needs to cross large distances within the network. This ""over-squashing"" effect means important messages get diluted or lost, much like a whisper in a crowded room, hindering the GNN's ability to learn effectively. We developed a new technique that reorganizes the network’s connections to combat this. Our approach carefully enhances these long-range communication channels. Crucially, it does this while meticulously preserving the graph's essential inherent structure—its unique ""spectral signature""—and ensuring the network remains ""sparse"" (not overly dense with connections), which keeps computations fast.Our research helps these GNNs learn more effectively from complex, interconnected data. By improving information flow without sacrificing the graph’s core properties or its computational efficiency, our method leads to significantly more accurate predictions in tasks like classifying different items within the network."
Poster,Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn,https://ICML.cc//virtual/2025/poster/45929,"Hongyao Tang, Johan Obando-Ceron, Pablo Samuel Castro, Aaron Courville, Glen Berseth","Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability induced by the data in each training batch. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.","Natural intelligent creatures have the ability to learn within their lifetimes, like human beings are able to keep accepting new information and learning new tasks every day. However, this kind of learnability or plasticity is non-trivial for artificial intelligence (AI). Most existing AI achievements are built for doing specific tasks to a near-human or super-human level. The continual learning ability of AI agents is still an open challenge.In this paper, we study the plasticity issue of AI methods for learning a temporal sequence of tasks. We present that one cause of the plasticity issue is the inner unregularized generalization or interference behaviors in AI models. Based on formal analysis and empirical investigation, we propose a regularization method called C-CHAIN to suppress the inner behaviors, which is demonstrated to successfully mitigate the plasticity issue and improve the learnability of AI models in continual learning scenarios.Our work will help to better understand the learning behaviors and issues of AI models. Our method and the idea behind it can be an easy-to-implement choice for continual learning problems. Our findings also indicate the distinction between natural intelligence and AI, from which more inspiration could be drawn to realize AI models of a higher level of intelligence."
Poster,MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges,https://ICML.cc//virtual/2025/poster/46699,"Shixi Qin, Zhiyong Yang, Shilong Bao, Shi Wang, Qianqian Xu, Qingming Huang","This paper focuses on implanting multiple heterogeneous backdoor triggers in bridge-based diffusion models designed for complex and arbitrary input distributions. Existing backdoor formulations mainly address single-attack scenarios and are limited to Gaussian noise input models. To fill this gap, we propose MixBridge, a novel diffusion Schrödinger bridge (DSB) framework to cater to arbitrary input distributions (taking I2I tasks as special cases). Beyond this trait, we demonstrate that backdoor triggers can be injected into MixBridge by directly training with poisoned image pairs. This eliminates the need for the cumbersome modifications to stochastic differential equations required in previous studies, providing a flexible tool to study backdoor behavior for bridge models. However, a key question arises: can a single DSB model train multiple backdoor triggers?  Unfortunately, our theory shows that when attempting this, the model ends up following the geometric mean of benign and backdoored distributions, leading to performance conflict across backdoor tasks. To overcome this, we propose a Divide-and-Merge strategy to mix different bridges, where models are independently pre-trained for each specific objective (Divide) and then integrated into a unified model (Merge). In addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the stealthiness of MixBridge. Empirical studies across diverse generation tasks speak to the efficacy of MixBridge. The code is available at: https://github.com/qsx830/MixBridge.","We show how image-editing AI can be secretly hijacked to produce harmful content only when a hidden “trigger” is present, while behaving normally otherwise. Current attacks focus on models that generate images from random noise, but many real-world tools take existing images as input (e.g., photo upscalers or inpainting). In this work, we introduce MixBridge, a new framework that lets attackers embed multiple distinct triggers in these “image-to-image” systems. Instead of rewriting complex equations for each trigger, MixBridge simply trains on pairs of marked (poisoned) images and their intended malicious outputs. However, trying to teach one model all triggers at once leads to conflicts: it ends up blending benign and malicious behaviors and cannot satisfy either well. To solve this, we first train separate “expert” models for each task (clean editing or one of several backdoors), then merge them using a lightweight router that learns which expert to use based on subtle patterns in the input. A small “weight reallocation” penalty encourages the router to spread responsibility across experts, making backdoors harder to detect. Experiments show MixBridge can yield high-quality normal edits and, when the trigger is added, consistently produce different malicious outputs. We release our code so others can study and defend against these vulnerabilities."
Poster,Mixed-curvature decision trees and random forests,https://ICML.cc//virtual/2025/poster/43603,"Philippe Chlenski, Quentin Chu, Raiyan Khan, Kaizhu Du, Antonio Moretti, Itsik Pe&#x27;er","Decision trees (DTs) and their random forest (RF) extensions are workhorses of classification and regression in Euclidean spaces. However, algorithms for learning in non-Euclidean spaces are still limited. We extend DT and RF algorithms to product manifolds: Cartesian products of several hyperbolic, hyperspherical, or Euclidean components. Such manifolds handle heterogeneous curvature while still factorizing neatly into simpler components, making them compelling embedding spaces for complex datasets. Our novel angular reformulation respects manifold geometry while preserving the algorithmic properties that make decision trees effective. In the special cases of single-component manifolds, our method simplifies to its Euclidean or hyperbolic counterparts, or introduces hyperspherical DT algorithms, depending on the curvature. In benchmarks on a diverse suite of 57 classification, regression, and link prediction tasks, our product RFs ranked first on 29 tasks and came in the top 2 for 41. This highlights the value of product RFs as straightforward yet powerful new tools for data analysis in product manifolds. Code for our method is available at https://github.com/pchlenski/manify.","We are interested in how decision trees partition the input space, with an eye towards adapting decision trees and random forests to more complicated non-Euclidean geometries. We claim that decision trees and random forests are successful for Euclidean inputs because they have 5 desirable properties:1. **Continuity:** Their leaves are single regions of space, never islands;2. **(Geodesic) convexity:** For each leaf, the shortest path between any two points in that leaf stays completely inside that leaf;3. **Equidistance:** Splits fall exactly between the two nearest points to either side;4. **Efficiency:** We consider a manageable number of split candidates ($\mathcal{O}(nd)$ to be exact); and5. **Speed:** We can evaluate each split in constant time.We show a method to extend decision trees while preserving these 5 properties to mixed-curvature product manifolds: non-Euclidean spaces built up from simple components like spheres, hyperboloids, and even Euclidean subspaces. We show how many kinds of problems can be viewed as classification or regression on product manifolds, and then use this to benchmark our method on 57 different problems and find that it performs better than other methods on a majority of the benchmarks."
Poster,MixMin: Finding Data Mixtures via Convex Minimization,https://ICML.cc//virtual/2025/poster/43605,"Anvith Thudi, Evianne Rovers, Yangjun Ruan, Tristan Thrush, Chris Maddison","Modern machine learning pipelines are increasingly combining and mixing data from diverse and disparate sources, e.g., pre-training large language models. Yet, finding the optimal data mixture is a challenging and open problem. We formalize this data mixing problem as a bi-level objective: the best mixture is the one that would lead to the best model for a downstream objective. Unfortunately, this objective is generally intractable. In this paper, we make the observation that the bi-level data mixing objective becomes convex as our model class becomes larger. We develop and study a gradient-based approach for optimizing this convex objective, which we call MixMin, and test it on language modeling and chemistry tasks. MixMin was the only method that uniformly improved the data mixture in all our experiments. With MixMin, we improved the data mixture using less than 0.2% additional compute for a pythia-$410M$ model trained on $8.2B$ tokens, resulting between 1-5% relative improvement to negative log likelihood on PIQA, ARC Easy, SciQ, and OpenWebMath. Crucially, we found that MixMin mixtures for smaller models improved training of larger models, suggesting that MixMin mixtures may be scale-invariant. When mixing bioassay data to train an XGBoost model, we saw improvements to average precision scores of $0.03-0.15$.","Performant machine learning requires having a relevant dataset for the task you want to learn. When given many sources of data, the problem of knowing how to make a good dataset from these sources poses a typically hard optimization problem. In this paper we showed this optimization can be simplified if we first train a (cheap) model on each of our sources of data. With this we provided a method to make better datasets, leading to improvements on language modeling and chemistry tasks. Our work paves a way for finding useful datasets for typically data-scarce tasks."
Poster,Mixture of Experts Made Intrinsically Interpretable,https://ICML.cc//virtual/2025/poster/46377,"Xingyi Yang, Constantin Venhoff, Ashkan Khakzar, Christian Schroeder de Witt, Puneet Dokania, Adel Bibi, Phil Torr","Neurons in large language models often exhibit \emph{polysemanticity}, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present \textbf{MoE-X}, a mixture-of-experts (MoE) language model designed to be \emph{intrinsically} interpretable. Our approach is motivated by the observation that, in language models, wider networks with sparse activations are more likely to capture interpretable factors. however, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting  the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.","## AI That ""Thinks"" Clearer: New Model Aims to Make Language AI Easier to UnderstandImagine trying to follow a conversation where each word has a dozen different meanings, all jumbled together. That's a bit like how current AI language models, the brains behind chatbots and translation tools, can sometimes work internally. Their ""neurons"" – the tiny processing units in these AI brains – often handle multiple unrelated concepts at once. This makes it incredibly difficult for us to understand exactly how they arrive at an answer, a problem scientists call ""polysemanticity.""This research paper introduces a new AI language model called **MoE-X** that's designed to be understandable from the get-go. Instead of trying to decode a jumbled mess after the fact, MoE-X is built to think in a more organized way.The key idea is inspired by how a very wide, but sparsely used, network might be easier to interpret – think of a massive library where only a few specific books are pulled out for any given topic. However, building such enormous networks directly is too computationally expensive.MoE-X cleverly sidesteps this by using a ""mixture-of-experts"" (MoE) approach. Imagine a team of specialist consultants. When a question comes in, the system smartly routes it to only the most relevant one or two experts, rather than every expert trying to chime in on every topic. This keeps things focused.MoE-X takes this a step further:- It's structured so that this ""team of experts"" system is equivalent to a very large, but sparsely used, internal network, making it efficient to train.- It encourages each ""expert"" to activate only for very specific features or concepts.- The system that decides which ""expert"" sees which piece of information is redesigned to pick experts that are very selective about what they respond to.The result? MoE-X performs as well as other popular language models (even outperforming the well-known GPT-2 in some measures) on tasks like understanding chess moves and general language. Crucially, it's significantly easier to look inside MoE-X and see which ""expert"" is handling which concept, making its decision-making process much clearer than traditional models, and even better than other methods specifically designed to improve this kind of transparency.In essence, MoE-X is a step towards AI that not only provides answers but can also show its working in a more straightforward and understandable way."
Poster,Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning,https://ICML.cc//virtual/2025/poster/46558,"Ryotaro Kawata, Kohsei Matsutani, Yuri Kinoshita, Naoki Nishikawa, Taiji Suzuki","Mixture of Experts (MoE), an ensemble of specialized models equipped with a router that dynamically distributes each input to appropriate experts, has achieved successful results in the field of machine learning. However, theoretical understanding of this architecture is falling behind due to its inherent complexity. In this paper, we theoretically study the sample and runtime complexity of MoE following the stochastic gradient descent when learning a regression task with an underlying cluster structure of single index models. On the one hand, we show that a vanilla neural network fails in detecting such a latent organization as it can only process the problem as a whole. This is intrinsically related to the concept of *information exponent* which is low for each cluster, but increases when we consider the entire task. On the other hand, with a MoE, we show that it succeeds in dividing the problem into easier subproblems by leveraging the ability of each expert to weakly recover the simpler function corresponding to an individual cluster. To the best of our knowledge, this work is among the first to explore the benefits of the MoE framework by examining its SGD dynamics in the context of nonlinear regression.","Mixture of Experts (MoE) models dynamically route inputs to specialized sub-models called experts, enabling remarkable efficiency in large-scale learning settings such as large language models (LLMs). Yet, the theoretical understanding of MoE remains limited. This paper investigates how MoE models, when trained via stochastic gradient descent (SGD), can provably detect and learn hidden cluster structures in nonlinear regression tasks. We demonstrate that a standard neural network cannot separate these clusters, as it treats the problem as a whole. In contrast, MoE models, under appropriate conditions, leverage the specialization of experts to divide the task into simpler subproblems, enabling more efficient learning. To the best of our knowledge, this is among the first works to analyze the dynamics of MoE under SGD in nonlinear regression."
