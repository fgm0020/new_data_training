type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Calibrated Language Models and How to Find Them with Label Smoothing,https://ICML.cc//virtual/2025/poster/43814,"Jerry Huang, Peng Lu, QIUHAO Zeng","Recent advances in natural language processing (NLP) have opened up greater opportunities to enable fine-tuned large language models (LLMs) to behave as more powerful interactive agents through improved instruction-following ability. However, understanding how this impacts confidence calibration for reliable model output has not been researched in full. In this work, we examine various open-sourced LLMs, identifying significant calibration degradation after instruction tuning in each. Seeking a practical solution, we look towards label smoothing, which has been shown as an effective method to regularize for overconfident predictions but has yet to be widely adopted in the supervised fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing is sufficient to maintain calibration throughout the SFT process. However, settings remain where the effectiveness of smoothing is severely diminished, in particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to stem from the ability to become over-confident, which has a direct relationship with the hidden size and vocabulary size, and justify this theoretically and experimentally. Finally, we address an outstanding issue regarding the memory footprint of the cross-entropy loss computation in the label smoothed loss setting, designing a customized kernel to dramatically reduce memory consumption without sacrificing speed or performance in comparison to existing solutions for non-smoothed losses.","LLMs, despite their abilities in various real-world language tasks, have significant underlying issues where they are highly overconfident in their response, which can lead to problematic behaviour. In this work, we research why this occurs, examining various open LLMs from which we identify a recurring pattern of confidence calibration issues. We then examine label smoothing, a widely used technique, and observe that it can significantly reduce overconfidence while retaining accuracy. In particular, we observe its benefits when instruction-tuning LLMs. Thus we provide greater insight into this behaviour and when it can be useful. In addition, we address an outstanding issue regarding the memory footprint of the cross-entropy loss computation in the label smoothed loss setting, designing a customized kernel to dramatically reduce memory consumption without sacrificing speed or performance in comparison to existing solutions for non-smoothed losses."
Poster,Calibrated Physics-Informed Uncertainty Quantification,https://ICML.cc//virtual/2025/poster/44881,"Vignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Daniel Giles, Matt Kusner, Stanislas Pamela, Marc Deisenroth","Simulating complex physical systems is crucial for understanding and predicting phenomena across diverse fields, such as fluid dynamics and heat transfer, as well as plasma physics and structural mechanics. Traditional approaches rely on solving partial differential equations (PDEs) using numerical methods, which are computationally expensive and often prohibitively slow for real-time applications or large-scale simulations. Neural PDEs have emerged as efficient alternatives to these costly numerical solvers, offering significant computational speed-ups. However, their lack of robust uncertainty quantification (UQ) limits deployment in critical applications. We introduce a model-agnostic, physics-informed conformal prediction (CP) framework that provides guaranteed uncertainty estimates without requiring labelled data. By utilising a physics-based approach, we can quantify and calibrate the model's inconsistencies with the physics rather than the uncertainty arising from the data. Our approach utilises convolutional layers as finite-difference stencils and leverages physics residual errors as nonconformity scores, enabling data-free UQ with marginal and joint coverage guarantees across prediction domains for a range of complex PDEs. We further validate the efficacy of our method on neural PDE models for plasma modelling and shot design in fusion reactors.","This paper addresses a critical problem in using AI for scientific simulations: while neural networks can predict physical systems like weather or plasma behaviour 1000x faster than traditional methods, we don't know when to trust their predictions. We developed CP-PRE (Conformal Prediction with Physics Residual Error), a method that adds reliable ""confidence scores"" to AI predictions by checking how well they obey fundamental physics laws (like conservation of energy) rather than requiring expensive validation data. When tested on applications ranging from fluid dynamics to fusion reactor modelling, the method successfully identifies which predictions are trustworthy with statistical guarantees - essentially adding a physics-based ""confidence meter"" to AI models. This work presents a significant step in safely deploying AI in critical applications like nuclear fusion or aerospace engineering, where you need both speed and reliability, potentially accelerating scientific discovery while maintaining the safety standards these fields require."
Poster,Calibrated Value-Aware Model Learning with Probabilistic Environment Models,https://ICML.cc//virtual/2025/poster/44513,"Claas Voelcker, Anastasiia Pedan, Arash Ahmadian, Romina Abachi, Igor Gilitschenski, Amir-massoud Farahmand","The idea of value-aware model learning, that models should produce accurate value estimates, has gained prominence in model-based reinforcement learning.The MuZero loss, which penalizes a model's value function prediction compared to the ground-truth value function, has been utilized in several prominent empirical works in the literature.However, theoretical investigation into its strengths and weaknesses is limited.In this paper, we analyze the family of value-aware model learning losses, which includes the popular MuZero loss.We show that these losses, as normally used, are uncalibrated surrogate losses, which means that they do not always recover the correct model and value function.Building on this insight, we propose corrections to solve this issue.Furthermore, we investigate the interplay between the loss calibration, latent model architectures, and auxiliary losses that are commonly employed when training MuZero-style agents.We show that while deterministic models can be sufficient to predict accurate values, learning calibrated stochastic models is still advantageous.","This paper analyzes Value-Aware Model Learning (VAML), including the MuZero loss, in model-based reinforcement learning. VAML-based losses train a model to predict accurate estimates of the value of taking an action in each state, instead of training it to predict states themselves accurately. It identifies that current sampled-based VAML losses are ""uncalibrated"" when used with stochastic environment models. This means that they learn solutions that are slightly flawed and produce suboptimal results when used for learning and planning. This error stems from the fact that the model's prediction become too confident (reducing the variance of the prediction) and too smooth (assigning similar values to states that should potentially not have them). The paper proposes ""Corrected VAML"" (CVAML) by adding a variance correction term. The authors formally prove that this enables more accurate value function and model recovery. In addition, they provide empirical evidence which suggests calibrated stochastic models can be advantageous in difficult control tasks over uncalibrated or deterministic models."
Poster,Calibrating Video Watch-time Predictions with Credible Prototype Alignment,https://ICML.cc//virtual/2025/poster/44788,"Chao, Shisong Tang, Fan Li, Jiechao Gao, Hechang Chen","Accurately predicting user watch-time is crucial for enhancing user stickiness and retention in video recommendation systems. Existing watch-time prediction approaches typically involve transformations of watch-time labels for prediction and subsequent reversal, ignoring both the natural distribution properties of label and the \textit{instance representation confusion} that results in inaccurate predictions. In this paper, we propose ProWTP, a two-stage method combining prototype learning and optimal transport for watch-time regression prediction, suitable for any deep recommendation model. Specifically, we observe that the watch-ratio (the ratio of watch-time to video duration) within the same duration bucket exhibits a multimodal distribution. To facilitate incorporation into models, we use a hierarchical vector quantised variational autoencoder (HVQ-VAE) to convert the continuous label distribution into a high-dimensional discrete distribution, serving as credible prototypes for calibrations. Based on this, ProWTP views the alignment between prototypes and instance representations as a Semi-relaxed Unbalanced Optimal Transport (SUOT) problem, where the marginal constraints of prototypes are relaxed. And the corresponding optimization problem is reformulated as a weighted Lasso problem for solution. Moreover, ProWTP introduces the assignment and compactness losses to encourage instances to cluster closely around their respective prototypes, thereby enhancing the prototype-level distinguishability. Finally, we conducted extensive offline experiments on two industrial datasets, demonstrating our consistent superiority in real-world application.","Video platforms like TikTok or YouTube try to predict how long you’ll watch a video, so they can recommend content you’re more likely to enjoy. But this is harder than it seems. People behave differently depending on video length, and existing prediction systems often miss these patterns or become confused when similar behaviors look different in the data.We introduce a new method, called ProWTP, that helps recommendation models better understand and organize viewing behavior. First, we summarize how people watch videos of different lengths into representative behavior patterns — kind of like identifying key “viewing styles.” Then we teach the model to align its internal understanding with these patterns using a technique from mathematics that’s often used to match two sets of things fairly.Our method works with any existing recommendation model and leads to more accurate watch-time predictions. This could help platforms make smarter content suggestions, saving users time and helping creators reach the right audience more effectively."
Poster,CALM: Consensus-Aware Localized Merging for Multi-Task Learning,https://ICML.cc//virtual/2025/poster/45426,"Kunda Yan, Min Zhang, Sen Cui, Qu Zikun, Bo Jiang, Feng Liu, Changshui Zhang","Model merging aims to integrate the strengths of multiple fine-tuned models into a unified model while preserving task-specific capabilities. Existing methods, represented by task arithmetic,  are typically classified into global- and local-aware methods. However, global-aware methods inevitably cause parameter interference, while local-aware methods struggle to maintain the effectiveness of task-specific details in the merged model. To address these limitations, we propose a Consensus Aware Localized Merging (CALM) method which incorporates localized information aligned with global task consensus, ensuring its effectiveness post-merging.  CALM consists of three key components: (1) class-balanced entropy minimizationsampling, providing a more flexible and reliable way to leverage unsupervised data; (2) an efficient-aware framework, selecting a small set of tasks for sequential merging with high scalability; (3) a consensus-aware mask optimization, aligning localized binary masks with global task consensus and merging them conflict-free. Experiments demonstrate the superiority and robustness of our CALM, significantly outperforming existing methods and achieving performance close to traditional MTL.","This work investigates effective methods for merging multiple fine-tuned task-specific models into a unified multi-task model. Existing approaches either optimize merging parameters for individual tasks, inevitably introducing parameter interference, or focus solely on extracting localized parameters for each task, which may lack generalizability across tasks. To address these limitations, we propose a Consensus-Aware Localized Merging (CALM) framework. Our key insight is that localized parameter selection should align with global task-level consensus to ensure compatibility and effectiveness in multi-task scenarios. CALM consists of three key components: (1) class-balanced entropy-minimization sampling, which provides a flexible and reliable mechanism to leverage unlabeled data; (2) an efficiency-aware sequential merging framework that selects a minimal subset of tasks for scalable integration; (3) consensus-aware mask optimization, which aligns localized binary masks with global task consensus and enables conflict-free merging. Experiments demonstrate that CALM significantly outperforms existing model merging methods and achieves performance comparable to traditional multi-task learning, while maintaining superior scalability and robustness."
Poster,Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example,https://ICML.cc//virtual/2025/poster/44078,"Yuhan Helena Liu, Guangyu Robert Yang, Christopher Cueva","Understanding how the brain learns may be informed by studying biologically plausible learning rules. These rules, often approximating gradient descent learning to respect biological constraints such as locality, must meet two critical criteria to be considered an appropriate brain model: (1) good neuroscience task performance and (2) alignment with neural recordings. While extensive research has assessed the first criterion, the second remains underexamined. Employing methods such as Procrustes analysis on well-known neuroscience datasets, this study demonstrates the existence of a biologically plausible learning rule — namely e-prop, which is based on gradient truncation and has demonstrated versatility across a wide range of tasks — that can achieve neural data similarity comparable to Backpropagation Through Time (BPTT) when matched for task accuracy. Our findings also reveal that model architecture and initial conditions can play a more significant role in determining neural similarity than the specific learning rule. Furthermore, we observe that BPTT-trained models and their biologically plausible counterparts exhibit similar dynamical properties at comparable accuracies. These results underscore the substantial progress made in developing biologically plausible learning rules, highlighting their potential to achieve both competitive task performance and neural data similarity.","The brain learns by adjusting the strength of connections between neurons. But how are these countless connection updates coordinated so that learning leads to symphony — where neurons work together to solve a task — rather than cacophony? To investigate this, computational neuroscientists look to the standard training methods in deep learning for inspiration. While powerful, these methods don't align with known biological mechanisms.To bridge this gap, researchers have proposed more biologically plausible learning models — inspired by standard deep learning methods but grounded in known biological processes. But how good are these models? In particular: (1) can they support good task performance, and (2) do they produce brain-like neural activity?While (1) has been studied widely, our study focuses on (2). We show that standard deep learning training can match the brain-like activity of one such biologically plausible learning model — if the architecture and initialization are well chosen. This suggests that brain-like learning in machines may be within reach, with implications for human- or animal-aligned AI and improved understanding of learning in the brain."
Poster,Can Classic GNNs Be Strong Baselines for Graph-level Tasks? Simple Architectures Meet Excellence,https://ICML.cc//virtual/2025/poster/44865,"Yuankai Luo, Lei Shi, Xiao-Ming Wu","Message-passing Graph Neural Networks (GNNs) are often criticized for their limited expressiveness, issues like over-smoothing and over-squashing, and challenges in capturing long-range dependencies. Conversely, Graph Transformers (GTs) are regarded as superior due to their employment of global attention mechanisms, which potentially mitigate these challenges. Literature frequently suggests that GTs outperform GNNs in graph-level tasks, especially for graph classification and regression on small molecular graphs. In this study, we explore the untapped potential of GNNs through an enhanced framework, GNN+, which integrates six widely used techniques: edge feature integration, normalization, dropout, residual connections, feed-forward networks, and positional encoding, to effectively tackle graph-level tasks. We conduct a systematic re-evaluation of three classic GNNs—GCN, GIN, and GatedGCN—enhanced by the GNN+ framework across 14 well-known graph-level datasets. Our results reveal that, contrary to prevailing beliefs, these classic GNNs consistently match or surpass the performance of GTs, securing top-three rankings across all datasets and achieving first place in eight. Furthermore, they demonstrate greater efficiency, running several times faster than GTs on many datasets. This highlights the potential of simple GNN architectures, challenging the notion that complex mechanisms in GTs are essential for superior graph-level performance. Our source code is available at https://github.com/LUOyk1999/GNNPlus.","Graph-level tasks — such as predicting whether a molecule is toxic, or determining the function of a protein — are essential in fields like drug discovery and chemistry. While Graph Neural Networks (GNNs) are a popular tool for these tasks, many researchers now favor more complex models like Graph Transformers (GTs), which can capture long-range relationships using attention mechanisms.In this study, we revisit the potential of classic GNNs for graph-level tasks. We introduce GNN+, a framework that enhances classic GNNs by incorporating six widely used techniques, including edge feature integration, normalization, dropout, residual connections, feed-forward networks, and positional encoding. We then re-evaluate 3 classic GNNs (GCN, GIN, and GatedGCN) on 14 well-known graph-level datasets across both classification and regression tasks.Contrary to common belief, we find that these classic GNNs often match or even outperform GTs, and do so with significantly higher efficiency. These findings show that simple, well-tuned GNNs remain powerful tools for graph-level learning, challenging the assumption that complex architectures are always better.The code is open-source at: https://github.com/LUOyk1999/GNNPlus."
Poster,Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression,https://ICML.cc//virtual/2025/poster/43871,"Peijie Dong, Zhenheng Tang, Xiang Liu, Lujun Li, Xiaowen Chu, Bo Li","Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks focus narrowly on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities—workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) 4-bit quantization (GPTQ, AWQ) and 50% pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5-7B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%--3% drop) but degrades real-world application accuracy by 10%--15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios, bridging the gap between algorithmic efficiency and real-world applicability.","Large language models (LLMs) like ChatGPT and Gemini are incredibly powerful but require massive computing resources, making them expensive and slow to run. To make them more efficient, researchers use **compression techniques**—methods that shrink these models while trying to preserve their intelligence. However, most existing benchmarks only test whether compressed models can still answer questions or write text well. But what if we want these models to **act as autonomous agents**—planning tasks, using tools, understanding long conversations, or solving real-world problems?  This paper introduces **ACBench**, the first benchmark designed to measure how well compressed LLMs perform in **agent-like scenarios**. The study evaluates:  - **Workflow planning** (breaking complex tasks into steps),  - **Tool use** (calling APIs or external software),  - **Long-context understanding** (remembering and retrieving information from long documents), and  - **Real-world applications** (handling tasks in robotics, gaming, or finance).  The results show that **4-bit quantization** (a compression method) works well for planning and tool use, with only a small performance drop (1-3%). However, it struggles more in real-world tasks, where accuracy can drop by **10-15%**. The paper also introduces new ways to analyze compression effects, such as:  - **ERank** (measuring how much the model’s internal structure changes),  - **Top-k ranking correlation** (checking if compressed models make similar predictions to original ones), and  - **Energy-based analysis** (evaluating confidence levels in responses).  Key findings:  ✅ **Quantization (e.g., GPTQ, AWQ) works better than pruning** for maintaining agent-like abilities.  ❌ **Distilled models** (from DeepSeek R1 series) often perform worse in agent tasks, despite being good at reasoning.  📉 **Long-context understanding degrades** when models are heavily compressed, especially beyond 32K tokens.  This research helps developers choose the best compression methods for AI agents, balancing efficiency with performance. It also highlights that **not all compression techniques are equal**—some preserve reasoning and planning abilities better than others.  For more details, check out the full paper and code at: [https://github.com/pprp/ACBench](https://github.com/pprp/ACBench)."
Poster,Can DBNNs Robust to Environmental Noise for Resource-constrained Scenarios?,https://ICML.cc//virtual/2025/poster/45126,"Wendong Zheng, Junyang Chen, Husheng Guo, Wenjian Wang","Recently, the potential of lightweight models for resource-constrained scenarios has garnered significant attention, particularly in safety-critical tasks such as bio-electrical signal classification and B-ultrasound-assisted diagnostic. These tasks are frequently affected by environmental noise due to patient movement artifacts and inherent device noise, which pose significant challenges for lightweight models (e.g., deep binary neural networks (DBNNs)) to perform robust inference. A pertinent question arises: can a well-trained DBNN effectively resist environmental noise during inference? In this study, we find that the DBNN's robustness vulnerability comes from the binary weights and scaling factors. Drawing upon theoretical insights, we propose L1-infinite norm constraints for binary weights and scaling factors, which yield a tighter upper bound compared to existing state-of-the-art (SOTA) methods. Finally, visualization studies show that our approach introduces minimal noise perturbations at the periphery of the feature maps. Our approach outperforms the SOTA method, as validated by several experiments conducted on the bio-electrical and image classification datasets. We hope our findings can raise awareness among researchers about the environmental noise robustness of DBNNs.","Recent advancements in lightweight AI models have sparked interest in their use for critical healthcare tasks—like analyzing bio-electrical signals or ultrasound images—especially in settings with limited computational resources. However, these models, such as deep binary neural networks (DBNNs), often struggle to perform reliably in real-world scenarios due to environmental noise caused by patient movements or device limitations. A key question emerges: Can these models maintain accuracy under such noisy conditions? Our study reveals that the vulnerability of DBNNs to noise stems from their simplified ""binary"" parameters and scaling factors. To address this, we propose a novel method that applies mathematical constraints (L1-infinity norms) to these components, significantly improving their noise resistance compared to existing techniques. Visual experiments confirm that our approach minimizes distortions at the edges of feature maps, a common source of errors. Tests on medical and image datasets demonstrate superior performance over current state-of-the-art methods."
Poster,Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?,https://ICML.cc//virtual/2025/poster/45949,"Yujin Han, Andi Han, Wei Huang, Chaochao Lu, Difan Zou","Despite the remarkable success of diffusion models (DMs) in data generation, they exhibit specific failure cases with unsatisfactory outputs. We focus on one such limitation: the ability of DMs to learn hidden rules between image features. Specifically, for image data with dependent features ($\mathbf{x}$) and ($\mathbf{y}$) (e.g., the height of the sun ($\mathbf{x}$) and the length of the shadow ($\mathbf{y}$)), we investigate whether DMs can accurately capture the inter-feature rule ($p(\mathbf{y}|\mathbf{x})$). Empirical evaluations on mainstream DMs (e.g., Stable Diffusion 3.5) reveal consistent failures, such as inconsistent lighting-shadow relationships and mismatched object-mirror reflections. Inspired by these findings, we design four synthetic tasks with strongly correlated features to assess DMs' rule-learning abilities. Extensive experiments show that while DMs can identify coarse-grained rules, they struggle with fine-grained ones. Our theoretical analysis demonstrates that DMs trained via denoising score matching (DSM) exhibit constant errors in learning hidden rules, as the DSM objective is not compatible with rule conformity. To mitigate this, we introduce a common technique - incorporating additional classifier guidance during sampling, which achieves (limited) improvements. Our analysis reveals that the subtle signals of fine-grained rules are challenging for the classifier to capture, providing insights for future exploration.","Our study reveals a new failure of diffusion models (DMs)—a previously underexplored and unsolved challenge!In our work, we ask ***Can image generative models like diffusion models truly capture the underlying rules embedded in image data?***  For instance, can DMs accurately understand fine-grained rules in the image, such as how the height of the sun influences the length of a shadow? This question is crucial for using DMs to faithfully reconstruct the physical world.By extensive experiments on both synthetic tasks and real-world datasets, our findings provide a clear answer: DMs can learn coarse rules (e.g., the sun and shadow should appear on opposite sides of an object), but they struggle to capture fine-grained rules (e.g., the precise geometric constrains between the sun’s height and the shadow’s length). And our theoretical analysis suggests that the root cause lies in a mismatch between the optimization objective of DMs and the underlying rules embedded in the data, which leads to persistent constant errors in rule learning. What’s worse, addressing this issue using conventional techniques—such as introducing guidance during sampling—has shown limited improvement. One of the key bottlenecks is that fine-grained rules typically manifest as weak signals within the data, making them difficult to capture and leverage for effective guidance."
