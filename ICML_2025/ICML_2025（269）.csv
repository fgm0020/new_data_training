type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation,https://ICML.cc//virtual/2025/poster/46649,"CHUANQI CHENG, Jian Guan, Wei Wu, Rui Yan","Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLAMP, a hierarchical video-language model that processes hour-long videos at ""mixed precision"" through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLAMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLAMP's superior performance across five video understanding benchmarks, particularly on long-form content. Notably, ViLAMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance. Code and model are available at https://github.com/steven-ccq/ViLAMP.","We developed ViLaMP, an AI model that can understand videos up to three hours long—all using just a single standard graphics card (A100 GPU). The idea is inspired by how humans watch videos: we pay close attention to important scenes and quickly skim through the rest.ViLaMP does something similar by using two smart techniques: (1) It identifies the most important moments in a video based on the task at hand and (2) It summarizes less important parts without losing their meaning. With these techniques, ViLaMP not only reduces computing costs but also beats other models on five major video understanding benchmarks. This makes it a practical and accurate tool for analyzing long videos, striking the right balance between detail and efficiency."
Poster,SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval,https://ICML.cc//virtual/2025/poster/43841,"Nikolaos Chaidos, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Stamou","Despite the dominance of convolutional and transformer-based architectures in image-to-image retrieval, these models are prone to biases arising from low-level visual features, such as color. Recognizing the lack of semantic understanding as a key limitation, we propose a novel scene graph-based retrieval framework that emphasizes semantic content over superficial image characteristics. Prior approaches to scene graph retrieval predominantly rely on supervised Graph Neural Networks (GNNs), which require ground truth graph pairs driven from image captions. However, the inconsistency of caption-based supervision stemming from variable text encodings undermine retrieval reliability.To address these, we present *SCENIR*, a Graph Autoencoder-based unsupervised retrieval framework, which eliminates the dependence on labeled training data. Our model demonstrates superior performance across metrics and runtime efficiency, outperforming existing vision-based, multimodal, and supervised GNN approaches.We further advocate for *Graph Edit Distance* (GED) as a deterministic and robust ground truth measure for scene graph similarity, replacing the inconsistent caption-based alternatives for the first time in image-to-image retrieval evaluation. Finally, we validate the generalizability of our method by applying it to unannotated datasets via automated scene graph generation, while substantially contributing in advancing state-of-the-art in counterfactual image retrieval.The source code is available at https://github.com/nickhaidos/scenir-icml2025.","State-of-the-Art AI models often fail to grasp the true meaning of images, focusing on surface-level details like color, which leads to biased and inaccurate image search results. Many systems also need extensive, manually labeled data to learn, a slow and costly process.  Our proposed system, SCENIR, teaches computers to ""see"" more deeply by utilizing ""scene graphs"" – structured maps of objects and their relationships within an image. Scene graphs enable SCENIR to focus on the key visual information within an image – the content that is most relevant and aligned with human understanding. SCENIR uniquely learns without needing pre-labeled examples (unsupervised learning), making it more efficient. We also propose a more reliable evaluation method (Graph Edit Distance) for this specific task.SCENIR delivers more accurate and less biased image search by understanding semantic content. It is faster than previous methods even on unannotated in-the-wild images."
Poster,SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields,https://ICML.cc//virtual/2025/poster/43656,"David K Park, Xihaier Luo, Guang Zhao, Seungjun Lee, Miruna Oprescu, Shinjae Yoo","Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints. These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties. In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning. SCENT unifies interpolation, reconstruction, and forecasting within a single architecture. Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions. We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability.","Scientific data, like information from sensor networks or traffic patterns, is often complex and messy. It can have missing values due to sensor failures, be spread out irregularly, and come in huge volumes, making it difficult to analyze and predict future trends. To address this, we developed a new AI framework called SCENT.  SCENT is designed to learn from this challenging spatiotemporal data (data that changes across space and time). It cleverly fills in missing information, reconstructs incomplete data, and forecasts future events, all within a single, unified system.  We built SCENT to be scalable, meaning it can efficiently handle large amounts of data and complex analyses. SCENT can help scientists and engineers gain clearer insights and make more accurate predictions from their complex data. This has the potential to improve our understanding in important areas like traffic forecasting or analyzing data from moving sensors, ultimately aiding in scientific discovery and addressing real-world problems."
Poster,Schwarz–Schur Involution: Lightspeed Differentiable Sparse Linear Solvers,https://ICML.cc//virtual/2025/poster/45286,"Yu Wang, Mazdak Abulnaga, Yaël Balbastre, Bruce Fischl","Sparse linear solvers are fundamental to science and engineering, applied in partial differential equations (PDEs), scientific computing, computer vision, and beyond. Indirect solvers possess characteristics that make them undesirable as stable differentiable modules; existing direct solvers, though reliable, are too expensive to be adopted in neural architectures. We substantially accelerate direct sparse solvers or generalized deconvolution by up to 3 orders-of-magnitude faster, violating common assumptions that direct solvers are too slow. We ``condense'' a sparse Laplacian matrix into a dense tensor, a compact data structure that batch-wise stores the Dirichlet-to-Neumann matrices, reducing the sparse solving to recursively merging pairs of dense matrices that are much smaller. The batched small dense systems are sliced and inverted in parallel to take advantage of dense GPU BLAS kernels, highly optimized in the era of deep learning. Our method is efficient, qualified as a strong zero-shot baseline for AI-based PDE solving and a reliable differentiable module integrable into machine learning pipelines.","We propose a very fast method for solving sparse linear systems, qualifying sparse solvers as an efficient reliable differentiable module in neural architectures. It can be used for solving partial differential equations or generalized deconvolution."
Poster,sciLaMA: A Single-Cell Representation Learning Framework to Leverage Prior Knowledge from Large Language Models,https://ICML.cc//virtual/2025/poster/46669,"Hongru Hu, Shuwen Zhang, Yongin Choi, Venkat Malladi, Gerald Quon","Single-cell RNA sequencing (scRNA-seq) enables high-resolution exploration of cellular diversity and gene regulation, yet analyzing such data remains challenging due to technical and methodological limitations. Existing task-specific deep generative models like Variational Auto-Encoder (VAE) and its variants struggle to incorporate external biological knowledge, while transformer-based foundational large Language Models (LLMs or large LaMs) face limitations in computational cost and applicability to tabular gene expression data. Here, we introduce sciLaMA (single-cell interpretable Language Model Adapter), a novel representation learning framework that bridges these gaps by integrating static gene embeddings from multimodal LaMs with scRNA-seq tabular data through a paired-VAE architecture. Our approach generates context-aware representations for both cells and genes and outperforms state-of-the-art methods in key single-cell downstream tasks, including batch effect correction, cell clustering, and cell-state-specific gene marker and module identification, while maintaining computational efficiency. sciLaMA offers a computationally efficient, unified framework for comprehensive single-cell data analysis and biologically interpretable gene module discovery.","Understanding how our cells work is key to figuring out how tissues grow, develop, and change during disease. Researchers now apply advanced assays to study individual cells, but the data they produce can be noisy and difficult to interpret. Our method, called sciLaMA, helps make sense of this complex data by combining it with existing biological knowledge learned from large language models (LLMs), such as the AI chatbot we are using daily. By bringing this knowledge into the analysis, sciLaMA more accurately identifies different cell states, fills in missing data, and highlights key genes that influence how cells behave over time or during disease progression. This makes the process more efficient and reveals new insights that could guide the discovery of biomarkers and potential treatment targets."
Poster,SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification,https://ICML.cc//virtual/2025/poster/45266,"Shuo Yang, Bardh Prenkaj, Gjergji Kasneci","Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ∼9.5% improvement on F1 for ViT on computer vision datasets and ∼11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.","When we train classifiers, they often learn to make judgement based on misleading patterns, what is widely known as shortcut learning. For example, a model trained on bussiness reviews might assume that any food-related term indicates the review is positive, even when it isn't. These shortcuts make models less reliable in real-world applications.In our research, we found that the shortcut learning does not just come from obvious features like specific words, but also from how similar examples are grouped in the model’s internal understanding of meaning. If these groups are unbalanced, the model may unfairly rely on shortcuts.To solve this, we created a new method called SCISSOR that helps the model avoid learning from these biases. SCISSOR changes how the model understands the data, encouraging it to focus on the true reasons. Unlike other solutions, our approach doesn’t need more balanced data, it works as a lightweight add-on to existing models. This study improved accuracy and fairness of machine learning models in real-world cases, fostering more robust, bias-resistant AI systems."
Poster,Score as Action: Fine Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45084,"Hanyang Zhao, Haoxian Chen, Ji Zhang, David Yao, Wenpin Tang","Reinforcement learning from human feedback (RLHF), which aligns a diffusion model with input prompt, has become a crucial step in building reliable generative AI models. Most works in this area uses a *discrete-time* formulation, which is prone to induced errors, and often not applicable to models with higher-order/black-box solvers. The objective of this study is to develop a disciplined approach to fine-tuning diffusion models using *continuous-time* RL, formulated as a stochastic control problem with a reward function that aligns the end result (terminal state) with input prompt. The key idea is to treat score matching as controls or actions, and thereby connecting to policy optimization and regularization in continuous-time RL. To carry out this idea, we lay out a new policy optimization framework for continuous-time RL, and illustrate its potential in enhancing the value networks design space via leveraging the structural property of diffusion models. We validate the advantages of our method by experiments in downstream tasks of fine-tuning large-scale Text2Image models, Stable Diffusion v1.5.","We often rely on feedback from humans to teach image-generation models how to produce results that match a user’s request. Current methods break down this teaching process into a fixed number of steps, which can introduce mistakes and can’t always work with newer, more flexible solvers. In our work, we propose a new approach that treats the model’s “denoising” steps as continuous actions, rather than a rigid sequence. By framing the problem this way—like guiding a car along a smooth path rather than handing it a checklist of directions—we can use well-known techniques from continuous-time decision making to more precisely steer the model toward the user’s prompt. To test this idea, we fine-tuned a popular text-to-image model (Stable Diffusion v1.5) using our continuous framework. The result is a model that better adapts to feedback and generates images more faithfully aligned with what users want, while avoiding the pitfalls that discrete-step methods often encounter. This makes the process of teaching large-scale generative models more robust and broadly applicable."
Poster,Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport,https://ICML.cc//virtual/2025/poster/46577,"Mingyang Sun, Pengxiang Ding, Weinan Zhang, Donglin Wang","Diffusion policies have shown promise in learning complex behaviors from demonstrations, particularly for tasks requiring precise control and long-term planning. However, they face challenges in robustness when encountering distribution shifts. This paper explores improving diffusion-based imitation learning models through online interactions with the environment. We propose OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement learning fine-tuning), a novel method that integrates diffusion policies with RL using optimal transport theory. OTPR leverages the Q-function as a transport cost and views the policy as an optimal transport map, enabling efficient and stable fine-tuning. Moreover, we introduce masked optimal transport to guide state-action matching using expert keypoints and a compatibility-based resampling strategy to enhance training stability. Experiments on three simulation tasks demonstrate OTPR's superior performance and robustness compared to existing methods, especially in complex and sparse-reward environments. In sum, OTPR provides an effective framework for combining IL and RL, achieving versatile and reliable policy learning.","Reinforcement learning (RL) algorithms often struggle to balance exploration and efficiency when adapting to complex environments, limiting their applicability. This paper explores improving diffusion-based imitation learning models through online interactions with the environment. We propose a new method, OTPR, that combines score-based diffusion models with optimal transport theory to uses trial-and-error learning to fine-tune policies, ensuring robots dynamically adjust to surprises while avoiding wasted effort.  Our method establishes a mathematical connection between trial-and-error learning and optimal transport planning, where the Q-function serves as transport cost and the policy operates as an optimal transport map.  Moreover, we introduce masked optimal transport to guide state-action matching using expert key-points and a compatibility-based resampling strategy to enhance training stability. Tests in virtual environments show OTPR outperforms existing methods, especially in complex tasks with sparse rewards, paving the way for robots that learn reliably in dynamic real-world settings."
Poster,Score-based Pullback Riemannian Geometry: Extracting the Data Manifold Geometry using Anisotropic Flows,https://ICML.cc//virtual/2025/poster/46179,"Willem Diepeveen, Georgios Batzolis, Zakhar Shumaylov, Carola-Bibiane Schönlieb","Data-driven Riemannian geometry has emerged as a powerful tool for interpretable representation learning, offering improved efficiency in downstream tasks. Moving forward, it is crucial to balance cheap manifold mappings with efficient training algorithms. In this work, we integrate concepts from pullback Riemannian geometry and generative models to propose a framework for data-driven Riemannian geometry that is scalable in both geometry and learning: score-based pullback Riemannian geometry. Focusing on unimodal distributions as a first step, we propose a score-based Riemannian structure with closed-form geodesics that pass through the data probability density. With this structure, we construct a Riemannian autoencoder (RAE) with error bounds for discovering the correct data manifold dimension. This framework can naturally be used with anisotropic normalizing flows by adopting isometry regularization during training. Through numerical experiments on diverse datasets, including image data, we demonstrate that the proposed framework produces high-quality geodesics passing through the data support, reliably estimates the intrinsic dimension of the data manifold, and provides a global chart of the manifold. To the best of our knowledge, this is the first scalable framework for extracting the complete geometry of the data manifold.","Most real-world data—from images to scientific measurements—exists in high-dimensional spaces that can be difficult to work with computationally. However, this data often has hidden structure: it actually lies on a lower-dimensional ""manifold"" (a curved surface) embedded within the larger space. Think of how a photograph of a face, while containing millions of pixel values, is really constrained by the underlying geometry of human facial features.This research introduces a new computational framework that can automatically discover both the true dimensionality of data and its geometric structure. The key innovation lies in learning what we call a ""pullback Riemannian metric""—essentially a mathematical tool that captures how distances, paths, and angles work on the data's curved surface.The method builds on Normalizing Flows, a type of machine learning model that can transform complex data distributions into simpler ones. We enhance these flows with two crucial components: regularization that preserves local geometric relationships (ensuring nearby points stay nearby), and a learnable base distribution that automatically identifies which dimensions matter most.This design creates a natural compression mechanism with a unique capability. During training, the model learns to assign larger importance to dimensions that capture meaningful features and near-zero importance to dimensions containing noise or redundant information. As a result, it simultaneously discovers how many dimensions are actually necessary to represent the data and builds an encoder-decoder system that operates precisely at that intrinsic dimension—something no previous approach has achieved, to our knowledge.What sets this framework apart is that it goes beyond compression. The learned geometric structure provides closed-form mathematical expressions for computing optimal paths (geodesics), distances, and transformations on the data manifold, giving researchers powerful tools for analysis and interpretation that were previously unavailable.We tested our approach on datasets ranging from high-dimensional Euclidean datasets to synthetic image manifolds and handwritten digits (MNIST). On datasets that align with the method's assumptions, it accurately estimated the intrinsic dimension and achieved near-perfect compression with minimal information loss. Even on more challenging datasets like MNIST, which deviates from our modeling assumptions due to its multimodal nature spanning ten distinct digit classes, the method demonstrated remarkable versatility by still identifying compact latent representations, albeit with a modest overestimation of the intrinsic dimension.This work establishes a practical foundation for learning the geometry of high-dimensional data at scale, unlocking computational possibilities that were previously beyond reach. The method provides efficient ways to calculate optimal routes and measure true distances within complex datasets, advancing how artificial intelligence systems understand similarity and structure. It also produces compression systems with clear, interpretable representations of data's essential features. Perhaps most significantly, this framework enables the creation of optimization methods that operate directly on the data's curved surface—enabling novel approaches to any problem that requires optimization while staying within realistic data boundaries. This capability could transform fields ranging from inverse problems like medical image reconstruction to controllable content generation."
Poster,Score Matching with Missing Data,https://ICML.cc//virtual/2025/poster/44169,"Josh Givens, Song Liu, Henry Reeve","Score matching is a vital tool for learning the distribution of data with applications across many areas including diffusion processes, energy based modelling, and graphical model estimation. Despite all these applications, little work explores its use when data is incomplete. We address this by adapting score matching (and its major extensions) to work with missing data in a flexible setting where data can be partially missing over any subset of the coordinates. We provide two separate score matching variations for general use, an importance weighting (IW) approach, and a variational approach. We provide finite sample bounds for our IW approach in finite domain settings and show it to have especially strong performance in small sample lower dimensional cases. Complementing this, we show our variational approach to be strongest in more complex high-dimensional settings which we demonstrate on graphical model estimation tasks on both real and simulated data.","Score matching is an invaluable machine learning method which allows us to learn a distribution given data from it. This has many downstream application such as learning dependencies between different components of our data (graphical model edge detection), grouping data points (mode-seeking clustering), image generation (diffusion processes). In this paper we propose two adaptations of score matching which still learn the distribution of the clean data even when the samples in our observed data have some chance of being partially missing or corrupted. This is a common real world scenario,  encompassing cases such as missing measurements in medical data or corrupted pixels in image data.Some work has looked to study adaptation of the aforementioned downstream processes to missing data but very little has focussed on adapting score matching itself. By constructing such an adaptation, we provide a tool that can tackle all of these downstream tasks as well as any other task which utilises score matching. We compare our new adaptations to existing methods for score matching and show our approach to improve upon them."
