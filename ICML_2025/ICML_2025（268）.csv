type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection,https://ICML.cc//virtual/2025/poster/43670,"Louis Béthune, David Grangier, Dan Busbridge, Eleonora Gualdoni, Marco Cuturi, Pierre Ablin","A widespread strategy to obtain a language model that performs well on a target domain is to finetune a pretrained model to perform unsupervised next-token prediction on data from that target domain.Finetuning presents two challenges: \textit{(i)} if the amount of target data is limited, as in most practical applications, the model will quickly overfit, and \textit{(ii)} the model will drift away from the original model, forgetting the pretraining data and the generic knowledge that comes with it.Our goal is to derive scaling laws that quantify these two phenomena for various target domains, amounts of available target data, and model scales.We measure the efficiency of injecting pretraining data into the finetuning data mixture to avoid forgetting and mitigate overfitting.A key practical takeaway from our study is that injecting as little as $1\%$ of pretraining data in the finetuning data mixture prevents the model from forgetting the pretraining set.","A common approach in LLM is to train on generic data that is abundant but not specific, and then fine-tune on specific data that is scarce. The finetuning stage may provoke a catastrophic forgetting of the generic data. We quantify this forgetting, and we show we can counterbalance it simply by mixing the scarce specific data with generic data during the finetuning stage."
Poster,Scaling Laws for Pre-training Agents and World Models,https://ICML.cc//virtual/2025/poster/45787,"Tim Pearce, Tabish Rashid, David Bignell, Raluca Georgescu, Sam Devlin, Katja Hofmann","The performance of embodied agents has been shown to improve by increasing model parameters, dataset size, and compute. This has been demonstrated in domains from robotics to video games, when generative learning objectives on offline datasets (pre-training) are used to model an agent's behavior (imitation learning) or their environment (world modeling). This paper characterizes the role of scale in these tasks more precisely. Going beyond the simple intuition that `bigger is better', we show that the same types of power laws found in language modeling also arise in world modeling and imitation learning (e.g. between loss and optimal model size). However, the coefficients of these laws are heavily influenced by the tokenizer, task \& architecture -- this has important implications on the optimal sizing of models and data.","Bigger neural networks, more data, and more computing power have been shown to make “embodied” AI (e.g. robots or game-playing agents) work better. In particular, researchers have shown this when models learn from recorded examples: either how an expert acts (imitation learning) or how its world behaves (world modeling). This paper digs into exactly how scale helps. It shows that the same neat, straight-line “power-law” trends seen in language models—where error drops predictably as models grow—also appear in these embodied-AI tasks. But the exact slopes of those lines change a lot depending on three practical details:1) Tokenizer: how raw data are broken into pieces.2) Task: whether the agent is learning to copy actions or to predict its environment.3) Model design: the neural-network architecture used.Knowing this lets engineers trade-off model and dataset sizes more wisely instead of blindly assuming “bigger is always better.”"
Poster,Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream,https://ICML.cc//virtual/2025/poster/44987,"Abdulkadir Gokce, Martin Schrimpf","When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition behaviors and neural response patterns in the primate brain. While recent machine learning advances suggest that scaling compute, model size, and dataset size improves task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate visual ventral stream by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and behavior. We find that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive biases and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Our results suggest that while scaling current architectures and datasets might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream, highlighting the need for novel strategies in building brain models.","How do we build artificial intelligence systems that see the world like humans do? In the brain, a network of regions called the ventral visual stream helps us recognize objects in our environment. Scientists use computer models called neural networks to mimic this brain system, hoping to better understand both artificial and biological vision. A common observation in machine learning is that scaling up (using larger models and more training data) leads to better performance. But does it also bring us closer to how the brain works?We trained over 600 neural networks of varying sizes and on different amounts of images, then compared their internal activity and decision patterns to recordings from primate brains and behavioral tests. We fitted simple “scaling laws” to see how brain-alignment scores change as we increase model parameters, dataset size, and compute.We found that although larger models and more data keep improving behavior (how well AI decisions match primate choices), the similarity of model activity to actual brain neurons levels off. This means that more data and compute alone won’t yield better brain-models, and future work should explore new architectures and training methods to truly capture how our visual system works."
Poster,Scaling Laws for Upcycling Mixture-of-Experts Language Models,https://ICML.cc//virtual/2025/poster/44873,"Seng Pei Liew, Takuya Kato, Sho Takase","Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to MoE models, of which the scaling behavior remains underexplored. Through extensive experiments, we identify empirical scaling laws that describe how performance depends on dataset size and model configuration. Particularly, we show that, while scaling these factors improves performance, there is a novel interaction term between the dense and upcycled training dataset that limits the efficiency of upcycling at large computational budgets.   Based on these findings, we provide guidance to scale upcycling, and establish conditions under which upcycling outperforms from-scratch trainings within budget constraints.","Training large language models takes a lot of time and computing power. We focus on how to build larger and more efficient models that only activate parts of themselves when needed (called mixture-of-experts, or MoE), by reusing smaller ones (called upcycling), to reduce training costs. We find patterns that explain how performance changes depending on how big the dataset is and how the model is built. We also discover a new effect: when you reuse a model and give it more data, performance does not simply keep improving but is affected by how the original model was trained.Based on our results, we offer guidelines to get the most out of upcycling, and show when it can be better than starting training from scratch, especially when working within a limited budget."
Poster,"Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",https://ICML.cc//virtual/2025/poster/43735,"Feng Wang, Yaodong Yu, Wei Shao, Yuyin Zhou, Alan Yuille, Cihang Xie","Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a common image pre-processing approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1*1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models.","In modern computer vision, images are usually split into small patches before being analyzed by AI models. This makes computation faster but may lose important details. In our work, we investigate how much this “patchification” affects model performance. By using smaller and smaller patches—down to individual pixels—we find that models actually perform better across various tasks like image classification and object detection. Surprisingly, removing the patch step entirely and treating every pixel as its own token allows the model to reach top accuracy, even with very long input sequences. Our findings show that it's possible to move away from compressive methods and build more accurate models by simply using all the raw visual information."
Poster,Scaling Probabilistic Circuits via Monarch Matrices,https://ICML.cc//virtual/2025/poster/43706,"Honghua Zhang, Meihua Dang, Benjie Wang, Stefano Ermon, Nanyun Peng, Guy Van den Broeck","Probabilistic Circuits (PCs) are tractable representations of probability distributions allowing for exact and efficient computation of likelihoods and marginals. Recent advancements have improved the scalability of PCs either by leveraging their sparse properties or through the use of tensorized operations for better hardware utilization. However, no existing method fully exploits both aspects simultaneously. In this paper, we propose a novel sparse and structured parameterization for the sum blocks in PCs. By replacing dense matrices with sparse Monarch matrices, we significantly reduce the memory and computation costs, enabling unprecedented scaling of PCs. From a theory perspective, our construction arises naturally from circuit multiplication; from a practical perspective, compared to previous efforts on scaling up tractable probabilistic models, our approach not only achieves state-of-the-art generative modeling performance on challenging benchmarks like Text8, LM1B and ImageNet, but also demonstrates superior scaling behavior, achieving the same performance with substantially less compute as measured by the number of floating-point operations (FLOPs) during training.","Deep generative models like transformers and diffusion models have demonstrated huge success in generating texts and images. However  it is extremely hard to reliably control their behaviors/outputs. This is due to their limited tractability: that is, we can easily draw high-quality samples from these models, but they cannot reliably solve tasks such as ""given 50 pieces of text segments, generate a story using all of them following a specific order"" or ""given 10 pieces of fragments of a picture, construct a complete picture using all of them"". Probabilistic Circuits is one special family of deep generative models that can actually solve such challenging problems. However it is quite challenging to scale up the training of Probabilistic Circuits such that they match the performance of e.g. GPT3, and one fundamental bottleneck is that they are built on top of large dense matrices. In this work, we overcome this challenge by replacing these dense matrices with sparse, structured matrices, which are much more efficient while being amenable to efficient execution on GPUs. Our construction of such sparse structured matrices is a natural result of the multiplication of the probability distributions represented by two (or more) Probabilistic Circuits."
Poster,Scaling Sparse Feature Circuits For Studying In-Context Learning,https://ICML.cc//virtual/2025/poster/45531,"Dmitrii Kharlapenko, Stepan Shabalin, Arthur Conmy, Neel Nanda","Sparse autoencoders (SAEs) are a popular tool for interpreting large language model activations, but their utility in addressing open questions in interpretability remains unclear. In this work, we demonstrate their effectiveness by using SAEsto deepen our understanding of the mechanism behind in-context learning (ICL). We identify abstract SAE features that (i) encode the model’s knowledge of which task to execute and (ii) whose latent vectors causally induce the task zero-shot.This aligns with prior work showing that ICL is mediated by task vectors. We further demonstrate that these task vectors are well approximated by a sparse sum of SAE latents, including these task-execution features. To explore the ICL mechanism, we scale the sparse feature circuits methodology of Marks et al. (2024) to the Gemma 1 2B model for the more complex task of ICL. Through circuit finding, we discover task-detecting features with corresponding SAE latents that activate earlier in the prompt, that detect when tasks have been performed. They are causally linked with task-execution features through the attention and MLP sublayers.","Large language models like ChatGPT can learn new tasks just from seeing a few examples in their input, without any additional training. For instance, if you show them ""hot → cold, big → small"" and then ask ""fast →"", they'll correctly respond ""slow."" This ability, called in-context learning, is remarkable but poorly understood. We don't know how these models recognize what task they're supposed to perform or how they execute it internally.We used a recently developed tool called sparse autoencoders (SAEs) to peer inside AI models and map out exactly how in-context learning works. SAEs help scientists identify meaningful patterns in the complex neural activity of AI systems. Using SAEs on Google's Gemma model, we discovered two key types of neural patterns working together: some that detect what task is being demonstrated (like recognizing antonym examples) and others that execute the task (like generating opposite words). We also developed new methods, including an algorithm called Task Vector Cleaning, to isolate these important patterns and trace how information flows between them.This work demonstrates that SAEs can successfully reveal the mechanisms behind complex AI behaviors, not just simple ones. Understanding how AI models process examples and learn tasks is crucial for making them safer and more reliable. Our investigation of in-context learning provides a foundation for better interpreting AI behavior, detecting potential failures, and designing more robust systems. As AI becomes more powerful and widespread, having these kinds of analysis tools becomes increasingly important for ensuring these systems work as intended."
Poster,Scaling Test-Time Compute Without Verification or RL is Suboptimal,https://ICML.cc//virtual/2025/poster/44733,"Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar","Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: (i) distilling successful search or thinking traces; and (ii), using verification (e.g., 0/1 outcome rewards, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erdős 1945], implying a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF widening as test-time budget grows.We corroborate our theory empirically on didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.","Modern language models can ""think longer” at test time by generating many lines of reasoning before deciding on a final answer. But does simply letting a model think for twice as many tokens actually make it twice as smart?We show that the answer depends on how the model was trained. Training approaches that just copy expert solutions (""supervised fine-tuning (SFT)"" or “imitation learning”) struggle to turn extra tokens into better answers: their error rate shrinks slowly—even for very large models—because they suffer from expert heterogeneity, i.e., try to mimic every style and length of expert reasoning they see.In contrast, training that verifies each attempt—using a simple checker that says “right” or “wrong” and then rewards the model accordingly—scales far better. Our theory proves that, as you increase both (i) the model’s test‑time budget of tokens H and (ii) the amount of training data n, the performance gap between verification‑based and imitation‑based methods widens roughly like √H.Experiments on math benchmarks with 3/8/32B parameter models confirm the prediction: verification‑trained models keep improving when given longer to think, while imitation‑trained (SFT) models plateau. The takeaway is simple—if we want bigger models to keep getting smarter by “thinking longer,” we must train them with explicit feedback, not just imitation."
Poster,Scaling Trends in Language Model Robustness,https://ICML.cc//virtual/2025/poster/43784,"Nikolaus Howe, Ian McKenzie, Oskar Hollinsworth, Michał Zając, Tom Tseng, Aaron Tucker, Pierre-Luc Bacon, Adam Gleave","Increasing model size has unlocked a dazzling array of capabilities in language models.At the same time, even frontier models remain vulnerable to jailbreaks and prompt injections, despite concerted efforts to make them robust.As both attackers and defenders gain access to more compute, and as models become larger, what will be the effect on robustness?We argue that to answer this question requires a *scaling lens*, which we adopt in an extensive study of language model robustness across several classification tasks, model families, and adversarial attacks.We find that in the absence of explicit safety training, larger models are not consistently more robust; however, scale improves sample efficiency in adversarial training, though it worsens compute efficiency.Further, we find that increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models.Finally, after exploring robustness transfer across attacks and threat models, we combine attack and defense scaling rates to study the offense-defense balance.We find that while attack scaling outpaces adversarial training across all models studied, larger adversarially trained models might give defense the advantage in the long run.These results underscore the utility of the scaling lens, and provide a paradigm for evaluating future attacks and defenses on frontier models.Code for this project is available at https://github.com/AlignmentResearch/scaling-llm-robustness-paper.","Previous research has shown that one can reliably improve the performance of LLMs like ChatGPT and Claude by using a larger underlying model, training on larger datasets, and training for longer. Despite this recipe for success---which had led to an explosion of capabilities by these frontier models---even the best models can adversarially attacked, that is, be tricked into doing things they shouldn't, like providing misinformation or advising on how to build weapons. We wanted to find a way to predict if this vulnerability to adversarial attack will still exist in the future, when both defender and attacker have access to more compute (so defender can train larger models and for longer, but attacker can also attack harder).In this work, we lay the groundwork for such an approach, and showcase it by studying six tasks and three attacks, against model families with models ranging from 7 million to 14 billion parameters (about 28 MB to 56 GB in size). We find that in general, using a larger model does not automatically make the model robust. However, doing some adversarial training, that is, training the model on examples of the attack, does reliably improve robustness. On the attack side of the equation, increasing attack strength reliably improves attack success rate, regardless of whether the model being attacked has undergone safety training or not. Putting the results together, we show that, for all model sizes studied, attacker has the advantage. However, for significantly larger models, the trend suggests that defender might ultimately have the advantage."
Poster,Scaling Value Iteration Networks to 5000 Layers for Extreme Long-Term Planning,https://ICML.cc//virtual/2025/poster/44887,"Yuhui Wang, Qingyuan Wu, Dylan Ashley, Francesco Faccio, Weida Li, Chao Huang, Jürgen Schmidhuber","The Value Iteration Network (VIN) is an end-to-end differentiable neural network architecture for planning. It exhibits strong generalization to unseen domains by incorporating a differentiable planning module that operates on a latent Markov Decision Process (MDP). However, VINs struggle to scale to long-term and large-scale planning tasks, such as navigating a $100\times 100$ maze---a task that typically requires thousands of planning steps to solve. We observe that this deficiency is due to two issues: the representation capacity of the latent MDP and the planning module's depth. We address these by augmenting the latent MDP with a dynamic transition kernel, dramatically improving its representational capacity, and, to mitigate the vanishing gradient problem, introduce an ""adaptive highway loss"" that constructs skip connections to improve gradient flow. We evaluate our method on 2D/3D maze navigation environments, continuous control, and the real-world Lunar rover navigation task. We find that our new method, named Dynamic Transition VIN (DT-VIN), scales to 5000 layers and solves challenging versions of the above tasks. Altogether, we believe that DT-VIN represents a concrete step forward in performing long-term large-scale planning in complex environments.","Planning is an essential skill for intelligent agents, enabling them to figure out how to reach goals in complex environments. A popular method, called Value Iteration Networks (VINs), allows artificial agents to plan by mimicking how humans and robots think ahead. However, VINs fail when the environment becomes large or the task requires many steps to complete. In this work, we propose an improved version of VIN, called Dynamic Transition VIN (DT-VIN). It introduces two key ideas: (1) a more flexible internal model that better captures the structure of the environment, and (2) a special training technique that helps extremely deep networks learn efficiently. These changes allow our model to plan across 5,000 steps—far more than previous methods. We test DT-VIN in a range of tasks, from simple maze navigation to controlling robots and planning routes for lunar rovers. Across all these tasks, DT-VIN consistently outperforms existing methods, showing that it is better at solving long, complicated planning problems. Our work brings AI one step closer to handling real-world challenges that involve complex, long-term decision-making."
