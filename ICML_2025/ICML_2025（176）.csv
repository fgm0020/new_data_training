type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models,https://ICML.cc//virtual/2025/poster/44566,"Xin Zou, Yizhou WANG, Yibo Yan, Yuanhuiyi Lyu, Kening Zheng, Sirui Huang, Junkai Chen, Peijie Jiang, Jia Liu, Chang Tang, Xuming Hu","Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources.Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to ""amnesia"" about visual information.To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as “key-value memory” at the middle trigger layer. This look-twice mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead.","How do multimodal large language models (MLLMs) handle hallucinations? Hallucinations in MLLMs often arise from the text decoder's sensitivity to visual tokens, causing a kind of “amnesia” about visual information. To tackle this, we propose MemVR, a new decoding paradigm inspired by human behavior of looking at an image again when memory fails. MemVR treats visual tokens as evidence and re-injects them as “key-value memory”. This “look-twice” mechanism reduces hallucination, performs well in benchmarks without extra time cost. MemVR is a plug-and-play, task-agnostic method with wide applicability."
Poster,LoRA-Gen: Specializing Large Language Model via Online LoRA Generation,https://ICML.cc//virtual/2025/poster/44054,"Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Yixiao Ge, Xiu Li, Ying Shan","Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks.Besides, our method delivers a compress ratio of 10.1x with Gemma-2B on intelligent agent tasks.","Recent advances show that scaling language models improves NLP performance, but effectiveness and efficiency remain limited for small edge-side models. We introduce LoRA-Gen, a framework that leverages a large cloud model to generate LoRA parameters for edge models based on task descriptions. Using reparameterization, we integrate these parameters for flexible specialization, enabling efficient knowledge transfer and reducing input length for faster inference."
Poster,"LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently",https://ICML.cc//virtual/2025/poster/45618,"Yuanhe Zhang, Fanghui Liu, Yudong Chen","This paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA) (Hu et al., 2022) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately—applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation. Code is available at: https://github.com/YuanheZ/LoRA-One.","Large language models like ChatGPT are very popular in recent years. For example, they can understand what you say, write like humans, and even solve maths problems! But these models are huge, e.g., in the size of hundred billions. They need a lot of computing power and huge storage space. This work attempts to make these big models work well on a new task with few time and memory cost. LoRA-One, introduced in our paper, takes one careful look at how the full model wants to change in one step, and then setting this change at begining. In cooking terms, imagine you’re making soup, LoRA-One’s idea is to first taste the soup once to know exactly what flavor it needs, then add the perfect amount of spice all at once, instead of trial-and-error pinches. We build LoRA-One in mathematical way and empieirally evaluate it in practical tasks, even with more than 10x speedup comparing to other popular methods."
Poster,LoRA Training Provably Converges to a Low-Rank Global Minimum Or It Fails Loudly (But it Probably Won't Fail),https://ICML.cc//virtual/2025/poster/44076,"Junsu Kim, Jaeyeon Kim, Ernest Ryu","Low-rank adaptation (LoRA) has become a standard approach for fine-tuning large foundation models. However, our theoretical understanding of LoRA remains limited as prior analyses of LoRA's training dynamics either rely on linearization arguments or consider highly simplified setups. In this work, we analyze the LoRA loss landscape without such restrictive assumptions. We define two regimes: a ""special regime"", which includes idealized setups where linearization arguments hold, and a ""generic regime"" representing more realistic setups where linearization arguments do not hold. In the generic regime, we show that LoRA training converges to a global minimizer with low rank and small magnitude, or a qualitatively distinct solution with high rank and large magnitude. Finally, we argue that the zero-initialization and weight decay in LoRA training induce an implicit bias toward the low-rank, small-magnitude region of the parameter space—where global minima lie—thus shedding light on why LoRA training usually succeeds in finding global minima.","Most modern AI models are developed in two stages. First, a model learns general knowledge about a broad range of topics. Then, it is slightly adjusted to specialize in a specific task or domain. The most commonly used method for efficiently doing this second-stage adjustment is called LoRA (Low-Rank Adaptation).These two learning stages are both essential, yet they differ in significant ways. While there is extensive research on algorithms for the initial training phase, the second, task-specific adaptation stage has received comparatively little theoretical attention.Our research addresses this gap by mathematically proving the effectiveness of training algorithms used in the second stage. Importantly, unlike most prior studies, we avoid relying on oversimplified assumptions about the complex architecture of AI models. This improved theoretical understanding helps ensure more reliable and efficient use of AI across various specialized tasks."
Poster,Loss Functions and Operators Generated by f-Divergences,https://ICML.cc//virtual/2025/poster/45088,"Vincent Roulet, Tianlin Liu, Nino Vieillard, Michael Sander, Mathieu Blondel","The logistic loss (a.k.a. cross-entropy loss) is one of the most popular loss functions used for multiclass classification. It is also the loss function of choice for next-token prediction in language modeling. It is associated with the Kullback-Leibler (KL) divergence and the softargmax operator. In this work, we propose to construct new convex loss functions based on $f$-divergences.  Our loss functions generalize the logistic loss in two directions: i) by replacing the KL divergence with $f$-divergences and ii) by allowing non-uniform reference measures. We instantiate our framework for numerous $f$-divergences, recovering existing losses and creating new ones.By analogy with the logistic loss, the loss function generated by an $f$-divergence is associated with an operator, that we dub $f$-softargmax. We derive a novel parallelizable bisection algorithm for computing the $f$-softargmax associated with any $f$-divergence.On the empirical side, one of the goals of this paper is to determine the effectiveness of loss functions beyond the classical cross-entropy in a language model setting, including on pre-training, post-training (SFT) and distillation. We show that the loss function generated by the $\alpha$-divergence (which is equivalent to Tsallis $\alpha$-negentropy in the case of unit reference measures) with $\alpha=1.5$ performs well across several tasks.",We propose to build new cost objectives for deep learning by modifying their theoretical blueprint. We test these new losses on some real problems and observe that our approach can lead to some improvements on some language model tasks.
Poster,LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression,https://ICML.cc//virtual/2025/poster/46198,"Haotian Wu, Gongpu Chen, Pier Luigi Dragotti, Deniz Gunduz","We introduce and validate the lottery codec hypothesis, which states that untrained subnetworks within randomly initialized networks can serve as synthesis networks for overfitted image compression, achieving rate-distortion (RD) performance comparable to trained networks. This hypothesis leads to a new paradigm for image compression by encoding image statistics into the network substructure. Building on this hypothesis, we propose LotteryCodec, which overfits a binary mask to an individual image, leveraging an over-parameterized and randomly initialized network shared by the encoder and the decoder. To address over-parameterization challenges and streamline subnetwork search, we develop a rewind modulation mechanism that improves the RD performance. LotteryCodec outperforms VTM and sets a new state-of-the-art in single-image compression. LotteryCodec also enables adaptive decoding complexity through adjustable mask ratios, offering flexible compression solutions for diverse device constraints and application requirements.","Image compression helps reduce the size of images for storage and transmission, but high performance often requires training large neural networks. In this work, we explore an alternative approach: utilizing untrained parts of randomly initialized networks to compress images effectively. We call this idea the lottery codec hypothesis—suggesting that good subnetworks already exist within randomly created networks, ready to be “found” rather than trained. Building on this, we introduce a new compression approach named LotteryCodec, which finds a binary mask to search subnetworks tailored to each image. LotteryCodec not only surpasses traditional compression tools in terms of quality and size but also enables users to flexibly balance quality and decoding speed, depending on the device, making it ideal for both high-performance and resource-limited devices."
Poster,Low-Dimension-to-High-Dimension Generalization and Its Implications for Length Generalization,https://ICML.cc//virtual/2025/poster/44854,"Yang Chen, Long Yang, Yitao Liang, Zhouchen Lin","Low-Dimension-to-High-Dimension (LDHD) generalization, a subset of Out-of-Distribution (OOD) generalization, involves training on a low-dimensional subspace and testing in a high-dimensional space. Assuming instances are generated from latent variables reflecting problem scale, LDHD generalization captures the inherent scaling challenge of length generalization. We theoretically show that LDHD generalization is unattainable without appropriate inductive bias. Focusing on Boolean functions, we demonstrate that different architectures trained with (S)GD converge to *min-degree interpolators w.r.t. different linearly independent sets*, achieving LDHD generalization only when the target function aligns with this bias. From the perspective of LDHD generalization for length generalization, we explain the success of CoT in restructuring latent space for improved LDHD generalization. We further propose a principle for designing position embeddings to address both LDHD generalization and data format nuisances separately. Following the principle, we introduce RPE-Square, a novel embedding that enhances RPE to better handle data formats.","Why is it hard for machines to learn to solve large and complex problems by first practicing on smaller and simpler ones? In this paper, we show that there are two main reasons: First, problems often become fundamentally more complex as their size grows. Second, the way these problems are presented to the machine can make learning even harder.We prove that, in general, no single learning method can guarantee success in scaling up from small to large problems across all tasks. However, when we do have some prior knowledge about a particular problem, we can design machines that are better suited to learn this way.Our work helps researchers better understand the challenges machines face when generalizing from small to large problem instances. It also offers practical guidance on how to adapt machine designs when additional information about the task is available."
Poster,Low-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space,https://ICML.cc//virtual/2025/poster/46503,"Max van Spengler, Pascal Mettes","Embedding tree-like data, from hierarchies to ontologies and taxonomies, forms a well-studied problem for representing knowledge across many domains. Hyperbolic geometry provides a natural solution for embedding trees, with vastly superior performance over Euclidean embeddings. Recent literature has shown that hyperbolic tree embeddings can even be placed on top of neural networks for hierarchical knowledge integration in deep learning settings. For all applications, a faithful embedding of trees is needed, with combinatorial constructions emerging as the most effective direction. This paper identifies and solves two key limitations of existing works. First, the combinatorial construction hinges on finding highly separated points on a hypersphere, a notoriously difficult problem. Current approaches achieve poor separation, degrading the quality of the corresponding hyperbolic embedding. We propose highly separated Delaunay tree embeddings (HS-DTE), which integrates angular separation in a generalized formulation of Delaunay embeddings, leading to lower embedding distortion. Second, low-distortion requires additional precision. The current approach for increasing precision is to use multiple precision arithmetic, which renders the embeddings useless on GPUs in deep learning settings. We reformulate the combinatorial construction using floating point expansion arithmetic, leading to superior embedding quality while retaining utility on accelerated hardware.","Organizing and representing knowledge that has a tree-like structure, such as family trees, topic hierarchies, or classification systems, is an important and well-explored challenge in many fields. One powerful way to represent these structures is through hyperbolic geometry, which is much better suited than traditional Euclidean spaces for capturing the nature of hierarchical data. Recent research has shown that hyperbolic representations can even be used by deep learning models to better handle complex, layered knowledge. To obtain good results with such deep learning approaches, the hyperbolic representation of the tree must closely mirror its original structure. A class of methods known as combinatorial constructions has emerged as a promising approach for producing high-quality representations. However, current versions of these methods often lead to poor results for many types of trees.In this paper, we identify two key limitations in existing combinatorial constructions and propose solutions to both. The first issue arises from a step in the construction that depends on solving a long-standing mathematical challenge: the uniform placement of points on a high-dimensional sphere. Existing methods use crude approximations for this step, which leads to inaccurate representations. We introduce highly separated Delaunay tree embeddings (HS-DTE), which uses an improved approach to placing the points, resulting in representations that more faithfully capture the original tree structures. The second issue is computational. Previous methods rely on multiple precision arithmetic, which involves keeping many decimal digits. This type of computation is not supported on a GPU, which is the type of hardware that is needed for deep learning. We reformulate the computations using floating point expansion arithmetic, a technique which has the same benefits while still being compatible with GPUs. The result is a method that produces accurate representations and is suitable for use in modern deep learning systems."
Poster,Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers,https://ICML.cc//virtual/2025/poster/45425,"Alireza Amiribavandpour, Xinting Huang, Mark Rofin, Michael Hahn","Chain-of-thought reasoning and scratchpads have emerged as critical tools for enhancing the computational capabilities of transformers. While theoretical results show that polynomial-length scratchpads can extend transformers' expressivity from $TC^0$ to $PTIME$, their required length remains poorly understood. Empirical evidence even suggests that transformers need scratchpads even for many problems in $TC^0$, such as Parity or Multiplication, challenging optimistic bounds derived from circuit complexity. In this work, we initiate the study of systematic lower bounds for the number of CoT steps across different algorithmic problems, in the hard-attention regime. We study a variety of algorithmic problems, and provide bounds that are tight up to logarithmic factors. Overall, these results contribute to emerging understanding of the power and limitations of chain-of-thought reasoning.","LLMs get substantially better at solving reasoning problems when they are allowed to output intermediate steps -- known as ""chain-of-thought reasoning"". However, such chains of intermediate steps introduce extra computational cost. We theoretically show that, on various reasoning problems, LLMs likely need to provide a large number of intermediate steps when the input is long. This research matters because it shows fundamental barriers to efficiently solving reasoning tasks with LLMs."
Poster,LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits,https://ICML.cc//virtual/2025/poster/45872,"Zikai Zhou, Qizheng Zhang, Hermann Kumbong, Kunle Olukotun","Fine-tuning large language models (LLMs) is increasingly costly as models scale to hundreds of billions of parameters, and even parameter-efficient fine-tuning (PEFT) methods like LoRA remain resource-intensive.We introduce LowRA, the first framework to enable LoRA fine-tuning below 2 bits per parameter with minimal performance loss.LowRA optimizes fine-grained quantization—mapping, threshold selection, and precision assignment—while leveraging efficient CUDA kernels for scalable deployment.Extensive evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior performance–precision trade-off above 2 bits and remains accurate down to 1.15 bits, reducing memory usage by up to 50\%. Our results highlight the potential of ultra-low-bit LoRA fine-tuning for resource-constrained environments.","Large language models (LLMs) pack hundreds of billions of parameters, so even “lightweight” frameworks to adapt LLMs to downstream tasks (e.g., LoRA or QLoRA) still strain GPU memory. LowRA squeezes each parameter to about 2 bits—over 15× smaller than the 32-bit norm—while keeping accuracy nearly intact. It learns quantization encoders/decoders specific to each slice of parameters, assigns 1-/2-/4-bit budgets with a fast optimizer, and dequantizes on the fly with lightweight CUDA kernels, so there’s virtually no runtime cost. On four mainstream LLMs and benchmarks, LowRA beats existing quantizers above 2 bits and still works down to 1.15 bits, cutting memory by up to 50 percent. This unlocks personalized fine-tuning on laptops, phones, and other edge devices that previously couldn’t handle such large models."
