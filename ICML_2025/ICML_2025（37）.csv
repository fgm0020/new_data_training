type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions,https://ICML.cc//virtual/2025/poster/45976,"Eray Erturk, Fahad Kamran, Salar Abbaspourazad, Sean Jewell, Harsh Sharma, Yujie Li, Sinead Williamson, Nicholas Foti, Joseph Futoma","Wearable devices record physiological and behavioral signals that can improve health predictions. While foundation models are increasingly used for such predictions, they have been primarily applied to low-level sensor data, despite behavioral data often being more informative due to their alignment with physiologically relevant timescales and quantities. We develop foundation models of such behavioral signals using over 2.5B hours of wearable data from 162K individuals, systematically optimizing architectures and tokenization strategies for this unique dataset. Evaluated on 57 health-related tasks, our model shows strong performance across diverse real-world applications including individual-level classification and time-varying health state prediction. The model excels in behavior-driven tasks like sleep prediction, and improves further when combined with representations of raw sensor data. These results underscore the importance of tailoring foundation model design to wearables and demonstrate the potential to enable new health applications.","This study explores how wearable devices like smartwatches can be used to detect and monitor a wide range of health conditions. We develop a new machine learning model that learns from more processed, behavior-level signals derived from wearables. These higher-level signals, which reflect things like how a person walks and their activity trends, may be easier to interpret than prior approaches that modeled raw sensor data (e.g., from the optical heart rate sensor), and more useful for detecting real-world health events. The model was tested on a variety of tasks, including predicting a person’s age and sex, identifying long-term conditions like diabetes, and spotting short-term health changes like a respiratory infection or injury. The ability to detect short-term changes in health is especially valuable for personalized health, where early detection and continuous monitoring can support timely care and better health decisions. Overall, this study shows that wearable devices, when paired with advanced models, have the potential to move beyond fitness tracking and help monitor everyday health in more meaningful ways."
Poster,Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning,https://ICML.cc//virtual/2025/poster/43966,"Fan Shi, Bin Li, Xiangyang Xue","Abstract visual reasoning (AVR) enables humans to quickly discover and generalize abstract rules to new scenarios. Designing intelligent systems with human-like AVR abilities has been a long-standing topic in the artificial intelligence community. Deep AVR solvers have recently achieved remarkable success in various AVR tasks. However, they usually use task-specific designs or parameters in different tasks. In such a paradigm, solving new tasks often means retraining the model, and sometimes retuning the model architectures, which increases the cost of solving AVR problems. In contrast to task-specific approaches, this paper proposes a novel Unified Conditional Generative Solver (UCGS), aiming to address multiple AVR tasks in a unified framework. First, we prove that some well-known AVR tasks can be reformulated as the problem of estimating the predictability of target images in problem panels. Then, we illustrate that, under the proposed framework, training one conditional generative model can solve various AVR tasks. The experiments show that with a single round of multi-task training, UCGS demonstrates abstract reasoning ability across various AVR tasks. Especially, UCGS exhibits the ability of zero-shot reasoning, enabling it to perform abstract reasoning on problems from unseen AVR tasks in the testing phase.","Humans can solve visual puzzles by understanding abstract rules, for example, finding the odd image out or completing a missing piece in a pattern. These problems belong to abstract visual reasoning (AVR) tasks. Teaching computers to solve AVR tasks is a key step toward developing AI systems that reason more like humans. Most existing AI models are trained to handle just one type of AVR task. When facing a new task, they often need to be retrained or redesigned from scratch, which can be a time-consuming and expensive process.In this work, we propose a framework that can handle multiple AVR tasks like Raven's progressive matrix and visual analogy problems using one conditional generative model. Instead of training new models for new tasks, our framework can generalize its abstract reasoning ability to unseen tasks.This paper provides a unified perspective for solving AVR tasks. Our experiments show that the proposed framework demonstrates the ability to generalize to new kinds of AVR tasks. This opens up new possibilities for developing more flexible and broadly intelligent AI systems that can adapt to a wide variety of AVR tasks."
Poster,Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion,https://ICML.cc//virtual/2025/poster/43634,"Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li","Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry","Consider two AI models, one skilled in sentiment analysis and the other in logical reasoning. Our goal is to obtain a new model that performs well on both tasks, but building up such a model from scratch is expensive. A common approach is to directly fuse the two models into a single one, such averaging their parameters, so-called model merging. But this is harder than it sounds: even if two models do the same job, their “internal wiring” might be arranged differently, making direct merging ineffective.This paper presents a new method to align model parameters before merging. Imagine there is a space where each model corresponds to a vector. Our approach rotates one of the models within this space to bring it closer to the other, which is pretty similar to align a screwdriver with a screw head before turning it. Since we ensure that the models remain functionally equivalent before and after alignment, model merging can benefit from this alignment without any adverse side effects.Our experiments show that this method improves the performance of merged models on both text and image tasks. This work provides a simple and effective way to make different model merging methods work better."
Poster,Beyond The Rainbow: High Performance Deep Reinforcement Learning on a Desktop PC,https://ICML.cc//virtual/2025/poster/45085,"Tyler Clark, Mark Towers, Christine Evers, Jonathon Hare","Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent enhancements could significantly boost a reinforcement learning (RL) agent’s performance. In this paper, we present “Beyond The Rainbow” (BTR), a novel algorithm that integrates six improvements from across the RL literature to Rainbow DQN, establishing a new state-of-the-art for RL using a desktop PC, with a human-normalized interquartile mean (IQM) of 7.6 on Atari-60. Beyond Atari, we demonstrate BTR’s capability to handle complex 3D games, successfully training agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with minimal algorithmic changes. Designing BTR with computational efficiency in mind, agents can be trained using a high-end desktop PC on 200 million Atari frames within 12 hours. Additionally, we conduct detailed ablation studies of each component, analyzing the performance and impact using numerous measures.","Reinforcement Learning (RL) can teach computers to make decisions by trial and error, achieving superhuman performance in many domains such as video games and robotics. However, state-of-the-art RL methods often require days or weeks of training on expensive GPU clusters, placing them out of reach for most researchers, students, and small companies. In response, we introduce Beyond The Rainbow (BTR), an algorithm that combines six recent advancements from the field. On a modern desktop PC, BTR reaches near–state-of-the-art performance on 60 Atari games in under 12 hours. We further demonstrate that, with minimal changes, BTR can learn to play complex 3D titles such as Super Mario Galaxy and Mario Kart. By reducing both training time and hardware cost, BTR makes high-performance RL accessible to anyone with a consumer-grade machine. This democratization promises to broaden participation in RL research and accelerate innovation."
Poster,Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective,https://ICML.cc//virtual/2025/poster/44141,"Steve Azzolin, SAGAR MALHOTRA, Andrea Passerini, Stefano Teso","Self-Explainable Graph Neural Networks (SE-GNNs) are popular explainable-by-design GNNs, but their explanations' properties and limitations are not well understood.Our first contribution fills this gap by formalizing the explanations extracted by some popular SE-GNNs, referred to as Minimal Explanations (MEs), and comparing them to established notions of explanations, namely Prime Implicant (PI) and faithful explanations.Our analysis reveals that MEs match PI explanations for a restricted but significant family of tasks. In general, however, they can be less informative than PI explanations and are surprisingly misaligned with widely accepted notions of faithfulness.Although faithful and PI explanations are informative, they are intractable to find and we show that they can be prohibitively large.Given these observations, a natural choice is to augment SE-GNNs with alternative modalities of explanations taking care of SE-GNNs’ limitations. To this end, we propose Dual-Channel GNNs that integrate a white-box rule extractor and a standard SE-GNN, adaptively combining both channels.Our experiments show that even a simple instantiation of Dual-Channel GNNs can recover succinct rules and perform on par or better than widely used SE-GNNs.","Graph Neural Networks (GNNs) are a type of AI model that can analyze and make predictions about data that’s best represented as a network—like social networks or molecules. Some special GNNs, called Self-Explainable GNNs (SE-GNNs), are designed not just to make predictions, but to also explain why they made those predictions by pointing to parts of the input network that were most important. This paper looks closely at how good those explanations really are, and whether highlighting parts of the input is always the most sensible type of guidance to provide.While SE-GNNs often give simple and compact explanations, these explanations can sometimes be incomplete or misleading. For example, they may fail to show all the reasons behind a prediction or might give the same explanation for different reasons. We then investigate an alternative design choice, combining the usual explanation method with a second, simpler rule-based system. Together, the two parts help the model decide when to use basic patterns (like simple rules about node features) and when to focus on more complex network structures.Our findings serve as sensible guidance to practitioners in knowing the limits of SE-GNNs in providing explanations, and as a warning for stakeholders from blindly trusting explanations produced by these models."
Poster,Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics,https://ICML.cc//virtual/2025/poster/46275,"Shiwei Li, Xiandi Luo, Xing Tang, Haozhao Wang, Hao Chen, weihongluo, Yuhua Li, xiuqiang He, Ruixuan Li","Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning method. In standard LoRA layers, one of the matrices, $A$ or $B$, is initialized to zero, ensuring that fine-tuning starts from the pretrained model. However, there is no theoretical support for this practice.In this paper, we investigate the impact of non-zero initialization on LoRA's fine-tuning dynamics from an infinite-width perspective. Our analysis reveals that, compared to zero initialization, simultaneously initializing $A$ and $B$ to non-zero values improves LoRA's robustness to suboptimal learning rates, particularly smaller ones. Further analysis indicates that although the non-zero initialization of $AB$ introduces random noise into the pretrained weight, it generally does not affect fine-tuning performance. In other words, fine-tuning does not need to strictly start from the pretrained model.The validity of our findings is confirmed through extensive experiments across various models and datasets. The code is available at https://github.com/Leopold1423/non_zero_lora-icml25.","Fine-tuning large language models is extremely expensive, so researchers often turn to a technique called Low-Rank Adaptation (LoRA), which approximates the update of the pretrained weight matrix  using two smaller low-rank matrices. Typically, one of these matrices is initialized to zero to ensure that fine-tuning starts exactly from the pretrained model. However, there is no theoretical reason why zero initialization should be the optimal choice.This raises a simple question: what if we initialize both low-rank matrices with small, non-zero values? Through theoretical analysis and extensive experiments, we find that this non-zero initialization makes LoRA more robust to suboptimal, especially smaller learning rates. These findings challenge two long-standing assumptions in LoRA fine-tuning: first, that one of the low-rank matrices must be initialized to zero, and second, that fine-tuning must begin exactly from the pretrained model. Instead, we show that carefully scaled non-zero initialization not only works, but can improve robustness and overall accuracy."
Poster,BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly,https://ICML.cc//virtual/2025/poster/45407,"Yan Shen, Ruihai Wu, Yubin Ke, Xinyuan Song, Zeyi Li, Xiaoqi Li, Hongwei Fan, Haoran Lu, Hao Dong","Shape assembly, the process of combining parts into a complete whole, is a crucial skill for robots with broad real-world applications. Among the various assembly tasks, geometric assembly—where broken parts are reassembled into their original form (e.g., reconstructing a shattered bowl)—is particularly challenging. This requires the robot to recognize geometric cues for grasping, assembly, and subsequent bimanual collaborative manipulation on varied fragments. In this paper, we exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. To address the evaluation ambiguity caused by geometry diversity  of broken parts, we introduce a real-world benchmark featuring geometric variety and global reproducibility. Extensive experiments demonstrate the superiority of our approach over both previous affordance-based and imitation-based methods.","Geometric shape assembly is a fundamental problem across multiple domains, including archaeology, where fragmented artifacts must be reconstructed to support cultural heritage restoration, and robotics and manufacturing, where assembling broken or modular components is essential for tasks such as object repair, packaging, and furniture construction. For robots, assembling broken parts into their original shapes is especially challenging when the parts vary in shape and size. It requires strong visual understanding, precise manipulation, and coordinated use of both arms.In this work, we teach two-arm (bimanual) robots how to reassemble broken objects from different categories. The robot first learns to understand the geometry of each broken part, avoiding picking them up in ways that would make assembly difficult—such as grabbing them by their broken edges. Then, using both arms, the robot aligns two pieces at their broken edges, and gradually brings them together to complete the object. Our method builds on a concept called visual affordance, which helps the robot understand where and how to grasp based on the geometry of the parts. We extend this idea of affordance learning to support bimanual coordination over long-horizon action sequences to complete the assembly task.To evaluate our approach, we create a new benchmark with real-world examples of broken objects with diverse shapes. Our experiments show that the robot can successfully assemble different types of objects, both in simulation and in real-world settings."
Poster,Bifurcate then Alienate: Incomplete Multi-view Clustering via Coupled Distribution Learning with Linear Overhead,https://ICML.cc//virtual/2025/poster/45961,"Shengju Yu, Yiu-ming Cheung, Siwei Wang, Xinwang Liu, En Zhu","Despite remarkable advances, existing incomplete multi-view clustering (IMC) methods typically  leverage  either perspective-shared or perspective-specific determinants to encode cluster representations. To address this limitation, we introduce a BACDL algorithm designed to explicitly capture both concurrently, thereby exploiting heterogeneous data more effectively. It chooses to bifurcate feature clusters and further alienate them  to enlarge the discrimination. With distribution learning, it successfully couples view guidance into feature clusters to alleviate  dimension inconsistency. Then, building on the principle  that samples in one common cluster own similar marginal distribution and conditional distribution, it unifies the association between feature clusters and sample clusters to bridge all views. Thereafter, all incomplete sample clusters are reordered and mapped to a common one to formulate  clustering embedding. Last, the overall linear overhead endows it with a resource-efficient characteristic.","Current methods for clustering incomplete multi-source data (like combining medical scans from different machines) have a key limitation - they only look for either similarities across all sources or unique patterns in individual sources. Our new BACDL algorithm solves this by: 1. Simultaneously identifying both shared patterns and source-specific features2. Carefully separating feature groups to make clearer distinctions between clusters3. Using smart distribution matching to align data from different sourcesThe system then reorganizes all the partial data into a unified clustering structure. Importantly, it does this efficiently without requiring heavy computations.This approach is particularly useful for real-world situations where:1. Some data might be missing (like a patient missing one type of scan)2. Different sources provide different types of information3. You need to combine various data types while maintaining accuracy"
Poster,BILBO: BILevel Bayesian Optimization,https://ICML.cc//virtual/2025/poster/46280,"Ruth Wan Theng Chew, Quoc Phong Nguyen, Bryan Kian Hsiang Low","Bilevel optimization is characterized by a two-level optimization structure, where the upper-level problem is constrained by optimal lower-level solutions, and such structures are prevalent in real-world problems. The constraint by optimal lower-level solutions poses significant challenges, especially in noisy, constrained, and derivative-free settings, as repeating lower-level optimizations is sample inefficient and predicted lower-level solutions may be suboptimal. We present BILevel Bayesian Optimization (BILBO), a novel Bayesian optimization algorithm for general bilevel problems with blackbox functions, which optimizes both upper- and lower-level problems simultaneously, without the repeated lower-level optimization required by existing methods. BILBO samples from confidence-bounds based trusted sets, which bounds the suboptimality on the lower level. Moreover, BILBO selects only one function query per iteration, where the function query selection strategy incorporates the uncertainty of estimated lower-level solutions and includes a conditional reassignment of the query to encourage exploration of the lower-level objective. The performance of BILBO is theoretically guaranteed with a sublinear regret bound for commonly used kernels and is empirically evaluated on several synthetic and real-world problems.","(1) Many real-world problems involve hierarchical decision-making with two levels of optimization, such as pricing strategies and toll setting. Bilevel optimization can model such hierarchical structures, but many existing methods require inefficient, repeated optimizations at the lower level. (2) We proposed BILevel Bayesian Optimization (BILBO), a novel algorithm that optimizes for both levels simultaneously. We do this by maintaining a trusted set of probable solutions, and encouraging exploration of the lower level via conditional reassignment. (3) Our method is applicable to general bilevel problems, with no assumptions on the availability of gradients, unlike many existing methods. We also provided theoretical guarantees and presented empirical results to show the potential of enabling applications to complex real-world bilevel problems."
Poster,BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix Compression for Real Super-Resolution,https://ICML.cc//virtual/2025/poster/43527,"Kai Liu, Kaicheng Yang, Zheng Chen, Zhiteng Li, Yong Guo, Wenbo Li, Linghe Kong, Yulun Zhang","While super-resolution (SR) methods based on diffusion models (DM) have demonstrated inspiring performance, their deployment is impeded due to the heavy request of memory and computation. Recent researchers apply two kinds of methods to compress or fasten the DM. One is to compress the DM into 1-bit, aka binarization, alleviating the storage and computation pressure. The other distills the multi-step DM into only one step, significantly speeding up inference process. Nonetheless, it remains impossible to deploy DM to resource-limited edge devices. To address this problem, we propose BiMaCoSR, which combines binarization and one-step distillation to obtain extreme compression and acceleration. To prevent the catastrophic collapse of the model caused by binarization, we proposed sparse matrix branch (SMB) and low rank matrix branch (LRM). Both auxiliary branches pass the full-precision (FP) information but in different ways. SMB absorbs the extreme values and its output is high rank, carrying abundant FP information. Whereas, the design of LRMB is inspired by LoRA and is initialized with the top r SVD components, outputting low rank representation. The computation and storage overhead of our proposed branches can be safely ignored. Comprehensive comparison experiments are conducted to exhibit BiMaCoSR outperforms current state-of-the-art binarization methods and gains competitive performance compared with FP one-step model. Moreover, we achieve excellent compression and acceleration. BiMaCoSR achieves a 23.8x compression ratio and a 27.4x speedup ratio compared to FP counterpart. Our code and model are available at https://github.com/Kai-Liu001/BiMaCoSR","Super-resolution models improve image quality but are often too large and slow for devices with limited resources. To solve this, we developed BiMaCoSR, a method that combines two strategies: compressing the model and speeding up its process. We use two new techniques to preserve important details without adding extra cost. BiMaCoSR is 23.8 times smaller and 27.4 times faster than previous models, making it much more efficient. Our code is available at [https://github.com/Kai-Liu001/BiMaCoSR](https://github.com/Kai-Liu001/BiMaCoSR)."
