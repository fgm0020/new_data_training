type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization,https://ICML.cc//virtual/2025/poster/45946,"Junkang Wu, xue wang, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, Xiangnan He","Aligning large language models (LLMs) with human preferences requires balancing policy optimization with computational stability. While recent offline methods like DPO and SimPO bypass reinforcement learning’s complexity, they face critical limitations: DPO relies on static reference models that degrade with policy updates, and SimPO assumes a uniform target reward margin that ignores instance-wise preference strength. We propose AlphaDPO, an adaptive preference optimization framework that dynamically reparameterizes the reference distribution to address these issues. Our key innovation lies in an implicit reference model \(\hat{\pi}_{\text{ref}} \propto U(y|x)(\pi_\theta/\pi_{\text{ref}})^\alpha\), which interpolates between policy-driven specialization and uniform exploration while enabling instance-adaptive reward margins. Theoretically, we prove AlphaDPO implicitly controls sequential KL divergence between iterative policy updates, ensuring stability even with poorly calibrated reference models. Empirically, AlphaDPO achieves state-of-the-art performance on AlpacaEval 2 (58.7\% LC win rate) and Arena-Hard (35.7\% win rate) across Mistral2-7B, Llama3-8B, and Gemma2-9B, demonstrating robust alignment without multi-stage training. Our work establishes adaptive reference reparameterization as a principled mechanism for preference optimization.","AI models, like chatbots, are being trained to better understand and follow human instructions and preferences. However, current training methods can sometimes be inflexible. Some rely on fixed 'guidance' that doesn't adapt as the AI learns new things, while others treat every piece of human feedback as equally important, potentially missing subtle differences in what users prefer.Our new method, AlphaDPO, acts like a more dynamic and personalized 'teacher' for these AI models. It continuously refines its teaching approach based on the AI's progress and also recognizes that some user preferences might be stronger or more critical to get right than others. This adaptive way of teaching helps the AI learn human values more effectively and reliably. As a result, AlphaDPO helps create AI assistants that are better aligned with user expectations and have shown top-level performance in evaluations."
Poster,AlphaPO: Reward Shape Matters for LLM Alignment,https://ICML.cc//virtual/2025/poster/45569,"Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Siyu Zhu, Parag Agrawal, Natesh Pillai, Sathiya Keerthi","Reinforcement Learning with Human Feedback (RLHF) and its variants have made huge strides toward the effective alignment of large language models (LLMs) to follow instructions and reflect human values. More recently, Direct Alignment Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is skipped by characterizing the reward directly as a function of the policy being learned. Some popular examples of DAAs include Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO). These methods often suffer from likelihood displacement, a phenomenon by which the probabilities of preferred responses are often reduced undesirably. In this paper, we argue that, for DAAs the reward (function) shape matters. We introduce \textbf{AlphaPO}, a new DAA method that leverages an $\alpha$-parameter to help change the shape of the reward function beyond the standard log reward. AlphaPO helps maintain fine-grained control over likelihood displacement and over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO leads to about 7\% to 10\% relative improvement in alignment performance for the instruct versions of Mistral-7B and Llama3-8B while achieving 15\% to 50\% relative improvement over DPO on the same models. The analysis and results presented highlight the importance of the reward shape and how one can systematically change it to affect training dynamics, as well as improve alignment performance.","Large language models are often fine-tuned to follow human instructions using methods like Reinforcement Learning with Human Feedback (RLHF), but newer Direct Alignment Algorithms (DAAs) such as DPO and SimPO skip the separate reward-modeling step and directly optimize for human preferences—sometimes at the cost of reducing the model’s likelihood of generating preferred responses, a problem known as likelihood displacement .This paper introduces AlphaPO, a simple yet powerful tweak: it adds a tunable parameter α to reshape the reward function itself, allowing precise control over how aggressively the model shifts probability mass toward preferred outputs without overshooting or under‐optimizing . By varying α, AlphaPO produces training trajectories that better balance margin improvement against maintaining high preferred-response probabilities, effectively mitigating both over-optimization and catastrophic likelihood displacement .In experiments on state-of-the-art 7–8 billion-parameter instruct models, AlphaPO boosts alignment performance by 7–10 % relative to SimPO and by 15–50 % relative to DPO—without longer or more verbose outputs—highlighting that the shape of the reward function is a crucial, previously underexplored knob for aligning LLMs to human values ."
Poster,AlphaQCM: Alpha Discovery in Finance with Distributional Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46517,"Zhoufan Zhu, Ke Zhu","For researchers and practitioners in finance, finding synergistic formulaic alphas is very important but challenging. In this paper, we reconsider the discovery of synergistic formulaic alphas from the viewpoint of sequential decision-making, and conceptualize the entire alpha discovery process as a non-stationary and reward-sparse Markov decision process. To overcome the challenges of non-stationarity and reward-sparsity, we propose the AlphaQCM method, a novel distributional reinforcement learning method designed to search for synergistic formulaic alphas efficiently. The AlphaQCM method first learns the Q function and quantiles via a Q network and a quantile network, respectively. Then, the AlphaQCM method applies the quantiled conditional moment method to learn unbiased variance from the potentially biased quantiles. Guided by the learned Q function and variance, the AlphaQCM method navigates the non-stationarity and reward-sparsity to explore the vast search space of formulaic alphas with high efficacy. Empirical applications to real-world datasets demonstrate that our AlphaQCM method significantly outperforms its competitors, particularly when dealing with large datasets comprising numerous stocks.","In finance, discovering interpretable and powerful signals (also known as formulaic alphas) for stock price prediction is both important and challenging. This task is naturally difficult by three key issues: the vast search space of possible alphas, the fact that most discovered alphas are weak, and the high correlation between the few strong alphas. Existing methods struggle to efficiently find synergistic formulaic alphas, particularly when the financial system is complex.To address these challenges, we reconsidered the alpha discovery problem from the viewpoint of sequential decision-making, and conceptualized the entire alpha discovery process as a non-stationary and reward-sparse Markov decision process. We then proposed the AlphaQCM method, a novel distributional reinforcement learning method designed to efficiently search for synergistic formulaic alphas. The core innovation of AlphaQCM is its ability to learn unbiased variance from the potentially biased quantiles, guiding the alpha discovery in the vast search space.Through experiments on real-world stock market datasets, AlphaQCM outperformed existing methods, especially when dealing with large and complex dataset. This innovation could substantially improve how financial practitioners and researchers discover predictive and interpretable alphas."
Poster,Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search,https://ICML.cc//virtual/2025/poster/44263,"Boyan Li, Jiayi Zhang, Ju Fan, Yanwei XU, Chong Chen, Nan Tang, Yuyu Luo","Text-to-SQL, which enables natural language interaction with databases, serves as a pivotal method across diverse industries.With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone. As an alternative, *zero-shot* Text-to-SQL, which leverages the growing knowledge and reasoning capabilities encoded in LLMs without task-specific fine-tuning, presents a promising and more challenging direction.To address this challenge, we propose Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS) framework to iteratively infer SQL construction actions based on partial reasoning states. To enhance the framework’s reasoning capabilities, we introduce *LLM-as-Action-Model* to dynamically generate SQL construction *actions* during the MCTS process, steering the search toward more promising SQL queries. Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the quality of candidate SQL queries, ensuring more accurate and efficient query generation. Experimental results show that Alpha-SQL achieves 69.7% execution accuracy on the BIRD development set, using a 32B open-source LLM without fine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by 2.5% on the BIRD development set.","Asking databases complex questions using natural language is challenging because translating them into precise database code (SQL) is very difficult. While powerful AI models (LLMs) can help, repeatedly retraining them for every new database or model is costly and time-consuming. This hinders the efficient application of the latest AI technology in database interaction.We developed Alpha-SQL, a new method that allows AI to translate natural language into SQL without expensive retraining. Alpha-SQL acts like a smart planner, guiding the AI to build SQL code step-by-step. It explores various paths for constructing the code (like navigating a maze) and uses the AI's own reasoning to decide the best ""next move"". It also cleverly self-checks by comparing the execution results of different attempts.Alpha-SQL enables smaller, more open AI models to achieve high accuracy in this complex task. For example, using our approach, a 32-billion-parameter open-source AI outperformed the much larger GPT-4o system on a key industry benchmark. This makes advanced natural language database interaction more accessible, cost-effective, and easier to deploy, helping more people gain insights from their data."
Poster,AlphaVerus: Bootstrapping Formally Verified Code Generation through Self-Improving Translation and Treefinement,https://ICML.cc//virtual/2025/poster/43780,"Pranjal Aggarwal, Bryan Parno, Sean Welleck","Automated code generation with large language models has gained significant traction, but there remains no guarantee of the correctness of generated code. We aim to use formal verification to provide mathematical guarantees that the generated code is correct. However, generating formally verified code with LLMs is hindered by the scarcity of training data and the complexity of formal proofs. To tackle this challenge, we introduce AlphaVerus, a self-improving framework that bootstraps formally verified code generation by iteratively translating programs from a higher-resource language and leveraging feedback from a verifier. AlphaVerus operates in three phases: exploration of candidate translations, Treefinement -- a novel tree search algorithm for program refinement using verifier feedback, and filtering misaligned specifications and programs to prevent reward hacking. Through this iterative process, AlphaVerus enables the LLaMA-3.1-70B model to generate verified code without human intervention or model finetuning. AlphaVerus shows an ability to generate formally verified solutions for HumanEval and MBPP, laying the groundwork for truly trustworthy code-generation agents.","When AI systems write computer code for critical applications like medical devices or autonomous vehicles, even tiny errors can have serious consequences. Current AI models, while impressive at generating code, cannot guarantee that their code will work correctly every time. We developed AlphaVerus, a system that teaches itself to write mathematically-proven, correct code. Like a student learning from both successes and mistakes, AlphaVerus tries different solutions, receives feedback from a mathematical verifier that checks if the code is correct, and in the process learns to generate more complicated code and fix its own mistakes. Our method achieves state-of-the-art performance in generating code that comes with a mathematical proof of correctness (known as formally verified code generation). AlphaVerus could enable AI to safely contribute to critical software systems where reliability is non-negotiable, from healthcare to aviation, while reducing the costly human effort currently required to verify code correctness."
Poster,A Machine Learning Approach to Duality in Statistical Physics,https://ICML.cc//virtual/2025/poster/44740,"Prateek Gupta, Andrea Ferrari, Nabil Iqbal","The notion of duality -- that a given physical system can have two different mathematical descriptions -- is a key idea in modern theoretical physics. Establishing a duality in lattice statistical mechanics models requires the construction of a dual Hamiltonian and a map from the original to the dual observables. By using neural networks to parameterize these maps and introducing a loss function that penalises the difference between correlation functions in original and dual models, we formulate the process of duality discovery as an optimization problem. We numerically solve this problem and show that our framework can rediscover the celebrated Kramers-Wannier duality for the 2d Ising model, numerically reconstructing the known mapping of temperatures. We further investigate the 2d Ising model deformed by a plaquette coupling and find families of ``approximate duals''.  We discuss future directions and prospects for discovering new dualities within this framework.","Must there be a unique, efficient mathematical description of a physical system? In the last century, we've learned that in general, this is not the case. A physical system may have several inequivalent descriptions that appear very different and are useful for different purposes; this phenomenon is referred to by physicists as a ""duality"". In this work, we explore the potential of generating entirely new types of ""dual"" descriptions using machine learning. The main new insight is that we use neural networks to parametrise potential descriptions of a given system, and train them so that many perfect ones can be found. We study this in some simple examples from statistical physics (closely related to how phenomena such as the magnetism of macroscopic objects are described at an atomic level) and show that we can automatically rediscover existing results, as well as make some progress towards discovering new dualities."
Poster,A Manifold Perspective on the Statistical Generalization of Graph Neural Networks,https://ICML.cc//virtual/2025/poster/46337,"Zhiyang Wang, Juan Cervino, Alejandro Ribeiro","Graph Neural Networks (GNNs) extend convolutional neural networks to operate on graphs. Despite their impressive performances in various graph learning tasks, the theoretical understanding of their generalization capability is still lacking. Previous GNN generalization bounds ignore the underlying graph structures, often leading to bounds that increase with the number of nodes – a behavior contrary to the one experienced in practice. In this paper, we take a manifold perspective to establish the statistical generalization theory of GNNs on graphs sampled from a manifold in the spectral domain. As demonstrated empirically, we prove that the generalization bounds of GNNs decrease linearly with the size of the graphs in the logarithmic scale, and increase linearly with the spectral continuity constants of the filter functions. Notably, our theory explains both node-level and graph-level tasks. Our result has two implications: i) guaranteeing the generalization of GNNs to unseen data over manifolds; ii) providing insights into the practical design of GNNs, i.e., restrictions on the discriminability of GNNs are necessary to obtain a better generalization performance. We demonstrate our generalization bounds of GNNs using synthetic and multiple real-world datasets.","In this paper, we take a manifold perspective to establish the statistical generalization theory of GNNs on graphs sampled from a manifold in the spectral domain. As demonstrated empirically, we prove that the generalization bounds of GNNs decrease linearly with the size of the graphs in the logarithmic scale, and increase linearly with the spectral continuity constants of the filter functions."
Poster,A Market for Accuracy: Classification Under Competition,https://ICML.cc//virtual/2025/poster/45280,"Ohad Einav, Nir Rosenfeld","Machine learning models play a key role for service providers looking to gain market share in consumer markets. However, traditional learning approaches do not take into account the existence of additional providers, who compete with each other for consumers. Our work aims to study learning in this market setting, as it affects providers, consumers, and the market itself. We begin by analyzing such markets through the lens of the learning objective, and show that accuracy cannot be the only consideration. We then propose a method for classification under competition, so that a learner can maximize market share in the presence of competitors. We show that our approach benefits the providers as well as the consumers, and find that the timing of market entry and model updates can be crucial. We display the effectiveness of our approach across a range of domains, from simple distributions to noisy datasets, and show that the market as a whole remains stable by converging quickly to  an equilibrium.","Machine learning models are widely used in consumer markets, but most ignore the fact that companies are in competition for the same users. We developed a new learning approach that helps models (i.e. service providers) compete more effectively by considering market dynamics. This can improve outcomes for both providers and consumers."
Poster,A Mathematical Framework for AI-Human Integration in Work,https://ICML.cc//virtual/2025/poster/43451,"L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi","The rapid rise of Generative AI (GenAI) tools has sparked debate over their role in complementing or replacing human workers across job contexts. We present a mathematical framework that models jobs, workers, and worker-job fit, introducing a novel decomposition of skills into decision-level and action-level subskills to reflect the complementary strengths of humans and GenAI. We analyze how changes in subskill abilities affect job success, identifying conditions for sharp transitions in success probability. We also establish sufficient conditions under which combining workers with complementary subskills significantly outperforms relying on a single worker. This explains phenomena such as *productivity compression*, where GenAI assistance yields larger gains for lower-skilled workers. We demonstrate the framework's practicality using data from O*NET and Big-Bench Lite, aligning real-world data with our model via subskill-division methods. Our results highlight when and how GenAI complements human skills, rather than replacing them.","The growing presence of Generative AI (GenAI) tools in the workplace has raised questions about whether these systems will replace human workers or enhance their capabilities. This paper introduces a mathematical framework to study this question systematically. The key idea is to break down job skills into two parts: decision-level skills (where humans typically excel) and action-level skills (where GenAI often performs better).Using this structure, we examine how a worker’s ability in these subskills affects their chances of completing a job successfully. We discover that small improvements in ability can lead to sudden, dramatic increases in success—especially when combining workers with complementary strengths. This helps explain real-world findings that AI tools often boost the performance of lower-skilled workers more than higher-skilled ones, a phenomenon known as ""productivity compression.""We support our theoretical insights with data from U.S. job databases and AI benchmarks, and show how this framework can be used to evaluate human-AI teams, guide worker upskilling, and improve hiring or task allocation decisions. Overall, the work highlights that GenAI is best viewed not as a replacement for humans, but as a collaborator that can amplify human strengths."
Poster,am-ELO: A Stable Framework for Arena-based LLM Evaluation,https://ICML.cc//virtual/2025/poster/45944,"Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, Shijin Wang","Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating’s probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.","Current arena-based LLM evaluation frameworks using the ELO rating system suffer from instability due to ranking inconsistencies caused by data order sensitivity and neglect of annotator ability variations, which undermines evaluation credibility.To solve this problem, we propose am-ELO, a stable framework enhancing ELO by replacing iterative updates for consistent rankings and modifying the probability function to model annotator abilities.Experiments show our method reduces ELO score inconsistency to 30% compared to traditional methods, improves prediction accuracy, and robustly handles perturbed annotations."
