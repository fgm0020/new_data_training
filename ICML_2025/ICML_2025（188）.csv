type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Model Immunization from a Condition Number Perspective,https://ICML.cc//virtual/2025/poster/43720,"Amber Yijia Zheng, Cedar Site Bai, Brian Bullins, Raymond A. Yeh","Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num.","When powerful AI models are open-sourced, there is a risk that it could be fine-tuned to produce harmful content. This paper addresses how to train these models in a way that makes misuse more difficult while still ensuring they remain useful for safe purposes. We explore this issue by examining how easily a model can be optimized after its initial training. By optimizing the condition number during training, we improve the model's resistance of being fine-tuned with harmful data. Empirically, our method works both in theory and practice, yielding promising results. We hope this approach contributes to making AI models safer for public release."
Poster,Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training,https://ICML.cc//virtual/2025/poster/44516,"Minghao Xu, Jiaze Song, Keming Wu, Xiangxin Zhou, Bin Cui, Wentao Zhang","Understanding the various properties of glycans with machine learning has shown some preliminary promise. However, previous methods mainly focused on modeling the backbone structure of glycans as graphs of monosaccharides (i.e., sugar units), while they neglected the atomic structures underlying each monosaccharide, which are actually important indicators of glycan properties. We fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide nodes representing its global backbone structure and atom nodes representing its local atomic-level structures. Based on such a graph, GlycanAA performs hierarchical message passing to capture from local atomic-level interactions to global monosaccharide-level interactions. To further enhance model capability, we pre-train GlycanAA on a high-quality unlabeled glycan dataset, deriving the PreGlycanAA model. We design a multi-scale mask prediction algorithm to endow the model about different levels of dependencies in a glycan. Extensive benchmark results show the superiority of GlycanAA over existing glycan encoders and verify the further improvements achieved by PreGlycanAA. We maintain all resources at https://github.com/kasawa1234/GlycanAA.","In this project, we teach the machine about the concept of glycans. Glycans are a very important kind of biomolecules in our life playing various biological functions. Basically, a glycan performs its functions by the atoms composing it. Therefore, we study how a machine can understand glycan functions by modeling their atoms. We develop the GlycanAA model for modeling the atomic-level structures of glycans. Furthermore, we endow GlycanAA with the knowledge underlying abundant unlabeled glycan structures, deriving the PreGlycanAA model."
Poster,Modeling Multi-Task Model Merging as Adaptive Projective Gradient Descent,https://ICML.cc//virtual/2025/poster/45923,"Yongxian Wei, Anke Tang, Li Shen, Zixuan Hu, Chun Yuan, Xiaochun Cao","Merging multiple expert models offers a promising approach for performing multi-task learning without accessing their original data. Existing methods attempt to alleviate task conflicts by sparsifying task vectors or promoting orthogonality among them. However, they overlook the fundamental target of model merging: the merged model performs as closely as possible to task-specific models on respective tasks. We find these methods inevitably discard task-specific information that, while causing conflicts, is crucial for performance. Based on our findings, we frame model merging as a constrained optimization problem ($\textit{i.e.}$, minimizing the gap between the merged model and individual models, subject to the constraint of retaining shared knowledge) and solve it via adaptive projective gradient descent. Specifically, we align the merged model with individual models by decomposing and reconstituting the loss function, alleviating conflicts through $\textit{data-free}$ optimization of task vectors. To retain shared knowledge, we optimize this objective by projecting gradients within a $\textit{shared subspace}$ spanning all tasks. Moreover, we view merging coefficients as adaptive learning rates and propose a task-aware, training-free strategy. Experiments show that our plug-and-play approach consistently outperforms previous methods, achieving state-of-the-art results across diverse architectures and tasks in both vision and NLP domains.","Merging multiple models presents a promising approach to multi-task learning. We posit that the fundamental objective of model merging is for the merged model to perform as closely as possible to the task-specific models on their respective tasks. Building on this insight, we formulate model merging as a constrained optimization problem—minimizing the performance gap between the merged model and individual models while preserving shared knowledge—and solve it using adaptive projective gradient descent. Our method is entirely data-free. Experiments demonstrate that this plug-and-play approach consistently achieves state-of-the-art results across diverse architectures and tasks in both vision and NLP domains."
Poster,Models of Heavy-Tailed Mechanistic Universality,https://ICML.cc//virtual/2025/poster/43908,"Liam Hodgkinson, Zhichao Wang, Michael Mahoney","Recent theoretical and empirical successes in deep learning, including the celebrated neural scaling laws, are punctuated by the observation that many objects of interest tend to exhibit some form of heavy-tailed or power law behavior. In particular, the prevalence of heavy-tailed spectral densities in Jacobians, Hessians, and weight matrices has led to the introduction of the concept of *heavy-tailed mechanistic universality* (HT-MU). Multiple lines of empirical evidence suggest a robust correlation between heavy-tailed metrics and model performance, indicating that HT-MU may be a fundamental aspect of deep learning efficacy. Here, we propose a general family of random matrix models---the *high-temperature Marchenko-Pastur (HTMP) ensemble*---to explore attributes that give rise to heavy-tailed behavior in trained neural networks. Under this model, spectral densities with power laws on (upper and lower) tails arise through a combination of three independent factors (complex correlation structures in the data; reduced temperatures during training; and reduced eigenvector entropy), appearing as an implicit bias in the model structure, and they can be controlled with an ""eigenvalue repulsion'' parameter. Implications of our model on other appearances of heavy tails, including neural scaling laws, optimizer trajectories, and the five-plus-one phases of neural network training, are discussed.","Deep learning models often work best when certain internal patterns follow a “heavy-tailed” shape, meaning most values are small, but a few are extremely large. This strange but consistent pattern, seen in things like weight and gradient matrices, has been linked to better performance, but no one fully understands why it happens. To investigate this, we introduced a new mathematical tool, the high-temperature Marchenko-Pastur (HTMP) model, that helps explain how and why heavy tails emerge during training. We found that heavy-tailed patterns naturally appear when three things are present: complex data, optimal training settings, and hidden structure in the model itself. In fact, our model is able to tune the extent of these heavy tails by changing only one number. This model matters because it connects heavy tails to deeper principles behind learning dynamics, scaling behavior with data, and the stages of training. Our findings suggest that heavy tails aren’t just a curiosity, they may be a core reason why deep learning is so effective."
Poster,Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws,https://ICML.cc//virtual/2025/poster/45349,"Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. Thai, Tianbao Yang","This paper formalizes an emerging learning paradigm that uses a trained model as a reference to guide and enhance the training of a target model through strategic data selection or weighting, named **model steering**. While ad-hoc methods have been used in various contexts, including the training of large foundation models,  its underlying principles remain insufficiently understood, leading to sub-optimal performance. In this work, we propose a theory-driven framework for model steering called **DRRho risk minimization**, which is rooted in Distributionally Robust Optimization (DRO). Through a generalization analysis, we provide theoretical insights into why this approach improves generalization and data efficiency compared to training without a reference model. To the best of our knowledge, this is the first time such theoretical insights are provided for the new learning paradigm, which significantly enhance our understanding and practice of model steering.  Building on these insights and the connection between contrastive learning and DRO, we introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments validate the theoretical insights, reveal a superior scaling law compared to CLIP without a reference model, and demonstrate its strength over existing heuristic approaches. Code is released at [github.com/Optimization-AI/DRRho-CLIP](https://github.com/Optimization-AI/DRRho-CLIP)","Training powerful AI models often requires massive amounts of data and computing power. In this paper, we try to find an approach for the following question: How can AI models learn more efficiently by following the guidance of an existing AI model?First we developed a novel framework, named DRRho risk minimization, to show how the guidance from an existing AI model helps the new AI learn more effectively and perform better on new tasks. We provide theoretical results showing that our framework helps new AI model learn better than existing frameworks.Then we proposed DRRho-CLIP, a practical application of our framework for AIs that help them better understand both images and text. This approach allows the new AI to learn with significantly less data, sometimes even outperform its guiding AI, and improve more rapidly as computing resources increase."
Poster,Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence,https://ICML.cc//virtual/2025/poster/46170,"Shangbin Feng, Zifeng Wang, Yike Wang, Sayna Ebrahimi, Hamid Palangi, Lesly Miculicich, Achin Kulshrestha, Nathalie Rauschmayr, Yejin Choi, Yulia Tsvetkov, Chen-Yu Lee, Tomas Pfister","We propose Model Swarms, a collaborative search algorithm to adapt LLMs via swarm intelligence, the collective behavior guiding individual systems. Specifically, Model Swarms starts with a pool of LLM experts and a utility function. Guided by the best-found checkpoints across models, diverse LLM experts collaboratively move in the weight space and optimize a utility function representing model adaptation objectives. Compared to existing model composition approaches, Model Swarms offers tuning-free model adaptation, works in low-data regimes with as few as 200 examples, and does not require assumptions about specific experts in the swarm or how they should be composed. Extensive experiments demonstrate that Model Swarms could flexibly adapt LLM experts to a single task, multi-task domains, reward models, as well as diverse human interests, improving over 12 model composition baselines by up to 21.0% across tasks and contexts. Further analysis reveals that LLM experts discover previously unseen capabilities in initial checkpoints and that Model Swarms enable the weak-to-strong transition of experts through the collaborative search process.",Multiple language models collaboratively explore the model weight space for adaptation.
Poster,Model Uncertainty Quantification by Conformal Prediction in Continual Learning,https://ICML.cc//virtual/2025/poster/45394,"Rui Gao, Weiwei Liu","Continual learning has attracted increasing research attention in recent years due to its promising experimental results in real-world applications. In this paper, we study the issue of calibration in continual learning which reliably quantifies the uncertainty of model predictions. Conformal prediction (CP) provides a general framework for model calibration, which outputs prediction intervals or sets with a theoretical high coverage guarantee as long as the samples are exchangeable. However, the tasks in continual learning are learned in sequence, which violates the principle that data should be exchangeable. Meanwhile, the model learns the current task with limited or no access to data from previous tasks, which is not conducive to constructing the calibration set. To address these issues, we propose a CP-based method for model uncertainty quantification in continual learning (CPCL), which also reveals the connection between prediction interval length and forgetting. We analyze the oracle prediction interval in continual learning and theoretically prove the asymptotic coverage guarantee of CPCL. Finally, extensive experiments on simulated and real data empirically verify the validity of our proposed method.","Continual learning has attracted increasing research attention in recent years due to its promising experimental results in real-world applications. However, existing works to date have ignored the issue of calibration in continual learning.In this paper, we study the issue of calibration in continual learning which reliably quantifies the uncertainty of model predictions. We propose a  conformal prediction based method for model uncertainty quantification in continual learning (CPCL). We analyze the oracle prediction interval in continual learning and theoretically prove the asymptotic coverage guarantee of CPCL. Finally, extensive experiments on simulated and real data empirically verify the validity of our proposed method.Our work reveals the connection between prediction interval length and forgetting."
Poster,Modified K-means Algorithm with Local Optimality Guarantees,https://ICML.cc//virtual/2025/poster/43694,"Mingyi Li, Michael R. Metel, Akiko Takeda","The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.","The K-means algorithm is one of the most widely used algorithms for clustering datasets into homogeneous groups. It also seems to be largely accepted, from at least the 1980s, that the K-means algorithm converges to locally optimal solutions. In this work we first show, by counterexample, that this is not true in general. We then develop simple modifications to the K-means algorithm which guarantee that it converges to a locally optimal solution, while also keeping the same computational complexity as the original K-means algorithm. We performed extensive experiments on both synthetic and real-world datasets and confirmed that the K-means algorithm does not always converge to locally optimal solutions in practice, while also verifying that our algorithms generate improved locally optimal solutions with reduced clustering loss."
Poster,Modular Duality in Deep Learning,https://ICML.cc//virtual/2025/poster/44428,"Jeremy Bernstein, Laker Newhouse","An old idea in optimization theory says that since the gradient is a dual vector it may not be subtracted from the weights without first being mapped to the primal space where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call modular dualization, forms a unifying theoretical basis for training algorithms that are a) fast and b) scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We derive GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers—the latter two methods are based on a Newton-Schulz iteration. We conclude with small experiments demonstrating the speed, scalability and novel numerical properties of duality-based optimizers. Our methods were used in the Muon optimizer, which recently set speed records for training NanoGPT and was scaled up to a 1.5 billion parameter transformer.","**Problem:** Vanilla gradient descent has a fundamental issue---it directly subtract gradients from weights without considering that different parts of the network may have very different geometric properties. This is like trying to subtract apples from oranges, and the result is that gradient descent can be slow and scale poorly across different neural network sizes.**Solution:** We developed ""modular dualization"", a systematic way to create proper conversion maps (called duality maps) to apply to the gradients of any neural network architecture, allowing the converted gradients to be subtracted from the weights in a sensible way.  Our method works in three steps: first, we assign appropriate geometric measures to individual layers based on what each layer actually does; second, we create conversion rules for each layer type; third, we combine these layer-wise rules to build a single conversion map for the entire network. We also created efficient GPU algorithms to compute these conversions quickly for the most common layer types like linear and convolutional layers.**Impact:** This approach unifies two important but seemingly different optimization methods---maximal update parameterization (μP) for scalable training and Shampoo for fast training---showing they're both approximations of our single theoretical idea. Our methods have already led to significant speedups in practice: the Muon optimizer based on our theory recently set new speed records for training language models, scaling from small networks to 1.5 billion parameter transformers. Beyond speed, our approach reveals novel properties of neural network training, such as allowing weights to move much further from their starting values than traditional methods, challenging conventional wisdom about how neural networks learn."
Poster,Modularized Self-Reflected Video Reasoner for Multimodal LLM with Application to Video Question Answering,https://ICML.cc//virtual/2025/poster/44609,"Zihan Song, Xin Wang, Zi Qian, Hong Chen, Longtao Huang, Hui Xue&#x27;, Wenwu Zhu","Multimodal Large Language Models (Multimodal LLMs) have shown their strength in Video Question Answering (VideoQA). However, due to the black-box nature of end-to-end training strategies, existing approaches based on Multimodal LLMs suffer from the lack of interpretability for VideoQA: they can neither present reasoning paths nor indicate where the answers are derived from the video. To address this issue, we propose **MSR-ViR** (**M**odularized **S**elf-**R**eflected **Vi**deo **R**easoner), which for the first time integrates modular networks to Multimodal LLMs, capable of providing VideoQA with explicit reasoning paths for more interpretability. Specifically, a **MoST-Grounding** (Modularized Spatial-Temporal Grounding) network is proposed to decompose complex questions via tree-structured policies, localizing relevant temporal and spatial segments within videos through step-by-step reasoning. The proposed MoST-Grounding network provides explicit visually grounded information for Multimodal LLMs with clear reasoning paths, thus enhancing interpretability for the predicted answers. To further improve the reasoning quality, we design an **Alternate Self-reflection Training Strategy** to jointly optimize policy generation and Multimodal LLMs. Experiments on real-world datasets demonstrate the superiority of our proposed MSR-ViR framework in video understanding, reasoning transparency, and providing explicit localization evidence for answers.","Today’s powerful Video Large Language Models can watch a video and answer questions about it—like “What did the person do after picking up the cup?”—but they often work like black boxes. We don’t know how they arrived at their answers or where in the video they got the information. To make this process more understandable, our paper introduces MSR-ViR, a new system that breaks the task into smaller, explainable steps. It uses a structured reasoning process to figure out which parts of the video are important and why. Think of it like a detective walking you through their thinking step by step. At the heart of this system is MoST-Grounding, a module that carefully selects the relevant moments and areas in the video based on the question. This information is then fed into a Large Language Model that answers the question—but now with a clear trail showing how the answer is reached. We also introduce a special training method that helps the system learn from its mistakes and improve its reasoning over time. Tests on real-world video datasets show that our method not only answers questions more accurately but also makes it easier to see and trust how those answers were formed."
