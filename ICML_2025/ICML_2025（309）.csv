type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees,https://ICML.cc//virtual/2025/poster/46113,"Zehong Wang, Zheyuan Zhang, Tianyi MA, Nitesh Chawla, Chuxu Zhang, Yanfang Ye","Foundation models are pretrained on large-scale corpora to learn generalizable patterns across domains and tasks---such as contours, textures, and edges in images, or tokens and sentences in text. In contrast, discovering such generalities in graph-structured data, especially across heterogeneous graph tasks, remains an open challenge. To address this, we propose a novel approach to cross-task generalization in graphs via task-trees, which serve as unified learning instances aligning node-, edge-, and graph-level tasks. We theoretically analyze the stability, transferability, and generalization properties of task-trees, showing that pretraining a graph neural network (GNN) on diverse task-trees with a reconstruction objective induces transferable knowledge. This enables efficient adaptation to downstream tasks with minimal fine-tuning. To validate our framework, we introduce Graph Generality Identifier on Task-Trees (GIT), a graph foundation model that demonstrates strong performance on over 30 graphs across five domains via fine-tuning, in-context learning, and zero-shot generalization. Code and data are available at https://github.com/Zehong-Wang/GIT.","Graph-structured data is everywhere---from social networks to molecular structures---but building general-purpose models for graphs has been difficult due to the wide variety of graph types and tasks. Inspired by the success of foundation models in text and vision, this work introduces a new approach to generalize across different graph tasks using a concept called “task-trees.” A task-tree is a structure that captures the essential parts of a graph relevant to a specific task (e.g., classifying a node or predicting a link), and unifies different types of graph tasks into a common format. The paper further proposes a model called GIT (Graph Generality Identifier on Task-Trees), which is pretrained on task-trees from diverse graphs. GIT demonstrates strong performance in fine-tuning, few-shot learning, and even zero-shot generalization across 30+ datasets in five domains. Theoretical analysis supports the effectiveness of task-trees for learning transferable patterns. Overall, this work provides a scalable and principled foundation for training general-purpose graph models, advancing the field toward graph foundation models similar to GPTs for text or CLIP for vision."
Poster,Towards Learning to Complete Anything in Lidar,https://ICML.cc//virtual/2025/poster/43669,"Ayça Takmaz, Cristiano Saltori, Neehar Peri, Tim Meinhardt, Riccardo de Lutio, Laura Leal-Taixé, Aljosa Osep","We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild. This is closely related to Lidar-based semantic/panoptic scene completion. However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets. Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects. These are then distilled into a Lidar-only instance-level completion and recognition model. Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset. We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies.","Advanced self-driving cars and robots often use Lidar sensors to perceive their 3D surroundings. However, Lidar sensors only capture surfaces that are in direct view, resulting in incomplete observations of scene structures and objects. For example, if the car passes a parked bus, the Lidar sensor may only capture the side of the bus facing the vehicle, missing a significant portion of its overall shape. This can lead to poor decisions, such as turning too sharply during a maneuver or misjudging the available parking space. Our method, called Complete Anything in Lidar (CAL), learns to complete the missing parts of any object from just a single Lidar scan. While traditional systems are often trained to recognize a fixed set of object types based on manually labeled data, we generate training data for CAL by using video and Lidar recordings of real-world scenes without any annotations. As a vehicle moves, it naturally sees the same object from different angles. We use these observations to reconstruct more complete object shapes and generate training examples without requiring any manual labels. Once trained, CAL can infer the full shape of objects such as vehicles, buildings, or trees from a single scan at test time. It can also complete and identify previously unseen objects, like trailers, delivery carts, or roadside equipment. By learning to complete objects from partial observations, CAL helps autonomous systems make safer and more informed decisions, even in complex urban environments with heavily occluded objects."
Poster,Towards Lifelong Model Editing via Simulating Ideal Editor,https://ICML.cc//virtual/2025/poster/45062,"Yaming Guo, Siyang Guo, Hengshu Zhu, Ying Sun","Model editing plays a crucial role in the cost-effective development of large language models, and the challenge of evolving knowledge facilitates its sequential extension, namely lifelong model editing. However, progress on standard and lifelong editing has historically followed separate tracks, overlooking the potential of generalizing standard methods to lifelong scenarios. By establishing this bridge, we can provide robust baselines in lifelong scenarios and ensure that lifelong editing benefits from the ongoing advancements in standard editing technologies. In response, this paper proposes a general framework, ***Sim**ulating **I**deal **E**ditor* (SimIE), which restores the strong performance of parameter-modifying methods from standard model editing in a lifelong context. SimIE formulates the ideal parameter shift as the minimum-norm solution to a linear system, constructed using the Moore-Penrose inverse, and subsequently enables recursive updates by truncating the limiting expression of the Moore-Penrose inverse under two mild assumptions. Theoretically, we demonstrate that if either assumption is not met, the solution provided by SimIE remains near-optimal in a statistical sense or stable against perturbations introduced by the sequential editing, but a trade-off between optimality and stability arises when both assumptions fail. Extensive experiments validate the effectiveness of SimIE, which allows standard algorithms to achieve performance comparable to specialized lifelong model editing methods. Our code is available at https://github.com/YamingGuo98/SimIE.","Large language models, such as ChatGPT, are powerful tools, but updating them with new information or correcting errors can be difficult and expensive. Historically, researchers have treated single-time updates (standard editing) and continuous updates (lifelong editing) as separate challenges, limiting the ability to utilize proven standard methods in lifelong scenarios.To bridge this gap, we introduce the Simulating Ideal Editor (SimIE), a general framework that restores the strong performance of standard model editing in a lifelong context. SimIE determines the ideal updates to a model’s parameters, ensuring effective and continuous knowledge integration. Even in imperfect conditions, SimIE is still valid, though  a trade-off is needed between maximizing optimality and preserving stability.Our experiments demonstrate that SimIE enables standard editing methods to perform as effectively as specialized lifelong approaches, thus benefiting lifelong editing from ongoing advancements made in standard techniques."
Poster,Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond,https://ICML.cc//virtual/2025/poster/43469,"Chongyu Fan, jinghan jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, Sijia Liu","The LLM unlearning technique has recently been introduced to comply with data regulations and address the safety and ethical concerns of LLMs by removing the undesired data-model influence. However, state-of-the-art unlearning methods face a critical vulnerability: they are susceptible to ``relearning'' the removed information from a small number of forget data points, known as relearning attacks. In this paper, we systematically investigate how to make unlearned models robust against such attacks. For the first time, we establish a connection between robust unlearning and sharpness-aware minimization (SAM) through a unified robust optimization framework, in an analogy to adversarial training designed to defend against adversarial attacks. Our analysis for SAM reveals that smoothness optimization plays a pivotal role in mitigating relearning attacks. Thus, we further explore diverse smoothing strategies to enhance unlearning robustness. Extensive experiments on benchmark datasets, including WMDP and MUSE, demonstrate that SAM and other smoothness optimization approaches consistently improve the resistance of LLM unlearning to relearning attacks. Notably, smoothness-enhanced unlearning also helps defend against (input-level) jailbreaking attacks, broadening our proposal's impact in robustifying LLM unlearning. Codes are available at https://github.com/OPTML-Group/Unlearn-Smooth.","Large language models (LLMs) can be taught to “forget” certain data to meet legal or ethical needs. But current methods have a flaw: if the model sees just a small part of the old data again, it might accidentally relearn it — a serious risk called a relearning attack.Our research shows that a smarter training approach, called sharpness-aware minimization (SAM), can make forgetting more reliable. It helps the model stay stable and less likely to pick up unwanted information again.We tested this on real benchmarks and found that it not only stops relearning but also protects against other attacks. Codes are available at https://github.com/OPTML-Group/Unlearn-Smooth."
Poster,"Towards Memorization Estimation: Fast, Formal and Free",https://ICML.cc//virtual/2025/poster/45633,"Deepak Ravikumar, Efstathia Soufleri, Abolfazl Hashemi, Kaushik Roy","Deep learning has become the de facto approach in nearly all learning tasks. It has been observed that deep models tend to memorize and sometimes overfit data, which can lead to compromises in performance, privacy, and other critical metrics.In this paper, we explore the theoretical foundations that connect memorization to sample loss, focusing on learning dynamics to understand what and how deep models memorize. To this end, we introduce a novel proxy for memorization: Cumulative Sample Loss (CSL).CSL represents the accumulated loss of a sample throughout the training process.CSL exhibits remarkable similarity to stability-based memorization, as evidenced by considerably high cosine similarity scores. We delve into the theory behind these results, demonstrating that low CSL leads to nontrivial bounds on the extent of stability-based memorization and learning time. The proposed proxy, CSL, is four orders of magnitude less computationally expensive than the stability-based method and can be obtained with zero additional overhead during training. We demonstrate the practical utility of the proposed proxy in identifying mislabeled samples and detecting duplicates where our metric achieves state-of-the-art performance.","Deep learning models, popular for their effectiveness in many applications like image and text processing, have a notable drawback: they often memorize the training data. This memorization can hurt their ability to perform well on new, unseen data, create privacy issues, and make them vulnerable to certain attacks. To tackle this, our research introduces a new way to measure memorization called Cumulative Sample Loss (CSL).CSL works by tracking how much each sample contributes to the model’s loss (or errors) throughout training. Interestingly, we discovered that samples with higher cumulative losses are more likely memorized by the model. Our CSL method is efficient, it can be calculated during training without extra computational costs, making it much faster than existing techniques.We validated CSL through experiments showing strong correlations with previous memorization metrics. CSL also excelled in practical applications, such as detecting mislabeled or duplicated data within datasets, significantly outperforming other approaches. By providing an efficient and effective way to measure memorization, CSL helps researchers build better, safer, and more reliable machine learning models."
Poster,Towards Practical Defect-Focused Automated Code Review,https://ICML.cc//virtual/2025/poster/44165,"Junyi Lu, Lili Jiang, Xiaojia Li, Jianbing Fang, Fengjun Zhang, Li Yang, Chun Zuo","The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2× improvement over standard LLMs and a 10× gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.","Spotting errors in software code, a vital process called code review, is challenging and time-consuming for developers. Many current automated tools aren't very helpful because they look at tiny, isolated pieces of code, focus on generating text that sounds like a human reviewer rather than actually finding important bugs, and don't fit well with how developers build software. This limits their practical use in real-world scenarios.Our research introduces an advanced AI system designed to review code more like an experienced human expert. When developers submit new code changes, our AI intelligently analyzes all the relevant code across the entire project. It uses a collaborative, team-like AI strategy to specifically hunt for critical errors and filters out distracting or incorrect suggestions. We also built it to smoothly integrate into developers' existing daily workflows.When tested on large-scale, complex software used by nearly 400 million daily active users, our system proved significantly more effective at identifying serious, real-world bugs compared to standard AI techniques and older methods—achieving up to a tenfold improvement. This work helps transform automated code review into a truly practical and powerful tool, ultimately leading to higher quality software and boosting developer productivity."
Poster,Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration,https://ICML.cc//virtual/2025/poster/45854,"Yuanchen Wu, Ke Yan, Shouhong Ding, Ziyin Zhou, Xiaoqiang Li","Large Vision-Language Models (LVLMs) have manifested strong visual question answering capability. However, they still struggle with aligning the rationale and the generated answer, leading to inconsistent reasoning and incorrect responses. To this end, this paper introduces Self-Rationale Calibration (SRC) framework to iteratively calibrate the alignment between rationales and answers. SRC begins by employing a lightweight “rationale fine-tuning” approach, which modifies the model’s response format to require a rationale before deriving answer without explicit prompts. Next, SRC searches a diverse set of candidate responses from the fine-tuned LVLMs for each sample, followed by a proposed pairwise scoring strategy using a tailored scoring model, R-Scorer, to evaluate both rationale quality and factual consistency of candidates. Based on a confidence-weighted preference curation process, SRC decouples the alignment calibration into a preference fine-tuning manner, leading to significant improvements of LVLMs in perception, reasoning, and generalization across multiple benchmarks. Our results emphasize the rationale-oriented alignment in exploring the potential of LVLMs.","This work introduces Self-Rationale Calibration (SRC), a framework to improve Large Vision-Language Models (LVLMs) by addressing the misalignment between their generated rationales and answers, which often leads to reasoning errors. SRC first fine-tunes LVLMs to explicitly output rationales before answers, then generates diverse rationale-answer candidates. A novel R-Scorer evaluates these pairs for rationale quality and factual consistency, informing a confidence-weighted preference fine-tuning process. This iterative calibration significantly enhances LVLM perception, reasoning, and generalization, underscoring the importance of rationale-answer alignment."
Poster,Towards Robust Influence Functions with Flat Validation Minima,https://ICML.cc//virtual/2025/poster/45098,"Xichen Ye, Yifan Wu, Weizhong Zhang, Cheng Jin, Yifan Chen","The Influence Function (IF) is a widely used technique for assessing the impact of individual training samples on model predictions.However, existing IF methods often fail to provide reliable influence estimates in deep neural networks, particularly when applied to noisy training data.This issue does not stem from inaccuracies in parameter change estimation, which has been the primary focus of prior research, but rather from deficiencies in loss change estimation, specifically due to the sharpness of validation risk.In this work, we establish a theoretical connection between influence estimation error, validation set risk, and its sharpness, underscoring the importance of flat validation minima for accurate influence estimation.Furthermore, we introduce a novel estimation form of Influence Function specifically designed for flat validation minima.Experimental results across various tasks validate the superiority of our approach.","Modern machine learning systems are trained on massive datasets, but not all data points are good: some are noisy, mislabeled, or even harmful, reducing model quality. Identifying which training examples help or hurt has become a major challenge, especially as models grow more complex.We tackled this problem using a technique called the “influence function,” which measures how much each training example affects a model’s predictions. However, we found that existing influence function methods often fail on modern deep networks because they assume overly simple conditions. To solve this, we proposed a new influence function formulation that specifically accounts for the ''flatness validation minima'', making influence estimates more accurate and robust.Our method improves the ability to detect harmful or misleading data points, enabling researchers to clean datasets, debug models, and improve fairness. Importantly, it scales well to large models and diverse tasks, including image generation and language processing. By making model training more trustworthy and interpretable, our work can help build AI systems that are safer and more reliable for society."
Poster,Towards Robustness and Explainability of Automatic Algorithm Selection,https://ICML.cc//virtual/2025/poster/45808,"Xingyu Wu, Jibin Wu, Yu Zhou, Liang Feng, KC Tan","Algorithm selection aims to identify the optimal performing algorithm before execution. Existing techniques typically focus on the observed correlations between algorithm performance and meta-features. However, little research has explored the underlying mechanisms of algorithm selection, specifically what characteristics an algorithm must possess to effectively tackle problems with certain feature values. This gap not only limits the explainability but also makes existing models vulnerable to data bias and distribution shift. This paper introduces directed acyclic graph (DAG) to describe this mechanism, proposing a novel modeling paradigm that aligns more closely with the fundamental logic of algorithm selection. By leveraging DAG to characterize the algorithm feature distribution conditioned on problem features, our approach enhances robustness against marginal distribution changes and allows for finer-grained predictions through the reconstruction of optimal algorithm features, with the final decision relying on differences between reconstructed and rejected algorithm features. Furthermore, we demonstrate that, the learned DAG and the proposed counterfactual calculations offer our approach with both model-level and instance-level explainability.","Choosing the right algorithm to solve a problem can significantly improve performance, but it's often difficult to know which algorithm works best before actually running them. Traditionally, researchers have relied on data patterns to make this decision, but such methods can be misleading when the data changes or contains bias. This paper introduces a new approach that mimics how humans might reason through the problem: by understanding why certain algorithms work well for certain types of problems. We use a technique called a causal graph—a type of map showing how problem characteristics influence the traits an ideal algorithm should have. This method not only makes the system more robust to changing conditions, but also allows it to explain its decisions more clearly. For example, it can tell us which specific problem features led to the choice of a certain algorithm, or even how small changes to a problem might lead to choosing a different algorithm. Our experiments on a well-known benchmark show that this new method is both more accurate and more interpretable than existing ones."
Poster,Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models,https://ICML.cc//virtual/2025/poster/44540,"Konstantin Donhauser, Kristina Ulicna, Gemma Moran, Aditya Ravuri, Kian Kenyon-Dean, Cian Eastwood, Jason Hartford","Sparse dictionary learning (DL) has emerged as a powerful approach to extract semantically meaningful concepts from the internals of large language models (LLMs) trained mainly in the text domain. In this work, we explore whether DL can extract meaningful concepts from less human-interpretable scientific data, such as vision foundation models trained on cell microscopy images, where limited prior knowledge exists about which high-level concepts should arise. We propose a novel combination of a sparse DL algorithm, Iterative Codebook Feature Learning (ICFL), with a PCA whitening pre-processing step derived from control data. Using this combined approach, we successfully retrieve biologically meaningful concepts, such as cell types and genetic perturbations. Moreover, we demonstrate how our method reveals subtle morphological changes arising from human-interpretable interventions, offering a promising new direction for scientific discovery via mechanistic interpretability in bioimaging.","Researchers in machine learning are increasingly interested in understanding how complex models process information internally: a field known as mechanistic interpretability. This area focuses on uncovering how models compute their outputs, rather than evaluating how well those outputs align with human intuition. One promising approach from this field, called sparse dictionary learning, has shown success in analyzing language models by identifying components inside the model that correspond to distinct patterns in language. In this work, we explore whether similar techniques can be used to study models trained not on text, but on scientific data such as microscopy images of cells. These models, known as vision foundation models, are trained to capture rich visual features but are much harder to interpret. We introduce a method that combines a sparse learning algorithm with a data-driven pre-processing step to help identify meaningful biological concepts. This approach enables us to extract meaningful biological patterns, such as differences between cell types and the effects of genetic perturbations. This approach reveals not only interpretable internal features, but also subtle morphological changes in cells, suggesting new avenues for using machine learning and mechanistic interpretability to advance scientific discovery in bioimage data analysis."
