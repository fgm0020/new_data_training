type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Federated Oriented Learning: A Practical One-Shot Personalized Federated Learning Framework,https://ICML.cc//virtual/2025/poster/44279,"Guan Huang, Tao Shu","Personalized Federated Learning (PFL) has become a promising learning paradigm, enabling the training of high-quality personalized models through multiple communication rounds between clients and a central server. However, directly applying traditional PFL in real-world environments where communication is expensive, limited, or infeasible is challenging, as seen in Low Earth Orbit (LEO) satellite constellations, which face severe communication constraints due to their high mobility, limited contact windows. To address these issues, we introduce Federated Oriented Learning (FOL), a novel four-stage one-shot PFL algorithm designed to enhance local model performance by leveraging neighboring models within stringent communication constraints. FOL comprises model pretraining, model collection, model alignment (via fine-tuning, pruning, post fine-tuning, and ensemble refinement), and knowledge distillation stages. We establish two theoretical guarantees on empirical risk discrepancy between student and teacher models and the convergence of the distillation process.  Extensive experiments on datasets Wildfire, Hurricane, CIFAR-10, CIFAR-100, and SVHN demonstrate that FOL consistently outperforms state-of-the-art one-shot Federated Learning (OFL) methods; for example, it achieves accuracy improvements of up to 39.24\% over the baselines on the Wildfire dataset.","When many devices such as Low-Earth-Orbit (LEO) satellites, delivery drones, or Internet of Things (IoT) sensors with intermittent connectivity collect data in different places, they could learn better if they shared what they know. Unfortunately, these devices often can only connect with one another briefly and have very limited communication windows, so traditional federated-learning algorithms (which send large models back and forth many times) are impractical.We introduce Federated Oriented Learning (FOL), a one-shot approach: each device exchanges models with its neighbors only once yet still ends up with a fully personalized model. For every model it receives, FOL (i) fine-tunes it on local data, (ii) prunes away irrelevant parameters, (iii) post-fine-tunes it again, and (iv) merges the best-matched models into a compact yet powerful teacher. The device then distills knowledge from this teacher into its own student model, while keeping it the same size and structure as the original local model.We prove that this distillation step converges and that the student’s error remains close to the teacher’s. On real wildfire and hurricane satellite imagery, as well as on standard image benchmarks, FOL outperforms existing one-shot approaches by as much as 39 percentage points, showing that personalized models can be learned effectively even under severe communication constraints."
Poster,FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning,https://ICML.cc//virtual/2025/poster/45310,"Ganyu Wang, Jinjie Fang, Maxwell (Juncheng) Yin, Bin Gu, Xi Chen, Boyu Wang, Yi Chang, Charles X. Ling","Black-Box Discrete Prompt Learning (BDPL) is a prompt-tuning method that optimizes discrete prompts without accessing model parameters or gradients, making the prompt tuning on a cloud-based Large Language Model (LLM) feasible.Adapting Federated Learning (FL) to BDPL could further enhance prompt tuning performance by leveraging data from diverse sources. However, all previous research on federated black-box prompt tuning had neglected the substantial query cost associated with the cloud-based LLM service. To address this gap, we conducted a theoretical analysis of query efficiency within the context of federated black-box prompt tuning. Our findings revealed that degrading FedAvg to activate only one client per round, a strategy we called \textit{FedOne}, enabled optimal query efficiency in federated black-box prompt learning. Building on this insight, we proposed the FedOne framework, a federated black-box discrete prompt learning method designed to maximize query efficiency when interacting with cloud-based LLMs.We conducted numerical experiments on various aspects of our framework, demonstrating a significant improvement in query efficiency, which aligns with our theoretical results.","Large language models like ChatGPT are often accessed through paid services that don’t let users see or change the model's internal components. To customize these models for specific tasks, users must repeatedly ""query"" them, which is both costly and slow. This paper explores how many users can work together to fine-tune these models without sharing their data, using a method called federated learning. But in this setup, the cost multiplies: each participating user has to make many queries to the LLMs. Making it impractical. We introduce FedOne, a new approach that trains the model by activating only one user at a time. Our analysis shows that this setup is not only far more efficient in reducing expensive queries but also retains strong performance. We tested this idea on real-world tasks using models like GPT-3.5 and showed that FedOne is both effective and cost-efficient. FedOne makes it easier for people and organizations to adapt powerful AI tools to their needs at a lower cost."
Poster,FedPHA: Federated Prompt Learning for Heterogeneous Client Adaptation,https://ICML.cc//virtual/2025/poster/43552,"Chengying Fang, Wenke Huang, Guancheng Wan, Yihao Yang, Mang Ye","Federated Prompt Learning (FPL) adapts pre-trained Vision-Language Models (VLMs) to federated learning through prompt tuning, leveraging their transferable representations and strong generalization capabilities. Traditional methods often require uniform prompt lengths for federated aggregation, limiting adaptability to clients with diverse prompt lengths and distribution biases. In this paper, we propose **Fed**erated **P**rompt Learning for **H**eterogeneous Client **A**daptation (FedPHA), a novel framework that combines a fixed-length global prompt for efficient aggregation with local prompts of varying lengths to capture client-specific data characteristics. Additionally, FedPHA designs Singular Value Decomposition (SVD) based projection and bidirectional alignment to disentangle global conflicts arising from client heterogeneity, ensuring that personalized client tasks effectively utilize non-harmful global knowledge. This approach ensures that global knowledge improves model generalization while local knowledge preserves local optimization. Experimental results validate the effectiveness of FedPHA in achieving a balance between global and personalized knowledge in federated learning scenarios.","Federated learning allows devices like smartphones, hospitals, or schools to train AI models together without sharing sensitive data. But a key challenge is that every device has different amounts and types of data, and forcing them to follow the same learning process can lead to poor performance.Our work introduces FedPHA, a method designed to address this problem by balancing shared learning with personalization. It gives each device a common ""global prompt"" to ensure consistent collaboration, while also allowing it to have its own ""local prompt"" that better fits its unique data. We also designed a mathematical technique to ensure these global and local prompts don’t conflict, helping devices learn effectively from both shared and personalized knowledge.This approach allows AI systems to benefit from diverse data sources, improving overall learning while ensuring that each device’s unique needs are respected."
Poster,FedSMU: Communication-Efficient and Generalization-Enhanced Federated Learning through Symbolic Model Updates,https://ICML.cc//virtual/2025/poster/45089,"Xinyi Lu, Hao Zhang, Chenglin Li, Weijia Lu, ZHIFEI YANG, Wenrui Dai, xiaodong Zhang, Xiaofeng Ma, Can Zhang, Junni Zou, Hongkai Xiong","The significant communication overhead and client data heterogeneity have posed an important challenge to current federated learning (FL) paradigm. Existing compression-based and optimization-based FL algorithms typically focus on addressing either the model compression challenge or the data heterogeneity issue individually, rather than tackling both of them. In this paper, we observe that by symbolizing the client model updates to be uploaded (i.e., normalizing the magnitude for each model parameter at local clients), the model heterogeneity, essentially stemmed from data heterogeneity, can be mitigated, and thereby helping improve the overall generalization performance of the globally aggregated model at the server. Inspired with this observation, and further motivated by the success of Lion optimizer in achieving the optimal performance on most tasks in the centralized learning, we propose a new FL algorithm, called FedSMU, which simultaneously reduces the communication overhead and alleviates the data heterogeneity issue. Specifically, FedSMU splits the standard Lion optimizer into the local updates and global execution, where only the symbol of client model updates commutes between the client and server. We theoretically prove the convergence of FedSMU for the general non-convex settings. Through extensive experimental evaluations on several benchmark datasets, we demonstrate that our FedSMU algorithm not only reduces the communication overhead, but also achieves a better generalization performance than the other compression-based and optimization-based baselines.","Federated learning is a distributed learning paradigm to enable multiple devices to collaboratively train an AI model but without sharing the raw data. Under this framework, two bottlenecks become serious: the large amount of model updates would incur very high communication overhead, while each device’s unique data can pull the overall AI model to update in a diversity of different directions.Our proposed algorithm, FedSMU, can tackle both of these two challenges at the same time. It allows that each device sends only the sign (i.e., “+” or “–”) of every weight change in the AI model, thus reducing the communication overhead to a few bits while naturally dampening the conflicts of update directions originated from the data heterogeneity across different devices. We further pair this “symbol-only” trick with a split version of the Lion optimizer, which is run partly on devices and partly on the server. We prove that FedSMU still converges for the non-convex settings of current AI models, and on image and text benchmarks it cuts the bandwidth up to ten-fold while outperforming the leading compression-based or optimization-based federated learning methods. This paves the way for a faster, fairer, and more private AI in the bandwidth-constrained settings."
Poster,FedSSI: Rehearsal-Free Continual Federated Learning with Synergistic  Synaptic Intelligence,https://ICML.cc//virtual/2025/poster/46211,"Yichen Li, Yuying Wang, Haozhao Wang, Yining Qi, Tianzhe Xiao, Ruixuan Li","Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding \textit{knowledge forgetting} of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named \textbf{FedSSI}, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.","Continual Federated Learning (CFL) enables devices to collaboratively learn new knowledge without sharing data while retaining previous knowledge. Current methods require repeatedly storing previous data, which is memory-intensive and poses privacy risks. To address this, we propose FedSSI, a lightweight solution that skips data storage and intelligently adjusts learning focus to tackle performance drops caused by data variations. Experiments confirm FedSSI outperforms existing approaches while saving resources and safeguarding privacy."
Poster,Feedforward Few-shot Species Range Estimation,https://ICML.cc//virtual/2025/poster/43753,"Christian Lange, Max Hamilton, Elijah Cole, Alexander Shepard, Samuel Heinrich, Angela Zhu, Subhransu Maji, Grant Horn, Oisin Mac Aodha","Knowing where a particular species can or cannot be found on Earth is crucial for ecological research and conservation efforts. By mapping the spatial ranges of all species, we would obtain deeper insights into how global biodiversity is affected by climate change and habitat loss. However, accurate range estimates are only available for a relatively small proportion of all known species. For the majority of the remaining species, we typically only have a small number of records denoting the spatial locations where they have previously been observed. We outline a new approach for few-shot species range estimation to address the challenge of accurately estimating the range of a species from limited data. During inference, our model takes a set of spatial locations as input, along with optional metadata such as text or an image, and outputs a species encoding that can be used to predict the range of a previously unseen species in a feedforward manner. We evaluate our approach on two challenging benchmarks, where we obtain state-of-the-art range estimation performance, in a fraction of the compute time, compared to recent alternative approaches.","There are many animal species that we do not know much about. We may have only seen them a few times in a few locations, and otherwise we may only have a photo of them or a brief description of where they are found or what they look like. Estimating what areas on Earth these species live in (the range of the species) is important if we want to protect them or find out more about them.We introduce a machine learning model called FS-SINR to help predict the range of a species from a few locations, or from text, or images, or any combination of those. Our model can take in information for a species that it has never seen before and predict the range straight away, without having to take time learning from the information we have for that species.We find that our model makes ranges that are closer to those made by experts, compared to other recent models, and it also makes them more quickly, as we don't have to spend time teaching our model about the species before it can make the range."
Poster,Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models,https://ICML.cc//virtual/2025/poster/44128,"Yao Shu, Wenyang Hu, See-Kiong Ng, Bryan Kian Hsiang Low, Fei Yu","Large Language Models (LLMs) have become indispensable in numerous real-world applications. However, fine-tuning these models at scale, especially in federated settings where data privacy and communication efficiency are critical, presents significant challenges. Existing approaches often resort to parameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but this typically comes at the cost of model accuracy. To this end, we propose *federated full-parameter tuning at scale for LLMs* (Ferret), **the first first-order method with shared randomness** to enable scalable full-parameter tuning of LLMs across decentralized data sources while maintaining competitive model accuracy. Ferret accomplishes this through three aspects: **(i)** it employs widely used first-order methods for efficient local updates; **(ii)** it projects these updates into a low-dimensional space to considerably reduce communication overhead; and **(iii)** it reconstructs local updates from this low-dimensional space with shared randomness to facilitate effective full-parameter global aggregation, ensuring fast convergence and competitive final performance. Our rigorous theoretical analyses and insights along with extensive experiments, show that Ferret significantly enhances the scalability of existing federated full-parameter tuning approaches by achieving high computational efficiency, reduced communication overhead, and fast convergence, all while maintaining competitive model accuracy. Our implementation is available at [https://github.com/allen4747/Ferret](https://github.com/allen4747/Ferret).","(1) Problem: Training large AI models, especially in situations where data is private and spread across many different locations (like hospitals or banks), is a huge challenge. Current methods often make the models less accurate to save on communication costs. We want to train these powerful AI models fully, without sacrificing accuracy, even when data is decentralized and communication is limited.(2) Solution: We developed Ferret, a new method that allows for full training of these large AI models in a federated setting. Ferret uses efficient local updates, then cleverly compresses these updates into a much smaller size for communication. Crucially, it uses a shared ""randomness"" to reconstruct the full updates on the central server, allowing for accurate and complete model adjustments.(3) Impact: Ferret makes it possible to train the most powerful AI models on private, distributed data without compromising their performance. This means AI can be deployed more widely and effectively in sensitive areas like healthcare or finance, while respecting data privacy and significantly reducing the time and resources needed for training."
Poster,Few-Shot Learner Generalizes Across AI-Generated Image Detection,https://ICML.cc//virtual/2025/poster/43709,"Shiyu Wu, Jing Liu, Jing Li, Yequan Wang","Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, these detectors suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space for effectively distinguishing unseen fake images using very few samples. Experiments show that FSD achieves state-of-the-art performance by $+11.6\%$ average accuracy on the GenImage dataset with only $10$ additional samples. More importantly, our method is better capable of capturing the intra-category commonality in unseen images without further training. Our code is available at https://github.com/teheperinko541/Few-Shot-AIGI-Detector.","Current tools for spotting AI-generated fakes struggle when new image generators emerge. They're only reliable for specific models they were trained on, and collecting enough training images for every new AI system is often impossible or prohibitively expensive.We developed FSD (Few-Shot Detector), a novel approach that can identify fake images from unseen AI models using just 10 sample images, with no retraining required. FSD quickly learns to recognize the hidden patterns specific to each generator by comparing image features in a specialized metric space, which allows it to instantly adapt to new generators with minimal data.As AI image generators rapidly evolve, FSD provides a practical, low-cost solution that can keep pace with emerging threats. This makes FSD an important technology  for reducing cheating, misinformation, and harm on Internet platforms and social media."
Poster,"Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts",https://ICML.cc//virtual/2025/poster/45056,"Marta Skreta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, Alan Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, Kirill Neklyudov","While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional `corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation.","Diffusion models are powerful tools for generating data like images, molecules, or text, but it is generally difficult to control their generation process. This paper introduces a method called Feynman-Kac Correctors (FKC), which allows for precise control over what a diffusion model generates without retraining it. FKC works by adjusting the way samples are drawn from the model, based on the Sequential Monte Carlo framework and, in particular, the Feynman-Kac formula. This enables a principled approach to sampling from combined target distributions, like mixtures or products of multiple pretrained models, or temperature-annealed target distributions. We show that FKC improves sampling in three settings: 1. classifier-free guidance, which is widely used in text-to-image generation, 2. generating molecules that satisfy multiple objectives (binding to two proteins simultaneously) and 3. sampling from physical systems at different temperatures using a model trained at a single temperature. Unlike traditional methods, FKC allows for flexible and efficient sampling with little added computation. This opens up new possibilities for applications in AI, drug discovery, and scientific simulations."
Poster,FG-CLIP: Fine-Grained Visual and Textual Alignment,https://ICML.cc//virtual/2025/poster/44568,"Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, Yuhui Yin","Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. We construct a comprehensive dataset, termed FineHARD, by integrating high-quality region-specific annotations with challenging fine-grained negative samples. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.","Modern AI systems often struggle to understand the fine details in images when paired with descriptive text. While models like CLIP have made great progress in matching images and text at a high level, they often struggle with fine-grained understanding due to their focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), a new method designed to enhance the model’s ability to capture and understand detailed visual information. Our approach includes three key innovations. First, we leverage large multimodal models to generate 1.6 billion caption-image pairs with long, detailed descriptions. This helps the model learn global-level semantic details more effectively. Second, we construct a dataset containing 12 million images and 40 million region-specific bounding boxes aligned with detailed captions. This ensures precise and context-rich representations of objects within images. Third, we incorporate 10 million challenging fine-grained negative samples to improve the model's ability to distinguish subtle semantic differences, making it better at recognizing small but important details. By integrating these three types of data into our comprehensive dataset, we meticulously design corresponding training methods to optimize performance. Extensive experiments show that FG-CLIP outperforms existing methods on various tasks requiring detailed understanding, including image-text retrieval, open-vocabulary object detection, and fine-grained recognition. These results demonstrate that FG-CLIP not only captures nuanced visual content more accurately but also enhances the overall capability of vision-language models. We construct a comprehensive dataset, termed FineHARD, by integrating high-quality region-specific annotations with challenging fine-grained negative samples. We release our dataset, code, and models at https://github.com/360CVGroup/FG-CLIP."
