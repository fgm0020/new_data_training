type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Concept-Centric Token Interpretation for Vector-Quantized Generative Models,https://ICML.cc//virtual/2025/poster/43803,"Tianze Yang, Yucheng Shi, Mengnan Du, Xuansheng Wu, Qiaoyu Tan, Jin Sun, Ninghao Liu","Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for image generation. However, the key component of VQGMs---the codebook of discrete tokens---is still not well understood, e.g., which tokens are critical to generate an image of a certain concept?This paper introduces Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting VQGMs by identifying concept-specific token combinations. Our framework employs two methods: (1) a sample-level explanation method that analyzes token importance scores in individual images, and (2) a codebook-level explanation method that explores the entire codebook to find globally relevant tokens. Experimental results demonstrate CORTEX's efficacy in providing clear explanations of token usage in the generative process, outperforming baselines across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX is useful in applications such as targeted image editing and shortcut feature detection. Our code is available at https://github.com/YangTianze009/CORTEX.","Generative models can create impressive images from text, but it’s often unclear how they represent different concepts internally. For example, when asked to generate “a bird” or “a doctor,” what parts of the model are actually responsible for shaping the result?We developed a method called CORTEX that helps uncover which visual building blocks, known as tokens, are most important for generating a specific concept. It works in two ways: first, by identifying the key tokens used in individual images, and second, by searching the model’s entire vocabulary to find the combinations that define a concept.CORTEX helps us understand what generative models have learned and how they use that to create images. It can reveal when certain concepts are shown with bias and identify which visual tokens are responsible. This also allows for targeted image editing by changing only the relevant parts."
Poster,Concept Reachability in Diffusion Models: Beyond Dataset Constraints,https://ICML.cc//virtual/2025/poster/44102,"Marta Aparicio Rodriguez, Xenia Miscouridou, Anastasia Borovykh","Despite significant advances in quality and complexity of the generations in text-to-image models, *prompting* does not always lead to the desired outputs. Controlling model behaviour by directly *steering* intermediate model activations has emerged as a viable alternative allowing to *reach* concepts in latent space that may otherwise remain inaccessible by prompt. In this work, we introduce a set of experiments to deepen our understanding of concept reachability. We design a training data setup with three key obstacles: scarcity of concepts, underspecification of concepts in the captions, and data biases with tied concepts. Our results show: (i) concept reachability in latent space exhibits a distinct phase transition, with only a small number of samples being sufficient to enable reachability, (ii) *where* in the latent space the intervention is performed critically impacts reachability, showing that certain concepts are reachable only at certain stages of transformation, and (iii) while prompting ability rapidly diminishes with a decrease in quality of the dataset, concepts often remain reliably reachable through steering. Model providers can leverage this to bypass costly retraining and dataset curation and instead innovate with user-facing control mechanisms.","Creating images from text descriptions using AI has come a long way, but these systems still don’t always generate exactly what users ask for. Even well-crafted prompts can fail to generate certain concepts in images. In our work, we explore two key questions: when prompting falls short, can alternative methods still recover those concepts? And are there limitations in the training data so severe that some concepts can’t be generated at all?To investigate this, we use synthetic data in controlled experiments that let us compare concept extraction methods. Our setup targets three common issues: concepts that are very rare, concepts present in images but missing from captions (such as backgrounds), and cases where concepts are biased or entangled with others.Our results show that training data quality is critical. We show that a small number of examples can often make a concept reachable, interventions in the model’s internal process can at times succeed even when prompting fails due to poor data, and success depends heavily on where in the model you intervene to extract concepts. These insights suggest that developers can focus on giving users more direct and flexible tools for controlling image generation as an alternative to retraining."
Poster,Concurrent Reinforcement Learning with Aggregated States via  Randomized Least Squares Value Iteration,https://ICML.cc//virtual/2025/poster/45535,"Yan Chen, Jerry Bai, Yiteng Zhang, Maria Dimakopoulou, Shi Dong, Qi Sun, Zhengyuan Zhou","Designing learning agents that explore efficiently in a complex environment has been widely recognized as a fundamental challenge in reinforcement learning. While a number of works have demonstrated the effectiveness of  techniques based on randomized value functions on a single agent, it remains unclear, from a theoretical point of view, whether injecting randomization can help a society of agents concurently explore an environment. The theoretical results established in this work tender an affirmative answer to this question. We adapt the concurrent learning framework to randomized least-squares value iteration (RLSVI) with aggregated state representation. We demonstrate polynomial worst-case regret bounds in both finite- and infinite-horizon environments.In both setups the per-agent regret decreases at an optimal rate of $\Theta\left(\frac{1}{\sqrt{N}}\right)$, highlighting the advantage of concurent learning. Our algorithm exhibits significantly lower space complexity compared to Russo (2019) and Agrawal et. al (2021). We reduce the space complexity by a factor of $K$ while incurring only a $\sqrt{K}$ increase in the worst-case regret bound, compared to Russo (2019) and Agrawal et. al (2021). Interestingly, our algorithm improves the worst-case regret bound of Russo (2019) by a factor of $H^{1/2}$, matching the improvement in Agrawal et. al (2021). However, this result is achieved through a fundamentally different algorithmic enhancement and proof technique. Additionally, we conduct numerical experiments to demonstrate our theoretical findings.","Reinforcement learning (RL) trains computer programs (agents) to make optimal decisions through trial-and-error interactions with complex environments. One significant challenge is helping multiple agents explore efficiently when working together, as current methods mainly address single-agent scenarios. In our study, we developed a new approach that helps groups of agents collectively learn faster and more efficiently. We adapted a method called concurrent randomized least-squares value iteration (RLSVI) with aggregated states, allowing multiple agents to explore concurrently by sharing and simplifying their knowledge of the environment. Our theoretical results demonstrate that each agent learns faster as more agents participate, significantly speeding up overall learning. Additionally, our method greatly reduces the memory requirements compared to existing work, making it practical for systems with limited resources. Compared to previous research, our approach achieves better performance through simpler computations and innovative theoretical insights. Through experiments, we confirmed that our theory translates into practical performance improvements. This work advances our understanding of multi-agent learning, providing a scalable solution for real-world applications."
Poster,Conditional Diffusion Model with Nonlinear Data Transformation for Time Series Forecasting,https://ICML.cc//virtual/2025/poster/44243,"RISHI JINKA, Venkata Sai Mothish Gonugunta, Deepak N. Subramani","Time-series forecasting finds application across domains such as finance, climate science, and energy systems. We introduce the Conditional Diffusion  with Nonlinear Data Transformation Model (CN-Diff), a generative framework that employs novel nonlinear transformations and learnable conditions in the forward process for time series forecasting. A new loss formulation for training is proposed, along with a detailed derivation of both forward and reverse process. The new additions improve the diffusion model's capacity to capture complex time series patterns, thus simplifying the reverse process. Our novel condition facilitates learning an efficient prior distribution. This also reduces the gap between the true negative log-likelihood and its variational approximation. CN-Diff is shown to perform better than other leading time series models on nine real-world datasets. Ablation studies are conducted to elucidate the role of each component of CN-Diff.","We introduce the Conditional Diffusion  with Nonlinear Data Transformation Model (CN-Diff), a generative framework that employs novel nonlinear transformations and learnable conditions in the forward process for time series forecasting. A new loss formulation for training is proposed, along with a detailed derivation of both forward and reverse process. CN-Diff is shown to perform better than other leading time series models on nine real-world datasets. Ablation studies are conducted to elucidate the role of each component of CN-Diff."
Poster,Conditioning Diffusions Using Malliavin Calculus,https://ICML.cc//virtual/2025/poster/46698,"Jakiw Pidstrigach, Elizabeth Baker, Carles Domingo i Enrich, George Deligiannidis, Nikolas Nüsken","In generative modelling and stochastic optimal control, a central computational task is to modify a reference diffusion process to maximise a given terminal-time reward. Most existing methods require this reward to be differentiable, using gradients to steer the diffusion towards favourable outcomes. However, in many practical settings, like diffusion bridges, the reward is singular, taking an infinite value if the target is hit and zero otherwise.We introduce a novel framework, based on Malliavin calculus and centred around a generalisation of the Tweedie score formula to nonlinear stochastic differential equations, that enables the development of methods robust to such singularities.This allows our approach to handle a broad range of applications, like diffusion bridges, or adding conditional controls to an already trained diffusion model.We demonstrate that our approach offers stable and reliable training, outperforming existing techniques. As a byproduct, we also introduce a novel score matching objective. Our loss functions are formulated such that they could readily be extended to manifold-valued and infinite dimensional diffusions.","In many stochastic systems, one is interested in simulating outcomes that satisfy a prescribed condition. For instance, one might wish to generate chemical reaction trajectories that terminate in a specific state, or simulate weather patterns consistent with a particular scenario. In generative artificial intelligence, this includes tasks such as generating images with a given depth map or designing proteins that exhibit certain desired properties.We introduce a new method rooted in Malliavin calculus to address these challenges. This mathematical framework yields a highly general formulation, enabling the control of diffusion processes conditioned on a wide variety of events."
Poster,Confidence Difference Reflects Various Supervised Signals in Confidence-Difference Classification,https://ICML.cc//virtual/2025/poster/46314,"Yuanchao Dai, Ximing Li, Changchun Li","Training a precise binary classifier with limited supervision in weakly supervised learning scenarios holds considerable research significance in practical settings. Leveraging pairwise unlabeled data with confidence differences has been demonstrated to outperform learning from pointwise unlabeled data. We theoretically analyze the various supervisory signals reflected by confidence differences in confidence difference (ConfDiff) classification and identify challenges arising from noisy signals when confidence differences are small. To address this, we partition the dataset into two subsets with distinct supervisory signals and propose a consistency regularization-based risk estimator to encourage similar outputs for similar instances, mitigating the impact of noisy supervision. We further derive and analyze its estimation error bounds theoretically. Extensive experiments on benchmark and UCI datasets demonstrate the effectiveness of our method. Additionally, to effectively capture the influence of real-world noise on the confidence difference, we artificially perturb the confidence difference distribution and demonstrate the robustness of our method under noisy conditions through comprehensive experiments.","When training models with limited supervision, comparing pairs of examples often provides better learning signals than labeling individual examples. However, we discover a critical challenge: small confidence differences between paired examples can introduce unreliable signals, as these examples might belong to either the same or different classes, potentially misleading the model during training.To solve this problem, we develop a method that separates data pairs based on their confidence differences. For pairs with small differences, we apply a special technique called consistency regularization that encourages the model to produce similar outputs, assuming they likely belong to the same class. For pairs with large differences, we maintain their strong guidance signals since they reliably indicate different classes.Our experiments across multiple datasets demonstrate that this approach consistently outperforms existing methods, even when data contains significant noise. This makes our technique particularly valuable for real-world applications like medical diagnosis, where precise labeling is challenging but comparing patient cases is often more straightforward and reliable."
Poster,Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention,https://ICML.cc//virtual/2025/poster/45527,"Stephan Rabanser, Ali Shahin Shamsabadi, Olive Franzese, Xiao Wang, Adrian Weller, Nicolas Papernot","Cautious predictions—where a machine learning model abstains when uncertain—are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model’s proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.","When artificial intelligence (AI) systems are unsure, they often choose to “abstain” from making a prediction. This cautious behavior helps avoid harmful mistakes in high-stakes settings like medicine, finance, or criminal justice. But what if that very mechanism—meant to promote safety—could be twisted into a tool for harm? In our work, we reveal a troubling possibility: an organization could deliberately make its AI system appear uncertain for certain people—not because the task is genuinely hard, but to quietly deny them services like loans or benefits. We call this deceptive strategy Mirage, an attack that reduces the AI's confidence in specific cases while still performing well overall. This makes it hard for outside observers to notice anything suspicious. To stop such misuse, we introduce Confidential Guardian, a new system that allows independent auditors to check whether an AI’s cautious behavior is real or artificially manufactured. It does this by analyzing how the AI behaves on trusted test cases, and verifying its behavior using a technique that ensures honesty—without revealing the model’s inner workings. Our findings highlight a hidden danger in today’s AI systems, and offer a path toward greater transparency and fairness—ensuring that caution is used for safety, not for discrimination."
Poster,Conformal Anomaly Detection in Event Sequences,https://ICML.cc//virtual/2025/poster/46037,"Shuai Zhang, Chuan Zhou, Yang Liu, PENG ZHANG, Xixun Lin, Shirui Pan","Anomaly detection in continuous-time event sequences is a crucial task in safety-critical applications. While existing methods primarily focus on developing a superior test statistic, they fail to provide guarantees regarding the false positive rate (FPR), which undermines their reliability in practical deployments. In this paper, we propose CADES (Conformal Anomaly Detection in Event Sequences), a novel test procedure based on conformal inference for the studied task with finite-sample FPR control. Specifically, by using the time-rescaling theorem, we design two powerful non-conformity scores tailored to event sequences, which exhibit complementary sensitivities to different abnormal patterns. CADES combines these scores with Bonferroni correction to leverage their respective strengths and addresses non-identifiability issues of existing methods. Theoretically, we prove the validity of CADES and further provide strong guarantees on calibration-conditional FPR control. Experimental results on synthetic and real-world datasets, covering various types of anomalies, demonstrate that CADES outperforms state-of-the-art methods while maintaining FPR control.","Anomaly detection in event sequences is essential for ensuring safety in areas like healthcare, finance, and information security, where unexpected sequences can have serious consequences. Existing methods typically focus on detecting these anomalies, but they do not offer rigorous control over false positives, meaning they might mistakenly flag normal sequences as anomalies. In this paper, we introduce a new method called CADES, based on a statistical framework called conformal prediction, which improves anomaly detection by controlling false positives. Additionally, CADES can identify anomalies that current methods fail to detect. Through a series of experiments, we show that CADES outperforms existing methods while maintaining control over false positive rate. This approach provides a more trustworthy way of detecting anomalies, which is crucial for applications where safety is a top priority."
Poster,Conformal Prediction as Bayesian Quadrature,https://ICML.cc//virtual/2025/poster/45390,"Jake Snell, Thomas Griffiths","As machine learning-based prediction systems are increasingly used in high-stakes situations, it is important to understand how such predictive models will perform upon deployment. Distribution-free uncertainty quantification techniques such as conformal prediction provide guarantees about the loss black-box models will incur even when the details of the models are hidden. However, such methods are based on frequentist probability, which unduly limits their applicability. We revisit the central aspects of conformal prediction from a Bayesian perspective and thereby illuminate the shortcomings of frequentist guarantees. We propose a practical alternative based on Bayesian quadrature that provides interpretable guarantees and offers a richer representation of the likely range of losses to be observed at test time.","Machine learning can be used to make predictions in high-stakes settings. In these settings, we want to decide if we should use a particular prediction algorithm or not. We can first measure the performance of the algorithm on some data. Then we can estimate whether the algorithm is appropriate based on this. Previous methods to do this estimation have two main issues. Some require strong assumptions about the algorithm itself. Others produce only a single estimate rather than a range of possibilities. We show how to produce a range of plausible estimates without strong assumptions. In this way, we can better decide whether we can safely use the algorithm."
Poster,Conformal Prediction with Cellwise Outliers: A Detect-then-Impute Approach,https://ICML.cc//virtual/2025/poster/43829,"Qian Peng, Yajie Bao, Haojie Ren, Zhaojun Wang, Changliang Zou","Conformal prediction is a powerful tool for constructing prediction intervals for black-box models, providing a finite sample coverage guarantee for exchangeable data. However, this exchangeability is compromised when some entries of the test feature are contaminated, such as in the case of cellwise outliers. To address this issue, this paper introduces a novel framework called *detect-then-impute conformal prediction*. This framework first employs an outlier detection procedure on the test feature and then utilizes an imputation method to fill in those cells identified as outliers. To quantify the uncertainty in the processed test feature, we adaptively apply the detection and imputation procedures to the calibration set, thereby constructing exchangeable features for the conformal prediction interval of the test label. We develop two practical algorithms, $\texttt{PDI-CP}$ and $\texttt{JDI-CP}$, and provide a distribution-free coverage analysis under some commonly used detection and imputation procedures. Notably, $\texttt{JDI-CP}$ achieves a finite sample $1-2\alpha$ coverage guarantee. Numerical experiments on both synthetic and real datasets demonstrate that our proposed algorithms exhibit robust coverage properties and comparable efficiency to the oracle baseline.","When test data contains corrupted values (e.g., a patient’s age mistakenly recorded as 200), traditional prediction interval methods fail because they assume clean, exchangeable data. We propose a *detect-then-impute conformal prediction*: first identify outliers in test features (like flagging implausible values), then replace them with plausible estimates (e.g., using mean). By applying the same detection and imputation steps to both the test data and the calibration data, our method can ultimately construct reliable prediction intervals."
