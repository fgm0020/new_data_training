type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,BSO: Binary Spiking Online Optimization Algorithm,https://ICML.cc//virtual/2025/poster/45087,"Yu Liang, Yu Yang, Wenjie Wei, Ammar Belatreche, Shuai Wang, Malu Zhang, Yang Yang","Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at \url{https://github.com/hamingsi/BSO}.","How can we create truly efficient brain-like computers that use simple binary switches and learn from timing patterns? Scientists have been developing neural networks that combine two powerful ideas: processing information through brief spikes (like real neurons firing) and using only binary connections that are either fully ""on"" (+1) or completely ""off"" (-1). This combination promises revolutionary efficiency—imagine a computer that thinks in rapid spikes using only simple switches. But there's been a fundamental contradiction: training these spike-and-switch networks required secretly maintaining precise decimal backup copies of every connection.Our breakthrough makes both the spikes and switches genuinely efficient. Our Binary Spiking Online (BSO) method eliminates all hidden decimal weights, treating connections as true binary switches that flip based on accumulated feedback momentum. Our advanced T-BSO version adds a crucial insight: it tracks how learning signals change over time, automatically adjusting sensitivity when spike patterns are weak versus when they're chaotic. This temporal awareness—understanding that brain-like networks naturally have different learning rhythms."
Poster,Byzantine-Resilient Federated Alternating Gradient Descent and Minimization for Partly-Decoupled Low Rank Matrix Learning,https://ICML.cc//virtual/2025/poster/44370,"Ankit Pratap Singh, Ahmed Abbasi, Namrata Vaswani","This work has two contributions. First, we introduce a provably secure (Byzantine-resilient) sample- and communication-efficient alternating gradient descent (GD) and minimization based algorithms for solving the federated low rank matrix completion (LRMC) problem. This involves learning a low rank (LR) matrix from a small subset of its entries. Second, we extend our ideas to show how a special case of our algorithms also solves another partly-decoupled vertically federated LR matrix learning problem, that is  LR column-wise sensing (LRCS), also referred to as multi-task linear representation learning in the literature. Finally, we also show how our results can be extended for the LR phase retrieval problem.  In all problems, we consider column-wise or vertical federation, i.e. each node observes a small subset of entries of a disjoint column sub-matrix of the entire LR matrix. For the LRMC problem, horizontal federation is equivalent since it is symmetric across rows and columns; while for the other two it is not. In all problems, the data at  different nodes is heterogeneous (not identically distributed), making it harder to obtain provable guarantees.","Imagine you want to predict lung disease using chest X-rays collected from different hospitals. To protect patient privacy, each hospital trains a model locally and only shares updates. However, some hospitals might use unreliable or compromised systems that send incorrect updates. These issues, known as Byzantine failures, can harm the overall learning process.We developed a method called Byzantine Resilient Federated Alternating Gradient Descent that can still train accurate models even when several participants are Byzantine. It uses robust statistics and takes advantage of the low-rank structure in the data to learn in a sample efficient way.It works for many low rank matrix recovery problems. One useful example is a web-based recommender system that suggests movies to users based on their ratings and reviews. Our method also applies to compressed sensing, used in Magnetic Resonance Imaging to speed up scans. It also reduces training time by allowing large language models to be trained faster using multiple GPUs or servers."
Poster,C2IQL: Constraint-Conditioned Implicit Q-learning for Safe Offline Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46250,"Zifan LIU, Xinran Li, Jun Zhang","Safe offline reinforcement learning aims to develop policies that maximize cumulative rewards while satisfying safety constraints without the need for risky online interaction. However, existing methods often struggle with the out-of-distribution (OOD) problem, leading to potentially unsafe and suboptimal policies. To address this issue, we first propose Constrained Implicit Q-learning (CIQL), a novel algorithm designed to avoid the OOD problem. In particular, CIQL expands the implicit update of reward value functions to constrained settings and then estimates cost value functions under the same implicit policy. Despite its advantages, the further performance improvement of CIQL is still hindered by the inaccurate discounted approximations of constraints. Thus, we further propose Constraint-Conditioned Implicit Q-learning (C2IQL). Building upon CIQL, C2IQL employs a cost reconstruction model to derive non-discounted cumulative costs from discounted values and incorporates a flexible, constraint-conditioned mechanism to accommodate dynamic safety constraints. Experiment results on DSRL benchmarks demonstrate the superiority of C2IQL compared to baseline methods in achieving higher rewards while guaranteeing safety constraints under different threshold conditions.","In safe offline reinforcement learning (SORL), a key challenge is to maximize rewards while ensuring constraint satisfaction, all from a pre-collected dataset. This difficulty is raised by the out-of-distribution (OOD) problem, where actions not in the dataset can be wrongly estimated during the Bellman backup process, leading to unsafe and suboptimal policies, especially in safety-sensitive applications.To address this, we developed Constraint-Conditioned Implicit Q-learning (C2IQL), which updates policies and value functions entirely within the dataset in constrained settings. This procedure is achieved based on expectile regression without querying any action out of the dataset. Additionally, we introduce a cost reconstruction model alongside a constraint-conditioned mechanism to ensure accurate and dynamic adherence to safety constraints.This research significantly advances the reliability and effectiveness of SORL by addressing the OOD problem completely. Besides, our findings also highlight a critical gap between non-discounted cost constraints and discounted value formulations in RL and provides a cost reconstruction model to address this gap."
Poster,C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation,https://ICML.cc//virtual/2025/poster/44398,"Guoxin Chen, Minpeng Liao, Peiying Yu, Dingmin Wang, Zile Qiao, Chao Yang, Xin Zhao, Kai Fan","Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior—typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.","Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior—typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities."
Poster,Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing,https://ICML.cc//virtual/2025/poster/44902,"Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, Long Chen","With the advance of diffusion models, today's video generation has achieved impressive quality. To extend the generation length and facilitate real-world applications, a majority of video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent clips conditioned on the last frame(s) of the previous clip. However, existing autoregressive VDMs are highly inefficient and redundant: The model must re-compute all the conditional frames that are overlapped between adjacent clips. This issue is exacerbated when the conditional frames are extended autoregressively to provide the model with long-term context. In such cases, the computational demands increase significantly (i.e., with a quadratic complexity w.r.t. the autoregression step). In this paper, we propose **Ca2-VDM**, an efficient autoregressive VDM with **Ca**usal generation and **Ca**che sharing. For **causal generation**, it introduces unidirectional feature computation, which ensures that the cache of conditional frames can be precomputed in previous autoregression steps and reused in every subsequent step, eliminating redundant computations. For **cache sharing**, it shares the cache across all denoising steps to avoid the huge cache storage cost. Extensive experiments demonstrated that our Ca2-VDM achieves state-of-the-art quantitative and qualitative video generation results and significantly improves the generation speed. Code is available: https://github.com/Dawn-LX/CausalCache-VDM","Nowadays, video synthesis technology has achieved impressive results,  thanks to a technology called ""video diffusion models"" (VDMs). Each video frame is synthesized through multiple iterations following the diffusion mechanism. Current methods generate videos in short clips, using previously generated clips to create new ones. However, these approaches are slow and repetitive. Because they wasted too much time recalculating generated frames (overlapped frames between chunks) when using them as references.Our paper introduces Ca2-VDM, a new method designed to make video generation faster and more efficient. It uses two key ideas: **causal generation** and **cache sharing**.  Causal generation means each frame is computed based only on the frames that came before it, so frame information can be pre-calculated and stored for future use. Cache sharing allows the method to reuse this stored information (i.e., cache) throughout all diffusion iterations, helping to use much less computer memory.With these innovations, our method speeds up video synthesis and reduces the amount of computer memory needed, all while maintaining the quality of the videos produced."
Poster,CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging,https://ICML.cc//virtual/2025/poster/44845,"Zongzhen Yang, Binhang Qi, Hailong Sun, Wenrui Long, Ruobing Zhao, Xiang Gao","Model merging based on task vectors, i.e., the parameter differences between fine-tuned models and a shared base model, provides an efficient way to integrate multiple task-specific models into a multitask model without retraining. Recent works have endeavored to address the conflicts between task vectors, one of the significant challenges faced by model merging, through sparsification; however, two issues significantly limit their performance: *high parameter overlap* and *unbalanced weight distribution*. To address these issues, we propose a simple yet effective framework called **CABS** (Conflict-Aware and Balanced Sparsification), consisting of **C**onflict-**A**ware Sparsification (CA) and **B**alanced **S**parsification (BS). CA reduces parameter overlap by applying masks during sequential pruning, ensuring that each task vector retains distinct, non-overlapping parameters. BS leverages $n$:$m$ pruning to preserve critical weights while maintaining an even distribution across layers. Our comprehensive experiments demonstrate that CABS outperforms state-of-the-art methods across diverse tasks and model sizes.","(1) Problem: Imagine you have several AI models, each an expert in a different task (like one for translating French, another for German). Combining them into a single, multi-talented AI is desirable but tricky. A common method, ""model merging,"" often fails because the experts' internal ""knowledge"" clashes. Current attempts to simplify this knowledge to reduce clashes often leave too much overlap between experts, or the remaining important knowledge is unevenly distributed, leading to a poorly performing merged AI.(2) Solution: We developed CABS (Conflict-Aware and Balanced Sparsification). First, its ""Conflict-Aware"" part carefully simplifies each expert's knowledge one by one, ensuring their unique contributions don't get muddled by using separate, non-overlapping internal ""brain regions."" Second, its ""Balanced Sparsification"" part ensures that the most critical remaining knowledge is evenly spread throughout the model, not all clumped in one area, preventing performance bottlenecks.(3) Impact: CABS creates merged AI models that perform significantly better across a wide variety of tasks and model sizes. In our experiments, models merged with CABS were more versatile and often even outperformed a hypothetical ""ideal"" scenario where one would just pick the single best expert for each individual task. This research helps build more efficient and powerful multi-talented AI systems."
Poster,Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models,https://ICML.cc//virtual/2025/poster/46067,"Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Surkov Nikita, Ivan Ermakov, Dan Alistarh","Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of gigabytes of device memory, as they store vector representations for each token and layer. Recent work has shown that the cached vectors can be compressed through quantization, pruning or merging, but these techniques often compromise quality towards higher compression rates. In this work, we aim to improve Key \& Value compression by exploiting two observations: 1) the inherent dependencies between keys and values across different layers, and 2) the existence of high-compression methods for internal network states (e.g. attention Keys \& Values). We propose AQUA-KV, an adaptive quantization for Key-Value caches that relies on compact adapters to exploit existing dependencies between Keys and Values, and aims to ""optimally"" compress the information that cannot be predicted. AQUA-KV significantly improves compression rates, while maintaining high accuracy on state-of-the-art LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5 bits per value with under $1\%$ relative error in perplexity and LongBench scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a single GPU within 1-6 hours, even for 70B models.","When LLMs generate text, they need to maintain a memory of previous tokens in the form of attention keys and values: tens of thousands of numbers for each token.For tasks where LLM deals with long texts, this adds up to tens of gigabytes of GPU memory for every sequence in a batch.To avoid running out of GPU memory, people have been compressing KV vectors — quantizing or pruning them.We propose a better way of compressing these keys and values: instead of quantizing them individually, we exploit the mutual information between different layers to quantize them together.Our approach fits a simple linear classifier to predict adjacent layer key-values and only store the part that cannot be predicted.This allows us to compress KV vectors with significantly better accuracy, especially for extreme 2-bit quantization."
Poster,CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation,https://ICML.cc//virtual/2025/poster/45732,"Aditya Gorla, Ryan Wang, Zhengtong Liu, Ulzee An, Sriram Sankararaman","We present CACTI, a masked autoencoding approach for imputing tabular data that leverages the structure in missingness patterns and contextual information. Our approach employs a novel median truncated copy masking training strategy that encourages the model to learn from empirical patterns of missingness while incorporating semantic relationships between features — captured by column names and text descriptions — to better represent feature dependence. These dual sources of inductive bias enable CACTIto outperform state-of-the-art methods —  an average $R^2$ gain of 7.8\% over the next best method (13.4%, 6.1%, and 5.3% under missing not at random, at random and completely at random, respectively) — across a diverse range of datasets and missingness conditions. Our results highlight the value of leveraging dataset-specific contextual information and missingness patterns to enhance imputation performance.","Imagine trying to complete a puzzle where some pieces are missing—this is what data scientists face daily when working with incomplete datasets. Missing information in medical records, survey responses, or business data can lead to flawed analyses and poor decisions. Current methods for filling these gaps treat all missing data the same way, like assuming puzzle pieces disappeared randomly. We created CACTI, a new machine learning approach that recognizes that data often goes missing in patterns. CACTI learns these real-world patterns by reusing observed missingness structures to improve its predictions for filling in missing data. CACTI also reads column descriptions to understand relationships between different types of information, much like understanding that ""blood pressure"" and ""heart rate"" are related health measurements. When tested on real datasets, CACTI outperformed the best existing methods by an average of 7.8%, reaching up to 13.4% improvement in the most complex cases. This means researchers and organizations can now extract more accurate insights from incomplete data, more reliable findings, better analysis and improved downstream decisions—all from the same imperfect datasets they already have."
Poster,CaDA: Cross-Problem Routing Solver with Constraint-Aware Dual-Attention,https://ICML.cc//virtual/2025/poster/46063,"Han Li, Fei Liu, Zhi Zheng, Yu Zhang, Zhenkun Wang","Vehicle routing problems (VRPs) are significant combinatorial optimization problems (COPs) holding substantial practical importance. Recently, neural combinatorial optimization (NCO), which involves training deep learning models on extensive data to learn vehicle routing heuristics, has emerged as a promising approach due to its efficiency and the reduced need for manual algorithm design. However, applying NCO across diverse real-world scenarios with various constraints necessitates cross-problem capabilities. Current cross-problem NCO methods for VRPs typically employ a constraint-unaware model, limiting their cross-problem performance. Furthermore, they rely solely on global connectivity, which fails to focus on key nodes and leads to inefficient representation learning. This paper introduces a \underline{C}onstraint-\underline{A}ware \underline{D}ual-\underline{A}ttention Model (CaDA), designed to address these limitations. CaDA incorporates a constraint prompt that efficiently represents different problem variants. Additionally, it features a dual-attention mechanism with a global branch for capturing broader graph-wide information and a sparse branch that selectively focuses on the key node connections. We comprehensively evaluate our model on 16 different VRPs and compare its performance against existing cross-problem VRP solvers. CaDA achieves state-of-the-art results across all tested VRPs. Our ablation study confirms that each component contributes to its cross-problem learning performance. The source code for CaDA is publicly available at \url{https://github.com/CIAM-Group/CaDA}.","Real-world routing problems involving multiple attributes are challenging. Current neural solvers usually require training separate models for each type of problem, making them costly and impractical for companies. We developed CaDA, a neural solver capable of solving diverse routing problems using just one model. CaDA introduces two key innovations: first, it clearly informs the model about the specific rules (such as delivery time windows) using a constraint prompt. Second, it adoptes a dual-attention approach, combining a big-picture view of the entire routing network with a focused view of key nodes that are most informative and important for next step route optimization. CaDA outperforms existing methods across 16 different types of vehicle routing problems."
Poster,CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing,https://ICML.cc//virtual/2025/poster/44580,"Yu Yuan, Shizhao Sun, Qi Liu, Jiang Bian","Computer Aided Design (CAD) is indispensable across various industries. \emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored.Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints.We introduce \emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions.To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge.Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.","Designing 3D models for things like cars, machines, or buildings usually requires complex software and skilled human effort. But what if we could just tell the computer, in plain language, how to change a design — like saying “reduce the cylinder's height by half” or “drill four smaller holes through corners”?Our research introduces a new system called CAD-Editor that makes this possible. It allows people to modify existing 3D models using simple text instructions. To train this system, we created a large dataset by automatically generating design variations and describing the changes with the help of advanced AI models that understand both images and language.CAD-Editor breaks the editing task into two steps: first, it finds which part of the design needs to change, and then it figures out how to change it correctly. We use powerful large language models to handle both parts of the task.This work brings us closer to making computer-aided design more accessible, efficient, and intuitive — even for non-experts."
