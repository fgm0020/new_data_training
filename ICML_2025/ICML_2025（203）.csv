type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"One Arrow, Two Hawks: Sharpness-aware Minimization for Federated Learning via Global Model Trajectory",https://ICML.cc//virtual/2025/poster/46296,"Yuhang Li, Tong Liu, Yangguang Cui, Ming Hu, Xiaoqiang Li","Federated learning (FL) presents a promising strategy for distributed and privacy-preserving learning, yet struggles with performance issues in the presence of heterogeneous data distributions. Recently, a series of works based on sharpness-aware minimization (SAM) have emerged to improve local learning generality, proving to be effective in mitigating data heterogeneity effects.However, most SAM-based methods do not directly consider the global objective and require two backward pass per iteration, resulting in diminished effectiveness.To overcome these two bottlenecks, we leverage the global model trajectory to directly measure sharpness for the global objective, requiring only a single backward pass.We further propose a novel and general algorithm FedGMT to overcome data heterogeneity and the pitfalls of previous SAM-based methods.We analyze the convergence of FedGMT and conduct extensive experiments on visual and text datasets in a variety of scenarios, demonstrating that FedGMT achieves competitive accuracy with state-of-the-art FL methods while minimizing computation and communication overhead. Code is available at https://github.com/harrylee999/FL-SAM.","In federated learning (FL), data heterogeneity causes local models to diverge, harming global performance. Existing sharpness-aware methods are costly and miss global smoothness. We propose **FedGMT**, using a **Global Model Trajectory** with exponential moving averages (EMA) and KL divergence to guide local updates toward global flat minima. ADMM aligns local-global directions. Experiments show FedGMT cuts computation by 33%, converges at \(O(1/T)\), and outperforms baselines on diverse datasets, even with high heterogeneity. A lighter variant, FedGMT-v2, reduces communication for real-world use. This work enhances FL efficiency and privacy-preserving collaboration on edge devices."
Poster,One Diffusion Step to Real-World Super-Resolution via Flow Trajectory Distillation,https://ICML.cc//virtual/2025/poster/43875,"Jianze Li, Jiezhang Cao, Yong Guo, Wenbo Li, Yulun Zhang","Diffusion models (DMs) have significantly advanced the development of real-world image super-resolution (Real-ISR), but the computational cost of multi-step diffusion models limits their application. One-step diffusion models generate high-quality images in a one sampling step, greatly reducing computational overhead and inference latency. However, most existing one-step diffusion methods are constrained by the performance of the teacher model, where poor teacher performance results in image artifacts. To address this limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique based on flow matching models. We use the state-of-the-art diffusion model FLUX.1-dev as both the teacher model and the base model. First, we introduce Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step Real-ISR. Second, to improve image realism and address high-frequency artifact issues in generated images, we propose TV-LPIPS as a perceptual loss and introduce Attention Diversification Loss (ADL) as a regularization term to reduce token similarity in transformer, thereby eliminating high-frequency artifacts. Comprehensive experiments demonstrate that our method outperforms existing one-step diffusion-based Real-ISR methods. The code and model will be released at \url{https://github.com/JianzeLi-114/FluxSR}.","Imagine you’ve taken a grainy, low-resolution photo with your phone and wish you could see every detail as if it were captured by a high-end camera. Recent AI systems called *diffusion models* can do this by gradually “painting in” missing details, but they normally need to repeat this process many times—like an artist layering dozens of brushstrokes—so they run slowly and demand a lot of computing power. Our work, **FluxSR**, shows how to achieve the same sharp results in just one swift step. We start with today’s best multi-step model and teach a lighter “student” version to jump straight to the finished picture without those extra strokes. To keep faces, textures and edges looking natural—without the strange speckles that sometimes appear—we add two new training tricks: one that judges realism more like a human viewer would, and another that nudges the AI to pay attention to varied visual cues instead of repeating itself. The result is a faster, more efficient tool that upgrades blurry images to crisp, photo-like quality in a fraction of a second, opening the door to smooth super-resolution on everyday devices and in real-time applications such as video calls or mobile photography."
Poster,One-dimensional Path Convolution,https://ICML.cc//virtual/2025/poster/46220,"Xuanshu Luo, Martin Werner","Two-dimensional (2D) convolutional kernels have dominated convolutional neural networks (CNNs) in image processing. While linearly scaling 1D convolution provides parameter efficiency, its naive integration into CNNs disrupts image locality, thereby degrading performance. This paper presents path convolution (PathConv), a novel CNN design exclusively with 1D operations, achieving ResNet-level accuracy using only 1/3 parameters. To obtain locality-preserving image traversal paths, we analyze Hilbert/Z-order paths and expose a fundamental trade-off: improved proximity for most pixels comes at the cost of excessive distances for other sacrificed ones to their neighbors. We resolve this issue by proposing path shifting, a succinct method to reposition sacrificed pixels. Using the randomized rounding algorithm, we show that three shifted paths are sufficient to offer better locality preservation than trivial raster scanning. To mitigate potential convergence issues caused by multiple paths, we design a lightweight path-aware channel attention mechanism to capture local intra-path and global inter-path dependencies. Experimental results further validate the efficacy of our method, establishing the proposed 1D PathConv as a viable backbone for efficient vision models.","Modern computer vision systems use complex 2D operations to analyze images, making them computationally expensive and resource-intensive. While 1D operations could be more efficient, they typically disrupt the natural neighborhood relationships between pixels, leading to poor performance.We developed Path Convolution (PathConv), a novel approach that processes images using only 1D operations. We preserve local pixel relationships by carefully designing special pixel traversal paths and implementing a path-shifting technique. We adopt randomized rounding algorithms to show that just three carefully shifted paths provide better locality preservation than traditional raster scanning methods. We also created a lightweight attention mechanism that helps the system understand relationships within and between these paths.Our PathConv models achieve a similar level of accuracy as standard methods (ResNet) while using only one-third of model parameters, which makes image processing more accessible for devices with limited computing power, potentially enabling more efficient computer vision capabilities on smartphones, IoT devices, and other resource-constrained systems."
Poster,"One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",https://ICML.cc//virtual/2025/poster/46191,"Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, Philip Yu","Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of ""proof by counterexamples"" commonly used in human mathematics education, our work aims to enhance LLMs’ ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, COUNTERMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that COUNTERMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.","This paper focuses on enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) by addressing their reliance on ""drill-based learning"" (memorizing proof patterns from training data), which limits their deep understanding of mathematical concepts. Inspired by how humans use counterexamples to learn theorems (e.g., identifying exceptions to test the validity of statements), the researchers developed **COUNTERMATH**, a benchmark that evaluates LLMs' ability to prove or disprove mathematical statements using counterexamples.  This work highlights the need for LLMs to move beyond memorization and develop deeper conceptual understanding, with COUNTERMATH serving as a critical tool to measure this progress. The findings suggest that integrating counterexample-based reasoning into LLM training could unlock more human-like mathematical thinking, benefiting fields like theorem proving and academic research."
Poster,On Efficient Estimation of Distributional Treatment Effects under Covariate-Adaptive Randomization,https://ICML.cc//virtual/2025/poster/44271,"Undral Byambadalai, Tomu Hirata, Tatsushi Oka, Shota Yasui","This paper focuses on the estimation of distributional treatment effects in randomized experiments that use covariate-adaptive randomization (CAR). These include designs such as Efron's biased-coin design and stratified block randomization, where participants are first grouped into strata based on baseline covariates and assigned treatments within each stratum to ensure balance across groups. In practice, datasets often contain additional covariates beyond the strata indicators. We propose a flexible distribution regression framework that leverages off-the-shelf machine learning methods to incorporate these additional covariates, enhancing the precision of distributional treatment effect estimates. We establish the asymptotic distribution of the proposed estimator and introduce a valid inference procedure. Furthermore, we derive the semiparametric efficiency bound for distributional treatment effects under CAR and demonstrate that our regression-adjusted estimator attains this bound. Simulation studies and analyses of real-world datasets highlight the practical advantages of our method.","When researchers evaluate new interventions—whether a medical treatment, education policy, or product feature—they often randomize participants so treatment and control groups look alike on a few key traits such as age or gender.  Yet the data they collect record many other details, and those are usually ignored. We show how to feed these extra details into off-the-shelf machine-learning tools to track how the full range of outcomes—winners, losers, and everyone in between—shifts under the interventions, not just the average effect. Our mathematical results show the method squeezes every drop of information the data can offer, so no rival approach can do better. Sharper, distribution-wide evidence helps policymakers target resources where they matter most instead of relying on one-size-fits-all averages  drawn from traditional randomized trials."
Poster,OneForecast: A Universal Framework for Global and Regional Weather Forecasting,https://ICML.cc//virtual/2025/poster/46192,"Yuan Gao, Hao Wu, Ruiqi Shu, huanshuo dong, Fan Xu, Rui Chen, Yibo Yan, Qingsong Wen, Xuming Hu, Kun Wang, Jiahao Wu, Qing Li, Hui Xiong, Xiaomeng Huang","Accurate weather forecasts are important for disaster prevention, agricultural planning, etc. Traditional numerical weather prediction (NWP) methods offer physically interpretable high-accuracy predictions but are computationally expensive and fail to fully leverage rapidly growing historical data. In recent years, deep learning models have made significant progress in weather forecasting, but challenges remain, such as balancing global and regional high-resolution forecasts, excessive smoothing in extreme event predictions, and insufficient dynamic system modeling. To address these issues, this paper proposes a global-regional nested weather forecasting framework (OneForecast) based on graph neural networks. By combining a dynamic system perspective with multi-grid theory, we construct a multi-scale graph structure and densify the target region to capture local high-frequency features. We introduce an adaptive messaging mechanism, using dynamic gating units to deeply integrate node and edge features for more accurate extreme event forecasting. For high-resolution regional forecasts, we propose a neural nested grid method to mitigate boundary information loss. Experimental results show that OneForecast performs excellently across global to regional scales and short-term to long-term forecasts, especially in extreme event predictions. Codes link: \url{https://github.com/YuanGao-YG/OneForecast}.","Accurate weather forecasts are crucial for protecting communities from disasters, planning agriculture, and managing daily life. Traditional methods of weather prediction utilize well-understood physical principles but require substantial computing power and don't fully leverage the vast amounts of historical weather data now available. Recently, artificial intelligence (AI) has shown promise in improving weather forecasts, but existing AI models still struggle with precisely forecasting extreme weather events and balancing global coverage with local detail.To tackle these challenges, we develop OneForecast, an AI-based weather prediction system that effectively combines large-scale global forecasting with detailed local predictions.Our approach enables seamless weather forecasting and more accurate prediction of extreme events. In summary, our approach fosters the development of next-generation data-driven weather forecasting systems."
Poster,One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework,https://ICML.cc//virtual/2025/poster/45434,"Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Xiaochun Cao, Qingming Huang","Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (**efficacy**) while minimizing the impact on other benign concepts (**usability**), as illustrated in Fig.1. In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (**Co-Erasing**) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability.","We aim to improve how stable diffusion models generators ""forget"" specific harmful or unwanted concepts. Current methods often struggle because they rely solely on written text prompts to describe what should be erased. This approach overlooks a key challenge: stable diffusion models interpret text and images differently, making it hard to fully remove the unwanted concept without affecting other useful ones.To overcome this, we introduce Co-Erasing, a new framework that combines both text and images to guide the erasure process. Instead of only describing what to erase with words, we also provide the model with actual images representing the concept. This joint representation helps the model better understand what should be removed and why.We also design a text-guided image refinement mechanism that fine-tunes the process, ensuring that only the targeted concept is affected while leaving others intact. This improves both the efficacy (removing the unwanted concept) and usability (keeping other concepts intact).Our experiments show that Co-Erasing improves previous approaches, offering a more accurate and reliable way to align image generation models with human intent."
Poster,One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with Semantic-Aware Views for Efficient Visual Representation,https://ICML.cc//virtual/2025/poster/43759,"Xiaoyu Yang, Lijian Xu, Hongsheng Li, Shaoting Zhang","This paper proposes a scalable and straightforward pre-training paradigm for efficient visual conceptual representation called occluded image contrastive learning (OCL). Our OCL approach is simple: we randomly mask patches to generate different views within an image and contrast them among a mini-batch of images. The core idea behind OCL consists of two designs. First, masked tokens have the potential to significantly diminish the conceptual redundancy inherent in images, and create distinct views with substantial fine-grained differences on the semantic concept level instead of the instance level. Second, contrastive learning is adept at extracting high-level semantic conceptual features during the pre-training, circumventing the high-frequency interference and additional costs associated with image reconstruction. Importantly, OCL learns highly semantic conceptual representations efficiently without relying on hand-crafted data augmentations or additional auxiliary modules. Empirically, OCL demonstrates high scalability with Vision Transformers, as the ViT-L/16 can complete pre-training in 133 hours using only 4 A100 GPUs, achieving 85.8\% accuracy in downstream fine-tuning tasks. Code is available at https://github.com/XiaoyuYoung/OCL.","The success of the pre-training with large models in NLP has prompted people to replicate this paradigm in the field of vision. However, visual pre-training requires a huge amount of computing resources that we cannot afford. Therefore, we designed a method to occlude most of the image to reduce the amount of pre-training calculations. This allows us to build larger and deeper visual models and speed up pre-training."
Poster,One-Pass Feature Evolvable Learning with Theoretical Guarantees,https://ICML.cc//virtual/2025/poster/44326,"Cun-Yuan Xing, Meng-Zhang Qian, Wu-Yang Chen, Wei Gao, Zhi-Hua Zhou","Feature evolvable learning studies the scenario where old features will vanish and new features will emerge when learning with data streams, and various methods have been developed by utilizing some useful relationships from old features to new features, rather than re-training from scratch. In this work, we focus on two fundamental problems: How to characterize the relationships between two different feature spaces, and how to exploit those relationships for feature evolvable learning. We introduce the Kernel Ortho-Mapping (KOM) discrepancy to characterize relationships between two different feature spaces via kernel functions, and correlate with the optimal classifiers learned from different feature spaces. Based on this discrepancy, we develop the one-pass algorithm for feature evolvable learning, which requires going through all instances only once without storing the entire or partial training data. Our basic idea is to take online kernel learning with the random Fourier features and incorporate some feature and label relationships via the KOM discrepancy for feature evolvable learning.  We finally validate the effectiveness of our proposed method both theoretically and empirically.","When the set of features used by a machine learning model changes, because some sensors degrade or new ones are introduced, our method enables a seamless transition without discarding prior knowledge. We begin by constructing a lightweight alignment between the old and new feature spaces to capture their most salient correlations. Using this alignment, we then perform a simple projection-based initialization that produces a “warm start” model in the new feature space. As new data points stream in under the revised feature regime, the algorithm updates its parameters in a single pass, eliminating the need to store or revisit past examples. Both our theoretical analysis and empirical results demonstrate that this approach converges nearly as rapidly and achieves accuracy comparable to fully retraining on the new features, yet does so with substantially reduced computational effort and memory requirements."
Poster,One-Shot Heterogeneous Federated Learning with Local Model-Guided Diffusion Models,https://ICML.cc//virtual/2025/poster/45366,"Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue","In recent years, One-shot Federated Learning (OSFL) methods based on Diffusion Models (DMs) have garnered increasing attention due to their remarkable performance. However, most of these methods require the deployment of foundation models on client devices, which significantly raises the computational requirements and reduces their adaptability to heterogeneous client models. In this paper, we propose FedLMG, a heterogeneous one-shot Federated learning method with Local Model-Guided diffusion models. In our method, clients do not need access to any foundation models but only train and upload their local models, which is consistent with traditional FL methods. On the clients, we employ classification loss and batch normalization loss to capture the broad category features and detailed contextual features of the client distributions. On the server, based on the uploaded client models, we utilize backpropagation to guide the server’s DM in generating synthetic datasets that comply with the client distributions, which are then used to train the aggregated model. By using the local models as a medium to transfer client knowledge, our method significantly reduces the computational requirements on client devices and effectively adapts to scenarios with heterogeneous clients. Extensive quantitation and visualization experiments on three large-scale real-world datasets, along with theoretical analysis, demonstrate that the synthetic datasets generated by FedLMG exhibit comparable quality and diversity to the client datasets, which leads to an aggregated model that outperforms all compared methods and even the performance ceiling, further elucidating the significant potential of utilizing DMs in FL.","We present a new one-shot federated learning method that enables devices to collaboratively train a model without sharing their private data. Unlike previous methods that require large, complex models on each device, the clients in our method train a lightweight model locally, which is then used by the server to guide a diffusion model that generates a synthetic dataset. This synthetic dataset compy the data distributions on the clients without leaking the privacy-sensitive information, enabling the server to build a strong aggregated model. Our method achieves strong results on real-world datasets and highlights the great potential of utilizing diffusion models in federated learning."
