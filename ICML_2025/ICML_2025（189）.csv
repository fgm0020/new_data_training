type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Modulated Diffusion:  Accelerating Generative Modeling with Modulated Quantization,https://ICML.cc//virtual/2025/poster/43551,"Weizhi Gao, Zhichao Hou, Junqi Yin, Feiyi Wang, Linyu Peng, Xiaorui Liu","Diffusion models have emerged as powerful generative models, but their high computation cost in iterative sampling remains a significant bottleneck. In this work, we present an in-depth and insightful study of state-of-the-art acceleration techniques for diffusion models, including caching and quantization, revealing their limitations in computation error and generation quality. To break these limits, this work introduces Modulated Diffusion (MoDiff), an innovative, rigorous, and principled framework that accelerates generative modeling through modulated quantization and error compensation. MoDiff not only inherents the advantages of existing caching and quantization methods but also serves as a general framework to accelerate all diffusion models. The advantages of MoDiff are supported by solid theoretical insight and analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate that MoDiff significant reduces activation quantization from 8 bits to 3 bits without performance degradation in post-training quantization (PTQ). Our code implementation is available at https://github.com/WeizhiGao/MoDiff.","Diffusion models are powerful generative frameworks capable of producing high-quality images. However, their generation process involves multiple iterative steps, leading to high computational costs. In this work, we investigate whether a unified framework can inherit the advantages of existing acceleration techniques to improve sampling efficiency.We propose MoDiff, a general framework that not only inherits the advantages of existing caching and quantization techniques but also provides a unified solution for accelerating various diffusion models. Both theoretical analysis and experimental results demonstrate that MoDiff effectively enhances the performance of existing quantization methods.MoDiff enables faster sampling, thereby facilitating the practical deployment of diffusion models and introducing a new direction for accelerating diffusion processes in research."
Poster,MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45086,"Yifu Yuan, Zhenrui Zheng, Zibin Dong, Jianye Hao","Multi-objective Reinforcement Learning (MORL) seeks to develop policies that simultaneously optimize multiple conflicting objectives, but it requires extensive online interactions. Offline MORL provides a promising solution by training on pre-collected datasets to generalize to any preference upon deployment. However, real-world offline datasets are often conservatively and narrowly distributed, failing to comprehensively cover preferences, leading to the emergence of out-of-distribution (OOD) preference areas. Existing offline MORL algorithms exhibit poor generalization to OOD preferences, resulting in policies that do not align with preferences. Leveraging the excellent expressive and generalization capabilities of diffusion models, we propose MODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs a preference-conditioned diffusion model as a planner to generate trajectories that align with various preferences and derive action for decision-making. To achieve accurate generation, MODULI introduces two return normalization methods under diverse preferences for refining guidance. To further enhance generalization to OOD preferences, MODULI proposes a novel sliding guidance mechanism, which involves training an additional slider adapter to capture the direction of preference changes. Incorporating the slider, it transitions from in-distribution (ID) preferences to generating OOD preferences, patching, and extending the incomplete Pareto front. Extensive experiments on the D4MORL benchmark demonstrate that our algorithm outperforms state-of-the-art Offline MORL baselines, exhibiting excellent generalization to OOD preferences.","Offline multi-objective reinforcement learning is a method that enables intelligent agents to automatically make decisions by balancing multiple objectives, using previously collected data for training instead of requiring extensive new experiments. This makes it particularly suitable for real-world applications. However, the data collected in practice often reflects only a limited range of preferences and choices, which means agents may struggle to make optimal decisions when faced with new or unseen objectives. To address this challenge, we propose a novel approach called MODULI. MODULI leverages advanced diffusion models to understand better and adapt to diverse objective preferences. Additionally, we introduce a new technique that helps the model gradually adjust from familiar to unfamiliar preferences, effectively filling in the gaps left by the original dataset. Extensive experiments demonstrate that MODULI not only achieves strong performance on known preferences but also generalizes well to novel, previously unseen objectives. This advancement paves the way for more flexible and versatile decision-making in real-world scenarios such as robotics and autonomous driving."
Poster,MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance,https://ICML.cc//virtual/2025/poster/46674,"Zhixuan Chen, Xing Hu, Dawei Yang, Zukang Xu, XUCHEN, Zhihang Yuan, Sifan Zhou, JiangyongYu","Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic routing and sparse activation to enhance efficiency and scalability, have achieved higher performance while reducing computational costs. However, these models face significant memory overheads, limiting their practical deployment and broader adoption. Post-training quantization (PTQ), a widely used method for compressing LLMs, encounters severe accuracy degradation and diminished generalization performance when applied to MoE models. This paper investigates the impact of MoE’s sparse and dynamic characteristics on quantization and identifies two primary challenges: (1) Inter-expert imbalance, referring to the uneven distribution of samples across experts, which leads to insufficient and biased calibration for less frequently utilized experts; (2) Intra-expert imbalance, arising from MoE's unique aggregation mechanism, which leads to varying degrees of correlation between different samples and their assigned experts. To address these challenges, we propose MoEQuant, a novel quantization framework tailored for MoE LLMs. MoEQuant includes two novel techniques: 1) Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that efficiently constructs a calibration set with balanced expert distributions by leveraging the cumulative probabilities of tokens and expert balance metrics as guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates affinities between experts and samples into the quantization process, thereby accurately assessing the impact of individual samples on different experts within the MoE layer. Experiments demonstrate that MoEQuant achieves substantial performance gains (more than 10 points accuracy gain in the HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.","Large language models (LLMs) have made remarkable progress, and Mixture-of-Experts (MoE) LLMs, with their dynamic routing and sparse activation, offer high performance at a reduced cost. However, when it comes to practical deployment, these models face a significant hurdle - high memory demands. Post-training quantization (PTQ), a common method for shrinking LLMs, doesn't work well with MoE LLMs, causing a sharp decline in accuracy.The root of the problem lies in two imbalances within MoE LLMs. First, samples are unevenly distributed among experts. Some experts get a lot of samples during calibration, while others get too few, leading to inaccurate calibration. Second, the connection strength between samples and their assigned experts varies, but traditional PTQ methods overlook this.To solve these issues, the researchers developed MoEQuant. It's like a smart toolkit for MoE LLMs. One part of it, Expert-Balanced Self-Sampling (EBSS), is a bit like a careful gardener. It uses the model's own ability to sample data and some guiding factors to create a calibration set where all experts are used evenly. This set is also a good match for the model's original data distribution. The other part, Affinity-Guided Quantization (AGQ), is like a precision - tuning device. It takes into account the connection strength between samples and experts during the quantization process. This makes the calculation of quantization errors more accurate and helps the model perform better.Tests on different MoE LLMs, such as DeepSeek-MoE-16B, Qwen-MoE-14B, and Mixtral-8x7B, show that MoEQuant is very effective. It can improve the model's performance significantly, even when using low-bit quantization. It also boosts the model's generalization ability, especially in instruction - tuned models. What's more, MoEQuant can speed up inference and save a lot of memory, making it possible to run MoE LLMs on regular consumer - level devices. Overall, MoEQuant is a big step forward in making MoE LLMs more practical and accessible."
Poster,MoE-SVD: Structured Mixture-of-Experts LLMs Compression via Singular Value Decomposition,https://ICML.cc//virtual/2025/poster/44786,"Wei Li, Lujun Li, Hao Gu, Youliang Huang, Mark Lee, Shengjie Sun, Wei Xue, Yike Guo","Mixture of Experts (MoE) architecture improves Large Language Models (LLMs) with better scaling, but its higher parameter counts and memory demands create challenges for deployment. In this paper, we present MoE-SVD, a new decomposition-based compression framework tailored for MoE LLMs without any extra training. By harnessing the power of Singular Value Decomposition (SVD), MoE-SVD addresses the critical issues of decomposition collapse and matrix redundancy in MoE architectures.   Specifically, we first decompose experts into compact low-rank matrices, resulting in accelerated inference and memory optimization. In particular, we propose selective decomposition strategy by measuring sensitivity metrics based on weight singular values and activation statistics to automatically identify decomposable expert layers. Then, we share a single V-matrix across all experts and employ a top-k selection for U-matrices. This low-rank matrix sharing and trimming scheme allows for significant parameter reduction while preserving diversity among experts.  Comprehensive experiments on Mixtral, Phi-3.5, DeepSeek, and Qwen2 MoE LLMs show MoE-SVD outperforms other compression methods, achieving a 60\% compression ratio and 1.5× faster inference with minimal performance loss. Codes are available at: https://github.com/lliai/MoE-SVD.","We introduce MoE-SVD, a decomposition-based compression approach specifically designed for Mixture of Experts (MoE) Large Language Models (LLMs). Leveraging Singular Value Decomposition (SVD), our method reduces parameter redundancy and memory requirements without requiring additional training. We propose selective decomposition using sensitivity metrics, employing a shared V-matrix across experts and trimming U-matrices through top-k selection. Experiments conducted on various MoE models such as Mixtral, Phi-3.5, DeepSeek, and Qwen2 demonstrate a 60% compression ratio and 1.5× faster inference speed with minimal performance degradation."
Poster,MOGIC: Metadata-infused Oracle Guidance for Improved Extreme Classification,https://ICML.cc//virtual/2025/poster/43708,"Suchith Chidananda Prabhu, Bhavyajeet Singh, Anshul Mittal, Siddarth Asokan, Shikhar Mohan, Deepak Saini, Yashoteja Prabhu, Lakshya Kumar, Jian Jiao, Amit Singh, Niket Tandon, Manish Gupta, Sumeet Agarwal, Manik Varma","Retrieval-augmented classification and generation models benefit from *early-stage fusion* of high-quality text-based metadata, often called memory, but face high latency and noise sensitivity. In extreme classification (XC), where low latency is crucial, existing methods use *late-stage fusion* for efficiency and robustness. To enhance accuracy while maintaining low latency, we propose MOGIC, a novel approach to metadata-infused oracle guidance for XC. We train an early-fusion oracle classifier with access to both query-side and label-side ground-truth metadata in textual form and subsequently use it to guide existing memory-based XC disciple models via regularization. The MOGIC algorithm improves precision@1 and propensity-scored precision@1 of XC disciple models by 1-2% on six standard datasets, at no additional inference-time cost. We show that MOGIC can be used in a plug-and-play manner to enhance memory-free XC models such as NGAME or DEXA. Lastly, we demonstrate the robustness of the MOGIC algorithm to missing and noisy metadata. The code is publicly available at [https://github.com/suchith720/mogic](https://github.com/suchith720/mogic).","In a text classification task, given a query, we aim to predict the labels relevant to that query. In many cases, additional information about the query or the label is available (called metadata or memory) and can be leveraged to make better predictions. Currently popular methods, such as RAG, which retrieve this metadata, and augment it with the model input for classification or generation, tend to have high latency and are sensitive to noise.We propose a two-stage technique to utilize this extra information while also meeting the latency constraints. First, we train a powerful oracle model that takes advantage of the metadata assuming a best-case scenario where this information is also available during inference. Then, we train an off-the-shelf classifier model as a disciple that learns to mimic the behaviour of the oracle, but under a more challenging scenario, wherein the metadata is not know apriori. In this way, we get the best of both worlds — A model that is both efficient and fast, while benefiting from the improved accuracy gained by learning from the powerful oracle model.We observe that this training technique (which we call Metadata-infused Oracle Guidance for Improved Extreme Classification, or MOGIC) improves overall accuracy of any existing classifier, while also offering a novel approach to incorporating extra information into the classification settings."
Poster,MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition,https://ICML.cc//virtual/2025/poster/44834,"Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun","Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems.","Understanding speech in noisy environments is difficult for both humans and machines. While combining audio and visual signals (like lip movements) can help, current audio-visual speech recognition (AVSR) systems struggle to scale without becoming inefficient. To solve this, we develop MoHAVE, a new AI model that smartly divides the work between specialized audio and visual components. These components, called “experts”, are activated based on the input, so the model only uses what it needs—saving time and computing power. Our approach includes a clever decision system that learns how to balance these expert groups dynamically, depending on the level and type of noise. As a result, MoHAVE is both more accurate and more efficient than existing AVSR models."
Poster,MoH: Multi-Head Attention as Mixture-of-Head Attention,https://ICML.cc//virtual/2025/poster/44579,"Peng Jin, Bo Zhu, Li Yuan, Shuicheng YAN","In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to reduce computational costs while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%$\sim$90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.","Transformers are the backbone of many modern AI models, helping computers understand language, recognize images, and even generate art. A key part of how transformers work is called “multi-head attention,” which allows the model to focus on different parts of the input simultaneously. However, not all of these attention “heads” are equally useful—some do more work than others. In our research, we introduce a smarter version of this system called Mixture-of-Head Attention (MoH). Instead of using all attention heads all the time, MoH lets the model choose only the most helpful ones for each piece of input, like picking the best team members for a job. This makes the model faster and more efficient without hurting its performance—and often makes it even better. We tested MoH on a range of tasks, including understanding images, generating pictures, and answering questions. In every case, MoH matched or outperformed traditional methods, even when using fewer attention heads. We also showed that existing models, like LLaMA3, can be upgraded to use MoH, making it easy to apply our method to today’s top AI systems. MoH is a promising step toward making powerful AI models faster, cheaper, and more adaptable."
Poster,Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts,https://ICML.cc//virtual/2025/poster/45201,"Xu Liu, Juncheng Liu, Gerald Woo, Taha Aksu, Yuxuan Liang, Roger Zimmermann, Chenghao Liu, Junnan Li, Silvio Savarese, Caiming Xiong, Doyen Sahoo","Achieving effective unified pretraining on large time series corpora remains an open challenge in developing time series foundation models. Existing methods, such as Moirai, introduce multiple projection layers for time series of different frequencies to account for high data heterogeneity. We identify major drawbacks to this human-imposed frequency-level model specialization. First, frequency is not a reliable indicator for grouping pretraining data. Second, time series can display varied distributions even within a short window. Frequency-level specialization overlooks the diversity at this granularity. To address these issues, this paper introduces Moirai-MoE, excluding human-defined data groupings while delegating the modeling of diverse time series patterns to the sparse mixture of experts (MoE) within Transformers. With this design, Moirai-MoE eliminates reliance on heuristics and enables automatic token-level specialization. Extensive evaluations on 39 datasets demonstrate the superiority of Moirai-MoE over state-of-the-art foundation models. This study also conducts comprehensive model analyses to explore the inner workings of time series MoE foundation models.","Time series data — like temperature changes, stock prices, or heart rate signals — can vary widely in how fast they change and what patterns they show. Traditional AI models try to handle this variety by grouping the data based on how frequently it changes. However, this approach has serious limitations: data that changes at the same speed can still look very different, and data that changes at different speeds can sometimes follow similar patterns. Our research proposes a new method called Moirai-MoE that avoids these rigid groupings. Instead of relying on human-defined categories, our model uses a technique called a “Mixture of Experts” to let the AI automatically specialize based on the patterns it sees in the data. This allows the model to adapt more flexibly and accurately to the rich variety found in real-world time series. We tested Moirai-MoE on 39 diverse datasets and found it consistently outperformed current top models. This work advances our ability to build general-purpose AI systems that can better understand and learn from complex time-based data."
Poster,MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition,https://ICML.cc//virtual/2025/poster/46159,"Yuhuan Yang, Chaofan Ma, Zhenjie Mao, Jiangchao Yao, Ya Zhang, Yanfeng Wang","Video understanding is a complex challenge that requires effective modeling of spatial-temporal dynamics. With the success of image foundation models (IFMs) in image understanding, recent approaches have explored parameter-efficient fine-tuning (PEFT)  to adapt IFMs for video. However, most of these methods tend to processspatial and temporal information separately,which may fail to capture the full intricacy of video dynamics. In this paper, we propose MoMa, an efficient adapter framework that achieves full spatial-temporal modeling by integrating Mamba's selective state space modeling into IFMs. We propose a novel SeqMod operation to inject spatial-temporal information into pre-trained IFMs, without disrupting their original features. By incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances video understanding while maintaining computational efficiency. Extensive experiments on multiple video benchmarks demonstrate the effectiveness of MoMa, achieving superior performance with reduced computational cost. Codes will be released upon publication.","Today’s AI models struggle to fully understand videos. While they excel at analyzing static images, they often treat what’s happening (like a ball flying) separately from where it’s happening (like a basketball court). This limits their ability to grasp complex actions, such as a gymnast’s fluid routine.We developed MoMa, an add-on tool that upgrades image-based AI to understand motion naturally. Inspired by how humans track movement, MoMa uses SeqMod—a technique that weaves together changes over time (temporal) and scene details (spatial) without overwriting the AI’s original knowledge. Its efficient ""Divide-and-Modulate"" design avoids heavy computations.MoMa outperformed existing methods on sports, surveillance, and action-recognition tasks while using 40% less computing power."
Poster,Momentum-Driven Adaptivity: Towards Tuning-Free Asynchronous Federated Learning,https://ICML.cc//virtual/2025/poster/44676,"Wenjing Yan, Xiangyu Zhong, Xiaolu Wang, Angela Yingjun Zhang","Asynchronous federated learning (AFL) has emerged as a promising solution to address system heterogeneity and improve the training efficiency of federated learning. However, existing AFL methods face two critical limitations: 1) they rely on strong assumptions about bounded data heterogeneity across clients, and 2) they require meticulous tuning of learning rates based on unknown system parameters. In this paper, we tackle these challenges by leveraging momentum-based optimization and adaptive learning strategies. We first propose MasFL, a novel momentum-driven AFL framework that successfully eliminates the need for data heterogeneity bounds by effectively utilizing historical descent directions across clients and iterations. By mitigating the staleness accumulation caused by asynchronous updates, we prove that MasFL achieves state-of- the-art convergence rates with linear speedup in both the number of participating clients and local updates. Building on this foundation, we further introduce AdaMasFL, an adaptive variant that incorporates gradient normalization into local updates. Remarkably, this integration removes all dependencies on problem-specific parameters, yielding a fully tuning-free AFL approach while retaining theoretical guarantees. Extensive experiments demonstrate that AdaMasFL consistently outperforms state-of-the-art AFL methods in run- time efficiency and exhibits exceptional robustness across diverse learning rate configurations and system conditions.","Asynchronous Federated Learning (AFL) is a promising approach to address system heterogeneity and improve training efficiency in federated learning. However, existing AFL methods face two major challenges: they rely on strong assumptions about bounded data heterogeneity across clients and require tedious tuning of learning rates based on unknown system parameters. To address these issues, we propose MasFL, a momentum-driven AFL framework that eliminates the need for heterogeneity bounds by leveraging historical descent directions across clients and iterations. By mitigating staleness accumulation caused by asynchronous updates, MasFL achieves state-of-the-art convergence rates with linear speedup in the number of clients and local updates. Building on this framework, we introduce AdaMasFL, an adaptive variant that incorporates gradient normalization into local updates. This fully tuning-free approach removes all dependencies on problem-specific parameters while retaining strong theoretical guarantees. Extensive experiments demonstrate that AdaMasFL consistently outperforms state-of-the-art AFL methods in runtime efficiency and offers exceptional robustness across diverse learning rate configurations and system conditions."
