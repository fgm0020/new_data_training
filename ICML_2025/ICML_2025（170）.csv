type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Leveraging Predictive Equivalence in Decision Trees,https://ICML.cc//virtual/2025/poster/46051,"Hayden McTavish, Zachery Boner, Jon Donnelly, Margo Seltzer, Cynthia Rudin","Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.","Decision trees are a popular machine learning tool because they explain decisions with a simple flowchart structure.To obtain predictions from these trees, common practice is to follow flowcharts from the top down. However, in many cases, there are a variety of flowcharts which make equivalent predictions (hence the name, ""predictive equivalence""), even though they present different top-down decision paths.To solve this, we introduced a way to represent decision trees using Boolean logic. This representation keeps the prediction behavior the same but removes the confusion caused by having many versions of the same model.It also has some surprising benefits: it helps decision trees handle missing data better than expected; gives a more accurate view of which features are important; and we introduce a way to reduce the cost of making predictions with a given tree."
Poster,Leveraging Randomness in Model and Data Partitioning for Privacy Amplification,https://ICML.cc//virtual/2025/poster/46537,"Andy Dong, Wei-Ning Chen, Ayfer Ozgur","We study how inherent randomness in the training process—where each sample (or client in federated learning) contributes only to a randomly selected portion of training—can be leveraged for privacy amplification. This includes (1) data partitioning, where a sample participates in only a subset of training iterations, and (2) model partitioning, where a sample updates only a subset of the model parameters. We apply our framework to model parallelism in federated learning, where each client updates a randomly selected subnetwork to reduce memory and computational overhead, and show that existing methods, e.g. model splitting or dropout, provide a significant privacy amplification gain not captured by previous privacy analysis techniques. Additionally, we introduce balanced iteration subsampling, a new data partitioning method where each sample (or client) participates in a fixed number of training iterations. We show that in certain regimes, this method yields stronger privacy amplification than Poisson (i.i.d.) sampling of data (or clients). Our results demonstrate that randomness in the training process, which is  structured rather than i.i.d. and interacts with data in complex ways, can be systematically leveraged for nontrivial privacy amplification.","Machine learning models typically protect privacy by adding random noise during training, but too much noise can harm accuracy. We show that hiding which parts of the model or which training steps each data point sees makes it harder to trace any one example. This hidden randomness gives a boost to privacy (so we can add less extra noise to achieve the same level of privacy) without changing the basic training algorithm. We are the first to explain and quantify exactly how much extra privacy this gives. Our approach is especially useful in federated learning, where devices have limited compute power: by training only on submodels, weaker devices can still participate, and we show how it also improves privacy guarantees."
Poster,Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration,https://ICML.cc//virtual/2025/poster/43987,"Max Wilcoxson, Qiyang Li, Kevin Frans, Sergey Levine","Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled offline trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-labels unlabeled trajectories with optimistic rewards and high-level action labels, transforming prior data into high-level, task-relevant examples that encourage novelty-seeking behavior. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. In our experiments, SUPE consistently outperforms prior strategies across a suite of 42 long-horizon, sparse-reward tasks.","How do we leverage unlabeled prior data to improve online learning and exploration of a reinforcement learning (RL) agent (a self-improving agent) in solving challenging tasks that require long-horizon reasoning? Our paper presents a method to utilize these data effectively as follows: (1) break the data into segments and turn them into a set of low-level skills that imitate these segments, and (2) determine which skills are the most appropriate to use by processing and analyzing the high-level structure of the data. These allow us to effectively learn a high-level agent that picks low-level skills at a fixed time interval during online learning. By using the high-level agent to carefully select low-level skills online, we are able to collect data in a structured manner, improving online data sample efficiency. As a result, our method is able to learn online efficiently with only limited online data and achieves strong performance on a set of 42 simulated robotic tasks compared all prior strategies."
Poster,Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective,https://ICML.cc//virtual/2025/poster/44091,"Yunzhen Yao, Lie He, Michael Gastpar","This paper considers the sample-efficiency of preference learning, which models and predicts human choices based on comparative judgments. The minimax optimal estimation error rate $\Theta(d/n)$ in classical estimation theory requires that the number of samples $n$ scales linearly with the dimensionality of the feature space $d$. However, the high dimensionality of the feature space and the high cost of collecting human-annotated data challenge the efficiency of traditional estimation methods. To remedy this, we leverage sparsity in the preference model and establish sharp error rates. We show that under the sparse random utility model, where the parameter of the reward function is $k$-sparse, the minimax optimal rate can be reduced to $\Theta(k/n \log(d/k))$. Furthermore, we analyze the $\ell_{1}$-regularized estimator and show that it achieves near-optimal rate under mild assumptions on the Gram matrix. Experiments on synthetic data and LLM alignment data validate our theoretical findings, showing that sparsity-aware methods significantly reduce sample complexity and improve prediction accuracy.","Training AI to follow human preferences often needs lots of data, which is costly and slow. We noticed that people usually care about only a few key factors when making choices. By focusing on this idea, we developed a method that learns much faster by ignoring unimportant details. This helps build AI systems—like helpful chatbots—that better understand what people want, using less data."
Poster,LEVIS: Large Exact Verifiable Input Spaces for Neural Networks,https://ICML.cc//virtual/2025/poster/45030,"Mohamad Chehade, Wenting Li, Brian Bell, Russell Bent, Saif Kazi, Hao Zhu","The robustness of neural networks is crucial in safety-critical applications, where identifying a reliable input space is essential for effective model selection, robustness evaluation, and the development of reliable control strategies. Most existing robustness verification methods assess the worst-case output under the assumption that the input space is known. However, precisely identifying a verifiable input space $ \mathcal{C} $, where no adversarial examples exist, is challenging due to the possible high dimensionality, discontinuity, and non-convex nature of the input space. To address this challenge, we propose a novel framework, **LEVIS**, comprising **LEVIS-$\alpha$** and **LEVIS-$\beta$**. **LEVIS-$\alpha$** identifies a single, large verifiable ball that intersects at least two boundaries of a bounded region $ \mathcal{C} $, while **LEVIS-$\beta$** systematically captures the entirety of the verifiable space by integrating multiple verifiable balls. Our contributions are fourfold: we introduce a verification framework, **LEVIS**, incorporating two optimization techniques for computing nearest and directional adversarial points based on mixed-integer programming (MIP); to enhance scalability, we integrate complementary constrained (CC) optimization with a reduced MIP formulation, achieving up to a 17-fold reduction in runtime by approximating the verifiable region in a principled way; we provide a theoretical analysis characterizing the properties of the verifiable balls obtained through **LEVIS-$\alpha$**; and we validate our approach across diverse applications, including electrical power flow regression and image classification, demonstrating performance improvements and visualizing the geometric properties of the verifiable region.","Neural networks are powerful tools used in applications like image recognition and energy systems, but they can make mistakes when their inputs are slightly changed. This is risky in safety-critical areas such as power grid control or self-driving cars. Our work introduces LEVIS, a method that finds input regions where a neural network is guaranteed to behave correctly, even in the presence of small changes.Rather than guessing safe regions or checking every possible input, LEVIS builds exact ""safe zones"" around certain points, like drawing protective bubbles. These zones are carefully calculated so that any input inside will not trigger errors or surprises in the network's decisions. We introduce two strategies: one that finds a single large bubble and one that builds many smaller ones to cover more space. LEVIS is both fast and reliable thanks to a smart combination of optimization tools.We tested LEVIS on real problems from power systems and image classification. It found safer regions more accurately and faster than previous methods. Our approach can help make AI systems more trustworthy and easier to analyze in the real world."
Poster,Lexico: Extreme KV Cache Compression via Sparse Coding over Universal Dictionaries,https://ICML.cc//virtual/2025/poster/44898,"Junhyuck Kim, Jongho Park, Jaewoong Cho, Dimitris Papailiopoulos","We introduce Lexico, a novel KV cache compression method that leverages sparse coding with a universal dictionary. Our key finding is that key-value cache in modern LLMs can be accurately approximated using sparse linear combination from a small, input-agnostic dictionary of ~4k atoms, enabling efficient compression across different input prompts, tasks and models. Using orthogonal matching pursuit for sparse approximation, Lexico achieves flexible compression ratios through direct sparsity control. On GSM8K, across multiple model families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the original performance while using only 15-25% of the full KV-cache memory, outperforming both quantization and token eviction methods. Notably, Lexico remains effective in low memory regimes where 2-bit quantization fails, achieving up to 1.7x better compression on LongBench and GSM8K while maintaining high accuracy.","Modern AI language models like ChatGPT need to remember everything you’ve said so far in a conversation. They store this information in what’s called a ""KV cache,"" which takes up a lot of memory—especially as conversations get longer or more complex. This creates a problem when trying to run these models efficiently on devices with limited memory, such as a single GPU.Our method, Lexico, reduces this memory cost by compressing the stored information using a technique called sparse coding. Instead of storing all the data in full, Lexico breaks it down into a small set of building blocks (like a dictionary of reusable parts) that can represent the original information with high accuracy. These dictionaries are universal, meaning they work across different tasks without needing to be retrained.Even under tight memory limits, Lexico maintains strong performance—matching the original model’s accuracy 90–95% of the time while using only 15–25% of the memory. It also outperforms popular alternatives like quantization and token deletion in challenging tasks. This makes Lexico a practical solution for running large language models more efficiently, especially when memory is a bottleneck."
Poster,LGDM: Latent Guidance in Diffusion Models for Perceptual Evaluations,https://ICML.cc//virtual/2025/poster/44206,"Shreshth Saini, Ru-Ling Liao, Yan Ye, Alan Bovik","Despite recent advancements in latent diffusion models that generate high-dimensional image data and perform various downstream tasks, there has been little exploration into perceptual consistency within these models on the task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we hypothesize that latent diffusion models implicitly exhibit perceptually consistent local regions within the data manifold. We leverage this insight to guide on-manifold sampling using perceptual features and input measurements. Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that utilizes pretrained latent diffusion models and perceptual quality features to obtain perceptually consistent multi-scale and multi-timestep feature maps from the denoising U-Net. We empirically demonstrate that these hyperfeatures exhibit high correlation with human perception in IQA tasks. Our method can be applied to any existing pretrained latent diffusion model and is straightforward to integrate. To the best of our knowledge, this paper is the first work on guiding diffusion model with perceptual features for NR-IQA. Extensive experiments on IQA datasets show that our method, LGDM, achieves state-of-the-art performance, underscoring the superior generalization capabilities of diffusion models for NR-IQA tasks.","Images on the internet often vary in quality because of noise, blur, or compression artifacts. Humans can easily tell when an image looks good or bad without seeing an original reference, but automated tools struggle to match human judgments. We discovered that advanced image-generation models, i.e. latent diffusion models, learn to represent images in a way that aligns with human perception. By tapping into these hidden representations, we can extract detailed features that relate directly to how people rate image quality. We introduce a simple algorithm, Perceptual Manifold Guidance (PMG), which uses these features to predict image quality without any additional training. Our method works out of the box on existing models and achieves top performance on a variety of standard benchmarks. This approach offers a practical and accurate way to assess image quality in applications ranging from photography and video streaming to medical imaging and remote sensing."
Poster,LieRE: Lie Rotational Positional Encodings,https://ICML.cc//virtual/2025/poster/43542,"Sophie Ostmeier, Brian Axelrod, Maya Varma, Michael Moseley, Akshay Chaudhari, Curtis Langlotz","Transformer architectures depend on explicit position encodings to capture token positional information. Rotary Position Encoding (RoPE) has emerged as a popular choice in language models due to its efficient encoding of relative position information through key-query rotations. However, RoPE faces significant limitations beyond language processing: it is constrained to one-dimensional sequence data and, even with learnable phases, offers limited representational capacity.We address these challenges with Lie Relative Encodings (LieRE), which generalizes RoPE to high-dimensional rotation matrices by leveraging their Lie group structure. Through extensive evaluation on three image datasets across 2D and 3D classification tasks, LieRE achieves 1.5% improvement over state-of-the-art baselines on 2D tasks and 1% on 3D tasks, while demonstrating superior generalization to higher resolutions. Our implementation is computationally efficient, with results reproducible on 4 A100 GPUs in 30 minutes on CIFAR100. Our code is available at https://github.com/StanfordMIMI/LieRE.","Transformers are widely used in AI, but they need position information to understand the structure of data like images or 3D scenes. For example, when processing an image, it is divided into small blocks called patches. Each patch becomes a token, but these tokens lack information about their original location unless position encodings are added.LieRE improves how transformers handle position encodings, especially in high-dimensional settings. It encodes token positions using rotation matrices, leveraging the relationship between skew-symmetric matrices and dense rotation matrices to preserve geometric structure.LieRE builds on RoPE (Rotary Position Embedding), which uses block-2D rotation matrices to encode relative positions directly into model computations. While RoPE was originally developed for one-dimensional sequence data like text, LieRE generalizes these ideas to higher dimensions, allowing the model to reason about both absolute and relative positions in complex spatial data."
Poster,LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning,https://ICML.cc//virtual/2025/poster/44967,"Zihang Liu, Tianyu Pang, Oleg Balabanov, Chaoqun Yang, Tianjin Huang, Lu Yin, Yaoqing Yang, Shiwei Liu","Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call *Principal Weights*. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: **L**ow-rank **I**nformed Sparse **F**ine-**T**uning ($\texttt{LIFT}$). $\texttt{LIFT}$ only updates the top 5% *Principal Weights* throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods.  In addition to strong performance on target domains such as arithmetic reasoning, $\texttt{LIFT}$ also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.","Modern language-based AI models learn to “reason” by adapting its weights to complex tasks through fine-tuning. As AI models continue to grow in size to billion-parameter level, it becomes crucial to develop a fine-tuning method that has both superior performance and better efficiency. To solve this problem, in this paper we study a method called sparse fine-tuning, which only changes a tiny subset of model weights. An important problem for sparse fine-tuning is to find the critical subset of weights. We find that the critical components for fine-tuning can be characterized by the top eigenspace of the weight matrix. Our study reveals that model weights that have the largest magnitude after performing low-rank approximation are the Principal Weights critical to fine-tuning. We then design a method that only fine-tunes the Principal Weights, and name this method Low-rank Informed Sparse Fine-Tuning (LIFT).From empirical studies, we found that LIFT achieves stronger results on reasoning tasks than dense fine-tuning method, while better preserving the knowledge the model already has. Furthermore, the memory overhead of LIFT is significantly lower than dense fine-tuning, comparable to the best efficient fine-tuning methods.By “lifting the veil” with low-rank approximation and fine-tuning the largest-magnitude weights, LIFT finds the “truth” within model weights that are critical to fine-tuning. This work provides insights for determining the critical components of model weights, and inspires future research on designing more efficient fine-tuning approaches that improve the reasoning ability of large AI models."
Poster,Liger: Linearizing Large Language Models to Gated Recurrent Structures,https://ICML.cc//virtual/2025/poster/46641,"Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, Yu Cheng","Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents \textbf{Liger}, short for \underline{\textbf{Li}}nearizing LLMs to \underline{\textbf{g}}at\underline{\textbf{e}}d \underline{\textbf{r}}ecurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\% of the Transformer-based LLM performance at 0.02\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters.","Current large language models (LLMs), such as those powering chatbots like ChatGPT, predominantly rely on Transformer architectures. While effective, these LLM architectures suffer from inefficiency. Recently proposed gated linear recurrent model architectures offer promising computational efficiency to address Transformer limitations, yet validating their potential remains challenging due to the prohibitive computational resources required for training LLMs.  In this work we present Liger, a novel technique for converting existing Transformer-based LLMs to gated linear recurrent structures. Unlike prior linearization methods that require multi-stage training, Liger constructs gating modules through parameter-free pooling operations, enabling full reuse of pretrained Transformer LLM  weights without newly introducing any learnable parameters. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93% of the Transformer-based LLM performance at 0.02% pre-training tokens during the linearization process.Experiments across diverse commonsense reasoning and knowledge benchmarks demonstrate Liger's superiority, featuring linear computational complexity and constant memory usage, and can be applied to various linear recurrent model structures with gating modules."
