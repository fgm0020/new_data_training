type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning",https://ICML.cc//virtual/2025/poster/46087,"Adrià López Escoriza, Nicklas Hansen, Stone Tao, Tongzhou Mu, Hao Su","Long-horizon tasks in robotic manipulation present significant challenges in reinforcement learning (RL) due to the difficulty of designing dense reward functions and effectively exploring the expansive state-action space. However, despite a lack of dense rewards, these tasks often have a multi-stage structure, which can be leveraged to decompose the overall objective into manageable sub-goals. In this work, we propose DEMO³, a framework that exploits this structure for efficient learning from visual inputs. Specifically, our approach incorporates multi-stage dense reward learning, a bi-phasic training scheme, and world model learning into a carefully designed demonstration-augmented RL framework that strongly mitigates the challenge of exploration in long-horizon tasks. Our evaluations demonstrate that our method improves data-efficiency by an average of 40% and by 70% on particularly difficult taskscompared to state-of-the-art approaches. We validate this across 16 sparse-reward tasks spanning four domains, including challenging humanoid visual control tasks using as few as five demonstrations.","Training robots to complete long, complex tasks, like picking up an object and placing it in a specific spot, is incredibly difficult. One major reason is that they usually receive very little feedback while learning, making it hard for them to know if they’re making progress.Our work introduces DEMO³, a learning method that teaches robots more effectively by giving them just a few example demonstrations and then helping them break big tasks into smaller steps. Each step provides feedback, which makes learning faster and more reliable. DEMO³ uses this step-based feedback to train the robot’s behavior, its understanding of the world, and its ability to evaluate progress, all at once.We tested our approach on 16 robot tasks across different environments. DEMO³ consistently outperformed other methods, especially in the most challenging tasks. It also worked well even with as few as five example demonstrations.This approach could make it easier and faster to train robots for real-world jobs, from household assistance to industrial automation."
Poster,Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks,https://ICML.cc//virtual/2025/poster/44646,"Rohit Sonker, Alexandre Capone, Andrew Rothstein, Hiro Kaga, Egemen Kolemen, Jeff Schneider","Machine learning algorithms often struggle to control complex real-world systems. In the case of nuclear fusion, these challenges are exacerbated, as the dynamics are notoriously complex, data is poor, hardware is subject to failures, and experiments often affect dynamics beyond the experiment's duration. Existing tools like reinforcement learning, supervised learning, and Bayesian optimization address some of these challenges but fail to provide a comprehensive solution. To overcome these limitations, we present a multi-scale Bayesian optimization approach that integrates a high-frequency data-driven dynamics model with a low-frequency Gaussian process. By updating the Gaussian process between experiments, the method rapidly adapts to new data, refining the predictions of the less reliable dynamical model. We validate our approach by controlling tearing instabilities in the DIII-D nuclear fusion plant. Offline testing on historical data shows that our method significantly outperforms several baselines. Results on live experiments on the DIII-D tokamak, conducted under high-performance plasma scenarios prone to instabilities, shows a 50\% success rate — marking a 117\% improvement over historical outcomes.","Controlling complex systems like nuclear fusion reactors is a major challenge for machine learning, mainly because the systems are unpredictable, data is limited, and things can go wrong during or after an experiment. Traditional machine learning methods don’t fully solve these issues.In this work, we introduce a new method that combines two types of machine learning models: one that learns from fast, real-time data and another that improves between experiments by learning from overall trends. This combination helps us make better decisions, even when conditions change or data is uncertain.We tested our method on a real fusion reactor (DIII-D) to prevent a common type of instability that disrupts experiments. First, we showed it works well on simulated experiments using past data. Then, we ran it in live experiments on a real tokamak and achieved a 50\% success rate— marking a 117\% improvement over historical outcomes."
Poster,Multi-Turn Code Generation Through Single-Step Rewards,https://ICML.cc//virtual/2025/poster/44806,"Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander Rush, Wenting Zhao, Sanjiban Choudhury","We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards.We propose a simple yet scalable approach, $\mu$CODE, that solves multi-turn code generation using only single-step rewards.Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn.$\mu$CODE iteratively trains both a generator to provide  code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code.Experimental evaluations show that our approach achieves significant improvements over state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of $\mu$CODE at utilizing the execution feedback.","We want agents that can generate correct code for us but doing so in one try can be difficult without unit test feedback, so we focus on multi-turn code generation where an agent can iteratively refine its solution using execution feedback. However, training agents with such feedback (correct/incorrect) using reinforcement learning is challenging due to sparse rewards signals which makes learning inefficient.Our work introduces $\mu$Code, a simple and scalable approach to make this process more effective. First, we observe that a correct code solution can be generated at any step, meaning the agent can ""recover"" in a step — we call this *one-step recoverability*. Second, instead of relying on sparse rewards, we *learn a verifier* to provide a richer score to make learning easier. These insights allow us to reduce the problem from complex reinforcement learning to imitation learning, making training more stable. In addition, by learning a verifier we can output multiple solutions and choose the highest scoring solution during inference-time in a *multi-turn Best-of-N search*.We release our models for generating code and verifying code for researchers to contribute to the self-improving model community. Developing stronger generators and verifiers in conjunction will produce agents that are stronger and more reliable at code generation over multiple steps."
Poster,Multivariate Conformal Selection,https://ICML.cc//virtual/2025/poster/44490,"Tian Bai, Yue Zhao, Xiang Yu, Archer Yang","Selecting high-quality candidates from large datasets is critical in applications such as drug discovery, precision medicine, and alignment of large language models (LLMs). While Conformal Selection (CS) provides rigorous uncertainty quantification, it is limited to univariate responses and scalar criteria. To address this, we propose Multivariate Conformal Selection (mCS), a generalization of CS designed for multivariate response settings. Our method introduces regional monotonicity and employs multivariate nonconformity scores to construct conformal $p$-values, enabling finite-sample False Discovery Rate (FDR) control. We present two variants: $\texttt{mCS-dist}$, using distance-based scores, and $\texttt{mCS-learn}$, which learns optimal scores via differentiable optimization. Experiments on simulated and real-world datasets demonstrate that mCS significantly improves selection power while maintaining FDR control, establishing it as a robust framework for multivariate selection tasks.","We've developed a new statistical method called Multivariate Conformal Selection (mCS) to help identify valuable candidates from large datasets, especially when dealing with multiple factors at once. Think of it like sifting through thousands of potential new drugs to find the best ones based on several desirable properties, not just one.Traditional methods, like Conformal Selection (CS), work well for single-factor decisions but struggle with complex, multi-factor choices. Our mCS method extends this by introducing a new way to measure how ""unusual"" a candidate is, allowing us to control the rate of false discoveries – meaning, we minimize selecting duds.We offer two versions: $\texttt{mCS-dist}$, which uses straightforward distance calculations, and $\texttt{mCS-learn}$, which uses machine learning to find the best way to evaluate candidates. Both versions significantly improve our ability to pick the right candidates while keeping errors low. This makes mCS a powerful tool for important tasks like drug discovery and refining large AI models."
Poster,Multi-View Graph Clustering via Node-Guided Contrastive Encoding,https://ICML.cc//virtual/2025/poster/46161,"Yazhou Ren, Junlong Ke, Zichen Wen, Tianyi Wu, Yang Yang, Xiaorong Pu, Lifang He","Multi-view clustering has gained significant attention for integrating multi-view information in multimedia applications. With the growing complexity of graph data, multi-view graph clustering (MVGC) has become increasingly important. Existing methods primarily use Graph Neural Networks (GNNs) to encode structural and feature information, but applying GNNs within contrastive learning poses specific challenges, such as integrating graph data with node features and handling both homophilic and heterophilic graphs. To address these challenges, this paper introduces Node-Guided Contrastive Encoding (NGCE), a novel MVGC approach that leverages node features to guide embedding generation. NGCE enhances compatibility with GNN filtering, effectively integrates homophilic and heterophilic information, and strengthens contrastive learning across views. Extensive experiments demonstrate its robust performance on six homophilic and heterophilic multi-view benchmark datasets.",NGCE (Node-Guided Contrastive Encoding) employs a node-guided encoding framework to address mixed homophilic and heterophilic patterns in multi-view graph clustering. This approach maintains the essential interactions between these patterns by avoiding their isolation and processing them within a unified framework. NGCE is designed to integrate graph data with node features effectively and strengthen contrastive learning across different views.
Poster,MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners,https://ICML.cc//virtual/2025/poster/45072,"Fang-Duo Tsai, Shih-Lun Wu, Weijaw Lee, Sheng-Ping Yang, Bo-Rui Chen, Hao-Chung Cheng, Yi-Hsuan Yang","We propose MuseControlLite, a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. The key finding is that positional embeddings, which have been seldom used by text-to-music generation models in the conditioner for text conditions, are critical when the condition of interest is a function of time. Using melody control as an example, our experiments show that simply adding rotary positional embeddings to the decoupled cross-attention layers increases control accuracy from 56.6% to 61.1%, while requiring 6.75 times fewer trainable parameters than state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion Transformer model of Stable Audio Open. We evaluate various forms of musical attribute control, audio inpainting, and audio outpainting, demonstrating improved controllability over MusicGen-Large and Stable Audio Open ControlNet at a significantly lower fine-tuning cost, with only 85M trainable parameters. Source code, model checkpoints, and demo examples are available at: https://MuseControlLite.github.io/web/","MuseControlLite is a fully open-source, controllable text-to-music model designed for low-cost training. It supports precise control over melody, rhythm, and dynamics, as well as audio inpainting and outpainting, and allows flexible combinations of these conditions. MuseControlLite achieves state-of-the-art performance in melody-conditioned music generation tasks."
Poster,Mutual Learning for SAM Adaptation: A Dual Collaborative Network Framework for Source-Free Domain Transfer,https://ICML.cc//virtual/2025/poster/46620,"Yabo Liu, Waikeung Wong, Chengliang Liu, Xiaoling Luo, Yong Xu, Jinghua Wang","Segment Anything Model (SAM) has demonstrated remarkable zero-shot segmentation capabilities across various visual tasks. However, its performance degrades significantly when deployed in new target domains with substantial distribution shifts. While existing self-training methods based on fixed teacher-student architectures have shown improvements, they struggle to ensure that the teacher network consistently outperforms the student under severe domain shifts. To address this limitation, we propose a novel Collaborative Mutual Learning Framework for source-free SAM adaptation, leveraging dual-networks in a dynamic and cooperative manner. Unlike fixed teacher-student paradigms, our method dynamically assigns the teacher and student roles by evaluating the reliability of each collaborative network in each training iteration. Our framework incorporates a dynamic mutual learning mechanism with three key components: a direct alignment loss for knowledge transfer, a reverse distillation loss to encourage diversity, and a triplet relationship loss to refine feature representations. These components enhance the adaptation capabilities of the collaborative networks, enabling them to generalize effectively to target domains while preserving their pre-trained knowledge. Extensive experiments on diverse target domains demonstrate that our proposed framework achieves state-of-the-art adaptation performance.","The Segment Anything Model (SAM) is an advantage tool that can automatically separate objects in images without needing extra training. While SAM works well in familiar settings, its performance drops when it is used in new environments that are very different from the ones it was designed for. Existing approaches to help SAM adapt, which rely on a fixed teacher-student learning setup, often fall short when the differences between environments are too large. To address this issue, we developed a new approach that allows two networks to work together and learn from each other in a flexible and dynamic way. Instead of having fixed roles, the networks switch between being ""teacher"" and ""student"" based on how well they perform during training. We also designed specific techniques to help the networks share knowledge, improve their understanding, and become better at handling new environments. Our method helps SAM adapt more effectively to new situations while keeping its original strengths. Tests on different types of data show that our approach achieves remarkable results, making it a promising tool for real-world applications."
Poster,MVA: Linear Attention with High-order Query-Keys Integration and Multi-level Vocabulary Decomposition,https://ICML.cc//virtual/2025/poster/45016,"ning wang, Zekun Li, Tongxin Bai, Man Yao, Zhen Qin, Guoqi Li","Linear attention offers the advantages of linear inference time and fixed memory usage compared to Softmax attention. However, training large-scale language models with linear attention from scratch remains prohibitively expensive and exhibits significant performance gaps compared to Softmax-based models. To address these challenges, we focus on transforming pre-trained Softmax-based language models into linear attention models. We unify mainstream linear attention methods using a **high-order QK integration theory** and a **multi-level vocabulary decomposition**. Specifically, the QK integration theory explains the efficacy of combining linear and sparse attention from the perspective of information collection across different frequency bands. The multi-level vocabulary decomposition exponentially expands memory capacity by recursively exploiting compression loss from compressed states. Through detailed error analysis, we demonstrate superior approximation of Softmax attention achieved by our approach. To further improve performance and reduce training costs, we adopt a **soft integration strategy** with attention scores, effectively combining a sliding window mechanism. With less than 100M tokens, our method fine-tunes models to achieve linear complexity while retaining 99\% of their original performance. Compared to state-of-the-art linear attention model and method, our approach improves MMLU scores by 1.2 percentage points with minimal fine-tuning. Furthermore, even without the sliding window mechanism, our method achieves state-of-the-art performance on all test sets with 10B tokens.","Modern AI language models are powerful but computationally expensive. While newer ""linear attention"" models promise faster, cheaper performance, training them from scratch remains costly and their results often lag behind traditional methods. Instead of building these efficient models from the ground up, we propose upgrading existing high-performing models to adopt linear attention’s benefits.Our approach works like a targeted retrofit: we analyze how existing models process information and systematically adjust their ""attention"" mechanisms—the part that determines which words or concepts the model focuses on. By breaking down how the model handles different types of language patterns and memory, we bridge the performance gap between traditional and linear models. After minor tweaks (using less than 1% of the original training data), our upgraded models retain 99% of their original accuracy while running significantly faster.This method outperforms other linear attention techniques and offers a practical path to deploy efficient AI without sacrificing quality. We’ve prioritized simplicity, ensuring even small-scale applications can benefit from these advancements."
Poster,MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design,https://ICML.cc//virtual/2025/poster/43995,"Haojie Duanmu, Xiuhong Li, Zhihang Yuan, Size Zheng, Jiangfei Duan, Xingcheng ZHANG, Dahua Lin","Mixture-of-Experts (MoE) models face deployment challenges due to their large parameter counts and computational demands. We explore quantization for MoE models and highlight two key insights: 1) linear blocks exhibit varying quantization sensitivity, and 2) divergent expert activation frequencies create heterogeneous computational characteristics. Based on these observations, we introduce MxMoE, a mixed-precision optimization framework for MoE models that considers both algorithmic and system perspectives. MxMoE navigates the design space defined by parameter sensitivity, expert activation dynamics, and hardware resources to derive efficient mixed-precision configurations. Additionally, MxMoE automatically generates optimized mixed-precision GroupGEMM kernels, enabling parallel execution of GEMMs with different precisions. Evaluations show that MxMoE outperforms existing methods, achieving 2.4 lower Wikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup over full precision, as well as up to 29.4% speedup over uniform quantization at equivalent accuracy with 5-bit weight-activation quantization. Our code is available at https://github.com/cat538/MxMoE.","Large AI models, like Mixture-of-Experts (MoE), are highly effective but often too bulky and slow for everyday use. These models consist of multiple specialized components, or ""experts,"" each handling different parts of a task. While this design boosts performance, it also leads to high memory usage and slower processing speeds.Our work introduces MxMoE, a new method that makes these large models faster and more efficient. MxMoE intelligently reduces different parts of the model by using lower-precision data type, a technique called quantization. What's unique about our approach is that it considers how often each expert is used and how sensitive each part is to quantization, allowing for smarter, more targeted reductions.We also developed specialized software that enables different parts of the model to run simultaneously, even if they're using different levels of precision. This model accuracy and computational efficiency co-design innovation leads to significant improvements: MxMoE can run up to 3.4 times faster than the original full-size model and outperforms existing methods in accuracy.By making powerful AI models more accessible and efficient, MxMoE opens the door to using advanced AI in a wider range of applications, from personal PC to smaller servers. Our implementation is open-source and available online."
Poster,N2GON: Neural Networks for Graph-of-Net with Position Awareness,https://ICML.cc//virtual/2025/poster/43890,"Yejiang Wang, Yuhai Zhao, Zhengkui Wang, Wen Shan, Ling Li, Qian Li, Miaomiao Huang, Meixia Wang, Shirui Pan, Xingwei Wang","Graphs, fundamental in modeling various research subjects such as computing networks, consist of nodes linked by edges. However, they typically function as components within larger structures in real-world scenarios, such as in protein-protein interactions where each protein is a graph in a larger network. This study delves into the Graph-of-Net (GON), a structure that extends the concept of traditional graphs by representing each node as a graph itself. It provides a multi-level perspective on the relationships between objects, encapsulating both the detailed structure of individual nodes and the broader network of dependencies. To learn node representations within the GON, we propose a position-aware neural network for Graph-of-Net which processes both intra-graph and inter-graph connections and incorporates additional data like node labels. Our model employs dual encoders and graph constructors to build and refine a constraint network, where nodes are adaptively arranged based on their positions, as determined by the network's constraint system. Our model demonstrates significant improvements over baselines in empirical evaluations on various datasets.","Many real-world systems, like biological interactions or even the internet, are complex networks. But what if each ""node"" or point in these networks is itself another intricate network? (For example, a protein in a Protein–protein interaction network is a complex graph of atoms). To address this, our research introduces a new way to see these systems, called ""Graph-of-Net"" (GoN). In a GoN, each node is recognized as its own detailed graph. We then developed an AI model, N2GON, specifically designed to learn from these multi-level structures. N2GON cleverly analyzes both the internal details of each ""node-graph"" and how these node-graphs are linked together in the overarching ""net."" It even considers their relative ""positions"" and distinct features to better understand their relationships.This approach allows for a much richer and more accurate understanding of complex, layered systems. For instance, it can help us better analyze how networks of proteins interact or how scientific papers, seen as text graphs, form citation networks. Our experiments show this method significantly improves our ability to analyze and interpret such intricate data compared to existing techniques."
