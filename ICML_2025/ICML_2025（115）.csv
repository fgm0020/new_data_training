type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,From Complex to Atomic: Enhancing Augmented Generation via Knowledge-Aware Dual Rewriting and Reasoning,https://ICML.cc//virtual/2025/poster/45398,"Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, Jiang Bian","Recent advancements in Retrieval-Augmented Generation (RAG) systems have significantly enhanced the capabilities of large language models (LLMs) by incorporating external knowledge retrieval. However, the sole reliance on retrieval is often inadequate for mining deep, domain-specific knowledge and for performing logical reasoning from specialized datasets. To tackle these challenges, we present an approach, which is designed to extract, comprehend, and utilize domain knowledge while constructing a coherent rationale. At the heart of our approach lie four pivotal components: a knowledge atomizer that extracts atomic questions from raw data, a query proposer that generates subsequent questions to facilitate the original inquiry, an atomic retriever that locates knowledge based on atomic knowledge alignments, and an atomic selector that determines which follow-up questions to pose guided by the retrieved information. Through this approach, we implement a knowledge-aware task decomposition strategy that adeptly extracts multifaceted knowledge from segmented data and iteratively builds the rationale in alignment with the initial query and the acquired knowledge. We conduct comprehensive experiments to demonstrate the efficacy of our approach across various benchmarks, particularly those requiring multihop reasoning steps. The results indicate a significant enhancement in performance, up to 12.6\% over the second-best method, underscoring the potential of the approach in complex, knowledge-intensive applications.","Current Retrieval-Augmented Generation (RAG) methods encounter significant challenges in domain-specific reasoning. A primary issue is that the corpus available may not always possess the necessary information to support each step of the complex reasoning processes that sophisticated language models might generate.Our approach involves tagging each original corpus document with atomic knowledge units and iteratively rewriting the user query based on the information already retrieved. This dual-rewriting mechanism bridges the gap between the user query and the available corpus. Moreover, the iterative process evolves step-by-step, culminating in a comprehensive response to the query. This mimics the meticulous approach researchers take in refining questions and gathering evidence, ensuring a thoroughly reasoned and substantiated conclusion.In public benchmarks featuring challenging multi-hop questions, our method demonstrated a 20% increase in accuracy over the second-best performing method. Moving forward, we plan to integrate in-context learning capabilities to dynamically select suitable examples and allow the components of our approach to learn from user feedback."
Poster,From Crowdsourced Data to High-quality Benchmarks: Arena-Hard and Benchbuilder Pipeline,https://ICML.cc//virtual/2025/poster/45630,"Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, Ion Stoica","The rapid evolution of Large Language Models (LLMs) has outpaced the development of model evaluation, highlighting the need for continuous curation of new, challenging benchmarks. However, manual curation of high-quality, human-aligned benchmarks is expensive and time-consuming. To address this, we introduce BenchBuilder, an automated pipeline that leverages LLMs to curate high-quality, open-ended prompts from large, crowd-sourced datasets, enabling continuous benchmark updates without human in the loop. We apply BenchBuilder to datasets such as Chatbot Arena and WildChat-1M, extracting challenging prompts and utilizing LLM-as-a-Judge for automatic model evaluation. To validate benchmark quality, we propose new metrics to measure a benchmark’s alignment with human preferences and ability to separate models. We release Arena-Hard-Auto, a benchmark consisting 500 challenging prompts curated by BenchBuilder. Arena-Hard-Auto provides 3x higher separation of model performances compared to MT-Bench and achieves 98.6% correlation with human preference rankings, all at a cost of $20. Our work sets a new framework for the scalable curation of automated benchmarks from extensive data.","This paper introduces BenchBuilder, an automated method for creating high-quality benchmarks for evaluating Large Language Models (LLMs) without costly human intervention. BenchBuilder takes crowdsourced queries and identifies challenging, prompts that clearly differentiate model capabilities. The resulting benchmark, Arena-Hard-Auto, achieves greater accuracy in distinguishing model performance and closely matches human preference rankings, outperforming existing benchmarks like MT-Bench at a significantly reduced cost. This method sets a new standard for scalable, reliable evaluation of advanced AI models."
Poster,From Debate to Equilibrium: Belief‑Driven Multi‑Agent LLM Reasoning via Bayesian Nash Equilibrium,https://ICML.cc//virtual/2025/poster/45279,"Yi Xie, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, Bo Han","Multi-agent frameworks can substantially boost the reasoning power of large language models (LLMs), but they typically incur heavy computational costs and lack convergence guarantees. To overcome these challenges, we recast multi-LLM coordination as an incomplete-information game and seek a Bayesian Nash equilibrium (BNE), in which each agent optimally responds to its probabilistic beliefs about the strategies of others. We introduce Efficient Coordination via Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that marries distributed reasoning with centralized final output. Under ECON, each LLM independently selects responses that maximize its expected reward, conditioned on its beliefs about co-agents, without requiring costly inter-agent exchanges.We mathematically prove that ECON attains a markedly tighter regret bound than non-equilibrium multi-agent schemes. Empirically, ECON outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks spanning complex reasoning and planning tasks. Further experiments demonstrate ECON’s ability to flexibly incorporate additional models, confirming its scalability and paving the way toward larger, more powerful multi-LLM ensembles. The code is publicly available at: https://github.com/tmlr-group/ECON.","(1) Problem: When multiple large language models (LLMs) work together as agents, they can solve complex problems better than working alone. However, current approaches require these agents to constantly exchange messages with each other, which is expensive and slow. This heavy communication also limits how many LLM agents can work together effectively.(2) Solution: We developed ECON, a new coordination method that eliminates the need for direct communication between LLM agents. Instead of sending messages back and forth, each agent maintains beliefs about what the others are likely to do and makes decisions based on these beliefs. We used game theory principles to ensure all agents reach an optimal collaborative strategy without talking to each other.(3) Impact: ECON outperformed existing multi-agent approaches by 11.2% on average across six challenging reasoning tasks while using 21.4% fewer computational resources. Our method can scale to coordinate up to nine LLM agents effectively, opening the door to building larger, more powerful agent teams that can tackle increasingly complex problems."
Poster,From Feature Interaction to Feature Generation: A Generative Paradigm of CTR Prediction Models,https://ICML.cc//virtual/2025/poster/45999,"MINGJIA YIN, Junwei Pan, Hao Wang, Ximei Wang, Shangyu Zhang, Jie Jiang, Defu Lian, Enhong Chen","Click-Through Rate (CTR) prediction models estimate the probability of users clicking on items based on feature interactions, inherently following a discriminative paradigm. However, this paradigm is prone to embedding dimensional collapse and information redundancy due to limitations of vanilla feature embeddings.This motivates us to reformulate it into a generative paradigm to generate new feature embeddings. Unlike sequential recommendation, which naturally fits a generative ""next-item prediction"" paradigm, it's hard to formulate CTR models into this paradigm without explicit feature order.Therefore, we propose a novel Supervised Feature Generation framework for CTR models, shifting from the discriminative ""feature interaction"" paradigm to the generative ""feature generation"" paradigm.Specifically, we predict each feature embedding based on the concatenation of all feature embeddings.Besides, this paradigm naturally accommodates a supervised binary cross-entropy loss to indicate whether the sample is positive or negative.The framework can reformulate nearly every existing CTR model and bring significant performance lifts.Moreover, it produces less-collapsed and redundancy-reduced feature embeddings, thereby mitigating the inherent limitations of the discriminative paradigm.The code can be found at https://github.com/USTC-StarTeam/GE4Rec.","CTR prediction models predict the probability of a user clicking on an item based on the direct interaction of a set of features. However, we have identified that this direct interaction results in a limitation of the information abundance of the learned features. We hypothesized that this issue could be addressed by avoiding direct interactions.Instead of interacting those features, we propose to generate them using a new paradigm. Our paper presents results that demonstrate this simple yet effective new paradigm can significantly enhance the information abundance of the learned features, as it can bypass direct feature interactions through a feature generation process.Our findings have revealed the inherent limitations of traditional CTR models. However, these models can be easily reformulated using our feature generation paradigm to increase the information abundance of learned features."
Poster,From Individual Experience to Collective Evidence: A Reporting-Based Framework for Identifying Systemic Harms,https://ICML.cc//virtual/2025/poster/43505,"Jessica Dai, Paula Gradu, Inioluwa Raji, Benjamin Recht","When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior? We study the *reporting database* problem, where individual reports of adverse events arrive sequentially, and are aggregated over time. In this work, our goal is to identify whether there are subgroups—defined by any combination of relevantfeatures—that are disproportionately likely to experience harmful interactions with the system. We formalize this problem as a sequential hypothesis test, and identify conditions on reporting behavior that are sufficient for making inferences about disparities in true rates of harm across subgroups. We show that algorithms for sequential hypothesis tests can be applied to this problem with a standard multiple testingcorrection. We then demonstrate our method on real-world datasets, including mortgage decisions and vaccine side effects; on each, our method (re-)identifies subgroups known to experience disproportionate harm using only a fraction of the data that was initially used to discover them.","When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior? In this work, we propose individual reporting as a pathway to identifying fairness problems in deployed systems. In our model, where individual reports of adverse events arrive sequentially to a reporting database, and are aggregated over time. We show how to use these reports to identify systemic performance issues that disproportionately affect particular subgroups. Finally, we use real-world data (from COVID vaccine side effects and mortgage decisions) as case studies for the effectiveness of our method."
Poster,From Jack of All Trades to Master of One: Specializing LLM-based Autoraters to a Test Set,https://ICML.cc//virtual/2025/poster/44938,"Mara Finkelstein, Daniel Deutsch, Parker Riley, Juraj Juraska, Geza Kovacs, Markus Freitag","As LLMs continue to become more powerful and versatile, human evaluation has become intractable at scale and reliance on automatic metrics has become the norm. Recently, it has been shown that LLMs are themselves state-of-the-art evaluators for many tasks. These *Autoraters* are typically designed so that they generalize to new systems *and* test sets. In practice, however, evaluation is performed on a small set of fixed, canonical test sets, which are carefully curated to measure the capabilities of interest and are not changed frequently. In this work, we design a method which specializes a prompted Autorater to a given test set, by leveraging historical ratings on the test set to construct in-context learning (ICL) examples. We evaluate our *Specialist* method on the task of fine-grained machine translation evaluation, and show that it dramatically outperforms the state-of-the-art XCOMET metric by 54% and 119% on the WMT'23 and WMT'24 test sets, respectively. We perform extensive analyses to understand the representations learned by our Specialist metrics, and how variability in rater behavior affects their performance. We also verify the generalizability and robustness of our Specialist method across different numbers of ICL examples, LLM backbones, systems to evaluate, and evaluation tasks.","As large language models (LLMs) continue to become more powerful and versatile, evaluating their performance across the wide range of tasks they are capable of becomes increasingly challenging. Traditionally, human evaluation has been considered the gold standard for evaluating LLM capabilities, but it has become intractable at scale and reliance on automatic metrics has become the norm. Automatic metrics can take many forms, from rule-based to model-based approaches. In the latter category, LLMs have themselves been shown to be state-of-the-art evaluators for many tasks.The key observation motivating this work is that, even though LLMs are typically evaluated using automatic metrics on standard test sets, the metrics and test sets are developed independently. This raises a crucial question: Can we design automatic metrics specifically to excel on the test sets we prioritize? We show that the answer is yes, by introducing the “Specialist” method for creating an LLM-based automatic metric. This method tailors an automatic metric to a specific test set by leveraging historical ratings on the test set for the same source segments as in-context learning examples. (These example are used, along with the evaluation task instruction, to prompt the LLM-based automatic metric.) We evaluate our Specialist method on the task of machine translation evaluation and show that it outperforms the existing state-of-the-art automatic metric for this task by a large margin.This work advances research on how to develop automatic metrics which perform even better than humans for evaluation of LLM capabilities. This work also shows how a small human evaluation budget can be used to directly improve automatic metrics, rather than continually relying on humans every time a new LLM needs to be evaluated."
Poster,From Kernels to Features: A Multi-Scale Adaptive Theory of Feature Learning,https://ICML.cc//virtual/2025/poster/44430,"Noa Rubin, Kirsten Fischer, Javed Lindner, Inbar Seroussi, Zohar Ringel, Michael Krämer, Moritz Helias","Feature learning in neural networks is crucial for their expressive power and inductive biases, motivating various theoretical approaches. Some approaches describe network behavior after training through a change in kernel scale from initialization, resulting in a generalization power comparable to a Gaussian process. Conversely, in other approaches training results in the adaptation of the kernel to the data, involving directional changes to the kernel. The relationship and respective strengths of these two views have so far remained unresolved. This work presents a theoretical framework of multi-scale adaptive feature learning bridging these two views. Using methods from statistical mechanics, we derive analytical expressions for network output statistics which are valid across scaling regimes and in the continuum between them. A systematic expansion of the network's probability distribution reveals that mean-field scaling requires only a saddle-point approximation, while standard scaling necessitates additional correction terms. Remarkably, we find across regimes that kernel adaptation can be reduced to an effective kernel rescaling when predicting the mean network output in the special case of a linear network. However, for linear and non-linear networks, the multi-scale adaptive approach captures directional feature learning effects, providing richer insights than what could be recovered from a rescaling of the kernel alone.","Understanding how neural networks learn structure from data is important for improving their performance, making them more interpretable, and allowing practitioners to catch errors in the model training. Researchers have developed different theories to explain this learning process. Some suggest that training mainly changes the amplitude of the network output by a scaling factor. Others believe that training causes the network to reshape its understanding of data in more complex ways, adapting its behavior depending on the features of the data it sees.These two perspectives have mostly been seen as separate, each with its own strengths. Our paper brings these views together under a new theoretical framework that shows how networks adapt to the data through learning, depending on the scale of the network output. Using tools from physics, we derive mathematical formulas that describe how a network behaves depending on the output scale. Surprisingly, we find that even when a network seems to behave simply, it's still quietly learning more complex patterns in the data. Our insights help understand how and when neural networks adapt to data."
Poster,From Language Models over Tokens to Language Models over Characters,https://ICML.cc//virtual/2025/poster/43839,"Tim Vieira, Benjamin LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian DuSell, John Terilla, Timothy O&#x27;Donnell, Ryan Cotterell","Modern language models are internally—and mathematically—distributions over *token* strings rather than *character* strings, posing numerous challenges for programmers building user applications on top of them.  For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model.  Thus, the tokenizer and consequent processing are very sensitive to the specification of the prompt (e.g., whether the prompt ends with a space or not).  This paper presents algorithms for converting token-level language models to character-level ones.  We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality.  Across four publicly available language models, we find that—even with a small computation budget—our method is able to accurately approximate the character-level distribution at reasonably fast speeds, and that a significant improvement in the language model's compression rate (bits/byte) is achieved.","Internally, modern language models don’t represent text as characters or words—they use tokens, which are variable-length chunks like `super`,`'cal`, `if`, `rag`, `il`, `ist`, `ice`, `xp`, `ial`, `id`, `ocious`. This setup makes models efficient to train and run, but introduces surprising and often frustrating behavior for users. For example, adding a single space to the end of a prompt can dramatically change the model’s output—even when the text looks the same to us.This paper resolves the issue by transforming any existing token-based language model into a character-based one—without any retraining. The result is a model that behaves more predictably and intuitively when given character-level prompts.But the benefits go deeper: token-based models actually assign probability mass to many different tokenizations of the same text, even though common interfaces only consider one. Our method accounts for all valid tokenizations, unlocking probability estimates that are both more accurate and more faithful to the underlying model!"
Poster,From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection,https://ICML.cc//virtual/2025/poster/45421,"Lincan Cai, Jingxuan Kang, Shuang Li, Wenxuan Ma, Binhui Xie, Zhida Qin, Jian Liang","Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive zero-shot capabilities on downstream tasks. Prior research highlights the crucial role of visual augmentation techniques, like random cropping, in alignment with fine-grained class descriptions generated by large language models (LLMs), significantly enhancing zero-shot performance by incorporating multi-view information. However, the inherent randomness of these augmentations can inevitably introduce background artifacts and cause models to overly focus on local details, compromising global semantic understanding. To address these issues, we propose an **A**ttention-**B**ased **S**election (**ABS**) method from local details to global context, which applies attention-guided cropping in both raw images and feature space, supplement global semantic information through strategic feature selection. Additionally, we introduce a soft matching technique to effectively filter LLM descriptions for better alignment. **ABS** achieves state-of-the-art performance on out-of-distribution generalization and zero-shot classification tasks. Notably, **ABS** is training-free and even rivals few-shot and test-time adaptation methods.","we propose ABS (Attention-Based Selection), a method that smartly selects the most important parts of an image, both in the original pixels and in the model’s internal features, which can align better with the detailed LLM descriptions of each image. This enhances the zero-shot classification performance of vision-language models."
Poster,From Logits to Hierarchies: Hierarchical Clustering made Simple,https://ICML.cc//virtual/2025/poster/43797,"Emanuele Palumbo, Moritz Vandenhirtz, Alain Ryser, Imant Daunhawer, Julia Vogt","The hierarchical structure inherent in many real-world datasets makes the modeling of such hierarchies a crucial objective in both unsupervised and supervised machine learning. While recent advancements have introduced deep architectures specifically designed for hierarchical clustering, we adopt a critical perspective on this line of research. Our findings reveal that these methods face significant limitations in scalability and performance when applied to realistic datasets.~Given these findings, we present an alternative approach and introduce a lightweight method that builds on pre-trained non-hierarchical clustering models. Remarkably, our approach outperforms specialized deep models for hierarchical clustering, and it is broadly applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our approach, we extend its application to a supervised setting, demonstrating its ability to recover meaningful hierarchies from a pre-trained ImageNet classifier. Our results establish a practical and effective alternative to existing deep hierarchical clustering methods, with significant advantages in efficiency, scalability and performance.","Many real-world problems involve organizing data into hierarchies. Think for instance of categorizing natural species at different taxonomy levels, such as species, genus, and family. Due to the wide spread of this problem, AI models designed specifically for categorizing unlabeled data in a hierarchy have been proposed. However, currently they do not perform well on large or complex datasets.To solve this problem we took a step back and explored an alternative strategy. There are highly effective AI models that learn to discover meaningful groupings (or clusters) in unlabelled data - though they are not able to model a hierarchy of these clusters. Notably, these models can perform very well even on large or complex datasets. Hence we designed an efficient algorithm that takes the outputs of these models and organizes the clusters that are found into a meaningful hierarchy.Surprisingly, this simple approach not only runs faster and scales better, but also consistently outperforms the more complex, specialized methods. Thereby we enable to discover meaningful hierarchies in datasets that are larger in size and complexity, compared to the ones that existing methods for this task could handle. Finally we show that our approach can be valuable also in setups where labels for the data are available."
