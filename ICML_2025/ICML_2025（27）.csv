type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A Two-Stage Learning-to-Defer Approach for Multi-Task Learning,https://ICML.cc//virtual/2025/poster/43923,"Yannis Montreuil, Shu Heng Yeo, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi","The Two-Stage Learning-to-Defer (L2D) framework has been extensively studied for classification and, more recently, regression tasks. However, many real-world applications require solving both tasks jointly in a multi-task setting. We introduce a novel Two-Stage L2D framework for multi-task learning that integrates classification and regression through a unified deferral mechanism. Our method leverages a two-stage surrogate loss family, which we prove to be both Bayes-consistent and $(\mathcal{G}, \mathcal{R})$-consistent, ensuring convergence to the Bayes-optimal rejector. We derive explicit consistency bounds tied to the cross-entropy surrogate and the $L_1$-norm of agent-specific costs, and extend minimizability gap analysis to the multi-expert two-stage regime. We also make explicit how shared representation learning—commonly used in multi-task models—affects these consistency guarantees. Experiments on object detection and electronic health record analysis demonstrate the effectiveness of our approach and highlight the limitations of existing L2D methods in multi-task scenarios.","Many real-world applications—such as object detection or electronic health record analysis—require solving classification and regression jointly. However, existing Learning-to-Defer methods treat these tasks separately, leading to suboptimal or inconsistent decisions. We propose a unified Two-Stage Learning-to-Defer framework for multi-task learning, enabling coordinated deferral across both tasks. Our method introduces theoretical guarantees, including Bayes-consistent surrogate losses, tight consistency bounds, and novel minimizability gap characterizations. It also provides insight into the interplay between shared representation learning, consistency bounds, and minimizability gap."
Poster,Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities,https://ICML.cc//virtual/2025/poster/43577,"Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, S Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, Bryan Catanzaro","Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert-annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach.","People and AI need to understand not only speech but also everyday sounds and music to interact meaningfully with the world. Traditional AI models can handle short snippets of audio but fail to make sense of longer, more complex sound events like a five-minute song or a sequence of machine noises. We created Audio Flamingo 2, a new AI system that learns to “listen” and “reason” about sounds over several minutes by training it on millions of audio clips paired with descriptive text. We improved the audio “ears” (an encoder called AF-CLAP) by feeding it diverse examples, including synthetic questions that teach the system how to answer complex “what” and “when” questions about sounds. We also used a step-by-step training plan that first aligns audio and text, then teaches reasoning on short clips, and finally extends to long audio segments up to five minutes. As a result, Audio Flamingo 2 outperforms larger models on a range of tests, like identifying instruments in music or tracking a conversation in noisy environments. By making AI better at understanding long sounds, our work opens doors to improved assistive devices, smarter monitoring systems, and more engaging audio-based applications."
Poster,Auditing $f$-differential privacy in one run,https://ICML.cc//virtual/2025/poster/45436,"Saeed Mahloujifar, Luca Melis, Kamalika Chaudhuri","Empirical auditing has emerged as a means of catching some of the flaws in the implementation of privacy-preserving algorithms. Existing auditing mechanisms, however, are either computationally inefficient -- requiring multiple runs of the machine learning algorithms —- or suboptimal in calculating an empirical privacy. In this work, we present a tight and efficient auditing procedure and analysis that can effectively assess the privacy of mechanisms. Our approach is efficient; Similar to the recent work of Steinke, Nasr and Jagielski (2023), our auditing procedure leverages the randomness of examples in the input dataset and requires only a single run of the target mechanism. And it is more accurate; we provide a novel analysis that enables us to achieve tight empirical privacy estimates by using the hypothesized $f$-DP curve of the mechanism, which provides a more accurate measure of privacy than the traditional $\epsilon,\delta$ differential privacy parameters. We use our auditing procure and analysis to obtain empirical privacy, demonstrating that our auditing procedure delivers tighter privacy estimates.","We design tests to check if a machine learning algorithm truly protects privacy as claimed. These tests are similar to known attacks that try to tell whether specific data was used during training, but our method only needs to run the algorithm once—making it more efficient. Compared to earlier approaches, our method catches more privacy failures."
Poster,Auditing Prompt Caching in Language Model APIs,https://ICML.cc//virtual/2025/poster/44473,"Chenchen Gu, Xiang Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto","Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.","When you send a prompt to a large language model (LLM) like ChatGPT, it may temporarily store your prompt so it can respond faster the next time a matching prompt is sent. We discovered that this method, called “prompt caching”, can accidentally reveal your private messages to other users.If a user sends a prompt to the LLM and receives an unusually fast response time, this means that the prompt is currently saved in the server’s memory. Therefore, the user can conclude that someone else recently sent that prompt. Since this is a violation of user privacy, we wanted to audit real-world LLMs to determine if they are performing prompt caching.We used careful statistical testing to develop and conduct our audits. We detected privacy risks from prompt caching in seven LLM companies, including OpenAI, the creator of ChatGPT.Before publicly releasing our findings, we contacted these companies and worked with them to fix these vulnerabilities. Our research helps improve privacy, security, and transparency in LLMs, making them safer and more trustworthy for the millions of people who use these AI tools."
Poster,A Unified Approach to Routing and Cascading for LLMs,https://ICML.cc//virtual/2025/poster/46183,"Jasper Dekoninck, Maximilian Baader, Martin Vechev","The availability of a wide range of large language models (LLMs) embedded in various agentic systems has significantly increased the potential of model selection strategies to improve the cost-performance tradeoff. Existing strategies involve either routing, where a single model is chosen per query, or cascading, which sequentially runs increasingly larger models until a satisfactory answer is found. However, current approaches face three key limitations: they (1) lack formal proofs of optimality, (2) fail to identify the conditions under which these strategies are most effective to improve the cost-performance tradeoff, and (3) are unable to combine both paradigms for further improvements. To address these issues, we first derive a novel optimal strategy for cascading and prove the optimality of an existing routing strategy. Further, we propose *cascade routing*, a unified framework that integrates routing and cascading into a theoretically optimal strategy. Through our analysis, we identify good quality estimators as the critical factor for the success of model selection paradigms.  Finally, in our experiments, we show that cascade routing consistently outperforms the individual approaches by a large margin and we analyze quality estimators to determine when routing and/or cascading are useful paradigms for model selection.","Language models—often called ""chatbots""—come in many sizes. But always using the largest model wastes money, time, and energy. This raises an important question: can we decide when a question actually requires the full power of a large model, and when a smaller, cheaper model would be enough? Existing solutions to this problem either pick a single model upfront (""routing"") or step through models from smallest to largest (""cascading""). However, both have limitations: they lack theoretical foundations and can be expensive. In our work, we develop a thorough mathematical understanding when each of strategy works best and where they fall short. Building on this, we introduce cascade routing: a flexible approach that combines the strengths of both routing and cascading. Instead of always running models in a fixed sequence or sticking to just one, cascade routing iteratively picks the best model, and thus can skip models, reorder them, or running only as few as needed. It turns out that this method can outperform the existing strategies by up to 14%! This makes AI systems more affordable and unlocks their use in settings where resources are limited. By providing a principled approach to model selection, our work lays the groundwork for smarter deployment of AI at scale."
Poster,A Unified Comparative Study with Generalized Conformity Scores for Multi-Output Conformal Regression,https://ICML.cc//virtual/2025/poster/45852,"Victor Dheur, Matteo Fontana, Yorick Estievenart, Naomi Desobry, Souhaib Ben Taieb","Conformal prediction provides a powerful framework for constructing distribution-free prediction regions with finite-sample coverage guarantees. While extensively studied in univariate settings, its extension to multi-output problems presents additional challenges, including complex output dependencies and high computational costs, and remains relatively underexplored. In this work, we present a unified comparative study of nine conformal methods with different multivariate base models for constructing multivariate prediction regions within the same framework. This study highlights their key properties while also exploring the connections between them. Additionally, we introduce two novel classes of conformity scores for multi-output regression that generalize their univariate counterparts. These scores ensure asymptotic conditional coverage while maintaining exact finite-sample marginal coverage. One class is compatible with any generative model, offering broad applicability, while the other is computationally efficient, leveraging the properties of invertible generative models. Finally, we conduct a comprehensive empirical evaluation across 13 tabular datasets, comparing all the multi-output conformal methods explored in this work. To ensure a fair and consistent comparison, all methods are implemented within a unified code base.","When an AI makes a prediction, how certain can we be? For complex tasks, like forecasting several related economic indicators at once, we need more than a single best guess, instead we need a reliable ""range of possibilities."" However, creating these reliable ranges for multiple predictions simultaneously is a difficult and relatively unexplored problem, with no clear guidance on which technique works best.We addressed this by first conducting a large-scale, fair comparison of nine existing methods to see how they stack up. Then, we designed two new, improved approaches as generalizations of single-prediction methods. Our first new method is highly flexible and works with many different types of AI models, while our second is designed to be computationally efficient.Our work provides a clear roadmap for researchers, helping them choose the right tool for building more trustworthy AI systems that can accurately report their own uncertainty. Our new techniques offer powerful and practical options, making it easier to develop reliable models for complex, real-world prediction challenges."
Poster,A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization,https://ICML.cc//virtual/2025/poster/45581,"Nuojin Cheng, Leonard Papenmeier, Stephen Becker, Luigi Nardi","Bayesian optimization is a widely used method for optimizing expensive black-box functions, with Expected Improvement being one of the most commonly used acquisition functions. In contrast, information-theoretic acquisition functions aim to reduce uncertainty about the function’s optimum and are often considered fundamentally distinct from EI. In this work, we challenge this prevailing perspective by introducing a unified theoretical framework, Variational Entropy Search, which reveals that EI and information-theoretic acquisition functions are more closely related than previously recognized. We demonstrate that EI can be interpreted as a variational inference approximation of the popular information-theoretic acquisition function, named Max-value Entropy Search. Building on this insight, we propose VES-Gamma, a novel acquisition function that balances the strengths of EI and MES. Extensive empirical evaluations across both low- and high-dimensional synthetic and real-world benchmarks demonstrate that VES-Gamma is competitive with state-of-the-art acquisition functions and in many cases outperforms EI and MES.","Imagine trying to find the best recipe for a cake by baking as few cakes as possible, since each one takes a lot of time and ingredients. This is similar to a problem in optimization where we want to find the best settings for a complex system without too many expensive experiments. Researchers have developed different strategies for this, but two popular approaches were thought to be fundamentally distinct, like choosing the next recipe based on either what is most likely to be the best (Expected Improvement) or what will teach us the most about all possible good recipes (Max-value Entropy Search).Our research shows that these two strategies are surprisingly connected. We developed a new framework that reveals the ""most likely to be best"" strategy is actually a simplified version of the ""learn the most"" strategy. This new understanding allowed us to create a hybrid strategy, called VES-Gamma, that combines the advantages of both.This matters because our new hybrid approach outperforms the individual strategies in many scenarios, from complex simulations to real-world problems. This enables scientists and engineers to find better solutions to their problems faster and with fewer costly experiments, accelerating discovery and innovation."
Poster,A Unified Framework for Generalization Error Analysis of Learning with Arbitrary Discrete Weak Features,https://ICML.cc//virtual/2025/poster/44244,"Kosuke Sugiyama, Masato Uchida","In many real-world applications, predictive tasks inevitably involve low-quality input features (Weak Features; WFs) which arise due to factors such as misobservations, missingness, or partial observations. While several methods have been proposed to estimate the true values of specific types of WFs and to solve a downstream task, a unified theoretical framework that comprehensively addresses these methods remains underdeveloped. In this paper, we propose a unified framework called Weak Features Learning (WFL), which accommodates arbitrary discrete WFs and a broad range of learning algorithms, and we demonstrate its validity. Furthermore, we introduce a class of algorithms that learn both the estimation model for WFs and the predictive model for a downstream task and perform a generalization error analysis under finite-sample conditions. Our results elucidate the interdependencies between the estimation errors of WFs and the prediction error of a downstream task, as well as the theoretical conditions necessary for the learning approach to achieve consistency. This work establishes a unified theoretical foundation, providing generalization error analysis and performance guarantees, even in scenarios where WFs manifest in diverse forms.","How the quality of input information affects the performance of predictive models trained using machine learning techniques? Specifically, we examine the relationship between the quality of input data and the resulting performance of predictive models trained on such improved inputs. We show that the quality of input data substantially influences the number of training samples required to achieve a target level of predictive accuracy. Our findings provide a foundation for theoretical investigations into the interplay between data quality and learning performance."
Poster,A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO,https://ICML.cc//virtual/2025/poster/44971,"Xingyu Zhou, Yulian Wu, Francesco Orabona","In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios.","When training AI systems to follow human instructions, it’s important to learn from human preferences while keeping those preferences private and safe from manipulation. In this work, we explore what happens when the labels—people’s feedback—are both noisy and privacy-protected. We compare two possible situations: one where privacy is applied first and then the data is tampered with, and another where tampering happens first and then privacy is applied. We find that the order of these steps matters: protecting data before tampering makes the learning task much harder. Our analysis helps explain why some methods for training AI systems might perform better than others when dealing with noisy, private data—and provides guidance on how to build more reliable and trustworthy AI."
Poster,A Unified View on Learning Unnormalized Distributions via Noise-Contrastive Estimation,https://ICML.cc//virtual/2025/poster/44988,"Jongha (Jon) Ryu, Abhin Shah, Gregory Wornell","This paper studies a family of estimators based on noise-contrastive estimation (NCE) for learning unnormalized distributions. The main contribution of this work is to provide a unified perspective on various methods for learning unnormalized distributions, which have been independently proposed and studied in separate research communities, through the lens of NCE. This unified view offers new insights into existing estimators. Specifically, for exponential families, we establish the finite-sample convergence rates of the proposed estimators under a set of regularity assumptions, most of which are new.","Some of the most flexible and expressive probabilistic models in machine learning, such as those used to model natural images or simulate physical systems, are unnormalized probability distributions, commonly known as energy-based models. These models assign a score, or ""energy"", to each possible outcome; lower energy corresponds to higher probability. This makes them powerful tools for capturing complex patterns in data. However, there is a trade-off: these models do not provide exact probabilities because they avoid computing a costly ""normalizing constant"". In other words, this flexibility is a double-edged sword, as it enables rich modeling but makes learning from data more difficult. Across statistics, computational physics, and machine learning, various methods have been proposed to learn with such unnormalized models. In this work, we show that many of these seemingly different approaches can be understood through a common principle called noise-contrastive estimation (NCE), which learns by contrasting real data with random noise. Our framework connects ideas across disciplines, clarifies how these methods work, and offers a unified and systematic view of how quickly they can learn from data. We believe these insights bring us a step closer to making energy-based models more practical and widely usable."
