type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,HYGMA: Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44143,"Chiqiang Liu, Dazi Li","Cooperative multi-agent reinforcement learning faces significant challenges in effectively organizing agent relationships and facilitating information exchange, particularly when agents need to adapt their coordination patterns dynamically. This paper presents a novel framework that integrates dynamic spectral clustering with hypergraph neural networks to enable adaptive group formation and efficient information processing in multi-agent systems. The proposed framework dynamically constructs and updates hypergraph structures through spectral clustering on agents' state histories, enabling higher-order relationships to emerge naturally from agent interactions. The hypergraph structure is enhanced with attention mechanisms for selective information processing, providing an expressive and efficient way to model complex agent relationships. This architecture can be implemented in both value-based and policy-based paradigms through a unified objective combining task performance with structural regularization. Extensive experiments on challenging cooperative tasks demonstrate that our method significantly outperforms state-of-the-art approaches in both sample efficiency and final performance. The code is available at: https://github.com/mysteryelder/HYGMA.","Teaching agents to work together as a team is challenging. Imagine coaching a soccer team where players need to figure out who should pass to whom or when to form a defensive line. That's similar to what we're trying to solve in AI. We've created a new approach that helps computer programs learn to collaborate more effectively. Instead of making all AI agents talk to everyone else (which gets chaotic) or putting them in permanent teams (which isn't flexible), our method lets them figure out their own groupings based on what they're doing. It's like how soccer players naturally form small groups during a game - defenders work together, forwards coordinate, but these groups change as the game evolves. Our system uses a clever math technique to spot these natural groupings as the agents interact. When we tested our approach in different team challenges, the agents learned to coordinate much better and faster than with existing methods. They could adapt their teamwork patterns when needed, just like good human teams do."
Poster,Hyperband-based Bayesian Optimization for Black-box Prompt Selection,https://ICML.cc//virtual/2025/poster/45571,"Lennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, Felice Antonio Merra","Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks, especially in black-box settings where models are only accessible via APIs.Black-box prompt selection is challenging due to potentially large, combinatorial search spaces, absence of gradient information, and high evaluation cost of prompts on a validation set.We propose HbBoPs, a novel method that combines a structural-aware deep kernel Gaussian Process with Hyperband as a multi-fidelity scheduler to efficiently select prompts.HbBoPs uses embeddings of instructions and few-shot exemplars, treating them as modular components within prompts.This enhances the surrogate model's ability to predict which prompt to evaluate next in a sample-efficient manner.Hyperband improves query-efficiency by adaptively allocating resources across different fidelity levels, reducing the number of validation instances required for evaluating prompts.Extensive experiments across ten diverse benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods in both performance and efficiency.","Large language models, like ChatGPT, can answer questions, solve problems, or write text.But how well they do often depends on how we ask them.Finding the best way to ask (called a ""prompt"") can be tricky, especially when using commercial models where we do not have access to their inner workings.Trying out lots of different prompts can be time-consuming and expensive.We created a new method, called HbBoPs, to help find better prompts more efficiently.It breaks each prompt into two parts, the instructions and the examples, and learns which combinations are most likely to work well. It also uses a clever way of testing prompts quickly and cheaply before spending more time and resources on the most promising ones.We tested HbBoPs across a wide range of tasks and language models.Compared to existing methods, It generally found better prompts while using fewer model calls.This means it can help people get more out of powerful language tools while saving time and cost and therefore makes these tools easier to use in everyday applications."
Poster,Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations,https://ICML.cc//virtual/2025/poster/45129,"Juwei Yue, Haikuo Li, Jiawei Sheng, Xiaodong Li, Taoyu Su, Tingwen Liu, Li Guo","Graph neural networks (GNNs) leverage message passing mechanisms to learn the topological features of graph data. Traditional GNNs learns node features in a spatial domain unrelated to the topology, which can hardly ensure topological features. In this paper, we formulates message passing as a system of hyperbolic partial differential equations (hyperbolic PDEs), constituting a dynamical system that explicitly maps node representations into a particular solution space. This solution space is spanned by a set of eigenvectors describing the topological structure of graphs. Within this system, for any moment in time, a node features can be decomposed into a superposition of the basis of eigenvectors. This not only enhances the interpretability of message passing but also enables the explicit extraction of fundamental characteristics about the topological structure. Furthermore, by solving this system of hyperbolic partial differential equations, we establish a connection with spectral graph neural networks (spectral GNNs), serving as a message passing enhancement paradigm for spectral GNNs.We further introduce polynomials to approximate arbitrary filter functions. Extensive experiments demonstrate that the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also significantly enhances the performance of various spectral GNNs across diverse graph tasks.","Graphs reflects relationships between objects of the real world, such as social networks with connections between individuals. Our aim is to learn the feature representations for nodes (i.e., people) through the structure of graphs, which can be useful for downstream applications such as node classification. Modern methods usually learn features of a node by passing messages from its neighboring nodes. This paper uses a mathematical tool named partial differential equation (PDE) to describe the message passing. PDE explains how nodes and their relationships evolve over time. By conducting extensive experiments, we discovered that PDEs have advantages in learning graph structural features, and can significantly enhance the learning capabilities of existing methods."
Poster,Hyper: Hyperparameter Robust Efficient Exploration in Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44124,"Yiran Wang, Chenshu Liu, Yunfan Li, Sanae Amani, Bolei Zhou, Lin Yang","The exploration \& exploitation dilemma poses significant challenges in reinforcement learning (RL). Recently, curiosity-based exploration methods achieved great success in tackling hard-exploration problems. However, they necessitate extensive hyperparameter tuning on different environments, which heavily limits the applicability and accessibility of this line of methods. In this paper, we characterize this problem via analysis of the agent behavior, concluding the fundamental difficulty of choosing a proper hyperparameter. We then identify the difficulty and the instability of the optimization when the agent learns with curiosity. We propose our method, hyperparameter robust exploration (\textbf{Hyper}), which extensively mitigates the problem by effectively regularizing the visitation of the exploration and decoupling the exploitation to ensure stable training. We theoretically justify that \textbf{Hyper} is provably efficient under function approximation setting and empirically demonstrate its appealing performance and robustness in various environments.",We design and build a novel exploration algorithm that is robust and efficient.
Poster,HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting,https://ICML.cc//virtual/2025/poster/43741,"Boyuan Li, Yicheng Luo, Zhen Liu, Junhao Zheng, Jianming Lv, Qianli Ma","Irregular multivariate time series (IMTS) are characterized by irregular time intervals within variables and unaligned observations across variables, posing challenges in learning temporal and variable dependencies. Many existing IMTS models either require padded samples to learn separately from temporal and variable dimensions, or represent original samples via bipartite graphs or sets.However, the former approaches often need to handle extra padding values affecting efficiency and disrupting original sampling patterns, while the latter ones have limitations in capturing dependencies among unaligned observations.To represent and learn both dependencies from original observations in a unified form, we propose HyperIMTS, a **Hyper**graph neural network for **I**rregular **M**ultivariate **T**ime **S**eries forecasting.Observed values are converted as nodes in the hypergraph, interconnected by temporal and variable hyperedges to enable message passing among all observations.Through irregularity-aware message passing, HyperIMTS captures variable dependencies in a time-adaptive way to achieve accurate forecasting. Experiments demonstrate HyperIMTS's competitive performance among state-of-the-art models in IMTS forecasting with low computational cost.Our code is available at [https://github.com/qianlima-lab/PyOmniTS](https://github.com/qianlima-lab/PyOmniTS).","Many real-world datasets, like medical records or sensor readings, track multiple measurements over time—but these measurements aren’t always taken at regular intervals or aligned with each other. This makes it hard for traditional models to comprehensively learn underlying patterns, as they often rely on fixed time steps or padding (adding artificial data), which can slower the processing speed and distort results. To tackle the problem, we developed HyperIMTS, a new method that treats each observed data point as part of a flexible network (a hypergraph), where connections capture both time and variable relationships between different measurements. Instead of forcing data into rigid structures, our model adapts to irregular gaps and mismatched observations, learning dependencies comprehensively and efficiently. Our approach improves forecasting accuracy for real-world irregular multivariate time series data—like predicting patient health trends—without heavy computational costs."
Poster,HyperIV: Real-time Implied Volatility Smoothing,https://ICML.cc//virtual/2025/poster/44077,"Yongxin Yang, Wenqi Chen, Chao Shu, Timothy Hospedales","We propose HyperIV, a novel approach for real-time implied volatility smoothing that eliminates the need for traditional calibration procedures. Our method employs a hypernetwork to generate parameters for a compact neural network that constructs complete volatility surfaces within 2 milliseconds, using only 9 market observations. Moreover, the generated surfaces are guaranteed to be free of static arbitrage. Extensive experiments across 8 index options demonstrate that HyperIV achieves superior accuracy compared to existing methods while maintaining computational efficiency. The model also exhibits strong cross-asset generalization capabilities, indicating broader applicability across different market instruments. These key features -- rapid adaptation to market conditions, guaranteed absence of arbitrage, and minimal data requirements -- make HyperIV particularly valuable for real-time trading applications. We make code available at https://github.com/qmfin/hyperiv.","In the fast-paced world of options trading, a complete and accurate implied volatility surface is crucial for tasks like pricing and hedging. However, creating this surface instantly, especially using only a few current market prices (common in high-frequency trading or for illiquid assets), and ensuring it is free from theoretical inconsistencies that allow risk-free profits (arbitrage), has been a major challenge.To address this, we developed HyperIV, an AI system that learns from vast amounts of historical market data. When presented with just a handful of live option prices – as few as nine – HyperIV instantly constructs the complete, smooth volatility surface in under two milliseconds, guaranteeing it is arbitrage-free.HyperIV provides traders with a highly accurate and near-instantaneous view of market volatility, even with very limited real-time data. This enables faster, more reliable trading decisions and improved risk management. Its strong performance across various financial instruments also indicates broad applicability, offering a significant advancement for real-time financial modelling in today's dynamic markets."
Poster,HyperNear: Unnoticeable Node Injection Attacks on Hypergraph Neural Networks,https://ICML.cc//virtual/2025/poster/45156,"Tingyi Cai, Yunliang Jiang, Ming Li, Lu Bai, Changqin Huang, Yi Wang","With the growing adoption of Hypergraph Neural Networks (HNNs) to model higher-order relationships in complex data, concerns about their security and robustness have become increasingly important. However, current security research often overlooks the unique structural characteristics of hypergraph models when developing adversarial attack and defense strategies. To address this gap, we demonstrate that hypergraphs are particularly vulnerable to node injection attacks, which align closely with real-world applications. Through empirical analysis, we develop a relatively unnoticeable attack approach by monitoring changes in homophily and leveraging this self-regulating property to enhance stealth. Building on these insights, we introduce HyperNear, i.e., $\underline{N}$ode inj$\underline{E}$ction $\underline{A}$ttacks on hype$\underline{R}$graph neural networks, the first node injection attack framework specifically tailored for HNNs. HyperNear integrates homophily-preserving strategies to optimize both stealth and attack effectiveness. Extensive experiments show that HyperNear achieves excellent performance and generalization, marking the first comprehensive study of injection attacks on hypergraphs. Our code is available at https://github.com/ca1man-2022/HyperNear.","Modern AI tools are getting better at understanding complex systems, such as how people interact on social media or how diseases spread. A new kind of AI model, called a hypergraph neural network, is especially good at this. But we found that it may also be easier to fool than expected.Our research shows that by adding just a few fake data points, these systems can be misled into making wrong predictions. These fake points can be carefully crafted to blend in, making the attack hard to notice. We built a tool called HyperNear to study this issue. It creates smart, hidden attacks that work well even when the attacker doesn’t know much about the system.This is the first study to show how vulnerable these models can be in such situations. We hope our work will help researchers build more secure AI systems that are ready for the real world."
Poster,Hyperspherical Normalization for Scalable Deep Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44241,"Hojoon Lee, Youngdo Lee, Takuma Seno, Donghu Kim, Peter Stone, Jaegul Choo","Scaling up the model size and computation has brought consistent performance improvements in supervised learning. However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization.In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by hyperspherical normalization; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains.","Scaling up models improves performance in supervised learning but often leads to instability in reinforcement learning due to non-stationary data. SimbaV2 addresses this by constraining the growth of parameter and gradient norms, stabilizing optimization. This enables effective scaling with larger models and compute, achieving state-of-the-art performance in reinforcement learning tasks."
Poster,Hyper-Transforming Latent Diffusion Models,https://ICML.cc//virtual/2025/poster/43516,"Ignacio Peis, Batuhan Koyuncu, Isabel Valera, Jes Frellsen","We introduce a novel generative framework for functions by integrating Implicit Neural Representations (INRs) and Transformer-based hypernetworks into latent variable models. Unlike prior approaches that rely on MLP-based hypernetworks with scalability limitations, our method employs a Transformer-based decoder to generate INR parameters from latent variables, addressing both representation capacity and computational efficiency. Our framework extends latent diffusion models (LDMs) to INR generation by replacing standard decoders with a Transformer-based hypernetwork, which can be trained either from scratch or via hyper-transforming—a strategy that fine-tunes only the decoder while freezing the pre-trained latent space. This enables efficient adaptation of existing generative models to INR-based representations without requiring full retraining. We validate our approach across multiple modalities, demonstrating improved scalability, expressiveness, and generalization over existing INR-based generative models. Our findings establish a unified and flexible framework for learning structured function representations.","Modern AI models often treat data like images, 3D shapes, or climate maps as fixed grids of values. This discretized view overlooks the continuous nature of real-world signals, potentially discarding rich structural information. Earlier approaches tried to address this with implicit neural representations (INRs), but often struggled to efficiently model complex data, limiting scalability across tasks and resolutions.We propose a generative model that represents data as continuous functions rather than discrete grids. By combining powerful diffusion models with a lightweight Transformer decoder, it learns flexible internal representations for generating, reconstructing, and completing data across diverse domains. Crucially, it builds on pretrained models and requires training only a small adapter, making it scalable and efficient.This work paves the way for AI systems that are more adaptable and resolution-independent. It has implications for fields like medical imaging, scientific simulations, and graphics, where high-quality, flexible data generation is essential."
Poster,HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking,https://ICML.cc//virtual/2025/poster/46506,"Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian, Enhong Chen, Feng Wu","Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning.However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks.To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning.The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner.We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines.Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6$\times$ performance improvement over o1-preview.","Many real-world tasks—such as planning a multi-day trip, organizing a complex schedule, or making long-term decisions—require advanced reasoning and step-by-step planning. Current large language models (LLMs) often struggle with these complex tasks due to their lack of explicit structure and long-term coherence.We present HyperTree Planning (HTP), a new method that enables LLMs to solve complex tasks by automatically decomposing them into subgoals using a tree-structured framework. This tree structure allows the model to plan and execute sub-tasks recursively, while a self-reflection mechanism guides the model to revise and improve its reasoning throughout the process. The entire pipeline is fully automated, without any human intervention.HTP significantly improves LLMs' ability to complete complex tasks accurately and efficiently. It outperforms strong baselines in travel planning, instructional generation, and embodied AI tasks. Our method demonstrates that LLM agents can autonomously perform multi-step reasoning and handle diverse real-world needs with high reliability and precision."
