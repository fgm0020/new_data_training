type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Sassha: Sharpness-aware Adaptive Second-order Optimization with Stable Hessian Approximation,https://ICML.cc//virtual/2025/poster/46316,"Dahun Shin, Dongyeop Lee, Jinseok Chung, Namhoon Lee","Approximate second-order optimization methods often exhibit poorer generalization compared to first-order approaches. In this work, we look into this issue through the lens of the loss landscape and find that existing second-order methods tend to converge to sharper minima compared to SGD.In response, we propose Sassha, a novel second-order method designed to enhance generalization by explicitly reducing sharpness of the solution, while stabilizing the computation of approximate Hessians along the optimization trajectory.In fact, this sharpness minimization scheme is crafted also to accommodate lazy Hessian updates, so as to secure efficiency besides flatness.To validate its effectiveness, we conduct a wide range of standard deep learning experiments where Sassha demonstrates its outstanding generalization performance that is comparable to, and mostly better than, other methods.We provide a comprehensive set of analyses including convergence, robustness, stability, efficiency, and cost.","How can we accelerate learning? One promising approach is to use second-order optimization methods, which utilize second-order derivatives to speed up convergence during training. Ironically, however, these methods often struggle to generalize to unseen data, thus missing the whole point of accelerating “learning”. What's going wrong, and how can we fix it?Our in-depth study suggests sharp minima as the culprit—a growing idea in recent research that makes intuitive sense: when the curvature of the loss landscape near minima is too steep, even small input changes can hurt performance. Building on this insight, we propose SASSHA, a novel second-order method designed to stably flatten the loss curvature. SASSHA not only improves generalization but also enhances efficiency by reducing the need for frequent second-order derivative computations, a primary source of computational overhead in second-order methods.In conclusion, we present a practical path for realizing the potential of second-order optimization methods through recovering their generalization and efficiency. We expect this methodology to further expand the applicability of second-order methods across a wide range of learning domains."
Poster,Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search,https://ICML.cc//virtual/2025/poster/44324,"Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan","Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: *Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM?* This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (*i.e.,* an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.","Since the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. This includes various approaches, including distillation using a strong teacher model, MCTS, and reward model guided search. This work aims to explore a new research direction: enabling LLMs with autoregressive search capabilities, i.e., a single LLM performs an extended reasoning process with self-reflection and self-exploration of new strategies. To achieve this, we develop a LLM post-training paradigm with several key concepts and ideas inspired by classical reinforcement learning (RL) communities. Our approach results in Satori, a 7B LLM trained on open-source model and data. Key features of Satori include: 1) Capable of self-reflection and self-exploration without external guidance; 2) Achieve state-of-the-art reasoning performance mainly through self-improvement (RL). 3) Exhibit transferability of reasoning capabilities on unseen domains beyond math."
Poster,SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion,https://ICML.cc//virtual/2025/poster/45891,"Junwei Su, shan Wu","Graph diffusion generative models (GDGMs) have emerged as powerful tools for generating high-quality graphs. However, their broader adoption faces challenges in \emph{scalability and size generalization}. GDGMs struggle to scale to large graphs due to their high memory requirements, as they typically operate in the full graph space, requiring the entire graph to be stored in memory during training and inference. This constraint limits their feasibility for large-scale real-world graphs. GDGMs also exhibit poor size generalization, with limited ability to generate graphs of sizes different from those in the training data, restricting their adaptability across diverse applications. To address these challenges, we propose the stochastic block graph diffusion (SBGD) model, which refines graph representations into a block graph space. This space incorporates structural priors based on real-world graph patterns, significantly reducing memory complexity and enabling scalability to large graphs. The block representation also improves size generalization by capturing fundamental graph structures.   Empirical results show that SBGD achieves significant memory improvements (up to 6$\times$) while maintaining comparable or even superior graph generation performance relative to state-of-the-art methods. Furthermore, experiments demonstrate that SBGD better generalizes to unseen graph sizes. The significance of SBGD extends beyond being a scalable and effective GDGM; \emph{it also exemplifies the principle of modularization in generative modelling, offering a new avenue for exploring generative models by decomposing complex tasks into more manageable components.}","The paper addresses the challenge of generating large-scale graphs using graph diffusion generative models (GDGMs), which typically struggle with high memory usage and poor adaptability to graph sizes different from their training data. To solve this, the authors introduce the stochastic block graph diffusion (SBGD) model. SBGD simplifies graph representation into modular ""blocks,"" significantly reducing memory requirements and enhancing scalability. This block-based approach also enables the model to better generalize to new graph sizes not seen during training."
Poster,Scaffold with Stochastic Gradients: New Analysis with Linear Speed-Up,https://ICML.cc//virtual/2025/poster/46580,"Paul Mangold, Alain Oliviero Durmus, Aymeric Dieuleveut, Eric Moulines","This paper proposes a novel analysis for the Scaffold algorithm, a popular method for dealing with data heterogeneity in federated learning. While its convergence in deterministic settings—where local control variates mitigate client drift—is well established, the impact of stochastic gradient updates on its performance is less understood. To address this problem, we first show that its global parameters and control variates define a Markov chain that converges to a stationary distribution in the Wasserstein distance. Leveraging this result, we prove that Scaffold achieves linear speed-up in the number of clients up to higher-order terms in the step size. Nevertheless, our analysis reveals that Scaffold retains a higher-order bias, similar to FedAvg, that does not decrease as the number of clients increases. This highlights opportunities for developing improved stochastic federated learning algorithms.","Federated learning is a way to train machine learning models across many devices (like smartphones) without needing to gather all their data in one place. A popular method in federated learning is Scaffold, which allows to learn correctly while reducing the number of communications. While Scaffold is well studied with exact updates, it’s less clear how it performs when updates are based on noisy or approximate information, which often happens in practice.This paper takes a fresh look at Scaffold under these realistic conditions. We show that, over time, the shared model and Scaffold's control variables settle into a stable state. Thanks to this insight, we prove that adding more devices helps the model learn faster, up to a certain limit. We also highlight that Scaffold still suffers from another bias that doesn’t go away, no matter how many devices are added. This suggests there’s still room to improve federated learning methods to make them more accurate and efficient in real-world settings."
Poster,Scalable Approximation Algorithms for $p$-Wasserstein Distance and Its Variants,https://ICML.cc//virtual/2025/poster/46422,"Nathaniel Lahn, Sharath Raghvendra, Emma Saarinen, Pouyan Shirzadian","The $p$-Wasserstein distance measures the cost of optimally transporting one distribution to another, where the cost of moving a unit mass from $a$ to $b$ is the $p^{th}$ power of the ground distance $\mathrm{d}(a,b)$ between them. Despite its strong theoretical properties, its use in practice  -- especially for $p \ge 2$ -- is limited due to two key challenges: sensitivity to noise and a lack of scalable algorithms.We identify noise sensitivity as a key reason why some existing approximation algorithms for $p=1$ fail to generalize to $p \ge 2$ and then present new algorithms for approximating the $p$-Wasserstein distance and its variant. First, when $\mathrm{d}(\cdot,\cdot)$ is a metric, for any constant $p \ge 2$, we present a novel relative $O(\log n)$-approximation algorithm to compute the $p$-Wasserstein distance between any two discrete distributions of size $n$. The algorithm runs in $O(n^2 \log U\log \Delta\log n)$ time, where $\log U$ is the bit-length of the input probabilities and $\Delta$ is the ratio of the largest to the smallest pairwise distance. We use $p$ hierarchically well-separated trees to define a distance that approximates the $p$-Wasserstein cost within a factor of $O(\log n)$ and then present a simple primal-dual algorithm to compute the $p$-Wasserstein cost with respect to this distance. Second, due to the noise sensitivity of the $p$-Wasserstein distance, we show that existing combinatorial approaches require $\Omega(n^2/\delta^p)$ time to approximate the $p$-Wasserstein distance within an additive error of $\delta$. In contrast, we show that, for any arbitrary distance $\mathrm{d}(\cdot,\cdot)$, a recent noise-resistant variant of the $p$-Wasserstein distance, called the $p$-RPW distance, can be approximated in $O(n^2/\delta^3)$ time.","The $p$-Wasserstein distance is a mathematical tool for measuring the similarity between two probability distributions. It quantifies the effort required to transform one distribution into another by moving probability mass, where the cost of moving a unit mass is given by the $p^{\text{th}}$ power of the ground distance between points. Despite its strong theoretical foundations, its practical use—especially for $p \geq 2$—is limited by high computational costs, largely due to its high sensitivity to noise. This research explains why algorithms that perform well for $p=1$ often fail to scale to higher values of $p$ and introduces new algorithms to address these challenges. The first algorithm offers a provably accurate approximation of the $p$-Wasserstein distance by using hierarchical, graph-based structures to efficiently approximate distances. The second part of the work shows that while traditional methods become prohibitively slow for higher values of $p$ due to increased sensitivity to noise, a newer and more robust variant—the $p$-RPW distance—can be approximated significantly faster, making it a practical alternative in such scenarios."
Poster,Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiation,https://ICML.cc//virtual/2025/poster/45307,"Yaowenhu, Wenxuan Tu, Yue Liu, Xinhang Wan, Junyi Yan, Taichun Zhou, Xinwang Liu","Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes in an attribute graph into different clusters, has seen substantial potential in various industrial scenarios like community detection and recommendation. However, the real-world attribute graphs, e.g., social networks interactions, are usually large-scale and attribute-missing. To solve these two problems, we propose a novel DGC method termed **C**omplementary **M**ulti-**V**iew **N**eighborhood **D**ifferentiation ($\textit{CMV-ND}$), which preprocesses graph structural information into multiple views in a complete but non-redundant manner. First, to ensure completeness of the structural information, we propose a recursive neighborhood search that recursively explores the local structure of the graph by completely expanding node neighborhoods across different hop distances. Second, to eliminate the redundancy between neighborhoods at different hops, we introduce a neighborhood differential strategy that ensures no overlapping nodes between the differential hop representations. Then, we construct $K+1$ complementary views from the $K$ differential hop representations and the features of the target node. Last, we apply existing multi-view clustering or DGC methods to the views. Experimental results on six widely used graph datasets demonstrate that CMV-ND significantly improves the performance of various methods.","Real-world graphs, such as those underlying social media and e-commerce platforms, are often massive and incomplete because many nodes lack attribute information. Grouping similar nodes in such graphs is essential for tasks like community detection and personalized recommendation. However, this becomes highly challenging when the graphs are too large or the attribute data is missing. We propose a new method that captures more comprehensive structural information for each node by examining multiple neighborhood levels—for example, direct neighbors, second-hop neighbors, and beyond. Each resulting “view” offers unique and complementary insights, contributing to a richer and more informative node representation. To avoid redundancy, we retain only the distinct information at each level, similar to peeling an onion to reveal non-overlapping layers. This layered perspective helps clustering algorithms more effectively identify structural patterns among nodes. Evaluated on six real-world datasets, our method consistently outperforms existing approaches, demonstrating its effectiveness in analyzing large-scale, incomplete graphs."
Poster,Scalable Equilibrium Sampling with Sequential Boltzmann Generators,https://ICML.cc//virtual/2025/poster/45137,"Charlie Tan, Joey Bose, Chen Lin, Leon Klein, Michael Bronstein, Alexander Tong","Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann generators tackle this problem by pairing normalizing flows with importance sampling to obtain uncorrelated samples under the target distribution. In this paper, we extend the Boltzmann generator framework with two key contributions, denoting our framework Sequential Boltzmann Generators (SBG). The first is a highly efficient Transformer-based normalizing flow operating directly on all-atom Cartesian coordinates. In contrast to the equivariant continuous flows of prior methods, we leverage exactly invertible non-equivariant architectures which are highly efficient during both sample generation and likelihood evaluation. This efficiency unlocks more sophisticated inference strategies beyond standard importance sampling. In particular, we perform inference-time scaling of flow samples using a continuous-time variant of sequential Monte Carlo, in which flow samples are transported towards the target distribution with annealed Langevin dynamics. SBG achieves state-of-the-art performance w.r.t. all metrics on peptide systems, demonstrating the first equilibrium sampling in Cartesian coordinates of tri-, tetra- and hexa-peptides that were thus far intractable for prior Boltzmann generators.","Simulating the behavior of molecules is essential for science but takes a huge amount of time. We built a machine learning model that can generate realistic molecular structures much faster than traditional methods. It learns from existing data and improves its guesses using a process inspired by physics. Our method scales to larger molecules than ever before, which could help accelerate drug and material discovery."
Poster,Scalable First-order Method for Certifying Optimal k-Sparse GLMs,https://ICML.cc//virtual/2025/poster/46512,"Jiachang Liu, Soroosh Shafiee, Andrea Lodi","This paper investigates the problem of certifying optimality for sparse generalized linear models (GLMs), where sparsity is enforced through an $\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can certify optimality by pruning nodes using dual bounds, existing methods for computing these bounds are either computationally intensive or exhibit slow convergence, limiting their scalability to large-scale problems. To address this challenge, we propose a first-order proximal gradient algorithm designed to solve the perspective relaxation of the problem within a BnB framework. Specifically, we formulate the relaxed problem as a composite optimization problem and demonstrate that the proximal operator of the non-smooth component can be computed exactly in log-linear time complexity, eliminating the need to solve a computationally expensive second-order cone program. Furthermore, we introduce a simple restart strategy that enhances convergence speed while maintaining low per-iteration complexity. Extensive experiments on synthetic and real-world datasets show that our approach significantly accelerates dual bound computations and is highly effective in providing optimality certificates for large-scale problems.","While continuous optimization has made great strides thanks to fast algorithms and GPU acceleration, discrete optimization — where some variables must take integer values — has lagged behind. In this work, we ask: can new tools from continuous optimization and modern hardware help scale discrete problems to much larger sizes? We focus on sparse generalized linear models (GLMs), where the goal is to find simple, accurate models using only ""$k$"" predictive features. This constraint makes the problem discrete and challenging, especially when optimality must be guaranteed.Our key contribution is a fast, scalable algorithm for computing tight lower bounds — a critical step in the standard framework, *i.e.*, the branch-and-bound algorithm. What’s surprising is that we discovered hidden mathematical structure in the model that allows us designing a first-order optimization method with three rare properties: (1) each step is very simple and only involves matrix-vector multiplication (making it GPU-friendly), (2) the method converges at the fastest possible rate (linear convergence rate) for this class of algorithms, and (3) each step is computationally cheap in practice.Our work makes it much faster to solve these problems and opens new possibilities for interpretable machine learning in high-stakes settings. It enables researchers to build sparse models that are optimal for large-scale datasets, potentially allowing us to identify key biomarkers in medical diagnosis or discover differential equations from empirical data in physics."
Poster,Scalable Gaussian Processes with Latent Kronecker Structure,https://ICML.cc//virtual/2025/poster/45475,"Jihao Andreas Lin, Sebastian Ament, Maximilian Balandat, David Eriksson, Jose Miguel Hernandez-Lobato, Eytan Bakshy","Applying Gaussian processes (GPs) to very large datasets remains a challenge due to limited computational scalability. Matrix structures, such as the Kronecker product, can accelerate operations significantly, but their application commonly entails approximations or unrealistic assumptions. In particular, the most common path to creating a Kronecker-structured kernel matrix is by evaluating a product kernel on gridded inputs that can be expressed as a Cartesian product. However, this structure is lost if any observation is missing, breaking the Cartesian product structure, which frequently occurs in real-world data such as time series. To address this limitation, we propose leveraging latent Kronecker structure, by expressing the kernel matrix of observed values as the projection of a latent Kronecker product. In combination with iterative linear system solvers and pathwise conditioning, our method facilitates inference of exact GPs while requiring substantially fewer computational resources than standard iterative methods. We demonstrate that our method outperforms state-of-the-art sparse and variational GPs on real-world datasets with up to five million examples, including robotics, automated machine learning, and climate applications.","Gaussian processes are a flexible machine learning model, but it can be difficult to apply them to large amounts of data because they require a lot of computation. For specific types of data, these computations can be done efficiently. For example, if the data consists of temperature measurements across different cities and days, we can simplify the computations by considering cities and days separately. However, this typically requires that a temperature measurement is available for each pair of city and day. We introduce a way to make computations efficient even if, for example, temperature measurements are not available for certain cities on some days. In experiments using real-world robotics, machine learning, and climate data, we demonstrate that our method is faster and performs better than other scalable alternatives."
Poster,Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching,https://ICML.cc//virtual/2025/poster/45412,"Tinglin Huang, Tianyu Liu, Mehrtash Babadi, Wengong Jin, ZHITAO YING","Spatial transcriptomics (ST) has emerged as a powerful technology for bridging histology imaging with gene expression profiling. However, its application has been limited by low throughput and the need for specialized experimental facilities. Prior works sought to predict ST from whole-slide histology images to accelerate this process, but they suffer from two major limitations. First, they do not explicitly model cell-cell interaction as they factorize the joint distribution of whole-slide ST data and predict the gene expression of each spot independently. Second, their encoders struggle with memory constraints due to the large number of spots (often exceeding 10,000) in typical ST datasets. Herein, we propose STFlow, a flow matching generative model that considers cell-cell interaction by modeling the joint distribution of gene expression of an entire slide. It also employs an efficient slide-level encoder with local spatial attention, enabling whole-slide processing without excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M benchmarks, STFlow substantially outperforms state-of-the-art baselines and achieves over 18% relative improvements over the pathology foundation models.","Understanding how genes are active in different parts of a tissue is important for learning more about diseases like cancer. In this study, we developed a new computational model, STFlow, that can predict gene activity using images of tissue samples. The model works step by step, gradually improving its predictions to be more accurate. This approach is both efficient and effective, helping researchers better identify important biological markers that may guide diagnosis or treatment."
