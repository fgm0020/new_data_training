type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Near-Optimal Decision Trees in a SPLIT Second,https://ICML.cc//virtual/2025/poster/46182,"Varun Babbar, Hayden McTavish, Cynthia Rudin, Margo Seltzer","Decision tree optimization is fundamental to interpretable machine learning. The most popular approach is to greedily search for the best feature at every decision point, which is fast but provably suboptimal. Recent approaches find the global optimum using branch and bound with dynamic programming, showing substantial improvements in accuracy and sparsity at great cost to scalability. An ideal solution would have the accuracy of an optimal method and the scalability of a greedy method. We introduce a family of algorithms called SPLIT (SParse Lookahead for Interpretable Trees) that moves us significantly forward in achieving this ideal balance. We demonstrate that not all sub-problems need to be solved to optimality to find high quality trees; greediness suffices near the leaves. Since each depth adds an exponential number of possible trees, this change makes our algorithms orders of magnitude faster than existing optimal methods, with negligible loss in performance. We extend this algorithm to allow scalable computation of sets of near-optimal trees (i.e., the Rashomon set).","Decision trees ask simple questions about data to make a prediction. They can be easily interpreted as a flowchart. However, it is difficult to find well performing decision trees for a given task.Most popular algorithms are ""greedy"": they focus on the next best question to ask at every step without considering whether it leads to the best overall outcome. They are fast, but the resulting flowcharts can be much bigger than needed, and their predictions may not be as accurate. We might instead find the mathematically ""optimal"" flowchart that is also simple. But this requires us to go through all possible simple flowcharts to prove we've found the best one. It's like choosing the best question only after thinking through all paths it could lead to in the future. It yields accurate flowcharts, but it can be quite slow.We bridge the gap between ""greedy"" and ""optimal"" algorithms by building flowcharts (aka decision trees) by asking questions based on information we acquire a few steps into the future, rather than thinking through all possibilities. The algorithm is almost as fast as greedy approaches, but yields comparable accuracy to optimal approaches."
Poster,Near Optimal Non-asymptotic Sample Complexity of 1-Identification,https://ICML.cc//virtual/2025/poster/44087,"Zitian Li, Wang Chi Cheung","Motivated by an open direction in existing literature, we study the 1-identification problem, a fundamental multi-armed bandit formulation on pure exploration. The goal is to determine whether there exists an arm whose mean reward is at least a known threshold $\mu_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-\delta$. Degenne & Koolen 2019 has established the asymptotically tight sample complexity for the 1-identification problem, but they commented that the non-asymptotic analysis remains unclear. We design a new algorithm Sequential-Exploration-Exploitation (SEE), and conduct theoretical analysis from the non-asymptotic perspective. Novel to the literature, we achieve near optimality, in the sense of matching upper and lower bounds on the pulling complexity. The gap between the upper and lower bounds is up to a polynomial logarithmic factor. The numerical result also indicates the effectiveness of our algorithm, compared to existing benchmarks.","We work on 1-identification, which is an open problem discussed by multiple papers. We propose a new algorithm SEE to solve the problem and conduct non-asymptotic theoretical analysis on SEE. Both the theoretical and numeric performance suggest the excellency of this new algorithm. And our work fills in a blank area in the academic community."
Poster,Near-optimal Regret Using Policy Optimization in Online MDPs with Aggregate Bandit Feedback,https://ICML.cc//virtual/2025/poster/45869,"Tal Lancewicki, Yishay Mansour","We study online finite-horizon Markov Decision Processes with adversarially changing loss and aggregate bandit feedback (a.k.a full-bandit). Under this type of feedback, the agent observes only the total loss incurred over the entire trajectory, rather than the individual losses at each intermediate step within the trajectory. We introduce the first Policy Optimization algorithms for this setting. In the known-dynamics case, we achieve the first *optimal* regret bound of $\tilde \Theta(H^2\sqrt{SAK})$, where $K$ is the number of episodes, $H$ is the episode horizon, $S$ is the number of states, and $A$ is the number of actions. In the unknown dynamics case we establish regret bound of $\tilde O(H^3 S \sqrt{AK})$, significantly improving the best known result by a factor of $H^2 S^5 A^2$.","We study the challenge of training reinforcement learning agents when feedback is only available as the total loss at the end of each episode, a situation known as aggregate bandit feedback. This is common in settings like robotics or dialogues with an LLM, where step-by-step feedback is typically unavailable. Our work introduces the first Policy Optimization algorithms for this problem. When the environment’s dynamics are known, our method achieves the first optimal regret bound (a common performance measure) for this setting. When the dynamics are unknown, our approach substantially improves regret compared to previous best results, marking a significant step forward for learning in environments with limited feedback."
Poster,Near-Optimal Sample Complexity for MDPs via Anchoring,https://ICML.cc//virtual/2025/poster/46059,"Jongmin Lee, Mario Bravo, Roberto Cominetti","We study a new model-free algorithm to compute  $\varepsilon$-optimal policies for average reward Markov decision processes, in the weakly communicating setting. Given a generative model, our procedure combines a recursive sampling technique with Halpern's anchored iteration, and computes an $\varepsilon$-optimal policy with sample and time complexity $\widetilde{O}(|\mathcal{S}||\mathcal{A}|\||h\||^{2}/\varepsilon^{2})$  both in high probability and in expectation.  To our knowledge, this is the best complexity among model-free algorithms, matching the known lower bound up to a factor $ \||h\|| $.  Although the complexity bound involves the span seminorm $ \||h\|| $ of the unknown  bias vector, the algorithm requires no  prior knowledge  and  implements a stopping rule which guarantees  with probability 1 that the procedure terminates in finite time. We also analyze how these techniques can be adapted for discounted MDPs.","We propose a new algorithm, SAVIA+, for solving average reward Markov decision processes (MDPs) in the generative model setting. To the best of our knowledge, SAVIA+ is the most efficient model-free algorithm to date, nearly matching the lower bound on sample complexity. Unlike many prior methods, it does not require any prior knowledge of the optimal solution. Our approach combines two key ideas, recursive sampling and Halpern's anchored iteration, and we also show how these techniques can be extended to address discounted MDPs."
Poster,Near-optimal Sketchy Natural Gradients for Physics-Informed Neural Networks,https://ICML.cc//virtual/2025/poster/44747,"Maricela Best Mckay, Avleen Kaur, Chen Greif, Brian Wetton","Natural gradient methods for PINNs have achieved state-of-the-art performance with errors several orders of magnitude smaller than those achieved by standard optimizers such as ADAM or L-BFGS. However, computing natural gradients for PINNs is prohibitively computationally costly and memory-intensive for all but small neural network architectures. We develop a randomized algorithm for natural gradient descent for PINNs that uses sketching to approximate the natural gradient descent direction. We prove that the change of coordinate Gram matrix used in a natural gradient descent update has rapidly-decaying eigenvalues for a one-layer, one-dimensional neural network and empirically demonstrate that this structure holds for four different example problems. Under this structure, our sketching algorithm is guaranteed to provide a near-optimal low-rank approximation of the Gramian. Our algorithm dramatically speeds up computation time and reduces memory overhead. Additionally, in our experiments, the sketched natural gradient outperforms the original natural gradient in terms of accuracy, often achieving an error that is an order of magnitude smaller. Training time for a network with around 5,000 parameters is reduced from several hours to under two minutes. Training can be practically scaled to large network sizes; we optimize a PINN for a network with over a million parameters within a few minutes, a task for which the full Gram matrix does not fit in memory.","Mathematical modeling and computing are key tools in scientific research. Among the many uses of simulations are: the identification of potential underlying mechanisms, fitting observations to theory,  gaining insight into inaccessible processes, and supplementing or replacing impractical physical experiments or measurements. Physics-based models are made up of systems of Partial Differential Equations (PDEs) that often span multiple physical or temporal scales and involve multiple interacting physical phenomena. These systems are solved numerically to generate a simulation.  Although numerous tools exist for analyzing and numerically solving a variety of PDEs, many problems of scientific interest remain computationally intractable or require significant simplification. Physics Informed Neural Networks (PINNs) have been used to model multi-scale and multi-physics phenomena, numerically solve high-dimensional systems of PDEs, and to combine incomplete mechanistic understanding with data. Although physics-informed learning has shown enormous potential, these networks can be very difficult to train and don't achieve high-levels of accuracy needed for some kinds of simulations. Recently, methods to train PINNs to higher accuracy have emerged, called energy natural gradients, but they are computationally costly for all but small networks, limiting their applicability.In our work, we come up with a method to scale natural gradient methods, making them more computationally efficient and usable for large network sizes. Our method also improves the accuracy of these natural gradients for PINNs."
Poster,NegMerge: Sign-Consensual Weight Merging for Machine Unlearning,https://ICML.cc//virtual/2025/poster/44843,"Hyo Seo Kim, Dongyoon Han, Junsuk Choe","Machine unlearning aims to selectively remove specific knowledge from a trained model. Existing approaches, such as Task Arithmetic, fine-tune the model on the forget set to create a task vector (i.e., a direction in weight space) for subtraction from the original model's weight. However, their effectiveness is highly sensitive to hyperparameter selection, requiring extensive validation to identify the optimal vector from many fine-tuned candidates. In this paper, we propose a novel method that utilizes all fine-tuned models trained with varying hyperparameters instead of a single selection. Specifically, we aggregate the computed task vectors by retaining only the elements with consistent shared signs. The merged task vector is then negated to induce unlearning on the original model. Evaluations on zero-shot and standard image recognition tasks across twelve datasets and four backbone architectures show that our approach outperforms state-of-the-art methods while requiring similar or fewer computational resources. Code is available at https://github.com/naver-ai/negmerge.","AI models learn from large amounts of data, but when someone asks for their data to be removed, it’s difficult to make the model forget just that part. Existing solutions, known as machine unlearning, are often sensitive to training settings and can hurt the model’s overall performance.To address this, we introduce a new method called NegMerge. Instead of picking just one model from many training runs and hoping it works, NegMerge uses all of them. It identifies what the models consistently agree should be forgotten and merges those parts to remove unwanted information, without affecting the rest.NegMerge helps AI to forget specific data while preserving the rest of its knowledge intact, without wasting time on choosing a single best model. This makes unlearning not only more effective but also faster and more reliable."
Poster,Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling,https://ICML.cc//virtual/2025/poster/44946,"Xinxing Shi, Xiaoyu Jiang, Mauricio Álvarez","Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by replacing the fully factorised Gaussian prior with a GP prior, thereby capturing richer correlations among latent variables. However, performing exact GP inference in large-scale GPVAEs is computationally prohibitive, often forcing existing approaches to rely on restrictive kernel assumptions or large sets of inducing points. In this work, we propose a neighbour-driven approximation strategy that exploits local adjacencies in the latent space to achieve scalable GPVAE inference. By confining computations to the nearest neighbours of each data point, our method preserves essential latent dependencies, allowing more flexible kernel choices and mitigating the need for numerous inducing points. Through extensive experiments on tasks including representation learning, data imputation, and conditional generation, we demonstrate that our approach outperforms other GPVAE variants in both predictive performance and computational efficiency.","Machines often learn from data that comes with a natural order—for example, the frames of a video, sensor readings over time, or measurements across a map. To understand such data, researchers use a tool called a variational auto-encoder (VAE), which compresses each data point into a small “code” inside the model. Classic VAEs treat every point as independent, so they miss the fact that neighbouring points—adjacent frames, nearby sensors—are usually related.Our work shows how to keep those neighbour relationships without slowing the model down. We give each hidden code a short “friends list,” asking it to pay attention only to its closest neighbours. This tiny change lets the model keep the benefits of neighbourhood awareness while staying computationally efficient. We build two versions of the idea and test them on tasks such as repairing missing video frames, predicting future steps in simulated robots, and filling gaps in large environmental maps. In every case, the neighbour-aware models are faster and more accurate than earlier approaches."
Poster,Nemotron-CORTEXA: Enhancing LLM Agents for Software Engineering Tasks via Improved Localization and Solution Diversity,https://ICML.cc//virtual/2025/poster/44274,"Atefeh Sohrabizadeh, Jialin Song, Mingjie Liu, Rajarshi Roy, Chankyu Lee, Jonathan Raiman, Bryan Catanzaro","Large Language Models (LLMs) have demonstrated significant potential in code generation by following natural language instructions.  Unfortunately, crucial real-world software engineering tasks, such as debugging or repository-level feature implementation, involve processing extensive contexts beyond current LLM context sizes and performing complex reasoning that is brittle using standard autoregressive decoding. Enhancing LLMs' performance in these scenarios requires careful consideration of the contextual information provided to the model, optimizing how the model leverages that, and identifying tools that enable more effective navigation of the development environment.To address these challenges, we introduce Nemotron-CORTEXA, an agentic system built on a predefined scaffold that enhances LLMs' ability to navigate and reason efficiently in complex software engineering contexts. Specifically, we develop a novel code embedding model that retrieves the most relevant files with greater precision, along with a localization agent that refines the granularity of the retrieval process. Additionally, we demonstrate that providing diverse contextual information and utilizing different prompt formats enable the model to identify and resolve issues more efficiently. We evaluate Nemotron-CORTEXA using SWE-bench, a benchmark derived from real-world GitHub issues. Compared to the widely used Agentless framework, Nemotron-CORTEXA achieves a higher issue resolution rate at a lower cost, highlighting its practical impact in addressing real-world software engineering challenges.","Advanced AI models, known as Large Language Models (LLMs), can now write code from plain English instructions. While they're great for simple tasks, they often hit a wall when faced with large, real-world software projects. These projects often contain too much code for a model to understand, and tasks like fixing bugs or adding features involve complex reasoning beyond the current models’ capability.We built Nemotron-CORTEXA, a system to help LLMs work more effectively in large software projects. It contains special search tools to find exact code files relevant to a task and then zooms in to find precise lines. It also presents information to the models in diverse ways, improving their understanding and increasing their chances of success.When tested on real software bugs from GitHub, Nemotron-CORTEXA fixed more issues than previous methods and did so more cost-effectively, highlighting its strong potential for tackling complex software challenges in the real world."
Poster,Nested Expectations with Kernel Quadrature,https://ICML.cc//virtual/2025/poster/45450,"Zonghao Chen, Masha Naslidnyk, Francois-Xavier Briol","This paper considers the challenging computational task of estimating nested expectations. Existing algorithms, such as nested Monte Carlo or multilevel Monte Carlo, are known to be consistent but require a large number of samples at both inner and outer levels to converge. Instead, we propose a novel estimator consisting of nested kernel quadrature estimators and we prove that it has a faster convergence rate than all baseline methods when the integrands have sufficient smoothness. We then demonstrate empirically that our proposed method does indeed require the fewest number of samples to estimate nested expectations over a range of real-world application areas from Bayesian optimisation to option pricing and health economics.","This paper addresses the problem of estimating nested expectations, or computing double integrals. We propose a novel estimator that achieves improved sample efficiency and lower computational cost. This problem is particularly important in scientific applications such as decision-making and risk management in finance."
Poster,Nesterov Method for Asynchronous Pipeline Parallel Optimization,https://ICML.cc//virtual/2025/poster/46245,"Thalaiyasingam Ajanthan, Sameera Ramasinghe, Yan Zuo, Gil Avraham, Alexander Long","Pipeline Parallelism (PP) enables large neural network training on small, interconnected devices by splitting the model into multiple stages. To maximize pipeline utilization, asynchronous optimization is appealing as it offers 100% pipeline utilization by construction. However, it is inherently challenging as the weights and gradients are no longer synchronized, leading to *stale (or delayed) gradients*. To alleviate this, we introduce a variant of Nesterov Accelerated Gradient (NAG) for asynchronous optimization in PP. Specifically, we modify the look-ahead step in NAG to effectively address the staleness in gradients. We theoretically prove that our approach converges at a sublinear rate in the presence of fixed delay in gradients. Our experiments on large-scale language modelling tasks using decoder-only architectures with up to **1B parameters**, demonstrate that our approach significantly outperforms existing asynchronous methods, even surpassing the synchronous baseline.","Training very large neural networks often requires splitting the model into parts and running them across several smaller devices. If the connection bandwidth between these devices is low (e.g., the internet), the devices would stay idle due to communication delays. Asynchronous optimization eliminates this idle time by ensuring all devices are active at all times. This comes at a cost of incorrect (or delayed) information being used for training, which often affects model performance. We address this by predicting the future state of the model using a look-ahead approach, effectively removing inaccuracies. We provide a theoretical guarantee that our approach still converges. Our experiments in training large language models show that our asynchronous method not only improves device utilization but also improves the final model performance compared to synchronized training. This shows the possibility of training large AI models using devices connected via the internet, instead of expensive centralized infrastructures."
