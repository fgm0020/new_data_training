type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A-PSRO: A Unified Strategy Learning Method with Advantage Metric for Normal-form Games,https://ICML.cc//virtual/2025/poster/43705,"Yudong Hu, Haoran Li, Congying Han, Tiande Guo, Bonan Li, Mingqiang Li","Solving the Nash equilibrium in normal-form games with large-scale strategy spaces presents significant challenges. Open-ended learning frameworks, such as PSRO and its variants, have emerged as effective solutions. However, these methods often lack an efficient metric for evaluating strategy improvement, which limits their effectiveness in approximating equilibria.In this paper, we introduce a novel evaluative metric called Advantage, which possesses desirable properties inherently connected to the Nash equilibrium, ensuring that each strategy update approaches equilibrium. Building upon this, we propose the Advantage Policy Space Response Oracle (A-PSRO), an innovative unified open-ended learning framework applicable to both zero-sum and general-sum games. A-PSRO leverages the Advantage as a refined evaluation metric, leading to a consistent learning objective for agents in normal-form games. Experiments showcase that A-PSRO significantly reduces exploitability in zero-sum games and improves rewards in general-sum games, outperforming existing algorithms and validating its practical effectiveness.","Game theory primarily studies strategic interactions among multiple rational agents, and it can be used to explain real-world scenarios in politics, economics, and common games such as chess and card games. Nash equilibrium represents a stable state achieved through strategic improvements by these agents and is often considered the strongest strategy in a game. Thus, solving for a Nash equilibrium is equivalent to finding the optimal solution of the game. Previous research proposed PSRO as an efficient algorithm for computing Nash equilibria, but its efficiency is affected by the randomness in strategy exploration. This paper introduces the advantage function as an evaluation metric for strategy exploration. With favorable theoretical properties, it accelerates the computation of Nash equilibria. Based on this, we propose the A-PSRO algorithm, which significantly improves equilibrium solving in games."
Poster,Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics Emulation,https://ICML.cc//virtual/2025/poster/44945,"Da Long, Zhitong Xu, Guang Yang, Akil Narayan, Shandian Zhe","Modern physics simulation often involves multiple functions of interests, and traditional numerical approaches are known to be complex and computationally costly. While machine learning-based surrogate models can offer significant cost reductions, most focus on a single task, such as forward prediction, and typically lack uncertainty quantification --- an essential component in many applications. To overcome these limitations, we propose Arbitrarily-Conditioned Multi-Functional Diffusion (ACM-FD), a versatile probabilistic surrogate model for multi-physics emulation. ACM-FD can perform a wide range of tasks within a single framework, including forward prediction, various inverse problems, and simulating data for entire systems or subsets of quantities conditioned on others. Specifically, we extend the standard Denoising Diffusion Probabilistic Model (DDPM) for multi-functional generation by modeling noise as Gaussian processes (GP). We propose a random-mask based, zero-regularized denoising loss to achieve flexible and robust conditional generation. We induce a Kronecker product structure in the GP covariance matrix, substantially reducing the computational cost and enabling efficient training and sampling. We demonstrate the effectiveness of ACM-FD across several fundamental multi-physics systems.","Simulating complex physical systems often involves many interacting components, such as pressure, velocity, and temperature. In real-world applications, scientists and engineers may want to predict some components based on others, complete missing information, estimate uncertainties of predictions, or simulate the entire system. Most machine learning models today are designed to do only one of these tasks at a time. This paper introduces ACM-FD (Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics Emulation), a probabilistic model that solves all these problems in one model within a single training. It uses diffusion models to generate and predict multiple physical functions simultaneously. This unified, multi-task capability makes modeling complex, interdependent physical systems more robust, flexible, efficient, and practical."
Poster,Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models,https://ICML.cc//virtual/2025/poster/46195,"Thomas Fel, Ekdeep Singh Lubana, Jacob Prince, Matthew Kowal, Victor Boutin, Isabel Papadimitriou, Binxu Wang, Martin Wattenberg, Demba Ba, Talia Konkle","Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine learning interpretability, enabling the unsupervised decomposition of model representations into a dictionary of abstract, human-interpretable concepts. However, we reveal a fundamental limitation: SAEs exhibit severe instability, as identical models trained on similar datasets can produce sharply different dictionaries, undermining their reliability as an interpretability tool. To address this issue, we draw inspiration from the Archetypal Analysis framework introduced by Cutler & Breiman (1994) and present Archetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the data’s convex hull. This geometric anchoring significantly enhances the stability and plausibility of inferred dictionaries, and their mildly relaxed variants RA-SAEs further match state-of-the-art reconstruction abilities. To rigorously assess dictionary quality learned by SAEs, we introduce two new benchmarks that test (i) plausibility, if dictionaries recover “true” classification directions and (ii) identifiability, if dictionaries disentangle synthetic concept mixtures. Across all evaluations, RA-SAEs consistently yield more structured representations while uncovering novel, semantically meaningful concepts in large-scale vision models.","Neural networks often make decisions using internal representations that are difficult for humans to interpret. One promising approach to explainability is to extract a set of internal “concepts” — directions in the model’s representation space that act like a dictionary the model uses to make sense of the world. These concepts can help us understand what features the model is using, and why it makes certain predictions.However, current methods for building these concept dictionaries are unstable: small changes in the data or random choices during training can lead to completely different explanations. This instability makes it hard to trust or reproduce the results.Our work introduces a new method, Archetypal Sparse Autoencoders, that builds more reliable and interpretable concept dictionaries by geometrically anchoring them to the training data. We also design new evaluation benchmarks to measure whether the learned concepts align with ground truth and remain consistent across training runs. Our approach improves the stability and quality of concept-based explanations in large vision models, helping researchers and practitioners better understand how these systems work — and why."
Poster,A Reasoning-Based Approach to Cryptic Crossword Clue Solving,https://ICML.cc//virtual/2025/poster/44268,"Martin Andrews, Sam Witteveen","Cryptic crossword clues are challenging language tasks for which new test sets are released daily by major newspapers on a global basis. Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and ‘wordplay’ that *proves* that the answer is correct (i.e. a human solver can be confident that an answer is correct without needing crossing words as confirmation). This work describes an LLM-based reasoning system built from open-licensed components that solves cryptic clues by (i) hypothesising answers; (ii) proposing wordplay explanations; and (iii) using a verifier system that operates on codified reasoning steps. Overall, this system establishes a new state-of-the-art performance on the challenging Cryptonite dataset of clues from The Times and The Telegraph newspapers in the UK. Because each proved solution is expressed in Python, interpretable wordplay reasoning for proven answers is available for inspection.","We're interested in a type of puzzle common in major newspapers (in the UK, and elsewhere) : Cryptic Crosswords. Each cryptic clue hints to its unique answer in two ways : a 'regular crossword definition' and 'wordplay'. Because the wordplay and definition must have the same answer, solvers know whether they've got the answer correct (even without having other answers in the crossword grid).Our work uses a large language model (LLM) to guess possible answers, and then justify the reasoning, finally delivering its solution in the Python programming language. By getting the reasoning as a small computer program, we can easily tell whether the LLM has got the reasoning correct, and this enables our method to beat ChatGPT and other models.Although we focused on Cryptic Crosswords, our ideas could also be applied to other linguistic tasks, opening them up to methods that are more commonly used for mathematics and programming problems."
Poster,A Recipe for Causal Graph Regression: Confounding Effects Revisited,https://ICML.cc//virtual/2025/poster/45617,"Yujia Yin, Tianyi Qu, Zihao Wang, Yifan Chen","Through recognizing causal subgraphs, causal graph learning (CGL) has risen to be a promising approach for improving the generalizability of graph neural networks under out-of-distribution (OOD) scenarios. However, the empirical successes of CGL techniques are mostly exemplified in classification settings, while regression tasks, a more challenging setting in graph learning, are overlooked. We thus devote this work to tackling causal graph regression (CGR); to this end we reshape the processing of confounding effects in existing CGL studies, which mainly deal with classification. Specifically, we reflect on the predictive power of confounders in graph-level regression, and generalize classification-specific causal intervention techniques to regression through a lens of contrastive learning. Extensive experiments on graph OOD benchmarks validate the efficacy of our proposals for CGR. The model implementation and the code are provided on https://github.com/causal-graph/CGR.","AI systems learning from complex networks (graphs) often struggle when faced with unfamiliar data. While ""causal learning"" helps these systems focus on true causes, making them more robust, it has primarily been applied to categorizing items rather than predicting specific numbers—a tougher challenge for AI on graphs.Our research introduces a new method for applying causal learning to predict numbers on graphs. This approach addresses misleading information, known as ""confounders,"" by adapting established techniques. Furthermore, we utilize ""contrastive learning."" This enables us to successfully extend causal methods, which were previously specific to classification (sorting items), to the task of regression (predicting numerical values).The result is AI that can make more reliable numerical predictions on graphs, even as data changes. This is crucial for developing trustworthy AI in fields like materials science or economics, where data is inherently complex and ever-evolving."
Poster,A Reduction Framework for Distributionally Robust Reinforcement Learning under Average Reward,https://ICML.cc//virtual/2025/poster/46132,"Zachary Roch, George Atia, Yue Wang","Robust reinforcement learning (RL) under the average reward criterion, which seeks to optimize long-term system performance in uncertain environments, remains a largely unexplored area. To address this challenge, we propose a reduction-based framework that transforms robust average reward optimization into the more extensively studied robust discounted reward optimization by employing a specific discount factor. Our framework provides two key advantages. **Data Efficiency**: We design a  model-based reduction algorithm that achieves near-optimal sample complexity, enabling efficient identification of optimal robust policies; **Scalability**: By bypassing the inherent challenges of scaling up average reward optimization, our framework facilitates the design of scalable, convergent algorithms for robust average reward optimization leveraging function approximation. Our algorithmic design, supported by theoretical and empirical analyses, provides a concrete solution to robust average reward RL with the first data efficiency and scalability guarantees, highlighting the framework’s potential to optimize long-term performance under model uncertainty in practical problems.","For complex systems such as self-driving cars or automated financial portfolio management, each decision, i.e. turning the car down a road or buying/selling a security, must add up to a strong “long-term average” performance. In simulation-based settings, such as video games, reinforcement learning typically performs well due to similarities between training and testing environments, but in more practical, real-world settings, we observe a deterioration in performance commonly referred to as the “Sim-to-Real” gap. This can result from a myriad of factors like modeling errors or environmental perturbations. To address this, our work shows a way to convert the difficult “long-term average” goal into the more manageable “discounted” step-by-step formulation by finding a specific discount factor. This allows us to find robust strategies to optimally trade off the immediate and future rewards while accounting for various types of environmental uncertainty, i.e. safely driving to a location on a sunny day versus on a rainy day. Our method learns robust, long-term strategies that optimize the average reward in a data-efficient manner by constructing a model of the environment as well as a scalable version that bypasses this step to tackle large-scale, complex problems."
Poster,A Reductions Approach to Risk-Sensitive Reinforcement Learning with Optimized Certainty Equivalents,https://ICML.cc//virtual/2025/poster/44224,"Kaiwen Wang, Dawen Liang, Nathan Kallus, Wen Sun","We study risk-sensitive RL where the goal is learn a history-dependent policy that optimizes some risk measure of cumulative rewards.We consider a family of risks called the optimized certainty equivalents (OCE), which captures important risk measures such as conditional value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. In this setting, we propose two meta-algorithms: one grounded in optimism and another based on policy gradients, both of which can leverage the broad suite of risk-neutral RL algorithms in an augmented Markov Decision Process (MDP). Via a reductions approach, we leverage theory for risk-neutral RL to establish novel OCE bounds in complex, rich-observation MDPs. For the optimism-based algorithm, we prove bounds that generalize prior results in CVaR RL and that provide the first risk-sensitive bounds for exogenous block MDPs. For the gradient-based algorithm, we establish both monotone improvement and global convergence guarantees under a discrete reward assumption. Finally, we empirically show that our algorithms learn the optimal history-dependent policy in a proof-of-concept MDP, where all Markovian policies provably fail.","In high-stakes settings (e.g., healthcare, finance, systems), we often care not only about the average outcome, but also about avoiding bad outcomes, tail events, or reducing variance. Our paper proposes a framework for solving these risk-sensitive applications via reinforcement learning with the optimized certainty equivalent—a broad class of risk measures that captures important cases such as Conditional Value-at-Risk (CVaR) and mean-variance. We reduce the challenging risk-sensitive RL problem into a standard RL problem, enabling the use of many existing algorithms from the literature. By combining our reduction with risk-neutral RL methods, we derive strong theoretical guarantees even in tasks with high-dimensional state spaces, such as exogenous block MDPs. In sum, our work shows that practical, risk-sensitive objectives can be addressed using well-established RL techniques through a principled reduction framework."
Poster,Are High-Quality AI-Generated Images More Difficult for Models to Detect?,https://ICML.cc//virtual/2025/poster/43842,"Yao Xiao, Binbin Yang, Weiyan Chen, Jiahao Chen, Zijie Cao, ZiYi Dong, Xiangyang Ji, Liang Lin, Wei Ke, Pengxu Wei","The remarkable evolution of generative models has enabled the generation of high-quality, visually attractive images, often perceptually indistinguishable from real photographs to human eyes. This has spurred significant attention on AI-generated image (AIGI) detection. Intuitively, higher image quality should increase detection difficulty. However, our systematic study on cutting-edge text-to-image generators reveals a counterintuitive finding: AIGIs with higher quality scores, as assessed by human preference models, tend to be more easily detected by existing models. To investigate this, we examine how the text prompts for generation and image characteristics influence both quality scores and detector accuracy. We observe that images from short prompts tend to achieve higher preference scores while being easier to detect. Furthermore, through clustering and regression analyses, we verify that image characteristics like saturation, contrast, and texture richness collectively impact both image quality and detector accuracy. Finally, we demonstrate that the performance of off-the-shelf detectors can be enhanced across diverse generators and datasets by selecting input patches based on the predicted scores of our regression models, thus substantiating the broader applicability of our findings. Code and data are available at \href{https://github.com/Coxy7/AIGI-Detection-Quality-Paradox}{GitHub}.","As AI-generated images become incredibly realistic, telling them apart from real photos is getting harder, posing challenges for content authenticity and security risks. This problem makes developing reliable AI image detectors crucial.Intriguingly, we've found a surprising twist: high-quality AI-generated images, which people usually prefer, are usually easier for current detectors to spot. To understand why, we explored what makes these high-quality images unique. Our research shows that images with certain visual traits, like high color saturation, strong contrast, rich textures, or simpler structures, tend to be both human-preferred and easier for AI detectors to identify.Our findings not only help explain this unexpected link between image quality and detectability but also show how we can use this knowledge. By focusing detection on parts of images that have these ""easy-to-spot"" characteristics, we can make existing AI image detectors perform better. This work guides future efforts to build more effective tools for identifying AI-generated content in the real world."
Poster,Are Large Brainwave Foundation Models Capable Yet ? Insights from Fine-Tuning,https://ICML.cc//virtual/2025/poster/45713,"Na Lee, Konstantinos Barmpas, Yannis Panagakis, Dimitrios Adamos, Nikolaos Laskaris, Stefanos Zafeiriou","Foundation Models have demonstrated significant success across various domains in Artificial Intelligence (AI), yet their capabilities for brainwave modeling remain unclear. In this paper, we comprehensively evaluate current Large Brainwave Foundation Models (LBMs) through systematic fine-tuning experiments across multiple Brain-Computer Interface (BCI) benchmark tasks, including memory tasks and sleep stage classification. Our extensive analysis shows that state-of-the-art LBMs achieve only marginal improvements (0.5\%) over traditional deep architectures while requiring significantly more parameters (millions vs thousands), raising important questions about their efficiency and applicability in BCI contexts. Moreover, through detailed ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce trainable parameters without performance degradation, while demonstrating that architectural and training inefficiencies limit LBMs' current capabilities. Our experiments span both full model fine-tuning and parameter-efficient adaptation techniques, providing insights into optimal training strategies for BCI applications. We pioneer the application of LoRA to LBMs, revealing that performance benefits generally emerge when adapting multiple neural network components simultaneously. These findings highlight the critical need for domain-specific development strategies to advance LBMs, suggesting that current architectures may require  redesign to fully leverage the potential of foundation models in brainwave analysis.","We tested Large Brainwave Foundation Models (LBMs) on tasks like movement and sleep stage classification. Despite their size, these models only slightly outperformed smaller ones while having used far more resources. By using techniques like LoRA, we reduced training demands without losing accuracy. Our findings suggest current LBMs are inefficient and need further redesigning to fully leverage their potential in brainwave analysis."
Poster,Are Large Language Models Ready for Multi-Turn Tabular Data Analysis?,https://ICML.cc//virtual/2025/poster/44506,"Jinyang Li, Nan Huo, Yan Gao, Jiayi Shi, Yingxiu Zhao, Qu Ge, Bowen Qin, Yurong Wu, Xiaodong Li, Chenhao Ma, Jian-Guang Lou, Reynold Cheng","Conversational Tabular Data Analysis, a collaboration between humans and machines, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic conversational logs for tabular data analysis hinder comprehensive quantitative evaluation of Large Language Models (LLMs) in this task. To mitigate this issue, we introduce **CoTA**, a new benchmark to evaluate LLMs on conversational tabular data analysis. **CoTA** contains 1013 conversations, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, **CoTA** is constructed by an economical multi-agent environment, Decision Company, with few human efforts. This environment ensures efficiency and scalability of generating new conversational data. Our comprehensive study, conducted by data analysis experts, demonstrates that Decision Company is capable of producing diverse and high-quality data, laying the groundwork for efficient data annotation. We evaluate popular and advanced LLMs in **CoTA**, which highlights the challenges of conversational tabular data analysis. Furthermore, we propose Adaptive Conversation Reflection (ACR), a self-generated reflection strategy that guides LLMs to learn from successful histories. Experiments demonstrate that ACR can evolve LLMs into effective conversational data analysis agents, achieving a relative performance improvement of up to 35.14%.","What's this research about? Imagine chatting naturally with AI about your data: asking ""Which products sold best?"" then following up with ""Show me the trend for those items"", like talking to a human analyst. This research tackles the problem that we don't have good ways to test how well AI handles these natural data conversations. The team created CoTA, a benchmark with over 1,000 realistic conversations generated by ""Decision Company,"" a virtual office where AI agents play different workplace roles and naturally discuss data. When they tested current AI agents, even advanced models struggled significantly with complex, multi-turn data conversations. Their solution, ACR (Adaptive Conversation Reflection), helps AI learn from successful past conversations, which is like a student reviewing their best work, leading to 35% better performance and bringing us closer to AI that can truly understand how we naturally want to explore data through conversation."
