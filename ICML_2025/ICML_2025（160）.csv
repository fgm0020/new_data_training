type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Learngene Tells You How to Customize: Task-Aware Parameter Initialization at Flexible Scales,https://ICML.cc//virtual/2025/poster/45736,"Jiaze Xu, Shiyu Xia, Xu Yang, JIAQI LYU, Xin Geng","Appropriate parameter initialization strategies are essential for reducing the high computational costs of training large pretrained models in various task scenarios. Graph HyperNetwork (GHN), a parameter initialization method, has recently demonstrated strong performance in initializing models.However, GHN still faces several challenges, including limited effectiveness in initializing larger models, poor performance on smaller datasets, and the requirement of task-specific GHN training, where each new task necessitates retraining the GHN model, leading to increased computational and storage overhead.To overcome these challenges, motivated by the recently proposed Learngene framework, we propose a novel method called **T**ask-**A**ware **L**earngene (**TAL**). Briefly, our approach pretrains a TAL model under the guidance of a well-trained model and then performs multi-task tuning to obtain a shared TAL model that enables parameter prediction based on both model architectures and task-specific characteristics.Extensive experiments show the superiority of TAL.Models initialized with TAL outperform those initialized using GHN method by an average of 24.39\% in terms of accuracy across Decathlon datasets.","Modern AI models often require enormous computing resources to train, especially when they are adapted to new tasks. One way to make this process more efficient is to give these models a good “starting point” — a smart guess of what their internal settings should be, before training begins.An existing method called Graph HyperNetwork (GHN) has shown promise in doing this. However, GHN struggles when models are large or when data is limited. It also needs to be retrained from scratch every time a new task comes up, which is time-consuming and inefficient.To address these problems, we introduce a new method called Task-Aware Learngene (TAL). TAL learns from a strong, existing model and then adapts to many tasks at once. This way, it can make smart predictions about how a model should start — not just based on the model's structure, but also on the nature of the task it will solve."
Poster,Learning Adaptive Lighting via Channel-Aware Guidance,https://ICML.cc//virtual/2025/poster/44791,"Qirui Yang, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Huanjing Yue, Jingyu Yang","Learning lighting adaptation is a crucial step in achieving good visual perception and supporting downstream vision tasks. Current research often addresses individual light-related challenges, such as high dynamic range imaging and exposure correction, in isolation. However, we identify shared fundamental properties across these tasks:i) different color channels have different light properties, and ii) the channel differences reflected in the spatial and frequency domains are different. Leveraging these insights, we introduce the channel-aware Learning Adaptive Lighting Network (LALNet), a multi-task framework designed to handle multiple light-related tasks efficiently. Specifically, LALNet incorporates color-separated features that highlight the unique light properties of each color channel, integrated with traditional color-mixed features by Light Guided Attention (LGA). The LGA utilizes color-separated features to guide color-mixed features focusing on channel differences and ensuring visual consistency across all channels. Additionally, LALNet employs dual domain channel modulation for generating color-separated features and a mixed channel modulation and light state space module for producing color-mixed features. Extensive experiments on four representative light-related tasks demonstrate that LALNet significantly outperforms state-of-the-art methods on benchmark tests and requires fewer computational resources.  We provide an anonymous online demo at [LALNet](https://xxxxxx2025.github.io/LALNet/).","Adapting to different lighting conditions is essential for both how humans and computers see the world. While recent research often focuses on solving single lighting-related problems, like making dark photos brighter or handling very bright and dark areas in the same image, we notice these challenges share some common traits. Specifically, different colors (like red, green, and blue) react to light in different ways, and these differences show up both in the details and the overall look of an image. Building on these insights, we introduce the channel-aware Learning Adaptive Lighting Network (LALNet), a system designed to handle a variety of lighting problems all at once. LALNet learns to treat each color channel separately, capturing what’s unique about how each one responds to light, and then combines this information using a process we call Light Guided Attention. This approach ensures the final image looks natural and balanced, with all colors working together smoothly. Our experiments show that LALNet outperforms leading methods on several important lighting tasks, and does so using fewer computing resources. You can try it out for yourself on our online demo: [LALNet](https://xxxxxx2025.github.io/LALNet/)."
Poster,Learning Adversarial MDPs with Stochastic Hard Constraints,https://ICML.cc//virtual/2025/poster/43855,"Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti","We study online learning in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints, under bandit feedback. We consider three scenarios. In the first one, we address general CMDPs, where we design an algorithm attaining sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that constraints are satisfied at every episode with high probability. In the last scenario, we only assume the existence of a strictly feasible policy, which is not known to the learner, and we design an algorithm attaining sublinear regret and constant cumulative positive constraints violation. Finally, we show that in the last two scenarios, a dependence on the Slater's parameter is unavoidable. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Thus, our algorithms can deal with general non-stationary environments subject to requirements much stricter than those manageable with existing ones, enabling their adoption in a much wider range of applications.","This work investigates online learning in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints, using bandit feedback. We propose algorithms for three scenarios, achieving sublinear regret and constraint control under various assumptions about the feasibility of policies. This study is the first to address CMDPs with both adversarial losses and hard constraints, broadening the applicability of CMDPs to more complex and demanding environments."
Poster,Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible Representation Learning,https://ICML.cc//virtual/2025/poster/45638,"Ngoc Bui, Menglin Yang, Runjin Chen, Leonardo Neves, Mingxuan Ju, ZHITAO YING, Neil Shah, Tong Zhao","Backward compatible representation learning enables updated models to integrate seamlessly with existing ones, avoiding to reprocess stored data. Despite recent advances, existing compatibility approaches in Euclidean space neglect the uncertainty in the old embedding models and force the new model to replicate outdated representations regardless of their quality, and thereby hindering the learning process. In this paper, we switch perspectives to hyperbolic geometry, where we treat time as a natural axis for capturing a model’s confidence and evolution. By lifting embeddings into hyperbolic space and constraining updated embeddings to lie within the entailment cone of the old ones, we maintain generational consistency across models while accounting for uncertainties in the representations. To further enhance compatibility, we introduce a robust contrastive alignment loss that dynamically adjusts alignment weights based on the uncertainty of the old embeddings. Experiments validate the superiority of the proposed method in achieving compatibility, paving the way for more resilient and adaptable machine learning systems.","As machine learning models evolve, their internal ""representations"" — the way they understand and organize information — often change. But this causes a problem: older data that was processed using earlier versions of these systems might no longer work with the updated versions. Reprocessing all that data can be expensive, time-consuming, and risky, especially when privacy is involved. Our research tackles this issue by designing a way for new models to stay compatible with old ones — a concept known as *backward compatibility*. Instead of forcing new models to exactly mimic outdated behavior, we treat learning as an evolving process over time. To do this, we turn to a different kind of geometry — hyperbolic space — which is better suited for modeling the evolution and uncertainty of the embedding model. By placing both old and new representations into this space, and guiding the new ones to stay consistent with the old without being limited by them, we enable models to evolve and stay compatible with previous versions. This makes machine learning systems more adaptable, future-proof, and safe to update without reprocessing all previous data."
Poster,Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization,https://ICML.cc//virtual/2025/poster/43868,"Peng Wang, Yong Li, Lin Zhao, Xiu-Shen Wei","Fine-grained hashing has become a powerful solution for rapid and efficient image retrieval, particularly in scenarios requiring high discrimination between visually similar categories. To enable each hash bit to correspond to specific visual attributes, we propose a novel method that harnesses learnable queries for attribute-aware hash code learning. This method deploys a tailored set of queries to capture and represent nuanced attribute-level information within the hashing process, thereby enhancing both the interpretability and relevance of each hash bit. Building on this query-based optimization framework, we incorporate an auxiliary branch to help alleviate the challenges of complex landscape optimization often encountered with low-bit hash codes. This auxiliary branch models high-order attribute interactions, reinforcing the robustness and specificity of the generated hash codes. Experimental results on benchmark datasets demonstrate that our method generates attribute-aware hash codes and consistently outperforms state-of-the-art techniques in retrieval accuracy and robustness, especially for low-bit hash codes, underscoring its potential in fine-grained image hashing tasks.","With the explosive growth of fine-grained data in real applications, fine-grained image retrieval has a wide range of application scenarios. Fine-grained hashing, which maps images to compact binary codes, has emerged as a promising solution for large-scale retrieval tasks, as it significantly reduces storage costs and improves retrieval speed.Our paper proposes an effective solution. We model the hashing problem as a set prediction task, where each bit of the generated hash codes indicates whether the image possesses a specific visual attribute. Additionally, we analyze the limitations caused by a large number of categories and small feature dimensions in low-bit scenarios. Based on our analysis, we seamlessly incorporate an auxiliary branch with a query learning mechanism, which significantly improves the model's performance.Extensive experimental results demonstrate that our attribute-aware hashing method not only achieves excellent retrieval performance but also shows that each bit of the hash codes is interpretable."
Poster,Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors,https://ICML.cc//virtual/2025/poster/44771,"Matei Gabriel Cosa, Marek Elias","Combining algorithms is one of the key techniques in learning-augmented algorithms.We consider the following problem:We are given $\ell$ heuristicsfor Metrical Task Systems (MTS), where each might be tailored to a different typeof input instances.While processing an input instance received online,we are allowed to query the action of only one of the heuristics at each time step.Our goal is to achieve performance comparable to the best of the given heuristics.The main difficulty of our setting comes from the fact thatthe cost paid by a heuristic at time $t$ cannot be estimatedunless the same heuristic was also queried at time $t-1$.This is related to Bandit Learning against memory boundedadversaries (Arora et al., 2012).We show how to achieve regret of $O(\text{OPT}^{2/3})$and prove a tight lower bound based on the constructionof Dekel et al. (2013).","Many complex tasks in power management, routing, production systems, and investing are characterized by a need for efficient and dynamic decision-making in the context of limited information and switching costs. One needs to make choices sequentially, without knowing what comes in the future and without being able to change the decisions already taken. Machine learning models are capable of producing informative predictions about the future, however these predictions may be arbitrarily poor. Moreover, obtaining predictions may be expensive both from a computational and financial perspective. Our task is to create an algorithm that combines the advice received from a portfolio of predictors in such away as to perform as well as the best one, while only asking for the advice of one of the models at each time. We provide such an algorithm and show that it is optimal for a wide range of problems. Our work can be used, for example, to improve the performance of data centers or to make computers faster by improving memory efficiency."
Poster,Learning-Augmented Hierarchical Clustering,https://ICML.cc//virtual/2025/poster/45883,"Vladimir Braverman, Jon C. Ergun, Chen Wang, Samson Zhou","Hierarchical clustering (HC) is an important data analysis technique in which the goal is to recursively partition a dataset into a tree-like structure while grouping together similar data points at each level of granularity. Unfortunately, for many of the proposed HC objectives, there exist strong barriers to approximation algorithms with the hardness of approximation. We consider the problem of hierarchical clustering given auxiliary information from natural oracles in the learning-augmented framework. Our main results are algorithms that, given learning-augmented oracles, compute efficient approximate HC trees for the celebrated Dasgupta's and Moseley-Wang objectives that overcome known hardness barriers.","We studied the hierarchical clustering (HC) problem, which aims to summarize data into increasingly precise sub-clusters based on similarity and dissimilarity. The optimization of hierarchical clustering is a major problem in machine learning with applications in artificial intelligence and medical science. Since the problem is hard in general, we considered a learning-augmented version that takes advantage of a (potentially erroneous) oracle obtained from machine learning models.This oracle can provide some 'hints' to the HC algorithm in the hope of getting improved results.Our paper showed that it is indeed possible to obtain such improved HC algorithms with the learning-augmented oracle. Our results shed light on advancing the optimization of HC using machine learning models, and these algorithms have the potential to be applied to a broad range of applications, e.g., in medical modeling."
Poster,Learning Bayesian Nash Equilibrium in Auction Games via Approximate Best Response,https://ICML.cc//virtual/2025/poster/44624,"Kexin Huang, Ziqian Chen, xue wang, Chongming Gao, Jinyang Gao, Bolin Ding, Xiang Wang","Auction plays a crucial role in many modern trading environments, including online advertising and public resource allocation. As the number of competing bidders increases, learning Bayesian Nash Equilibrium (BNE) in auctions faces significant scalability challenges. Existing methods often experience slow convergence in large-scale auctions. For example, in a classic symmetric auction setting, the convergence rate depends on the number of bidders quadratically.To address this issue, we propose the *Approximate Best Response Gradient* method, a new approach for learning BNE efficiently in auction games. We leverage an analytic solution for gradient estimation to enable efficient gradient computation during optimization. Moreover, we introduce the *Best Response Distance* objective, which serves as an upper bound of approximation quality to BNE. By optimizing the new objective, our method is proven to achieve a local convergence rate independent of bidder numbers and circumvent the traditional quadratic complexity in the classic symmetric setting.Extensive experiments across various auction formats demonstrate that our approach accelerates convergence and enhances learning efficiency in complex auction settings.","When many buyers compete in auctions like those used in online advertising, finding a stable equilibrium, where each competitor reaches a strategic balance, becomes extremely difficult and slow to calculate. We developed a new approach that helps calculate these market equilibria much more efficiently. Our method uses mathematical insights to bypass the usual computational bottlenecks, significantly speeding up the solution process. This advancement will help better understand and predict auction behaviors in applications ranging from internet advertising to government resource allocation, providing valuable insights for both market designers and participants."
Poster,Learning Cascade Ranking as One Network,https://ICML.cc//virtual/2025/poster/44498,"Yunli Wang, ZhenZhang, Zhiqiang Wang, Zixuan Yang, Yu Li, Jian Yang, Shiyang Wen, Peng Jiang, Kun Gai","Cascade Ranking is a prevalent architecture in large-scale top-k selection systems like recommendation and advertising platforms. Traditional training methods focus on single-stage optimization, neglecting interactions between stages. Recent advances have introduced interaction-aware training paradigms, but still struggle to 1) align training objectives with the goal of the entire cascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn effective collaboration patterns for different stages. To address these challenges, we propose LCRON, which introduces a novel surrogate loss function derived from the lower bound probability that ground truth items are selected by cascade ranking, ensuring alignment with the overall objective of the system. According to the properties of the derived bound, we further design an auxiliary loss for each stage to drive the reduction of this bound, leading to a more robust and effective top-k selection. LCRON enables end-to-end training of the entire cascade ranking system as a unified network. Experimental results demonstrate that LCRON achieves significant improvement over existing methods on public benchmarks and industrial applications, addressing key limitations in cascade ranking training and significantly enhancing system performance.","Modern recommendation systems help users find what they truly care about—from movies to products—by selecting the most relevant items from a large pool.We developed LCRON, a new method that improves how these systems rank and select top results. Unlike traditional approaches that treat each step separately, LCRON views the whole process as one system, learning how different stages work together to improve overall performance. A key benefit of LCRON is that it can be trained end-to-end, directly optimizing for the final goal: showing users the most relevant content. Our experiments show that LCRON outperforms existing methods on both public benchmarks and industrial data. This means it has great potential to enhance user experience across services like e-commerce, streaming platforms, and online ads."
Poster,Learning Changes in Graphon Attachment Network Models,https://ICML.cc//virtual/2025/poster/45662,"Xinyuan Fan, Bufan Li, Chenlei Leng, Weichi Wu","This paper introduces Graphon Attachment Network Models (GAN-M), a novel framework for modeling evolving networks with rich structural dependencies, grounded in graphon theory. GAN-M provides a flexible and interpretable foundation for studying network formation by leveraging graphon functions to define attachment probabilities, thereby combining the strengths of graphons with a temporal perspective. A key contribution of this work is a methodology for learning structural changes in these networks over time. Our approach uses graph counts—frequencies of substructures such as triangles and stars—to capture shifts in network topology. We propose a new statistic designed to learn changes in the resulting piecewise polynomial signals and develop an efficient method for change detection, supported by theoretical guarantees. Numerical experiments demonstrate the effectiveness of our approach across various network settings, highlighting its potential for dynamic network analysis.","We introduce a novel and flexible model for the dynamic growth of random networks, and address the problem of detecting structural change-points—time points at which the underlying network structure undergoes significant shifts—over time. Leveraging the fact that the expected subgraph counts grow polynomially in time, we develop a new multiple change-point detection method for sequences of such statistics. Theoretical guarantees are established, and simulation studies validate the method’s effectiveness. A real-data application demonstrates its ability to detect changes in previously intractable scenarios."
