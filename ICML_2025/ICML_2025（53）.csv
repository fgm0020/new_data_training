type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Componential Prompt-Knowledge Alignment for Domain Incremental Learning,https://ICML.cc//virtual/2025/poster/45337,"Kunlun Xu, Xu Zou, Gang Hua, Jiahuan Zhou","Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt.","Domain Incremental Learning (DIL) aims to adapt to continuous data streams with substantial domain shifts caused by variations in style, data quality degradation, and environmental changes.  Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt can be applied to a variety of practical scenarios and tasks, including the continuously evolving AI agents, the multi-domain downstream adaptation of large vision models, and the lifelong environment adaptation of autonomous driving systems."
Poster,Compositional Causal Reasoning Evaluation in Language Models,https://ICML.cc//virtual/2025/poster/45455,"Jacqueline Maasch, Alihan Hüyük, Xinnuo Xu, Aditya Nori, Javier Gonzalez","Causal reasoning and compositional reasoning are two core aspirations in AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed *compositional causal reasoning* (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate CCR evaluation for language models in the Llama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. CCR errors increased with the complexity of causal paths for all models except o1.","Humans are very good at *reasoning*: the process of drawing logical conclusions from new information and prior knowledge. The ability to reason is also desirable in AI, including language models (LMs) that generate text. However, today’s LMs are still better at recalling previously seen information than reasoning about new problems. Additionally, it is not always clear how reasoning abilities can be thoroughly measured, especially for certain kinds of reasoning. In this work, we focus on measuring two kinds of reasoning that LMs still struggle with: (1) *causal reasoning*, the ability to reason about cause-effect relationships and (2) *compositional reasoning*, the ability to recognize and produce new combinations of previously seen concepts. We explore these simultaneously under the umbrella of *compositional causal reasoning* (CCR): the ability to reason about how causal effects mathematically compose. Our framework allows us to detect four general error patterns when LMs perform CCR.  As AI becomes more sophisticated, distinguishing true reasoning from other behaviors will become more difficult. Rigorous evaluation frameworks will be necessary to prevent misleading results. In this work, we show one way that formal causal theory can be used for new reasoning evaluation approaches."
Poster,Compositional Condition Question Answering in Tabular Understanding,https://ICML.cc//virtual/2025/poster/44789,"Jun-Peng Jiang, Tao Zhou, De-Chuan Zhan, Han-Jia Ye","Multimodal Large Language Models (MLLMs) for tabular understanding have made significant progress in tasks such as financial report analysis and public data tests. However, our comprehensive analysis shows that these models are still limited in certain simple scenarios, particularly when handling compositional conditions in QA. Further investigation reveals that the poor performance can be attributed to two main challenges: the visual encoder's inability to accurately recognize the content of a row, and the model's tendency to overlook conditions in the question.To address these, we introduce a new Compositional Condition Tabular Understanding method, called {\sc CoCoTab}. Specifically, to capture the structural relationships within tables, we enhance the visual encoder with additional row and column patches. Moreover, we introduce the conditional tokens between the visual patches and query embeddings, ensuring the model focuses on relevant parts of the table according to the conditions specified in the query.Additionally, we also introduce the Massive Multimodal Tabular Understanding (MMTU) benchmark, which comprehensively assesses the full capabilities of MLLMs in tabular understanding. Our proposed method achieves state-of-the-art performance on both existing tabular understanding benchmarks and MMTU.Our code can be available at \url{https://github.com/LAMDA-Tabular/MMTU}.","Based on the characteristics of tabular data, we categorize existing benchmarks into four aspects---understanding individual elements (IE), interpreting rows and columns (RC), comprehending compositional conditions (CC), and performing basic calculations or reasoning (CR). Our experiments reveal that current multimodal large language models often fail at seemingly simple table question answering tasks, especially when multiple conditions are involved. We introduce CoCoTab, a method designed to improve performance in the most challenging CC cases, along with a comprehensive benchmark for tabular understanding, MMTU. Our approach achieves better results with limited computation cost and reveals critical weaknesses in current MLLMs’ tabular understanding capabilities."
Poster,Compositional Flows for 3D Molecule and Synthesis Pathway Co-design,https://ICML.cc//virtual/2025/poster/46473,"Tony Shen, Seonghwan Seo, Ross Irwin, Kieran Didi, Simon Olsson, Woo Youn Kim, Martin Ester","Many generative applications, such as synthesis-based 3D molecular design, involve constructing compositional objects with continuous features.Here, we introduce Compositional Generative Flows (CGFlow), a novel framework that extends flow matching to generate objects in compositional steps while modeling continuous states. Our key insight is that modeling compositional state transitions can be formulated as a straightforward extension of the flow matching interpolation process.We further build upon the theoretical foundations of generative flow networks (GFlowNets), enabling reward-guided sampling of compositional structures. We apply CGFlow to synthesizable drug design by jointly designing the molecule's synthetic pathway with its 3D binding pose.Our approach achieves state-of-the-art binding affinity and synthesizability on all 15 targets from the LIT-PCBA benchmark, and 4.2x improvement in sampling efficiency compared to 2D synthesis-based baseline.To our best knowledge, our method is also the first to achieve state of-art-performance in both Vina Dock (-9.42) and AiZynth success rate (36.1\%) on the CrossDocked2020 benchmark.","Developing new drugs means discovering molecules that not only work well but can also be realistically synthesized in the wet-lab. Current AI methods often fall short: either they design molecules that are hard to synthesize, or they don't fully consider how the molecule interacts with the target protein.Our new method, CGFlow, is like a smart architect for molecules. It designs the ligand binder within target protein binding site directly while simultaneously planning its step-by-step ""recipe"" (synthesis pathway).We've used CGFlow to create 3DSynthFlow, a system that excels at drug design. In multiple benchmark tests, 3DSynthFlow effectively discovered molecules better binding to targets compared to baseline models. Crucially, our approach ensures these molecules are synthesizable, overcoming a major hurdle in drug discovery. This co-design process could significantly accelerate the structure-based drug design."
Poster,Compositional Generalization via Forced Rendering of Disentangled Latents,https://ICML.cc//virtual/2025/poster/43873,"Qiyao Liang, Daoyuan Qian, Liu Ziyin, Ila R. Fiete","Composition—the ability to generate myriad variations from finite means—is believed to underlie powerful generalization. However, compositional generalization remains a key challenge for deep learning. A widely held assumption is that learning disentangled (factorized) representations naturally supports this kind of extrapolation. Yet, empirical results are mixed, with many generative models failing to recognize and compose factors to generate out-of-distribution (OOD) samples. In this work, we investigate a controlled 2D Gaussian ""bump"" generation task with fully disentangled $(x,y)$ inputs, demonstrating that standard generative architectures still fail in OOD regions when training with partial data, by re-entangling latent representations  in subsequent layers. By examining the model's learned kernels and manifold geometry, we show that this failure reflects a ""memorization"" strategy for generation via data superposition rather than via composition of the true factorized features. We show that when models are forced—through architectural modifications with regularization or curated training data—to render the disentangled latents into the full-dimensional representational (pixel) space, they can be highly data-efficient and effective at composing in OOD regions. These findings underscore that disentangled latents in an abstract representation are insufficient and show that if models can represent disentangled factors directly in the output representational space, it can achieve robust compositional generalization.","Humans effortlessly mix a few simple pieces—like words or shapes—to create endless new ideas, but computers usually must see every example to learn. To investigate, we asked an AI to draw a single shaded dot on a blank grid anywhere it was told, except we hid the center area during training. Even when we gave the exact ""x"" and ""y"" instructions, the AI simply stitched together bits of remembered examples instead of learning the underlying rule for placing the dot. Then we tried two small tweaks: one that makes the AI paint each instruction directly onto the final grid, and another that first teaches it simple horizontal and vertical lines. With either tweak, the AI truly learned to combine the two directions and instantly filled in the missing center—using far fewer examples. This shows that grounding each piece of information right where the AI acts can help future systems flexibly recombine known elements—whether for new word combinations, object layouts, or routes—without needing to relearn every possibility."
Poster,Compositional Risk Minimization,https://ICML.cc//virtual/2025/poster/44769,"Divyat Mahajan, Mohammad Pezeshki, Charles Arnal, Ioannis Mitliagkas, Kartik Ahuja, Pascal Vincent","Compositional generalization is a crucial step towards developing data-efficient intelligent machines that generalize in human-like ways. In this work, we tackle a challenging form of distribution shift, termed compositional shift, where some attribute combinations are completely absent at training but present in the test distribution. This shift tests the model's ability to generalize compositionally to novel attribute combinations in discriminative tasks. We model the data with flexible additive energy distributions, where each energy term represents an attribute, and derive a simple alternative to empirical risk minimization termed compositional risk minimization (CRM). We first train an additive energy classifier to predict the multiple attributes and then adjust this classifier to tackle compositional shifts. We provide an extensive theoretical analysis of CRM, where we show that our proposal extrapolates to special affine hulls of seen attribute combinations. Empirical evaluations on benchmark datasets confirms the improved robustness of CRM compared to other methods from the literature designed to tackle various forms of subpopulation shifts.","Modern AI systems often struggle to generalize to novel combinations of familiar attributes, a phenomenon known as compositional shift. We propose Compositional Risk Minimization (CRM), a method that separately models the influence of individual attributes on the data distribution and combines their effects additively. By enabling the model to reason about individual parts and how they work together,  CRM supports more robust generalization to unseen attribute compositions. We provide both theoretical guarantees and empirical results to validate our approach. This work advances reliable, data-efficient learning and brings models closer to human-like generalization in open-world settings."
Poster,Compositional Scene Understanding through Inverse Generative Modeling,https://ICML.cc//virtual/2025/poster/45676,"Yanbo Wang, Justin Dauwels, Yilun Du","Generative models have demonstrated remarkable abilities in generating high-fidelity visual content. In this work, we explore how generative models can further be used not only to synthesize visual content but also to understand the properties of a scene given a natural image. We formulate scene understanding as an inverse generative modeling problem, where we seek to find conditional parameters of a visual generative model to best fit a given natural image. To enable this procedure to infer scene structure from images substantially different than those seen during training, we further propose to build this visual generative model compositionally from smaller models over pieces of a scene. We illustrate how this procedure enables us to infer the set of objects in a scene, enabling robust generalization to new test scenes with an increased number of objects of new shapes. We further illustrate how this enables us to infer global scene factors, likewise enabling robust generalization to new scenes. Finally, we illustrate how this approach can be directly applied to existing pretrained text-to-image generative models for zero-shot multi-object perception. Code and visualizations are at https://energy-based-model.github.io/compositional-inference.","Recent advances in AI have made it possible for computers to generate incredibly realistic images. But what if we could also use these generative tools to help computers better understand what's in a photo—like identifying all the objects or describing how a scene is structured?We explore exactly that: it shows how we can reverse the usual process of image generation to instead analyze real-world scenes. Think of it like using a recipe (the generative model) to figure out the ingredients in a dish (the photo), even if you've never tasted that exact dish before.The key idea is to break down complex scenes into smaller parts, using a modular approach—like identifying each object in a room one by one. This makes it easier for the computer to recognize new and unfamiliar scenes by reusing these smaller pieces.We tested our method in several ways:Detecting objects in images with more complexity than the system saw during training—for example, images with more items, different shapes, or new backgrounds.Describing people’s faces across gender differences, even when trained on only one gender.Using powerful image-generation tools to recognize multiple objects in completely new web images, without needing additional training (a concept known as ""zero-shot learning"").Overall, our work offers a new way to teach computers to understand the world more like humans do—by learning in flexible, reusable parts—and it shows strong results in both simple and complex settings."
Poster,Compressed and distributed least-squares regression: convergence rates with applications to federated learning,https://ICML.cc//virtual/2025/poster/46714,"Constantin Philippenko, Aymeric Dieuleveut","In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H{{\""o}}lder regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning. More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced by the algorithm. We demonstrate despite the non-regularity of the stochastic field, that the limit variance term scales with $\mathrm{Tr}(\mathfrak{C}_{\mathrm{ania}} H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the number of iterations) generalizing the rate for the vanilla LSR case where it is $\sigma^2 \mathrm{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach and Moulines, 2013). Then, we analyze the dependency of $\mathfrak{C}_{\mathrm{ania}}$ on the compression strategy and ultimately its impact on convergence, first in the centralized case, then in two heterogeneous FL frameworks.",
Poster,Compressed Image Generation with Denoising Diffusion Codebook Models,https://ICML.cc//virtual/2025/poster/44687,"Guy Ohayon, Hila Manor, Tomer Michaeli, Michael Elad","We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples *along* with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed *Denoising Diffusion Codebook Model* (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results.More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations.Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered.Code and demo are available on our project's [website](https://ddcm-2025.github.io/).","We developed a new method for generating high-quality images using AI, while also creating a compact, compressed version of each generated image at the same time. This is useful because we can store or send these images more efficiently without losing quality. Our approach builds on diffusion models, a popular type of GenAI models, which usually rely on random noise to create images.Instead of using random noise, we select the noise from a fixed set of options — like choosing from a finite menu of available noises. Then, to re-create the exact same generated image later on, we only need to tell which items from the menu were selected, and re-select them.We leverage this property and propose a powerful method for compressing images, beating previous compression methods especially when aiming for small file sizes. We also show how this technique can be used for other tasks like restoring damaged images, all while keeping the stored data compact."
Poster,Compressing tree ensembles through Level-wise Optimization and Pruning,https://ICML.cc//virtual/2025/poster/46237,"Laurens Devos, Timo Martens, Deniz Oruc, Wannes Meert, Hendrik Blockeel, Jesse Davis","Tree ensembles (e.g., gradient boosting decision trees) are often used in practice because they offer excellent predictive performance while still being easy and efficient to learn. In some contexts, it is important to additionally optimize their size: this is specifically the case when models need to have verifiable properties (verification of fairness, robustness, etc. is often exponential in the ensemble's size), or when models run on battery-powered devices (smaller ensembles consume less energy, increasing battery autonomy).  For this reason, compression of tree ensembles is worth studying.  This paper presents LOP, a method for compressing a given tree ensemble by pruning or entirely removing trees in it, while updating leaf predictions in such a  way that predictive accuracy is mostly unaffected. Empirically, LOP achieves compression factors that are often 10 to 100 times better than that of competing methods.","Larger machine learning models tend to increase predictive performance. However, in some situations, like running on a phone with limited battery or ensuring fair predictions, it’s important to make these models smaller. Smaller models are easier to provide fairness guarantees for and use less energy. In this paper, we introduce a novel method called LOP to make a specific type of machine learning model much smaller: tree ensembles. We can do this by removing some parts of the model that minimally affect the model’s predictions. In tests, LOP was able to shrink models much more effectively, by 10 to 100 times, compared to other approaches."
