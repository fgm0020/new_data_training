type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Online Linear Classification with Massart Noise,https://ICML.cc//virtual/2025/poster/45073,"Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis","We study the task of online learning in the presence of Massart noise. Specifically, instead of assuming that the online adversary chooses an arbitrary sequence of labels, we assume that the context $\boldsymbol{x}$ is selected adversarially but the label $y$ presented to the learner disagrees with the ground-truth label of $\boldsymbol{x}$ with unknown probability {\em at most} $\eta$. We focus on the fundamental class of $\gamma$-margin linear classifiers and present the first computationally efficient algorithm that achieves mistake bound $\eta T + o(T)$.   We point out that the mistake bound achieved by our algorithm is qualitatively tight for computationally efficient algorithms; this follows from the fact that, even in the offline setting, achieving 0-1 error better than $\eta$ requires super-polynomial time under standard complexity assumptions.We extend our online learning model to a $k$-arm contextual bandit setting where the rewards---instead of satisfying commonly used realizability assumptions---are consistent, in expectation, with some linear ranking function with weight vector $\boldsymbol{w}^\ast$.   Given a list of contexts $\boldsymbol{x}_1,\ldots \boldsymbol{x}_k$, if $\boldsymbol{w}^*\cdot \boldsymbol{x}_i > \boldsymbol{w}^* \cdot \boldsymbol{x}_j$, the expected reward of action $i$must be larger than that of $j$ by at least $\Delta$.  We use our Massart online learner to design an efficient bandit algorithm that obtains expected reward at least $(1-1/k)~ \Delta T - o(T)$ bigger than choosing a random action at every round.","In the online classification setting, there is no fixed training set; instead, an adversary (or “nature”) sends us one feature vector at a time. We must predict its label on the spot, immediately see whether we were right, and then repeat—while the adversary is free to craft each new example after observing every mistake we have ever made.We investigate this sequential scenario under Massart noise, where each revealed label disagrees with the ground-truth label with some unknown probability $\eta<1/2$. Focusing on γ-margin linear classifiers, we show that a simple margin-based update rule makes at most $\eta T + o(T)$ mistakes over T rounds, thereby achieving the best-known error guarantees that were previously known only in the easier offline setting.  We further extend the approach to a $k$-arm contextual bandit problem whose rewards respect the same linear-margin structure, designing an algorithm whose expected cumulative reward beats a uniformly random policy by at least $(1 − 1/k) \Delta T − o(T)$, where $\Delta$ is the margin gap between optimal and sub-optimal actions. Collectively, our results demonstrate that near-optimal robustness to Massart noise can be attained in real time without sacrificing either computational efficiency or regret guarantees."
Poster,Online Pre-Training for Offline-to-Online Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46420,"Yongjae Shin, Jeonghye Kim, Whiyoung Jung, Sunghoon Hong, Deunsol Yoon, Youngsoo Jang, Geon-Hyeong Kim, Jongseong Chae, Youngchul Sung, Kanghoon Lee, Woohyung Lim","Offline-to-online reinforcement learning (RL) aims to integrate the complementary strengths of offline and online RL by pre-training an agent offline and subsequently fine-tuning it through online interactions. However, recent studies reveal that offline pre-trained agents often underperform during online fine-tuning due to inaccurate value estimation caused by distribution shift, with random initialization proving more effective in certain cases. In this work, we propose a novel method, Online Pre-Training for Offline-to-Online RL (OPT), explicitly designed to address the issue of inaccurate value estimation in offline pre-trained agents. OPT introduces a new learning phase, Online Pre-Training, which allows the training of a new value function tailored specifically for effective online fine-tuning. Implementation of OPT on TD3 and SPOT demonstrates an average 30\% improvement in performance across a wide range of D4RL environments, including MuJoCo, Antmaze, and Adroit.","Training AI agents using pre-collected data is efficient but often leads to poor performance when those agents are later deployed in the real world. This happens because the data the agent saw during training may differ from what it encounters during deployment, a problem known as distribution shift. In some cases, even starting from scratch performs better than starting from a poorly pre-trained agent.To address this, we introduce Online Pre-Training for Offline-to-Online RL (OPT), a new approach that gives the agent a chance to adapt before full online learning begins. OPT adds a brief intermediate phase where the agent learns a new decision-making component using a small amount of interaction with the real environment. This prepares the agent to fine-tune more effectively when online learning starts.We applied OPT to several common reinforcement learning tasks and found that it consistently improves performance, even outperforming prior state-of-the-art methods. Because OPT can be added to many existing algorithms, it provides a simple and effective way to make AI agents more reliable in dynamic, real-world settings."
Poster,Online Robust Reinforcement Learning Through Monte-Carlo Planning,https://ICML.cc//virtual/2025/poster/44177,"Tuan Dam, Kishan Panaganti, Brahim Driss, Adam Wierman","Monte Carlo Tree Search (MCTS) is a powerful framework for solving complex decision-making problems, yet it often relies on the assumption that the simulator and the real-world dynamics are identical. Although this assumption helps achieve the success of MCTS in games like Chess, Go, and Shogi, the real-world scenarios incur ambiguity due to their modeling mismatches in low-fidelity simulators. In this work, we present a new robust variant of MCTS that mitigates dynamical model ambiguities. Our algorithm addresses transition dynamics and reward distribution ambiguities to bridge the gap between simulation-based planning and real-world deployment. We incorporate a robust power mean backup operator and carefully designed exploration bonuses to ensure finite-sample convergence at every node in the search tree. We show that our algorithm achieves a convergence rate of $\mathcal{O}(n^{-1/2})$ for the value estimation at the root node, comparable to that of standard MCTS. Finally, we provide empirical evidence that our method achieves robust performance in planning problems even under significant ambiguity in the underlying reward distribution and transition dynamics.","Imagine you're learning to play a video game by practicing on a simulator, but when you finally play the real game, the physics are slightly different—maybe the character jumps a bit lower or moves a bit slower than in the simulator. This gap between practice and reality is a major challenge in artificial intelligence, where computer programs often train in simplified virtual environments before being deployed in the messy real world. This paper tackles this ""simulation-to-reality gap"" by making AI planning algorithms more robust—meaning they work well even when the real world differs from their training environment. The researchers focus on a popular AI technique called Monte Carlo Tree Search (MCTS), which is like playing out thousands of possible future scenarios in your head before making a decision. Think of MCTS like a chess player who considers many possible moves and counter-moves before choosing their next play. The difference here is that instead of assuming the game rules are perfectly known, the algorithm plans for uncertainty—it considers that the ""rules"" of the real world might be somewhat different from what it learned in simulation. The key innovation is building uncertainty directly into the decision-making process. Instead of assuming the best-case scenario, the algorithm prepares for reasonable worst-case scenarios. It's like a cautious driver who plans their route assuming there might be unexpected traffic, rather than optimistically assuming clear roads. The algorithm does this by considering multiple possible versions of how the world might behave, making decisions that work well across all these possibilities, and balancing between being too cautious and being too optimistic. This research is important because it helps bridge the gap between AI systems that work perfectly in labs and AI systems that work reliably in the real world. Applications could include autonomous vehicles that can handle unexpected road conditions, medical treatment planning that accounts for patient variability, financial trading systems that remain stable during market volatility, and robotics that can adapt when the real environment differs from simulations. The researchers proved mathematically that their robust algorithm maintains the same learning speed as traditional methods while being much more reliable when faced with unexpected conditions. They tested this in several scenarios, including gambling problems and navigation tasks, showing that the robust approach maintains steady performance even when the real environment differs significantly from what was expected. This work represents a step toward AI systems that are not just smart, but also reliable and trustworthy in real-world deployment. By explicitly planning for uncertainty rather than ignoring it, we can build AI that performs consistently across the messy, unpredictable conditions of the real world."
Poster,Online Sparsification of  Bipartite-Like Clusters in Graphs,https://ICML.cc//virtual/2025/poster/44321,"Joyentanuj Das, Suranjan De, He Sun","Graph clustering is an important algorithmic technique for analysing massive graphs, and has been widely applied in many research fields of data science. While the objective of most   graph clustering algorithms is to find a vertex set of low conductance, there has been a sequence of recent studies that highlight the importance of the inter-connection between clusters when analysing real-world datasets. Following this line of research, in this work we study bipartite-like clusters and present efficient and  online algorithms that  find such clusters in both undirected graphs and directed ones. We   conduct experimental studies on both synthetic and real-world datasets, and show that our algorithms significantly speedup the running time of existing clustering algorithms while preserving their effectiveness.","We introduce two sparsification algorithms designed to find bipartite-like clusters in both undirected and directed graphs. These cluster structures are prevalent in real-world networks like trade, migration, and communication. Unlike traditional methods that focus on within-group interactions, our approach highlights connections between groups.  Our algorithms create sparsifiers that significantly reduce graph size while preserving these essential bipartite structures. For undirected graphs, we guarantee the presence of $k$ bipartite clusters by maintaining the $k$-way dual Cheeger constant, a guarantee we extend to directed graphs. These algorithms run in nearly-linear time, rely purely on random sampling, and are simple to implement in an online setting. Our experimental results, spanning both real and synthetic datasets, demonstrate significant speed improvements compared to current methods, all while maintaining clustering accuracy."
Poster,On Measuring Long-Range Interactions in Graph Neural Networks,https://ICML.cc//virtual/2025/poster/46575,"Jacob Bamberger, Benjamin Gutteridge, Scott le Roux, Michael Bronstein, Xiaowen Dong","Long-range graph tasks --- those dependent on interactions between `distant' nodes --- are an open problem in graph neural network research. Real-world benchmark tasks, especially the Long Range Graph Benchmark, have become popular for validating the long-range capability of proposed architectures. However, this is an empirical approach that lacks both robustness and theoretical underpinning; a more principled characterization of the long-range problem is required. To bridge this gap, we formalize long-range interactions in graph tasks, introduce a **range measure** for operators on graphs, and validate it with synthetic experiments. We then leverage our measure to examine commonly used tasks and architectures, and discuss to what extent they are, in fact, long-range.We believe our work advances efforts to define and address the long-range problem on graphs, and that our range measure will aid evaluation of new datasets and architectures.","Long-range graph tasks --- those dependent on interactions between `distant' nodes --- are an open problem in graph neural network research. Real-world benchmark tasks, especially the Long Range Graph Benchmark, have become popular for validating the long-range capability of proposed architectures. However, this is an empirical approach that lacks both robustness and theoretical underpinning; a more principled characterization of the long-range problem is required. To bridge this gap, we formalize long-range interactions in graph tasks, introduce a **range measure** for operators on graphs, and validate it with synthetic experiments. We then leverage our measure to examine commonly used tasks and architectures, and discuss to what extent they are, in fact, long-range.We believe our work advances efforts to define and address the long-range problem on graphs, and that our range measure will aid evaluation of new datasets and architectures."
Poster,On Mitigating Affinity Bias through Bandits with Evolving Biased Feedback,https://ICML.cc//virtual/2025/poster/44376,"Matthew Faw, Constantine Caramanis, Jessica Hoffmann","Unconscious bias has been shown to influence how we assess our peers, with consequences for hiring, promotions and admissions. In this work, we focus on affinity bias, the component of unconscious bias which leads us to prefer people who are similar to us, despite no deliberate intention of favoritism. In a world where the people hired today become part of the hiring committee of tomorrow, we are particularly interested in understanding (and mitigating) how affinity bias affects this feedback loop. This problem has two distinctive features: 1) we only observe the _biased value_ of a candidate, but we want to optimize with respect to their _real value_ 2) the bias towards a candidate with a specific set of traits depends on the _fraction_ of people in the hiring committee with the same set of traits. We introduce a new bandits variant that exhibits those two features, which we call affinity bandits. Unsurprisingly, classical algorithms such as UCB often fail to identify the best arm in this setting. We prove a new instance-dependent regret lower bound, which is larger than that in the standard bandit setting by a multiplicative function of $K$. Since we  treat rewards that are _time-varying_ and _dependent on the policy's past actions_, deriving this lower bound requires developing proof techniques beyond the standard bandit techniques. Finally, we design an elimination-style algorithm which nearly matches this regret, despite never observing the real rewards.","Unconscious bias has been shown to influence how we assess our peers, with consequences for hiring, promotions and admissions. These assessments often have downstream consequences: for instance, the people we hire today may influence, indirectly or directly, the people we hire in the future. How do these unconscious biases affect this feedback loop?We focus on two essential features of unconscious bias: (1) we only observe the biased value of a candidate, but we want to optimize with respect to their real value 2) the bias towards a candidate with a specific set of traits depends on the fraction of people in the hiring committee with the same set of traits. We introduce a new bandits variant that exhibits those two features, which we call affinity bandits. We provide a near-tight characterization of this problem, deriving a new lower bound as well as an algorithm with performance nearly matching this bound.Our work highlights the many challenges that arise when making decisions in the presence of bias, even when the bias structure is known. We hope our techniques pave the way towards understanding when and how decision-makers can mitigate negative consequences of unconscious biases."
Poster,On Path to Multimodal Generalist: General-Level and General-Bench,https://ICML.cc//virtual/2025/poster/45047,"Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Weiming Wu, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng YAN, Hanwang Zhang","The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of language-based LLMs. Unlike their specialist predecessors, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting singular modalities to accommodating a wide array of or even arbitrary modalities. To assess the capabilities of various MLLMs, a diverse array of benchmark test sets has been proposed. This leads to a critical question: *Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI?*We argue that the answer is not as straightforward as it seems. In this project, we introduce an evaluation framework to delineate the capabilities and behaviors of current multimodal generalists. This framework, named **General-Level**, establishes 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI (Artificial General Intelligence). Central to our framework is the use of **Synergy** as the evaluative criterion, categorizing capabilities based on whether MLLMs preserve synergy across comprehension and generation, as well as across multimodal interactions.To evaluate the comprehensive abilities of various generalists, we present a massive multimodal benchmark, **General-Bench**, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI.Project Page: https://generalist.top/,Leaderboard: https://generalist.top/leaderboard/,Benchmark: https://huggingface.co/General-Level/.","Artificial intelligence (AI) systems are increasingly capable of handling diverse types of data—such as text, images, and audio. However, many of these systems excel only in specific tasks or data types, lacking the broad adaptability seen in human intelligence. Also the existing evaluation paradigm that simply assumes that higher performance across tasks indicates a stronger MLLM capability can be problematic.Our research introduces two tools: General-Level, a framework that assesses an AI model's ability to integrate and apply knowledge across different tasks and data types; and General-Bench, a comprehensive dataset comprising over 700 tasks and 325,000 examples designed to evaluate this integrative capability.By applying these tools to over 100 existing AI models, we discovered that while some models perform well on individual tasks, they often struggle to transfer knowledge between different types of tasks or data. This indicates a gap in achieving truly general-purpose multimodal AGI.Our work aims to guide the development of more versatile AI systems that can seamlessly understand and generate multiple forms of data, moving us closer to AI that mirrors human-like general intelligence."
Poster,On Teacher Hacking in Language Model Distillation,https://ICML.cc//virtual/2025/poster/43914,"Daniil Tiapkin, Daniele Calandriello, Johan Ferret, Sarah Perrin, Nino Vieillard, Alexandre Rame, Mathieu Blondel","Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model, leading to degraded performance on the true objective, in line with Goodhart's law.In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher.Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking.Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust LMs.","Large language models are often “distilled” into smaller ""student"" models by having the student imitate a larger “teacher” model, but this process can backfire: students sometimes learn to exploit quirky behaviors in the teacher rather than truly understanding how language works, a hidden flaw we call teacher hacking.In our experiment, we showed that if the student is trained on the same batch of teacher-generated examples over and over, it learns shortcuts instead of real language skills—but if you keep creating fresh examples as you train, or mix in a wide range of different prompts, the student stays on track and avoids exploiting the teacher’s flaws.Our findings suggest simple, practical fixes—online data sampling or richer, more diverse offline datasets—that make distilled models both more robust and more accurate to real language, improving their safety and usefulness in everyday applications."
Poster,On Temperature Scaling and Conformal Prediction of Deep Classifiers,https://ICML.cc//virtual/2025/poster/44049,"Lahav Dabah, Tom Tirer","In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied by some confidence indication. Two popular approaches for that aim are: 1) *Calibration*: modifies the classifier's softmax values such that the maximal value better estimates the correctness probability; and 2) *Conformal Prediction* (CP): produces a prediction set of candidate labels that contains the true label with a user-specified probability, guaranteeing marginal coverage but not, e.g., per class coverage.  In practice, both types of indications are desirable, yet, so far the interplay between them has not been investigated. Focusing on the ubiquitous *Temperature Scaling* (TS) calibration, we start this paper with an extensive empirical study of its effect on prominent CP methods. We show that while TS calibration improves the class-conditional coverage of adaptive CP methods, surprisingly, it negatively affects their prediction set sizes. Motivated by this behavior, we explore the effect of TS on CP *beyond its calibration application* and reveal an intriguing trend under which it allows to trade prediction set size and conditional coverage of adaptive CP methods. Then, we establish a mathematical theory that explains the entire non-monotonic trend.Finally, based on our experiments and theory, we offer guidelines for practitioners to effectively combine adaptive CP with calibration, aligned with user-defined goals.","Modern machine learning classifiers often output a ""confidence"" score alongside their predictions. While this is meant to indicate how likely the prediction is to be correct, these scores often do not reflect true likelihoods. This miscalibration can be dangerous in high-stakes applications, where decisions based on false confidence can have serious consequences.To address this, two key techniques have emerged: *calibration*, which adjusts confidence scores to better align with actual correctness probabilities, and *conformal prediction* (CP), which produces a set of possible classes guaranteed to contain the true label with a specified probability. However, the interplay between these methods remains largely unexplored. This paper addresses that gap: What happens when you apply calibration (specifically, temperature scaling) before using conformal prediction?Surprisingly, while temperature scaling improves coverage performance across different categories (e.g., image classes), it can also make the list of possible answers longer and less useful. We further experimented with different temperature values and found that this trade-off still holds. In addition, we developed a mathematical theory that explains the effect of temperature scaling on the prediction set sizes produced by CP methods.Finally, we distilled these insights into actionable guidelines to help practitioners—particularly in high-stakes domains—better tune CP methods for more reliable and informative AI predictions."
Poster,On the Adversarial Robustness of Multi-Kernel Clustering,https://ICML.cc//virtual/2025/poster/43590,"Hao Yu, Weixuan Liang, KE LIANG, Suyuan Liu, Meng Liu, Xinwang Liu","Multi-kernel clustering (MKC) has emerged as a powerful method for capturing diverse data patterns, offering robust and generalized representations of data structures. However, the increasing deployment of MKC in real-world applications raises concerns about its vulnerability to adversarial perturbations. While adversarial robustness has been extensively studied in other domains, its impact on MKC remains largely unexplored. In this paper, we address the challenge of assessing the adversarial robustness of MKC methods in a black-box setting. Specifically, we propose *AdvMKC*, a novel reinforcement-learning-based adversarial attack framework designed to inject imperceptible perturbations into data and mislead MKC methods. AdvMKC leverages proximal policy optimization with an advantage function to overcome the instability of clustering results during optimization. Additionally, it introduces a generator-clusterer framework, where a generator produces adversarial perturbations, and a clusterer approximates MKC behavior, significantly reducing computational overhead. We provide theoretical insights into the impact of adversarial perturbations on MKC and validate these findings through experiments. Evaluations across seven datasets and eleven MKC methods (seven traditional and four robust) demonstrate AdvMKC's effectiveness, robustness, and transferability.","Multi-Kernel Clustering (MKC) is a powerful tool for finding patterns in complex data, but its vulnerability to adversarial attacks is not well understood. These attacks make tiny, hard-to-detect changes to the data that can fool the clustering results. As MKC becomes more common in real-world applications, this weakness becomes a serious concern. To tackle this issue, we create AdvMKC, a new framework that uses reinforcement learning to perform black-box adversarial attacks on MKC. Our method adds small changes to the data to mislead the clustering process. We design it to be efficient by using a generator to create these changes and a clusterer to mimic MKC behavior. We test AdvMKC on seven datasets and eleven MKC methods and find that it works well, even on robust models. Our approach helps uncover hidden risks in MKC and offers a way to test and improve its security in practical settings."
