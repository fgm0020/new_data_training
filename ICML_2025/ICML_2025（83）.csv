type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation,https://ICML.cc//virtual/2025/poster/46187,"Yunbei Zhang, Akshay Mehra, Shuaicheng Niu, Jihun Hamm","Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions—they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose **DPCore**, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.","AI models, like those in autonomous vehicles, often falter when moving from familiar training grounds to the real world’s ever-changing conditions—sunny to rain, fog to tunnels. Current adaptation techniques aren't built for such dynamic, unpredictable shifts, often leading to errors, forgetting past lessons, or misusing learned knowledge when domains change rapidly or briefly appear.We introduce DPCore, a novel method that helps AI adapt efficiently to these challenges. DPCore uses adaptable ""visual prompts"" – small, adjustable instructions for the AI – and maintains a ""prompt coreset,"" which is a streamlined memory of key visual characteristics from past environments. When faced with a new situation, DPCore intelligently decides whether to adjust an existing prompt from its memory if the scene is similar to something seen before, or create a fresh prompt if the environment is distinctly new.DPCore enables AI to maintain strong performance even as surroundings change rapidly and erratically, significantly outperforming previous methods in these realistic dynamic settings. Remarkably, it achieves this with 99% fewer adaptable parts and 64% less computation time compared to earlier approaches. Our work also introduces a more realistic way to test AI adaptation, which we call ""Continual Dynamic Change,"" better reflecting real-world complexities."
Poster,DPO Meets PPO: Reinforced Token Optimization for RLHF,https://ICML.cc//virtual/2025/poster/45726,"Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, Liwei Wang","In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards---a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. We conduct extensive experiments to evaluate \texttt{RTO} against PPO and other direct preference learning algorithms. The results highlight the effectiveness of RTO, with the algorithm outperforming PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard.  Our code and models are available at \href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.","We formulate RLHF as token-wise MDPs instead of sentence-level bandits, and propose a provable and practical algorithm, RTO, under this framework."
Poster,DRAG: Data Reconstruction Attack using Guided Diffusion,https://ICML.cc//virtual/2025/poster/43496,"Wa-Kin Lei, Jun-Cheng Chen, Shang-Tse Chen","With the rise of large foundation models, split inference (SI) has emerged as a popular computational paradigm for deploying models across lightweight edge devices and cloud servers, addressing data privacy and computational cost concerns. However, most existing data reconstruction attacks have focused on smaller CNN classification models, leaving the privacy risks of foundation models in SI settings largely unexplored. To address this gap, we propose a novel data reconstruction attack based on guided diffusion, which leverages the rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on a large-scale dataset. Our method performs iterative reconstruction on the LDM’s learned image prior, effectively generating high-fidelity images resembling the original data from their intermediate representations (IR). Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, both qualitatively and quantitatively, in reconstructing data from deep-layer IRs of the vision foundation model. The results highlight the urgent need for more robust privacy protection mechanisms for large models in SI scenarios.","Modern AI systems often split computation between local devices and cloud servers to improve efficiency and privacy. However, when these systems share encoded information between devices and servers, malicious actors can steal users' data by reconstructing it from these encoded pieces. Previous studies focused on smaller models and overlooked privacy risks for the large, powerful vision models now widely used.We propose DRAG, a new privacy attack using advanced diffusion models to recover original images from encoded information generated by large vision models like CLIP and DINOv2. These large diffusion models' extensive knowledge of images provides malicious actors with powerful tools, enabling high-quality recovery of users' data, even after heavy processing through multiple layers, outperforming prior attacks. Our findings reveal serious privacy vulnerabilities in current split inference systems using large vision models, highlighting the urgent need for stronger privacy protections. Our insights will help researchers and practitioners better understand and mitigate privacy risks, ensuring AI technologies remain both effective and secure."
Poster,DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model,https://ICML.cc//virtual/2025/poster/44760,"Siwei Xia, Li Sun, Tiantian Sun, Qingli Li","Drag-based editing within pretrained diffusion model provides a precise and flexible way to manipulate foreground objects. Traditional methods optimize the input feature obtained from DDIM inversion directly, adjusting them iteratively to guide handle points towards target locations. However, these approaches often suffer from limited accuracy due to the low representation ability of the feature in motion supervision, as well as inefficiencies caused by the large search space required for point tracking. To address these limitations, we present DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation) adapters into the drag-based editing pipeline. To enhance the training of LoRA adapters, we introduce an additional denoising score distillation loss which regularizes the online model by aligning its output with that of the original model. Additionally, we improve the consistency of motion supervision by adapting the input features using the updated LoRA, giving a more stable and accurate input feature for subsequent operations. Building on this, we design an adaptive optimization scheme that dynamically toggles between two modes, prioritizing efficiency without compromising precision. Extensive experiments demonstrate that DragLoRA significantly enhances the control precision and computational efficiency for drag-based image editing. The Codes of DragLoRA are available at: https://github.com/Sylvie-X/DragLoRA.","We often want to tweak the shape or position of an object in an image by simply dragging it, but current methods struggle to move those “handles” exactly where you want and can be slow to adjust.To fix this, we introduce a new method called DragLoRA, which adds a lightweight module into an existing image-generation model so it can learn on the fly how to better follow your drag moves. Besides, we incorporate a strategy that dynamically accelerates the entire process so you get quick feedback without losing precision.As a result, DragLoRA lets users drag parts of an image more accurately and much faster than before, making it easier for anyone to reshape or reposition objects simply by pulling on intuitive handle points."
Poster,DragSolver: A Multi-Scale Transformer for Real-World  Automotive Drag Coefficient Estimation,https://ICML.cc//virtual/2025/poster/44542,"Ye Liu, Yuntian Chen","Automotive drag coefficient ($C_d$) is pivotal to energy efficiency, fuel consumption, and aerodynamic performance. However, costly computational fluid dynamics (CFD) simulations and wind tunnel tests struggle to meet the rapid-iteration demands of automotive design. We present DragSolver, a Transformer-based framework for rapid and accurate $C_d$ estimation from large-scale, diverse 3D vehicle models.DragSolver tackles four key real-world  challenges: (1) multi-scale feature extraction to capture both global shape and fine local geometry; (2) heterogeneous scale normalization to handle meshes with varying sizes and densities;(3) surface-guided gating to suppress internal structures irrelevant to external aerodynamics;and (4) epistemic uncertainty estimation via Monte Carlo dropout for risk-aware design. Extensive evaluations on three industrial-scale datasets (DrivaerNet, DrivaerNet++, and DrivaerML) show that DragSolver outperforms existing approaches in accuracy and generalization, achieving an average reduction of relative $L_2$ error by 58.7% across real-world datasets. Crucially, DragSolver is the first to achieve reliable, real-time $C_d$ inference on production-level automotive geometries.","Cars and trucks waste a surprising amount of energy just by pushing air aside. Engineers capture this “air-slipperiness” with a single number—the drag coefficient (C<sub>d</sub>). Each minor shape tweak has traditionally required hours-long computer-fluid simulations or costly wind-tunnel tests, bottlenecking the design loop. DragSolver is an AI system that learns the link between a vehicle’s 3-D surface and its drag coefficient from thousands of past designs. It examines shapes at two scales simultaneously: the global silhouette that steers the bulk airflow and the fine ridges and gaps that disturb it. The model automatically ignores interior parts that do not affect outside aerodynamics and flags when its own prediction may be unreliable. Across three industry-scale datasets, DragSolver cuts prediction error by about 50 % compared with previous methods while running in real time on a standard GPU. Designers can therefore evaluate far more concepts per day and ultimately build vehicles that travel farther on the same fuel or battery charge."
Poster,DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization,https://ICML.cc//virtual/2025/poster/45024,"Zhenglin Zhou, Xiaobo Xia, Fan Ma, Hehe Fan, Yi Yang, Tat-Seng Chua","Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then validates their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging relative preferences, DreamDPO reduces reliance on precise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves state-of-the-art results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced.","Creating 3D objects from text prompt — like turning “a rough rock” into a 3D model — has promising applications in gaming, design, and virtual reality. However, current methods often fail to match human expectations.We propose DreamDPO, an optimization-based framework that aligns 3D generation with human preferences. Instead of relying on strict quality scores, DreamDPO learns from pairwise comparisons — similar to choosing which of two versions looks better. This feedback helps produce more realistic, customizable, and appealing 3D models.By prioritizing human judgment over rigid metrics, DreamDPO enables more flexible and accurate 3D creation. Experiments show it outperforms existing methods, delivering higher-quality results and user control."
Poster,DriveGPT: Scaling Autoregressive Behavior Models for Driving,https://ICML.cc//virtual/2025/poster/45238,"Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa","We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.","* Our paper is the first published study of large-scale scaling laws in autonomous driving. The largest model was trained on 100M+ high-quality human demonstrations in dense urban driving with 1B+ parameters.* Our scaling experiments validate the benefits of increasing both data and compute, revealing better model scalability as training data increases -- consistent with trends observed in language models.* We quantitatively and qualitatively evaluate models across scales, including real-world deployment of our model as a real-time planner in complex urban scenarios.* Our model achieves state-of-the-art performance on the Waymo Open Motion Dataset across key geometric metrics, highlighting its strong generalization in motion prediction."
Poster,Drug-TTA: Test-Time Adaptation for Drug Virtual Screening via Multi-task Meta-Auxiliary Learning,https://ICML.cc//virtual/2025/poster/44855,"Ao Shen, Ming&#x27;zhi Yuan, Yingfan MA, Jie Du, qiao Huang, Manning Wang","Virtual screening is a critical step in drug discovery, aiming at identifying potential drugs that bind to a specific protein pocket from a large database of molecules. Traditional docking methods are time-consuming, while learning-based approaches supervised by high-precision conformational or affinity labels are limited by the scarcity of training data. Recently, a paradigm of feature alignment through contrastive learning has gained widespread attention. This method does not require explicit binding affinity scores, but it suffers from the issue of overly simplistic construction of negative samples, which limits their generalization to more difficult test cases. In this paper, we propose Drug-TTA, which leverages a large number of self-supervised auxiliary tasks to adapt the model to each test instance. Specifically, we incorporate the auxiliary tasks into both the training and the inference process via meta-learning to improve the performance of the primary task of virtual screening. Additionally, we design a multi-scale feature based Auxiliary Loss Balance Module (ALBM) to balance the auxiliary tasks to improve their efficiency. Extensive experiments demonstrate that Drug-TTA achieves state-of-the-art (SOTA) performance in all five virtual screening tasks under a zero-shot setting, showing an average improvement of 9.86\% in AUROC metric compared to the baseline without test-time adaptation.","How can we make AI drug discovery models work reliably on proteins and molecules they’ve never seen before? This is a major challenge in structure-based virtual screening, where most models struggle to generalize beyond training data. We tackle this problem by introducing Drug-TTA, a method that allows the model to adapt at test time—without access to labels. Our key idea is to leverage a set of self-supervised tasks to fine-tune the model on each test sample individually. Additionally, we use meta-learning during training to ensure that the model can quickly adapt at inference time.Surprisingly, this simple approach significantly boosts performance across diverse benchmarks. Our results show that even without test-time labels, a model can still learn to “specialize” to new proteins and molecules. Drug-TTA thus offers a practical and data-efficient way to improve virtual screening in real-world drug discovery."
Poster,"DSBRouter: End-to-end Global Routing via Diffusion Schr\""{o}dinger Bridge",https://ICML.cc//virtual/2025/poster/45255,"Liangliang Shi, Shenhui Zhang, Xingbo Du, Nianzu Yang, Junchi Yan","Global routing (GR) is a fundamental task in modern chip design and various learning techniques have been devised. However, a persistent challenge is the inherent lack of a mechanism to guarantee the routing connectivity in network's prediction results, necessitating post-processing search or reinforcement learning (RL) to enforce the connectivity. In this paper, we propose a neural GR solver called DSBRouter, leveraging the Diffusion Schr\""{o}dinger Bridge (DSB) model for GR. During training, unlike previous works that learn the mapping from noise to routes, we establish a bridge between the initial pins and the routing via DSB, which learns the forward and backward mapping between them. For inference, based on the evaluation metric (e.g. low overflow), we further introduce a sampling scheme with evaluation-based guidance to enhance the routing predictions. Note that DSBRouter is an end-to-end model that does not require a post-step to ensure connectivity. Empirical results show that it achieves SOTA performance on the overflow reduction in ISPD98 and part of ISPD07. In some cases, DSBRouter can even generate routes with zero overflow.","Modern chips contain millions of tiny wires that must be routed without crossing or overflow. Existing algorithms—and even recent machine-learning routers—still need slow, hand-tuned post-processing to make all wires connect, so designers lose days iterating on congestion fixes. Our research introduces **DSBRouter**, the first *end-to-end* neural global router. It builds on the Diffusion Schrödinger Bridge framework to learn both a “forward” path (from routes to clean layouts) and a “backward” path (from pins to finished routes), then refines each intermediate layout with an evaluation-guided sampler that explicitly reduces overflow. In public ISPD benchmarks, DSBRouter cut routing overflow by an average of 90%—and sometimes eliminated it entirely—while matching or even beating prior methods on wire length, all without any post-processing steps.By removing a critical manual bottleneck, DSBRouter can shorten chip design cycles, lower engineering costs, and pave the way for fully automated, AI-driven electronic design automation pipelines."
Poster,DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers,https://ICML.cc//virtual/2025/poster/44732,"Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, Yang You","Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.","Making AI Faster with Complex InformationModern AI often deals with very long sequences of information, like lengthy documents or videos. Processing this complex data can be slow and require a lot of computer memory.Current methods try to speed this up by dividing the work, but they're often rigid, only splitting the data in one way. This can cause slowdowns as different computer parts shuffle information back and forth.Our new approach, Dynamic Sequence Parallelism (DSP), is much more flexible. It intelligently changes how it divides the data based on the specific task the AI is performing at that moment. This smart, adaptive splitting significantly reduces the data shuffling.The result? DSP makes AI systems 32.2% to 10 times faster at handling long, complex information, all while using less than a quarter of the communication compared to older methods. This allows for more powerful and efficient AI."
