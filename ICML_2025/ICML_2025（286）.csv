type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Stochastic Deep Restoration Priors for Imaging Inverse Problems,https://ICML.cc//virtual/2025/poster/45624,"Yuyang Hu, Albert Peng, Weijie Gan, Peyman Milanfar, Mauricio Delbracio, Ulugbek Kamilov","Deep neural networks trained as image denoisers are widely used as priors for solving imaging inverse problems. We introduce Stochastic deep Restoration Priors (ShaRP), a novel framework that stochastically leverages an ensemble of deep restoration models beyond denoisers to regularize inverse problems. By using generalized restoration models trained on a broad range of degradations beyond simple Gaussian noise, ShaRP effectively addresses structured artifacts and enables self-supervised training without fully sampled data. We prove that ShaRP minimizes an objective function involving a regularizer derived from the score functions of minimum mean square error (MMSE) restoration operators. We also provide theoretical guarantees for learning restoration operators from incomplete measurements. ShaRP achieves state-of-the-art performance on tasks such as magnetic resonance imaging reconstruction and single-image super-resolution,  surpassing both denoiser- and diffusion-model-based methods without requiring retraining.","Recovering clear images from noisy or incomplete data, like MRIs or low-resolution photos, is a common challenge. While deep learning models often help by learning to remove simple noise, this isn't always enough for complex errors.We introduce ""ShaRP,"" a new system that uses a team of diverse image restoration models, not just single noise removers. This ensemble approach better tackles complex image artifacts. Crucially, ShaRP can sometimes learn to fix images even without needing perfect, clean examples. Our tests show ShaRP produces state-of-the-art results for tasks like MRI reconstruction and enhancing image resolution, outperforming current methods."
Poster,Stochastic Encodings for Active Feature Acquisition,https://ICML.cc//virtual/2025/poster/45534,"Alexander Norcliffe, Changhee Lee, Fergus Imrie, Mihaela van der Schaar, Pietro Lió","Active Feature Acquisition is an instance-wise, sequential decision making problem. The aim is to dynamically select which feature to measure based on current observations, independently for each test instance. Common approaches either use Reinforcement Learning, which experiences training difficulties, or greedily maximize the conditional mutual information of the label and unobserved features, which makes myopic acquisitions. To address these shortcomings, we introduce a latent variable model, trained in a supervised manner. Acquisitions are made by reasoning about the features across many possible unobserved realizations in a stochastic latent space. Extensive evaluation on a large range of synthetic and real datasets demonstrates that our approach reliably outperforms a diverse set of baselines.",Problem: In the real world not all information to solve a problem is all available at once. An AI model should be able to look at its current information and decide what to measure next to improve its prediction. Doctors do this when they diagnose a patient: based on their existing observations they choose what test to conduct next.Solution: We developed an AI-based method that addresses this problem. It makes decisions similarly to how a human might. It considers many possible situations and which measurement tends to be the best.Impact: With more development this method could be used as a diagnostic tool.
Poster,Stochastic Forward–Backward Deconvolution: Training Diffusion Models with Finite Noisy Datasets,https://ICML.cc//virtual/2025/poster/44994,"Haoye Lu, Qifan Wu, Yaoliang Yu","Recent diffusion-based generative models achieve remarkable results by training on massive datasets, yet this practice raises concerns about memorization and copyright infringement. A proposed remedy is to train exclusively on noisy data with potential copyright issues, ensuring the model never observes original content. However, through the lens of deconvolution theory, we show that although it is theoretically feasible to learn the data distribution from noisy samples, the practical challenge of collecting sufficient samples makes successful learning nearly unattainable. To overcome this limitation, we propose to pretrain the model with a small fraction of clean data to guide the deconvolution process. Combined with our Stochastic Forward--Backward Deconvolution (SFBD) method, we attain FID  6.31 on CIFAR-10 with just 4% clean images (and 3.58 with 10%). We also provide theoretical guarantees that SFBD learns the true data distribution. These results underscore the value of limited clean pretraining, or pretraining on similar datasets. Empirical studies further validate and enrich our findings.","Modern image generation models - such as those behind AI art tools - are typically trained on massive collections of images. However, this practice raises important concerns: some of the training data may be copyrighted, and models risk memorizing and reproducing such content too closely. One proposed solution is to train models only on noisy (blurred or altered) versions of the images, ensuring the originals are never directly seen. Yet in practice, we show that learning from noisy data alone is extremely difficult - it requires an impractically large number of samples to be effective.In this work, we focus on diffusion models and demonstrate that introducing even a small fraction of clean (original) data, just 4% or 10%, can make a substantial difference.  We propose a method called Stochastic Forward–Backward Deconvolution (SFBD), which alternates between denoising noisy samples using the current model and then retraining the model with those denoised results. This process helps the model gradually learn to generate realistic images, even when most of the training data is noisy. Our experiments show that SFBD achieves image quality close to models trained on fully clean datasets, while greatly reducing legal and ethical risks. This work offers a promising path toward training generative models more responsibly and efficiently."
Poster,Stochastic Layer-Wise Shuffle for Improving Vision Mamba Training,https://ICML.cc//virtual/2025/poster/46353,"Zizheng Huang, Haoxing Chen, Jiaqi Li, jun lan, Huijia Zhu, Weiqiang Wang, Limin Wang","Recent Vision Mamba (Vim) models exhibit nearly linear complexity in sequence length, making them highly attractive for processing visual data. However, the training methodologies and their potential are still not sufficiently explored. In this paper, we investigate strategies for Vim and propose Stochastic Layer-Wise Shuffle (SLWS), a novel regularization method that can effectively improve the Vim training. Without architectural modifications, this approach enables the non-hierarchical Vim to get leading performance on ImageNet-1K compared with the similar type counterparts. Our method operates through four simple steps per layer: probability allocation to assign layer-dependent shuffle rates, operation sampling via Bernoulli trials, sequence shuffling of input tokens, and order restoration of outputs. SLWS distinguishes itself through three principles: \textit{(1) Plug-and-play:} No architectural modifications are needed, and it is deactivated during inference. \textit{(2) Simple but effective:} The four-step process introduces only random permutations and negligible overhead. \textit{(3) Intuitive design:} Shuffling probabilities grow linearly with layer depth, aligning with the hierarchical semantic abstraction in vision models. Our work underscores the importance of tailored training strategies for Vim models and provides a helpful way to explore their scalability. Code and models are available at https://github.com/huangzizheng01/ShuffleMamba","Big pictures slow most AI models, because their running time grows faster than the image. Vision Mamba stays fast because time grows almost linearly with image size. Yet it has some issues when trained to larger versions. We improve this with a training method called Stochastic Layer-Wise Shuffle. This costs nothing at test time and needs no changes to the network."
Poster,Stochastic Online Conformal Prediction with Semi-Bandit Feedback,https://ICML.cc//virtual/2025/poster/45727,"Haosen Ge, Hamsa Bastani, Osbert Bastani","Conformal prediction has emerged as an effective strategy for uncertainty quantification by modifying a model to output sets of labels instead of a single label. These prediction sets come with the guarantee that they contain the true label with high probability. However, conformal prediction typically requires a large calibration dataset of i.i.d. examples. We consider the online learning setting, where examples arrive over time, and the goal is to construct prediction sets dynamically. Departing from existing work, we assume semi-bandit feedback, where we *only observe the true label if it is contained in the prediction set*. For instance, consider calibrating a document retrieval model to a new domain; in this setting, a user would only be able to provide the true label if the target document is in the prediction set of retrieved documents. We propose a novel conformal prediction algorithm targeted at this setting, and prove that it obtains sublinear regret compared to the optimal conformal predictor. We evaluate our algorithm on a retrieval task, an image classification task, and an auction price-setting task, and demonstrate that it empirically achieves good performance compared to several baselines.","Imagine a search engine that shows you a handful of results. If the one you're looking for is in that list, you can point it out. But if it's missing entirely, you can't really help—you just know it's not there. The system only gets feedback if it includes the right answer somewhere in the list.We look at how to train AI systems in exactly that kind of situation. The key idea comes from a technique called conformal prediction, which doesn’t pick a single answer—it outputs a set of possible answers, and promises that the correct one will be in that set most of the time.Normally, conformal prediction relies on having a lot of well-labeled data, but the real world doesn’t always work that way. Sometimes data comes in one example at a time, and you only find out the right answer if it was in your prediction set—like trying to learn from a quiz where the teacher only marks your answer if it's on a shortlist you came up with. To tackle this challenge, we propose a new algorithm that updates its predictions as data arrives, even when feedback is limited this way. We show that our method gradually improves and comes close to the best possible performance. This method could enable AI systems to learn more effectively in settings where full feedback is impractical—such as when there are too many candidate options."
Poster,Stochastic Poisson Surface Reconstruction with One Solve using Geometric Gaussian Processes,https://ICML.cc//virtual/2025/poster/45222,"Sidhanth Holalkere, David S Bindel, Silvia Sellán, Alexander Terenin","Poisson Surface Reconstruction is a widely-used algorithm for reconstructing a surface from an oriented point cloud. To facilitate applications where only partial surface information is available, or scanning is performed sequentially, a recent line of work proposes to incorporate uncertainty into the reconstructed surface via Gaussian process models. The resulting algorithms first perform Gaussian process interpolation, then solve a set of volumetric partial differential equations globally in space, resulting in a computationally expensive two-stage procedure. In this work, we apply recently-developed techniques from geometric Gaussian processes to combine interpolation and surface reconstruction into a single stage, requiring only one linear solve per sample. The resulting reconstructed surface samples can be queried locally in space, without the use of problem-dependent volumetric meshes or grids. These capabilities enable one to (a) perform probabilistic collision detection locally around the region of interest, (b) perform ray casting without evaluating points not on the ray's trajectory, and (c) perform next-view planning on a per-ray basis. They also do not requiring one to approximate kernel matrix inverses with diagonal matrices as part of intermediate computations, unlike prior methods. Results show that our approach provides a cleaner, more-principled, and more-flexible stochastic surface reconstruction pipeline.","(1) In computer graphics, recent work on Stochastic Poisson Surface Reconstruction offers a principled way to quantify uncertainty about how to reconstruct a surface from oriented point cloud data, but requires multiple linear solves as part of its computational pipeline. (2) We use ideas from geometric Gaussian processes to reduce this to one linear solve, which scales only with the size of the point cloud data, and not the volumetric grid around it. (3) This produces a computational pipeline for quantifying uncertainty in surface reconstruction which is simpler and more scalable."
Poster,Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints,https://ICML.cc//virtual/2025/poster/44818,"Ruichuan Huang, Jiawei Zhang, Ahmet Alacaoglu","We propose smoothed primal-dual algorithms for solving stochastic nonconvex optimization problems with linear \emph{inequality} constraints. Our algorithms are single-loop and only require a single (or two) samples of stochastic gradients at each iteration. A defining feature of our algorithm is that it is based on an inexact gradient descent framework for the Moreau envelope, where the gradient of the Moreau envelope is estimated using one step of a stochastic primal-dual (linearized) augmented Lagrangian algorithm. To handle inequality constraints and stochasticity, we combine the recently established global error bounds in constrained optimization with a Moreau envelope-based analysis of stochastic proximal algorithms. We establish the optimal (in their respective cases) $O(\varepsilon^{-4})$ and $O(\varepsilon^{-3})$ sample complexity guarantees for our algorithms and provide extensions to stochastic linear constraints. Unlike existing methods, iterations of our algorithms are free of subproblems, large batch sizes or increasing penalty parameters in their iterations and they use dual variable updates to ensure feasibility.","Current machine learning systems train neural networks with constraints, which can be for example safety limits or a desired functionality from the network. These problems are modeled by what is referred to as ""constrained optimization problems"". In particular, in modern machine learning, due to the structure and large size of the neural networks, vast amount of data, these problems lack the property called ""convexity"" and the algorithms used in practice are ""stochastic"", that is, they use only a fraction of the available data at every iteration.Our paper focuses on a special case of the above problem, one where the constraints are given as linear functions. We propose and theoretically analyze algorithms that are similar to ones used in practice and provide guarantees on the amount of computational resources they need to give us an ""approximately good"" point in terms of solving our problem. These guarantees are of the same order as the best possible guarantees that can be achievable by algorithms of the type we analyze for this problem.Theoretical guarantees for algorithms ensure practitioners that the method they use to solve a problem will behave correctly in practice. Moreover, this also guides the design of faster algorithms that will require less computational resources to output a point that is as good as more computationally heavy algorithms."
Poster,SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics,https://ICML.cc//virtual/2025/poster/45384,"Suyuan Zhao, YIZHEN LUO, Ganbo Yang, Yan Zhong, Hao Zhou, Zaiqing Nie","Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells.Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile.To address this challenge, we propose **SToFM**, a multi-scale **S**patial **T**ranscript**o**mics **F**oundation **M**odel.SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro- and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices.Additionally, we construct **SToCorpus-88M**, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data through capturing and integrating multi-scale information.","Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells.The form of ST data is tissue slices containing a large number of cells, with each cell containing a high-dimensional gene expression profile.We hope to establish a foundation model for the complex data, which requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile.To address this challenge, we propose **SToFM**, a multi-scale **S**patial **T**ranscript**o**mics **F**oundation **M**odel, to aggregate information from different scales and obtain high-quality cell representations.Additionally, we construct **SToCorpus-88M**, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data through capturing and integrating multi-scale information."
Poster,STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving,https://ICML.cc//virtual/2025/poster/43472,"Kefan Dong, Tengyu Ma","A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating  proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens generated during the training in Lean, STP proves 28.5% of the statements in the LeanWorkbook dataset,  doubling the previous best result of 13.1% achieved through expert iteration.The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (65.0\%), ProofNet-test (23.9\%) and PutnamBench (8/644) with pass@3200.","Proving mathematical theorems is one of the most rigorous forms of reasoning — and a major challenge for AI. Traditional systems rely heavily on large datasets of theorem statements, which are often hard to scale. In this work, we introduce STP (Self-play Theorem Prover), a new AI training method that takes a different approach: having the AI generate its own problems and attempt to solve them.STP consists of two components: a conjecturer, which proposes new conjectures that are challenging yet approachable, and a prover, which tries to prove them. These two components are trained together, providing feedback to each other. We evaluate STP on formal proof benchmarks in Lean and Isabelle — two well-known formal proof languages — and achieve state-of-the-art performance, significantly outperforming previous methods based on classic RL algorithms.STP mirrors how human mathematicians often work: by proposing new questions and learning through the process of solving them. This approach offers a promising step toward AI systems that can continue improving without needing more data from humans."
Poster,Strategic A/B testing via Maximum Probability-driven Two-armed Bandit,https://ICML.cc//virtual/2025/poster/46086,"Yu Zhang, Shanshan Zhao, Bokui Wan, Jinjuan Wang, Xiaodong Yan","Detecting a minor average treatment effect is a major challenge in large-scale applications, where even minimal improvements can have a significant economic impact. Traditional methods, reliant on normal distribution-based or expanded statistics, often fail to identify such minor effects because of their inability to handle small discrepancies with sufficient sensitivity. This work leverages a counterfactual outcome framework and proposes a maximum probability-driven two-armed bandit (TAB) process by weighting the mean volatility statistic, which controls Type I error. The implementation of permutation methods further enhances the robustness and efficacy. The established strategic central limit theorem (SCLT) demonstrates that our approach yields a more concentrated distribution under the null hypothesis and a less concentrated one under the alternative hypothesis, greatly improving statistical power. The experimental results indicate a significant improvement in the A/B testing, highlighting the potential to reduce experimental costs while maintaining high statistical power.","In industry, comparing the effectiveness of two experimental strategies—such as evaluating which of two advertising policies yields higher revenue—is both common and challenging. We propose a novel test statistic based on the two-armed bandit framework, rather than relying on the Central Limit Theorem. This statistic not only offers improved statistical power but also effectively controls the Type I error rate. Our work has important implications for enhancing the sensitivity of A/B testing, reducing their duration, and lowering experimental costs."
