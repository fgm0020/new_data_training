type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series,https://ICML.cc//virtual/2025/poster/45009,"Zachary Brown, David Carlson","The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.","In order to understand brain activity, neuroscientists currently have to explore an enormous amount of possible root causes, which can be costly and time consuming. We created an AI-based way to predict which possible root causes will actually be important for understanding brain behavior. This should help neuroscientists identify the causes of behaviors in the brain more quickly and at reduced cost."
Poster,Generation from Noisy Examples,https://ICML.cc//virtual/2025/poster/44036,"Ananth Raman, Vinod Raman","We continue to study the learning-theoretic foundations of generation by extending the results from Kleinberg and Mullainathan [2024] and Li et al. [2024] to account for noisy example streams. In the noiseless setting of Kleinberg and Mullainathan [2024] and Li et al. [2024], an adversary picks a hypothesis from a binary hypothesis class and provides a generator with a sequence of its positive examples. The goal of the generator is to eventually output new, unseen positive examples. In the noisy setting, an adversary still picks a hypothesis and a sequence of its positive examples. But, before presenting the stream to the generator, the adversary inserts a finite number of negative examples. Unaware of which examples are noisy, the goal of the generator is to still eventually output new, unseen positive examples. In this paper, we provide necessary and sufficient conditions for when a binary hypothesis class can be noisily generatable. We provide such conditions with respect to various constraints on the number of distinct examples that need to be seen before perfect generation of positive examples. Interestingly, for finite and countable classes we show that generatability is largely unaffected by the presence of a finite number of noisy examples.","How can machines learn to generate new, valid examples from noisy data? Our paper explores that question through a learning-theoretic lens. Inspired by prior work on ""generation in the limit,"" where a model must eventually produce novel, unseen examples from a concept class, our study extends the framework to more realistic scenarios where the data stream may contain a few incorrect or ""negative"" examples. We define several increasingly relaxed notions of ""noisy generatability"" and provide precise mathematical conditions under which generation remains possible despite the noise. We show that while a small amount of noise can make the task harder, for many useful classes—including all finite and countable ones—generation is still possible. This work contributes a formal foundation for robust generation, with implications for building systems like large language models that must cope with imperfect data."
Poster,Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction,https://ICML.cc//virtual/2025/poster/45195,"Shu-wen Yang, Byeonggeun Kim, Kuan Po Huang, Qingming Tang, Huy Phan, Bo-Ru Lu, Harshavardhan Sundar, Shalini Ghosh, Hung-yi Lee, Chieh-Chi Kao, Chao Wang","Autoregressive next-token prediction with the Transformer decoder has become a de facto standard in large language models (LLMs), achieving remarkable success in Natural Language Processing (NLP) at scale. Extending this paradigm to audio poses unique challenges due to its inherently continuous nature. We research audio generation with a causal language model (LM) without discrete tokens. We leverage token-wise diffusion to model the continuous distribution of the next continuous-valued token. Our approach delivers significant improvements over previous discrete solution, AudioGen, achieving 20% and 40% relative gains on AudioCaps in Frechet Audio Distance (FAD) and Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a novel masked next-token prediction task that incorporates masked prediction into the causal LM framework. On AudioCaps, the innovation yields 41% and 33% relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B) models, respectively, and is on par with the state-of-the-art (SOTA) diffusion models. Furthermore, we achieve these results with significantly fewer parameters—193M for our Base and 462M for our Large models.","Have you ever wondered how computers can generate realistic sounds, like music or environmental noises? Our research pushes this ability forward by teaching AI to create audio in a smoother and more natural way—more like how humans experience sound.Traditionally, powerful AI systems like ChatGPT learn to predict the next word when writing sentences. We use a similar method, but instead of words, we teach the AI to generate the next tiny slice of sound. This is especially tricky because sound isn’t made of clear-cut pieces like words—it’s continuous and complex.To tackle this, we designed a new method that helps the AI better understand and produce these continuous sound waves. As a result, our system creates audio that sounds much more natural than older methods, especially compared to a popular model called AudioGen. It’s also more efficient: our models are smaller and faster, but still match or even outperform state-of-the-art systems in quality.In short, we’ve taken a big step toward helping AI generate high-quality audio in a smarter and more efficient way—bringing us closer to lifelike sound generation for games, movies, accessibility tools, and beyond."
Poster,Generative Data Mining with Longtail-Guided Diffusion,https://ICML.cc//virtual/2025/poster/46120,"David Hayden, Mao Ye, Timur Garipov, Gregory Meyer, Carl Vondrick, Zhao Chen, Yuning Chai, Eric M. Wolff, Siddhartha Srinivasa","It is difficult to anticipate the myriad challenges that a predictive model will encounter once deployed. Common practice entails a reactive, cyclical approach: model deployment, data mining, and retraining. We instead develop a proactive longtail discovery process by imagining additional data during training. In particular, we develop general model-based longtail signals, including a differentiable, single forward pass formulation of epistemic uncertainty that does not impact model parameters or predictive performance but can flag rare or hard inputs. We leverage these signals as guidance to generate additional training data from a latent diffusion model in a process we call Longtail Guidance (LTG). Crucially, we can perform LTG without retraining the diffusion model or the predictive model, and we do not need to expose the predictive model to intermediate diffusion states. Data generated by LTG exhibit semantically meaningful variation, yield significant generalization improvements on numerous image classification benchmarks, and can be analyzed by a VLM to proactively discover, textually explain, and address conceptual gaps in a deployed predictive model.",We imbue predictive AI models with the ability to continuously dream up additional hard or rare data that can be used as additional training data for improving their own capabilities in uncommon real-world scenarios. We further provide techniques to reduce those hard or rare data to textual descriptions so that humans can better anticipate what an AI model might struggle with before release.
Poster,Generative Human Trajectory Recovery via Embedding-Space Conditional Diffusion,https://ICML.cc//virtual/2025/poster/45338,"KAIJUN LIU, Sijie Ruan, Liang Zhang, Cheng Long, Shuliang Wang, Liang Yu","Recovering human trajectories from incomplete or missing data is crucial for many mobility-based urban applications, e.g., urban planning, transportation, and location-based services. Existing methods mainly rely on recurrent neural networks or attention mechanisms. Though promising, they encounter limitations in capturing complex spatial-temporal dependencies in low-sampling trajectories. Recently, diffusion models show potential in content generation. However, most of proposed methods are used to generate contents in continuous numerical representations, which cannot be directly adapted to the human location trajectory recovery. In this paper, we introduce a conditional diffusion-based trajectory recovery method, namely, DiffMove. It first transforms locations in trajectories into the embedding space, in which the embedding denoising is performed, and then missing locations are recovered by an embedding decoder. DiffMove not only improves accuracy by introducing high-quality generative methods in the trajectory recovery, but also carefully models the transition, periodicity, and temporal patterns in human mobility. Extensive experiments based on two representative real-world mobility datasets are conducted, and the results show significant improvements (an average of 11% in recall) over the best baselines.","People increasingly rely on location-based services—like maps, ride-sharing, and recommendation apps—but often stop sharing their location due to privacy or battery concerns. This creates “gaps” in the data that make it hard for apps and planners to understand where people actually go. We developed a new AI method that treats these gaps like a puzzle, using a generative process by conditional diffusion model to fill in missing stops in someone’s daily route. Instead of predicting a single determined path, our approach can suggest some plausible locations by learning the spatial temporal features and obtain better estimates of locations a person might have traveled, reflecting real-life variability. By recovering those hidden steps, our system can power more accurate transit planning, personalized recommendations, and better traffic management—while still respecting users’ choice to share only part of their journey."
Poster,Generative Intervention Models for Causal Perturbation Modeling,https://ICML.cc//virtual/2025/poster/45875,"Nora Schneider, Lars Lorch, Niki Kilbertus, Bernhard Schölkopf, Andreas Krause","We consider the problem of predicting perturbation effects via causal models. In many applications, it is a priori unknown which mechanisms of a system are modified by an external perturbation, even though the features of the perturbation are available. For example, in genomics, some properties of a drug may be known, but not their causal effects on the regulatory pathways of cells. We propose a generative intervention model (GIM) that learns to map these perturbation features to distributions over atomic interventions in a jointly-estimated causal model. Contrary to prior approaches, this enables us to predict the distribution shifts of unseen perturbation features while gaining insights about their mechanistic effects in the underlying data-generating process. On synthetic data and scRNA-seq drug perturbation data, GIMs achieve robust out-of-distribution predictions on par with unstructured approaches, while effectively inferring the underlying perturbation mechanisms, often better than other causal inference methods.","Understanding how systems respond to small changes is a key goal in many scientific fields — for example, predicting how a cell reacts to different drugs. Since testing every possible drug experimentally is infeasible, we need computational methods to make such predictions. Often, we have data showing how a system responds to a set of known drugs, along with features that describe these drugs, such as their chemical properties. The challenge is to predict how the system will change under a new, previously unseen drug.We introduce an approach, called Generative Intervention Models (GIMs), that learns the system's internal structure and how different perturbations (the drugs) affect it. Specifically, we model the system using a causal graph that captures how variables, such as the genes of cells, influence each other. Then, using the features of each drug, we learn how that drug alters the system’s internal mechanisms.This approach allows us to predict the effects of new drugs and gain insight into how they act on the system. On synthetic and single-cell RNA sequencing (scRNA-seq) drug data, GIMs show strong performance and accurately infer the underlying mechanisms of the perturbations."
Poster,Generative Modeling Reinvents Supervised Learning: Label Repurposing with Predictive Consistency Learning,https://ICML.cc//virtual/2025/poster/45890,"Yang Li, Jiale Ma, Yebin Yang, Qitian Wu, Hongyuan Zha, Junchi Yan","Predicting labels directly from data has been the standard in label learning tasks, e.g., supervised learning, where models often prioritize feature compression and extraction from inputs under the assumption that label information is less complex. However, recent prediction tasks often face predicting complex labels, exacerbating the challenge of learning mappings from learned features to high-fidelity label representations. To this end, we draw inspiration from the consistency training concept in generative consistency models and propose predictive consistency learning (PCL), a novel learning paradigm that decomposes the full label information into a progressive learning procedure, mitigating the label capture challenge. Besides data inputs, PCL additionally receives input from noise-perturbed labels as an additional reference, pursuing predictive consistency across different noise levels. It simultaneously learns the relationship between latent features and a spectrum of label information, which enables progressive learning for complex predictions and allows multi-step inference analogous to gradual denoising, thereby enhancing the prediction quality. Experiments on vision, text, and graph tasks show the superiority of PCL over conventional supervised training in complex label prediction tasks.","In machine learning, labels are typically viewed as simple answers that models aim to predict from data. However, many real-world tasks involve complex labels that contain richer information beyond just a final answer. This work explores a fundamental question: when labels hold valuable information, can they be used to aid learning instead of merely serving as prediction targets?We introduce a novel approach to unlock this hidden value in labels by treating them not only as targets but also as informative references during training. Our method, Predictive Consistency Learning (PCL), inspired by generative consistency models, breaks down label information into a progressive learning process. Besides data inputs, PCL additionally receives input from noise-perturbed labels as an additional reference, pursuing predictive consistency across different noise levels.This strategy shows promise across diverse data types such as images, text, and graphs. By demonstrating the effectiveness of incorporating label information into model input for reference, this study opens new avenues for rethinking how labels are utilized in machine learning."
Poster,Generative Point Cloud Registration,https://ICML.cc//virtual/2025/poster/43513,"Haobo Jiang, Jin Xie, jian Yang, Liang Yu, Jianmin Zheng","In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometric-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled  conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.","Our research introduces a new 3D matching paradigm called Generative Point Cloud Registration. We've found a way to use advanced 2D generative AI to help with this 3D problem. Our key idea is to generate realistic image pairs from different viewpoints of the 3D scene. These images are carefully created to be consistent with both the 3D scene geometry and their visual appearance across views. By combining information from both the 3D scans and these generated 2D images, we can more accurately and reliably match the 3D objects.To achieve this, we developed Match-ControlNet, a specialized 2D generative model for 3D matching task. It uses information about the 3D depth of the scene to create images that are geometrically accurate. Additionally, it ensures that the generated images from different viewpoints have consistent textures, making them ideal for matching. Our proposed new paradigm is plug-and-play and can improve the performance of many existing 3D registration techniques. We've shown its effectiveness through extensive testing on widely used 3D datasets."
Poster,Generative Social Choice: The Next Generation,https://ICML.cc//virtual/2025/poster/45972,"Niclas Boehmer, Sara Fish, Ariel Procaccia","A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions.","On many online platforms, users share their opinions, ranging from product reviews on Amazon to public policy discussions on Pol.is. But when hundreds or thousands of people weigh in, it becomes impossible for any one person to read everything.We introduce the Proportional Slate Engine (PROSE), a system that creates a slate of statements from large, messy collections of opinions. Instead of repeating popular comments, PROSE uses a language model (like ChatGPT) to write and assess new statements that synthesize the beliefs of groups of users. Inspired by ideas from voting theory, PROSE aims for proportional fairness: each viewpoint receives space in the summary proportional to its popularity. A key challenge is ensuring both clarity and fairness. To keep things simple, PROSE respects a word limit, so the summary doesn't become too long. To ensure fairness, we provide mathematical guarantees: even if the language model makes small mistakes, the summary still proportionally reflects what people believe.We tested PROSE on real-world data like drug reviews and city improvement debates, and found it to be a powerful tool for summarizing public input in a readable and fair way."
Poster,GenMol: A Drug Discovery Generalist with Discrete Diffusion,https://ICML.cc//virtual/2025/poster/45647,"Seul Lee, Karsten Kreis, Srimukh Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, Arash Vahdat","Drug discovery is a complex process that involves multiple stages and tasks. However, existing molecular generative models can only tackle some of these tasks. We present *Generalist Molecular generative model* (GenMol), a versatile framework that uses only a *single* discrete diffusion model to handle diverse drug discovery scenarios. GenMol generates Sequential Attachment-based Fragment Embedding (SAFE) sequences through non-autoregressive bidirectional parallel decoding, thereby allowing the utilization of a molecular context that does not rely on the specific token ordering while having better sampling efficiency. GenMol uses fragments as basic building blocks for molecules and introduces *fragment remasking*, a strategy that optimizes molecules by regenerating masked fragments, enabling effective exploration of chemical space. We further propose *molecular context guidance* (MCG), a guidance method tailored for masked discrete diffusion of GenMol. GenMol significantly outperforms the previous GPT-based model in *de novo* generation and fragment-constrained generation, and achieves state-of-the-art performance in goal-directed hit generation and lead optimization. These results demonstrate that GenMol can tackle a wide range of drug discovery tasks, providing a unified and versatile approach for molecular design.","Drug discovery is a complicated process with many steps, but most current AI models for generating molecules can only handle a few of these tasks. This limits their usefulness in real-world drug development.In this work, we introduce a new framework named GenMol. GenMol is a new, flexible AI model designed to handle a wide range of drug discovery tasks using just one system. GenMol performs parallel generation, meaning it can generate molecules efficiently without depending on the order of tokens of molecular sequences. GenMol is endowed with versatile generation capabilities, including generating molecules by combining small molecular substructures called fragments and improves them by regenerating selected fragments through a process called fragment remasking. GenMol also adopts a scheme called molecular context guidance (MCG) to help it calibrate its own predictions to fully utilize given molecular information.GenMol outperforms existing methods, including those based on GPT, in several key tasks like designing new molecules and optimizing drug candidates. It offers a powerful, all-in-one tool for faster and more effective drug discovery."
