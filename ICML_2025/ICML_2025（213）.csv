type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Optimizing Noise Distributions for Differential Privacy,https://ICML.cc//virtual/2025/poster/43633,"Atefeh Gilani, Felipe Gomez, Shahab Asoodeh, Flavio Calmon, Oliver Kosut, Lalitha Sankar","We propose a unified optimization framework for designing continuous and discrete noise distributions that ensure differential privacy (DP) by minimizing Rényi DP, a variant of DP, under a cost constraint. Rényi DP has the advantage that by considering different values of the Rényi parameter $\alpha$, we can tailor our optimization for any number of compositions. To solve the optimization problem, we reduce it to a finite-dimensional convex formulation and perform preconditioned gradient descent. The resulting noise distributions are then compared to their Gaussian and Laplace counterparts. Numerical results demonstrate that our optimized distributions are consistently better, with significant improvements in $(\varepsilon, \delta)$-DP guarantees in the moderate composition regimes, compared to Gaussian and Laplace distributions with the same variance.","Protecting sensitive information is a major concern in the age of big data. Differential Privacy (DP) is a popular method for ensuring privacy by adding random noise to data, making it difficult to identify individuals. However, choosing the right type of noise is critical—too much noise can ruin data accuracy, and too little can fail to protect privacy. In this work, we introduce a new way to find the best noise distribution for a given privacy guarantee. Our method improves the accuracy of results while still meeting strong privacy standards. We show that our optimized noise works better than commonly used noise types, such as Gaussian or Laplace, across different datasets and privacy settings. This approach can help make privacy-preserving machine learning more reliable and effective in real-world applications."
Poster,Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach,https://ICML.cc//virtual/2025/poster/44667,"Xu Zhang, Kaidi Xu, Ziqing Hu, Ren Wang","Mixture of Experts (MoE) have shown remarkable success in leveraging specialized expert networks for complex machine learning tasks. However, their susceptibility to adversarial attacks presents a critical challenge for deployment in robust applications. This paper addresses the critical question of how to incorporate robustness into MoEs while maintaining high natural accuracy. We begin by analyzing the vulnerability of MoE components, finding that expert networks are notably more susceptible to adversarial attacks than the router. Based on this insight, we propose a targeted robust training technique that integrates a novel loss function to enhance the adversarial robustness of MoE, requiring only the robustification of one additional expert without compromising training or inference efficiency. Building on this, we introduce a dual-model strategy that linearly combines a standard MoE model with our robustified MoE model using a smoothing parameter. This approach allows for flexible control over the robustness-accuracy trade-off. We further provide theoretical foundations by deriving certified robustness bounds for both the single MoE and the dual-model. To push the boundaries of robustness and accuracy, we propose a novel joint training strategy JTDMoE for the dual-model. This joint training enhances both robustness and accuracy beyond what is achievable with separate models. Experimental results on CIFAR-10 and TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures demonstrate the effectiveness of our proposed methods.","Modern AI models are becoming more powerful—but also more vulnerable to tiny, invisible changes in input data. This is especially true for “Mixture-of-Experts” models, which divide work among several smaller networks called experts. Our research found that these expert networks are much easier to fool than the part of the system that assigns tasks to them. To fix this, we designed a new way to selectively train just one extra expert at a time, making the entire system more robust without slowing it down. We also created a two-part model that combines a normal version with a more defensive one, giving users a way to balance accuracy and safety. To support this, we developed new mathematical tools to show when and why our system can resist attacks. Our experiments on image datasets confirm that our method makes AI models both stronger and smarter. This work helps pave the way for safer AI systems in real-world applications like self-driving cars, medical imaging, or content moderation—where mistakes from small changes could be costly."
Poster,Optimizing Social Network Interventions via Hypergradient-Based Recommender System Design,https://ICML.cc//virtual/2025/poster/45353,"Marino Kühne, Panagiotis D. Grontas, Giulia De Pasquale, Giuseppe Belgioioso, Florian Dörfler, John Lygeros","Although social networks have expanded the range of ideas and information accessible to users, they are also criticized for amplifying the polarization of user opinions. Given the inherent complexity of these phenomena, existing approaches to counteract these effects typically rely on handcrafted algorithms and heuristics. We propose an elegant solution: we act on the network weights that model user interactions on social networks (e.g., ranking of users’ shared content in feeds), to optimize a performance metric (e.g., minimize polarization), while users’ opinions follow the classical Friedkin-Johnsen model. Our formulation gives rise to a challenging, large-scale optimization problem with non-convex constraints, for which we develop a gradient-based algorithm. Our scheme is simple, scalable, and versatile, as it can readily integrate different, potentially non-convex, objectives. We demonstrate its merit by: (i) rapidly solving complex social network intervention problems with 4.8 million variables based on the Reddit, LiveJournal, and DBLP datasets; (ii) outperforming competing approaches in terms of both computation time and disagreement reduction.","Social media platforms have made it easier to share ideas and connect with others, but they can also contribute to increasing polarization by reinforcing existing beliefs and limiting exposure to diverse viewpoints. Addressing this challenge is complex, and most current solutions rely on manually crafted rules or heuristics. In our work, we present a flexible approach that adjusts the interactions between users to reduce a wide range of objectives, such as polarization or disagreement. Our method is scalable, capable of handling networks with millions of users, and adaptable to a range of objectives beyond polarization. When evaluated on real-world data, our approach significantly outperforms existing techniques, both in reducing disagreement and in computational efficiency."
Poster,Optimizing Temperature for Language Models with Multi-Sample Inference,https://ICML.cc//virtual/2025/poster/43869,"Weihua Du, Yiming Yang, Sean Welleck","Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is temperature selection, which significantly impacts model performance. Existing approaches either rely on a fixed default temperature or require labeled validation data for tuning, which are often scarce and difficult to obtain. This paper addresses the challenge of automatically identifying the (near)-optimal temperature for different LLMs using multi-sample aggregation strategies, without relying on task-specific validation data. We provide a comprehensive analysis of temperature’s role in performance optimization, considering variations in model architectures, datasets, task types, model sizes, and predictive accuracy. Furthermore, we propose a novel entropy-based metric for automated temperature optimization, which consistently outperforms fixed-temperature baselines. Additionally, we incorporate a stochastic process model to enhance interpretability, offering deeper insights into the relationship between temperature and model performance.","This paper introduces a method for automatically selecting an appropriate temperature for different models and tasks under multi-sample aggregation strategies like majority voting and best-of-N, without requiring any labeled data. We find that entropy serves as a strong signal for temperature selection and propose an algorithm, TURN, based on this insight."
Poster,Optimizing Test-Time Compute via Meta Reinforcement Finetuning,https://ICML.cc//virtual/2025/poster/45154,"Yuxiao Qu, Matthew Yang, Amrith Setlur, Lewis Tunstall, Edward Beeching, Russ Salakhutdinov, Aviral Kumar","Training models to efficiently use test-time compute is crucial for improving the reasoning performance of LLMs. While current methods mostly do so via fine-tuning on search traces or running RL against the 0/1 outcome reward, do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute from the lens of exploration and exploitation. It also motivates the use of cumulative regret to measure the efficacy of test-time compute by viewing a long output stream as consisting of several episodes from the model. While current state-of-the-art models do not optimize regret, we show that regret can be minimized by running final 0/1 reward RL regularized by a dense reward bonus, given by the ""information gain"" from each subsequent block in the output stream. We prescribe an approach for quantifying information gain, which measures the utility of an intermediate segment of tokens towards improving accuracy of the final answer. We instantiate this idea to develop MRT, a new class of finetuning methods for optimizing test-time compute. Fine-tuning with MRT leads to substantial improvements in both performance and token efficiency on the AIME dataset.","Modern AI models like DeepSeek-R1 often generate long responses to solve complex problems. But more thinking doesn’t always mean better answers — sometimes, it’s just wasted effort. How can we train LLM to use its thinking time more wisely?In this work, we treat each step of the model’s reasoning as a decision — similar to how a person might weigh whether to keep working on a problem or stop. We developed a new training method called Meta Reinforcement Fine-Tuning (MRT) that encourages models to make meaningful progress at every step, not just aim for a final correct answer. We tested this idea on challenging math problems and found that MRT-trained models solve problems using fewer words and with higher accuracy. In fact, they often perform better by thinking less, as long as each step is purposeful.This research offers a new way to train LLMs to reason more efficiently, which could improve their performance in real-world applications without requiring more computational resources."
Poster,OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling,https://ICML.cc//virtual/2025/poster/46227,"Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren, Yuxuan Chen, Zaiwen Wen","Despite the rapid development of large language models (LLMs), a fundamental challenge persists: the lack of high-quality optimization modeling datasets hampers LLMs' robust modeling of practical optimization problems from natural language descriptions (NL). This data scarcity also contributes to the generalization difficulties experienced by learning-based methods.To address these challenges, we propose a scalable framework for synthesizing a high-quality dataset, named OptMATH. Starting from curated seed data with mathematical formulations (MF), this framework automatically generates problem data (PD) with controllable complexity. Then, a back-translation step is employed to obtain NL. To verify the correspondence between the NL and the PD, a forward modeling step followed by rejection sampling is used. The accepted pairs constitute the training part of OptMATH. Then a collection of rejected pairs is identified and further filtered. This collection serves as a new benchmark for optimization modeling, containing difficult instances whose lengths are much longer than these of NL4OPT and MAMO.Through extensive experiments, we demonstrate that models of various sizes (0.5B-32B parameters) trained on OptMATH achieve superior results on multiple modeling benchmarks, thereby validating the effectiveness and scalability of our approach. The OptMATH dataset and related resources are available at \url{https://github.com/optsuite/OptMATH}.","Imagine you want to find the best way to do something, like the most efficient route for deliveries or the most profitable way to schedule tasks. Describing these problems in everyday language is easy for us, but computers need precise mathematical instructions to solve them. Teaching computers to make this translation from words to math is a big challenge because they need many good examples to learn from, and these are often hard to come by.Our work introduces a new system called OptMATH that cleverly creates large numbers of high-quality examples for this purpose. OptMATH starts with the core mathematical structure of a problem and then generates a relatable word description for it. Afterwards, it carefully checks if this word description accurately reflects the underlying math.The correctly matched examples become excellent training material for artificial intelligence (AI) systems. Interestingly, problems that were initially mismatched but still valid are collected and refined to create a new set of tough test questions for these AIs. Our experiments show that AI systems trained with the examples generated by OptMATH become significantly better at understanding everyday problem descriptions and setting them up for solution."
Poster,Oracle-MoE: Locality-preserving Routing in the Oracle Space for Memory-constrained Large Language Model Inference,https://ICML.cc//virtual/2025/poster/43606,"Jixian Zhou, Fang DONG(董方), Ruijun Huang, Hengjie Cao, Mengyi Chen, Yifeng Yang, Anrui Chen, Mingzhi Dong, Yujiang Wang, Dongsheng Li, David Clifton, Qin Lv, Rui Zhu, Chun Zhang, Fan Yang, Tun Lu, Ning Gu, Li Shang","Mixture-of-Experts (MoE) is widely adopted to deploy Large Language Models (LLMs) on edge devices with limited memory budgets.Although MoE is, in theory, an inborn memory-friendly architecture requiring only a few activated experts to reside in the memory for inference, current MoE architectures cannot effectively fulfill this advantage and will yield intolerable inference latencies of LLMs on memory-constrained devices. Our investigation pinpoints the essential cause as the remarkable temporal inconsistencies of inter-token expert activations, which generate overly frequent expert swapping demands dominating the latencies. To this end, we propose a novel MoE architecture, Oracle-MoE, to fulfill the real on-device potential of MoE-based LLMs. Oracle-MoE route tokens in a highly compact space suggested by attention scores, termed the *oracle space*, to effectively maintain the semantic locality across consecutive tokens to reduce expert activation variations, eliminating massive swapping demands. Theoretical analysis proves that Oracle-MoE is bound to provide routing decisions with better semantic locality and, therefore, better expert activation consistencies. Experiments on the pretrained GPT-2 architectures of different sizes (200M, 350M, 790M, and 2B) and downstream tasks demonstrate that without compromising task performance, our Oracle-MoE has achieved state-of-the-art inference speeds across varying memory budgets, revealing its substantial potential for LLM deployments in industry.","Large language models (LLMs) are very powerful but hard to run on small devices such as smartphones, since these models usually need a lot of memory, slowing down their operation on memory-limited hardware. Mixture-of-Experts (MoE) works by activating only small parts (called ""experts"") of the model when needed, which are inborn memory-friendly architectures on limited memory budgets. But current MoE methods often move experts in and out of memory repeatedly. This frequent switching introduces intolerable inference latencies .We found that the reason for this problem lies in how MoE decides which expert to use. Traditional MoE assigns experts based on rapidly changing token information. These quick changes lead to frequent and inefficient expert swapping, finally intolerable memory communication pressure. To fix this, we introduced a new method called Oracle-MoE. Oracle-MoE chooses experts by using a simpler grouping method we call ""oracle space."" Oracle space groups tokens with similar semantics together, greatly reducing how often experts switch.Our experiments show that Oracle-MoE makes the model run much faster without losing accuracy. We also provide detailed explanations and instructions for implementation. This helps other researchers easily use powerful language models on devices with limited memory."
Poster,OR-Bench: An Over-Refusal Benchmark for Large Language Models,https://ICML.cc//virtual/2025/poster/46052,"Jiaxing Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh","Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful.Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs.This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses.We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models.","As language models become safer to prevent harmful outputs, they increasingly exhibit over-refusal—rejecting even harmless prompts. This reduces their usefulness and creates a trade-off between safety and helpfulness. However, the lack of a large-scale benchmark makes it difficult to study or quantify this issue systematically.We developed an automated pipeline that rewrites toxic prompts into safe yet borderline cases likely to be wrongly rejected. Using this, we built OR-Bench, the first large-scale benchmark for over-refusal, with 80,000 prompts across 10 categories, a hard subset of 1,000 challenging cases, and 600 toxic prompts for safety evaluation.Our benchmark enables consistent evaluation of over-refusal across models. We tested 32 leading LLMs and uncovered key trade-offs between safety and helpfulness. OR-Bench provides the community with a critical tool to design models that are both safe and practically useful."
Poster,OrcaLoca: An LLM Agent Framework for Software Issue Localization,https://ICML.cc//virtual/2025/poster/45561,"Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, Jishen Zhao","Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration.","Finding the exact location of bugs in large software projects is still a major challenge for AI assistants. We present OrcaLoca, an AI agent framework that improves bug localization by searching code more intelligently. It outperforms existing open-source tools and boosts bug-fixing success in real software projects."
Poster,Organize the Web: Constructing Domains Enhances Pre-Training Data Curation,https://ICML.cc//virtual/2025/poster/44718,"Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, Luca Soldaini","Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.","A central problem when curating training data for language models is how to combine data from different sources. Increasingly, the majority of training data consists of web pages, but most prior work treat the web as a single data source. We study the internal composition of web data by breaking it down into meaningful topic and format categories. Based on these topic and format annotations, we provide insights into existing data curation practices, but also improve them by changing the balance of topics and formats in the training data."
