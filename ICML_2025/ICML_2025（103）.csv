type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"FAB-PPI: Frequentist, Assisted by Bayes, Prediction-Powered Inference",https://ICML.cc//virtual/2025/poster/46317,"Stefano Cortinovis, Francois Caron","Prediction-powered inference (PPI) enables valid statistical inference by combining experimental data with machine learning predictions. When a sufficient number of high-quality predictions is available, PPI results in more accurate estimates and tighter confidence intervals than traditional methods. In this paper, we propose to inform the PPI framework with prior knowledge on the quality of the predictions. The resulting method, which we call frequentist, assisted by Bayes, PPI (FAB-PPI), improves over PPI when the observed prediction quality is likely under the prior, while maintaining its frequentist guarantees. Furthermore, when using heavy-tailed priors, FAB-PPI adaptively reverts to standard PPI in low prior probability regions. We demonstrate the benefits of FAB-PPI in real and synthetic examples.","Making accurate decisions requires a large amount of high-quality data, which is not always available. While machine learning (ML) predictions can help cheaply fill the gap, they may be biased, leading to incorrect conclusions. Prediction-powered inference (PPI) is a recent method that enables using ML predictions for making principled decisions by correcting for such bias using just a few gold standard observations. However, standard PPI does not take advantage of existing knowledge or ""prior beliefs"" about the likely quality of these ML predictions, which are often expected to be usually very good, but occasionally very off. Our work introduces FAB-PPI, a rigorous approach to incorporate such prior beliefs into standard PPI, while maintaining the latter's guarantees. FAB-PPI significantly improves over standard PPI when the predictions are good, and it reverts to standard PPI when they are very poor. Overall, FAB-PPI offers a principled way to leverage our knowledge about ML model performance for making decisions, with a built-in safety net."
Poster,FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems,https://ICML.cc//virtual/2025/poster/44576,"Arya Fayyazi, Mehdi Kamal, Massoud Pedram","We propose FACTER, a fairness-aware framework for LLM-based recommendation systems that integrates conformal prediction with dynamic prompt engineering. By introducing an adaptive semantic variance threshold and a violation-triggered mechanism, FACTER automatically tightens fairness constraints whenever biased patterns emerge. We further develop an adversarial prompt generator that leverages historical violations to reduce repeated demographic biases without retraining the LLM. Empirical results on MovieLens and Amazon show that FACTER substantially reduces fairness violations (up to 95.5%)  while maintaining strong recommendation accuracy, revealing semantic variance as a potent proxy of bias.","Modern AI systems, like large language models, are widely used in recommendations and decision-making but can sometimes produce biased or unfair results, such as favoring certain age groups or genders. Our work introduces FACTER, a method that detects and corrects such unfair behavior without retraining the model. By statistically monitoring model outputs and updating future prompts based on prior unfair responses, FACTER helps ensure more equitable outcomes while preserving model accuracy. It is efficient, easy to apply, and improves fairness across diverse real-world applications."
Poster,FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees,https://ICML.cc//virtual/2025/poster/43756,"Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang","The propensity of large language models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly classifying hallucinations as truthful content) is essential. Despite its importance, formal verification of LLM factuality with such guarantees remains largely unexplored.In this paper, we introduce FactTest, a novel framework that statistically assesses whether an LLM can provide correct answers to given questions with high-probability correctness guarantees. We formulate hallucination detection as a hypothesis testing problem to enforce an upper bound of Type I errors at user-specified significance levels. Notably, we prove that FactTest also ensures strong Type II error control under mild conditions and can be extended to maintain its effectiveness when covariate shifts exist. Our approach is distribution-free and works for any number of human-annotated samples. It is model-agnostic and applies to any black-box or white-box LM. Extensive experiments on question-answering (QA) benchmarks demonstrate that FactTest effectively detects hallucinations and enable LLMs to abstain from answering unknown questions, leading to an over 40% accuracy improvement.","Large Language Models (LLMs), like ChatGPT, are powerful tools capable of generating remarkably human-like text—but they have a troubling habit of presenting false or fabricated information, a phenomenon known as ""hallucinations."" Imagine a medical diagnosis or legal advice given with absolute confidence but entirely incorrect; the risks are enormous. To combat this problem, we created FactTest, a robust statistical tool that checks whether the answers from language models are trustworthy or not. FactTest operates by statistically evaluating the correctness of a model’s responses and enables the model to ""politely refuse"" to answer when it's uncertain, thereby dramatically reducing false statements. Unlike previous solutions, FactTest offers clear-cut mathematical guarantees to keep mistakes under tight control. Our method boosts the accuracy of responses by more than 40% over typical models, adapts gracefully when encountering new or different kinds of questions, and even enhances the trustworthiness of commercial ""black-box"" models, whose internal workings are inaccessible."
Poster,Fair Clustering via Alignment,https://ICML.cc//virtual/2025/poster/44309,"Kunwoong Kim, Jihu Lee, Sangchul Park, Yongdai Kim","Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute.While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice.To resolve these limitations, we propose a new fair clustering algorithm based on a novel decomposition of the fair $K$-means clustering objective function.The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space.A key advantage of FCA is that it theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice.Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.","(1) Fair clustering aims to balance cluster proportions by a sensitive attribute, but inherent complexity or approximation of existing methods often yields suboptimal utility or numerical instability.(2) We propose Fair Clustering via Alignment (FCA), which optimizes (i) a joint probability distribution to align data from different protected groups and (ii) cluster centers in the aligned space.(3) FCA guarantees approximately optimal clustering utility for any given fairness level without complex constraints and outperforms existing methods in the trade‐off between fairness and utility."
Poster,FairICP: Encouraging Equalized Odds via Inverse Conditional Permutation,https://ICML.cc//virtual/2025/poster/45147,"Yuheng Lai, Leying Guan","*Equalized odds*, an important notion of algorithmic fairness, aims to ensure that sensitive variables, such as race and gender, do not unfairly influence the algorithm's prediction when conditioning on the true outcome. Despite rapid advancements, current research primarily focuses on equalized odds violations caused by a single sensitive attribute, leaving the challenge of simultaneously accounting for multiple attributes under-addressed.  We bridge this gap by introducing an in-processing fairness-aware learning approach, FairICP, which integrates adversarial learning with a novel inverse conditional permutation scheme. FairICP offers a flexible and efficient scheme to promote equalized odds under fairness conditions described by complex and multi-dimensional sensitive attributes. The efficacy and adaptability of our method are demonstrated through both simulation studies and empirical analyses of real-world datasets.","Machine learning tools are widely used to support decision-making in areas like healthcare, employment, and public services. However, these systems can sometimes work better for certain groups than others—such as people of a particular age or sex. Addressing this issue requires tools that promote balanced performance across multiple factors simultaneously. Our study introduces a method called FairICP, which helps machine learning models make predictions that are both accurate and more balanced across different groups of people. FairICP adjusts how models learn from data, preventing them from forming misleading and spurious associations between sensitive personal attributes and outcomes—unless such associations are supported across population groups in the data. We tested FairICP on both simulated and real-world datasets, including health and social information, and found that it reduced disparities without significantly lowering accuracy. By supporting fairness across multiple factors at once, FairICP brings us closer to building responsible AI tools that are more inclusive and trustworthy for everyone."
Poster,Fairness on Principal Stratum: A New Perspective on Counterfactual Fairness,https://ICML.cc//virtual/2025/poster/46308,"Haoxuan Li, Zeyu Tang, Zhichao Jiang, Zhuangyan Fang, Yue Liu, zhi geng, Kun Zhang","Fairness in human and algorithmic decision-making is crucial in areas such as criminal justice, education, and social welfare. Recently, counterfactual fairness has drawn increasing research interest, suggesting that decision-making for individuals should remain the same when intervening with different values on protected attributes. Nevertheless, the question of ""which attributes and individuals should be protected"" is rarely discussed in the existing counterfactual fairness literature. For example, when considering leg disability as a protected attribute, the algorithms should not treat individuals with leg disabilities differently in college admissions, but one may naturally consider this factor when selecting runner athletes. In other words, when and how to enforce fairness is expected to depend on the causal relation between the protected attribute and the outcome of interest. Formally, this paper proposes principal counterfactual fairness using the concept of principal stratification from the causal inference literature, focusing on whether an algorithm is counterfactually fair for individuals whose protected attribute has no individual causal effect on the outcome of interest. To examine whether an algorithm satisfies principal counterfactual fairness, we derive the statistical bounds and propose a post-processing approach to achieving principal counterfactual fairness with minimal individual decision changes. Experiments are conducted using synthetic and real-world datasets to verify the effectiveness of our methods.","This paper extends the widely-used counterfactual fairness that 'the decision-making should be same between factual and counterfactual for all individuals' to the new counterfactual fairness notion that 'if some factors didn't influence an individual's outcome, then it shouldn't influence the individual's decision-making'."
Poster,Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective,https://ICML.cc//virtual/2025/poster/43830,"Firas Laakom, Haobo Chen, Jürgen Schmidhuber, Yuheng Bu","Despite substantial progress in promoting fairness in high-stake applications using machine learning models, existing methods often modify the training process, such as through regularizers or other interventions, but lack formal guarantees that fairness achieved during training will generalize to unseen data. Although overfitting with respect to prediction performance has been extensively studied, overfitting in terms of fairness loss has received far less attention. This paper proposes a theoretical framework for analyzing fairness generalization error through an information-theoretic lens. Our novel bounding technique is based on Efron–Stein inequality, which allows us to derive tight information-theoretic fairness generalization bounds with both Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical results validate the tightness and practical relevance of these bounds across diverse fairness-aware learning algorithms.Our framework offers valuable insights to guide the design of algorithms improving fairness generalization.","As machine learning is increasingly used in high-stakes areas like hiring, healthcare, and lending, ensuring fairness is more important than ever. Many existing methods aim to make models fair during training—but here’s the catch: does fairness on training data guarantee fairness in the real world, on unseen data? Our paper shed some light on this question and finds that models can exhibit ""fairness overfitting,"" where fairness achieved during training does not carry over to new data. While generalization in terms of accuracy is well studied, how fairness overfitting remains poorly understood. We address this gap by introducing a new theoretical framework that uses tools from information theory to measure fairness overfitting. We develop a novel mathematical proof that leads to tighter and more insightful bounds predicting how fairness on training data will generalize to unseen data. We test these bounds across several scenarios and find they work well in practice. This work offers a deeper understanding and practical guidance for designing machine learning models that stay fair beyond the training data."
Poster,FairPFN: A Tabular Foundation Model for Causal Fairness,https://ICML.cc//virtual/2025/poster/45748,"Jake Robertson, Noah Hollmann, Samuel Gabriel Müller, Noor Awad, Frank Hutter","Machine learning (ML) systems are utilized in critical sectors such as healthcare, law enforcement, and finance, but often rely on historical data that contains demographic biases, leading to decisions that perpetuate or intensify existing inequalities. Causal and counterfactual fairness provide a transparent, human-in-the-loop framework to mitigate algorithmic discrimination, aligning closely with legal doctrines of direct and indirect discrimination. However, current causal fairness frameworks hold a key limitation in that they assume prior knowledge of the correct causal model, restricting their applicability in complex fairness scenarios where causal models are unknown or difficult to identify. To bridge this gap, we propose FairPFN, a tabular foundation model pre-trained on synthetic causal fairness data to identify and mitigate the causal effects of protected attributes in its predictions. FairPFN's key contribution is that it requires no knowledge of the causal model and demonstrates strong performance across a diverse set of hand-crafted and real-world causal scenarios relative to robust baseline methods. FairPFN paves the way for a promising direction for future research, making causal fairness more accessible to a wider variety of complex fairness problems.","When a machine learning algorithm used to make critical decisions is trained on historical data containing evidence of discrimination, these algorithms are well known to reproduce, and even worsen these ethnic, gender, or age-based biases in their predictions. For example, a hiring algorithm trained on past hiring decisions may learn to discriminate against potential candidates on account of their gender, ethnicity, or physical ability. The research field focused on detecting and mitigating these biases in machine learning algorithms is called algorithmic fairness. While many past attempts to mitigate algorithmic biases focused on ""outcome-based"" fairness, similar in essence to Affirmative Action or quota strategies, recent approaches focus on first understanding the source of discrimination, and then coming up with a fair alternative decision making process. One way of understanding the mechanisms of discrimination is through so-called causal graphs and models, which depict a network describing the cause and effect relationships between variables. While process-based fairness offers numerous advantages over its outcome-based alternative, including a strong analogy to US and EU anti-discrimination law, coming up with a plausable causal model for a real-world scenario is often very challenging. In order to be able to perform ""causal fairness"" without strict knowledge of these causal mechanisms, we leverage recent advancements in machine learning called prior-data-fitted networks (PFNs), which allow us to simultaneously identify the mechanisms of discrimination for a given situation and perform the appropriate adjustment to achieve a fair decision making process. Our method, which we call FairPFN, is able to do this from only observational data, and thus makes causal fairness increasingly accessible to machine learning practitioners."
Poster,Falcon: Fast Visuomotor Policies via Partial Denoising,https://ICML.cc//virtual/2025/poster/44509,"Haojun Chen, Minghao Liu, Chengdong Ma, Xiaojian Ma, Zailin Ma, Huimin Wu, Yuanpei Chen, Yifan Zhong, Mingzhi Wang, Qing Li, Yaodong Yang","Diffusion policies are widely adopted in complex visuomotor tasks for their ability to capture multimodal action distributions. However, the multiple sampling steps required for action generation significantly harm real-time inference efficiency, which limits their applicability in real-time decision-making scenarios. Existing acceleration techniques either require retraining or degrade performance under low sampling steps. Here we propose Falcon, which mitigates this speed-performance trade-off and achieves further acceleration. The core insight is that visuomotor tasks exhibit sequential dependencies between actions. Falcon leverages this by reusing partially denoised actions from historical information rather than sampling from Gaussian noise at each step. By integrating current observations, Falcon reduces sampling steps while preserving performance. Importantly, Falcon is a training-free algorithm that can be applied as a plug-in to further improve decision efficiency on top of existing acceleration techniques. We validated Falcon in 48 simulated environments and 2 real-world robot experiments. demonstrating a 2-7x speedup with negligible performance degradation, offering a promising direction for efficient visuomotor policy design.","Robots using advanced ""diffusion policies"" can perform complex tasks flexibly, but they're too slow for real-world use because they need many computational steps to make each decision. We developed Falcon, a new method that makes robots act faster by cleverly reusing information from their previous actions. Unlike existing solutions that sacrifice performance for speed, Falcon maintains the robot's ability to perform tasks successfully while making decisions 2-7 times faster. Our method works as a simple plug-in to existing robot systems without requiring additional training. We proved Falcon's effectiveness across 48 simulated tasks and 2 real-world robot experiments, showing it can help robots respond more quickly in practical applications."
Poster,False Coverage Proportion Control for Conformal Prediction,https://ICML.cc//virtual/2025/poster/45792,"Alexandre Blain, Thirion Bertrand, Pierre Neuvial","Split Conformal Prediction (SCP) provides a computationally efficient way to construct confidence intervals in prediction problems. Notably, most of the theory built around SCP is focused on the single test point setting. In real-life, inference sets consist of multiplepoints, which raises the question of coverage guarantees for many points simultaneously. While *on average*, the False Coverage Proportion (FCP) remains controlled, it can fluctuate strongly around its mean, the False Coverage Rate (FCR). We observe that when adataset is split multiple times, classical SCP may not control the FCP in a majority of the splits. We propose CoJER, a novel method that achieves sharp FCP control in probability for conformal prediction, based on a recent characterization of the distribution of conformal $p$-values in a transductive setting. This procedure incorporates an aggregation scheme which provides robustness with respect to modeling choices. We show through extensive real data experiments that CoJER provides FCP control while standard SCP does not. Furthermore, CoJER yields shorter intervals than the *state-of-the-art method* for FCP control and only slightly larger intervals than standard SCP.","Conformal prediction is a popular method for producing confidence intervals that work with any predictive model, offering a reliable measure of uncertainty. While the standard approach, Split Conformal Prediction (SCP), provides guarantees for individual predictions, it often fails to control errors when applied to many predictions at once. We propose CoJER, a new method that ensures accurate error control across multiple predictions by combining recent theoretical insights with a robust aggregation scheme. CoJER consistently achieves tighter and more reliable confidence intervals than existing methods, making it a practical tool for large-scale applications in areas like healthcare, finance, and scientific analysis."
