type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Investigating Non-Transitivity in LLM-as-a-Judge,https://ICML.cc//virtual/2025/poster/44669,"Yi Xu, Laura Ruis, Tim Rocktäschel, Robert Kirk","Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0\% $\rightarrow$ 96.4\% and 82.1\% $\rightarrow$ 86.3\% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency.","When evaluating AI chatbots that follow human instructions, researchers often rely on automatic comparisons made by powerful language models. This typically involves comparing two chatbots at a time, based on an implicit assumption: if Chatbot A is better than Chatbot B, and Chatbot B is better than Chatbot C, then Chatbot A should also be better than Chatbot C. However, we find that this assumption does not always hold, and such inconsistencies can significantly distort the overall rankings of AI chatbots. We examine this issue in a widely used evaluation framework called AlpacaEval and observe clear evidence of these ranking inconsistencies. To address the problem, we introduce an evaluation method inspired by round-robin tournaments, where each chatbot is compared against many others. The outcomes are then aggregated using a statistical model called Bradley-Terry to produce more consistent and accurate rankings. This approach significantly improves the reliability of AI chatbot evaluations. To reduce the high computational cost of round-robin comparisons, we also propose a more efficient matching strategy called Swim tournaments, which preserves the benefits of round-robin evaluation while requiring far fewer comparisons."
Poster,Investigating the Overlooked Hessian Structure: From CNNs to LLMs,https://ICML.cc//virtual/2025/poster/44080,"Qian-Yuan Tang, Yufei Gu, Yunfeng Cai, Mingming Sun, Ping Li, zhou Xun, Zeke Xie","It is well-known that the Hessian of deep loss landscape matters to optimization and generalization of deep learning. Previous studies reported a rough Hessian structure in deep learning, which consists of two components, a small number of large eigenvalues and a large number of nearly-zero eigenvalues. To the best of our knowledge, we are the first to report that a simple but overlooked power-law Hessian structure exists in well-trained deep neural networks, including Convolutional Neural Networks (CNNs) and Large Language Models (LLMs). Moreover, we provide a maximum-entropy theoretical interpretation for the power-law Hessian structure and theoretically demonstrate the existence of robust and low-dimensional subspace of deep neural networks. Our extensive experiments using the proposed power-law spectral method demonstrate that the power-law Hessian spectra critically relate to multiple important behaviors of deep learning, including optimization, generalization, and overparameterization. Notably, we discover that the power-law Hessian structure of a given LLM can effectively predict generalization during training, while conventional sharpness-based generalization measures that often works well on CNNs become nearly useless for as a generalization predictor of LLMs.","Deep loss landscape matters to optimization and generalization of deep learning; however, the Hessian structure is often overlooked in previous studies. We report that a simple power-law Hessian structure exists in well-trained neural networks, including Convolutional Neural Networks (CNNs) and Large Language Models (LLMs). A maximum-entropy theoretical interpretation is provided and theoretically demonstrates the existence of a robust and low-dimensional subspace of deep neural networks. We further provide extensive empirical results under different experiment setups that demonstrate this power-law Hessian spectra critically relate to multiple important behaviors of deep learning, including optimization, generalization, and overparameterization. Our findings indicate that this power-law structure in the Hessian spectrum offers a novel and promising perspective for understanding neural networks and their behavior."
Poster,IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models,https://ICML.cc//virtual/2025/poster/44764,"Hanting Wang, Tao Jin, Wang Lin, Shulei Wang, Hai Huang, Shengpeng Ji, Zhou Zhao","Bridge models in image restoration construct a diffusion process from degraded to clear images. However, existing methods typically require training a bridge model from scratch for each specific type of degradation, resulting in high computational costs and limited performance. This work aims to efficiently leverage pretrained generative priors within existing image restoration bridges to eliminate this requirement. The main challenge is that standard generative models are typically designed for a diffusion process that starts from pure noise, while restoration tasks begin with a low-quality image, resulting in a mismatch in the state distributions between the two processes. To address this challenge, we propose a transition equation that bridges two diffusion processes with the same endpoint distribution. Based on this, we introduce the **IRBridge** framework, which enables the direct utilization of generative models within image restoration bridges, offering a more flexible and adaptable approach to image restoration. Extensive experiments on six image restoration tasks demonstrate that IRBridge efficiently integrates generative priors, resulting in improved robustness and generalization performance. Code will be available at GitHub.","Existing image restoration bridge models often require training a dedicated model from scratch for each specific restoration task, which is typically costly and time-consuming. In this work, we explore whether pretrained generative models can be leveraged to address this limitation, as they also aim to produce high-quality images as outputs.The main challenge lies in the fact that bridge models define a diffusion process that differs from that of standard generative models, resulting in mismatched state distributions along the process. To overcome this, we propose a transition equation that enables state transformation between two diffusion processes sharing the same endpoint distribution. Building on this, we introduce a new framework, IRBridge, which effectively incorporates generative priors by reusing powerful pretrained image generative models, reducing training costs while enhancing restoration performance."
Poster,"Is Best-of-N the Best of Them? Coverage, Scaling, and Optimality in Inference-Time Alignment",https://ICML.cc//virtual/2025/poster/45322,"Audrey Huang, Adam Block, Qinghua Liu, Nan Jiang, Akshay Krishnamurthy, Dylan Foster","Recent work on inference-time alignment has established benefits of increasing inference-time computation in language models, but naively scaling compute through techniques like Best-of-N sampling can cause performance to degrade due to reward hacking. Toward a theoretical understanding of how to best leverage additional computation, we formalize inference-time alignment as improving a pre-trained policy’s responses for a prompt of interest, given access to an imperfect reward model. We analyze the performance of inference-time alignment algorithms in terms of (i) response quality, and (ii) compute, and provide new results that highlight the importance of the pre-trained policy’s coverage over high-quality responses for performance and compute scaling: (1) We show that Best-of-N alignment with an ideal N can achieve optimal performance under stringent notions of coverage, but provably suffers from reward hacking when N is large, and fails to achieve tight guarantees under more realistic coverage conditions; (2) We introduce InferenceTimePessimism, a new algorithm which mitigates reward hacking through deliberate use of inference-time compute, implementing pessimism in the face of uncertainty; we prove that its performance is optimal and scaling-monotonic, i.e., does ot degrade as N increases. We complement our theoretical results with experiments that demonstrate the practicality of our algorithm across a variety of tasks and models.","Inference-time methods for language models alter and improve the model’s outputs at generation time, and have experienced great empirical success. One popular approach, called Best‐of‐N sampling, generates N candidates and returns the one with the highest score under a learned model. However, as N grows, reward model errors accumulate and output quality worsens, meaning that Best-of-N is unable to access the full improvement in response quality available at inference time. To solve this problem, we first built a clean mathematical framework for inference‐time alignment, and analyzed exactly how and why Best‐of‐N sampling breaks down. Then, we designed a new algorithm, InferenceTimePessimism, that deliberately penalizes highly-rewarded responses with high uncertainty, and utilizes essentially the same amount of computation as Best-of-N. Crucially, our method separates how much computation we spend from how strongly we penalize uncertainty, so adding more compute never hurts quality. We proved that this approach achieves the best possible trade‐off between reward‐model error and computational cost and showed in experiments that it reliably improves accuracy without the performance dips seen in standard Best‐of‐N sampling. In this way, our research offers both a theoretical roadmap and a practical tool for making large language models more robust and reliable at inference time."
Poster,Is Complex Query Answering Really Complex?,https://ICML.cc//virtual/2025/poster/45905,"Cosimo Gregucci, Bo Xiong, Daniel Hernández, Lorenzo Loconte, Pasquale Minervini, Steffen Staab, Antonio Vergari","Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum as a challenging reasoning task.In this paper, we show that the current benchmarks for CQA might not be as *complex* as we think, as the way they are built distorts our perception of progress in this field.For example, we find that in these benchmarks most queries (up to 98% for some query types) can be reduced to simpler problems, e.g., link prediction, where only one link needs to be predicted.The performance of state-of-the-art CQA models decreses significantly when such models are evaluated on queries that cannot be reduced to easier types.Thus, we propose a set of more challenging benchmarks composed of queries that *require* models to reason over multiple hops and better reflect the construction of real-world KGs.In a systematic empirical investigation, the new benchmarks show that current methods leave much to be desired from current CQA methods.","Knowledge graphs are collections of facts that can be queried to find specific information. However, when the knowledge graph is incomplete, retrieving all answers to a query becomes challenging—a task known as *complex query answering* (CQA). While benchmarks exist to evaluate CQA, we find that most answers to the queries can be found by predicting just a single missing link in the graph, making them less hard than intended.  To fix this, we introduce new, more challenging benchmarks containing queries that require multi-hop reasoning and offer a more balanced level of hardness.  Our experiments show that all existing models perform significantly worse on the new benchmarks, and no single method stands out as clearly superior. This highlights that CQA remains an open challenge and calls for the development of more sophisticated approaches."
Poster,Is Noise Conditioning Necessary for Denoising Generative Models?,https://ICML.cc//virtual/2025/poster/44004,"Qiao Sun, Zhicheng Jiang, Hanhong Zhao, Kaiming He","It is widely believed that noise conditioning is indispensable for denoising diffusion models to work successfully. This work challenges this belief. Motivated by research on blind image denoising, we investigate a variety of denoising-based generative models in the absence of noise conditioning. To our surprise, most models exhibit graceful degradation, and in some cases, they even perform better without noise conditioning. We provide a mathematical analysis of the error introduced by removing noise conditioning and demonstrate that our analysis aligns with empirical observations. We further introduce a noise-*unconditional* model that achieves a competitive FID of 2.23 on CIFAR-10, significantly narrowing the gap to leading noise-conditional models. We hope our findings will inspire the community to revisit the foundations and formulations of denoising generative models.","Denoising diffusion models are powerful tools that can generate realistic images from random noise. Traditionally, these models rely heavily on a technique called noise conditioning, which means they are told how much noise is in the image at each step of the generation process. Until now, it was widely assumed that this information is essential for the model to work well.In this study, the researchers challenge that assumption. Inspired by methods in blind image denoising—where the model doesn’t know how noisy an image is—they explore what happens when diffusion models are trained without any noise information. Surprisingly, many of these models still work quite well, and some even improve.The authors also provide a mathematical explanation for why removing noise conditioning doesn’t hurt performance as much as expected. To support their findings, they introduce a new model that doesn’t use noise conditioning at all, yet achieves strong performance on a standard image generation benchmark (CIFAR-10), coming close to the best existing models.This work opens up new possibilities for building simpler and potentially more robust image generation models by rethinking how noise is handled."
Poster,Isolated Causal Effects of Natural Language,https://ICML.cc//virtual/2025/poster/44884,"Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael","As language technologies become widespread, it is important to understand how changes in language affect reader perceptions and behaviors. These relationships may be formalized as the *isolated causal effect* of some *focal* language-encoded intervention (e.g., factual inaccuracies) on an external outcome (e.g., readers' beliefs). In this paper, we introduce a formal estimation framework for isolated causal effects of language. We show that a core challenge of estimating isolated effects is the need to approximate all *non-focal* language outside of the intervention. Drawing on the principle of *omitted variable bias*, we provide measures for evaluating the quality of both non-focal language approximations and isolated effect estimates themselves. We find that poor approximation of non-focal language can lead to bias in the corresponding isolated effect estimates due to omission of relevant variables, and we show how to assess the sensitivity of effect estimates to such bias along the two key axes of *fidelity* and *overlap*. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to correctly recover isolated effects and demonstrate the utility of our proposed measures.","Science often seeks to understand whether one variable (an intervention) causes another (an outcome), like whether a new medication actually improves health or whether a certain type of conversation affects mood. This process is called causal inference, which helps separate real cause-and-effect relationships from coincidences.Scientists have developed many methods to study cause and effect in structured data settings like numbers in a table. However, the growing availability of language data—such as experiences that people share online or in text conversations—opens new opportunities for discovery. For example, if thousands of people mention feeling better after taking a specific medicine, we might wonder whether the medicine is truly responsible or whether other factors are at play.A major challenge with language is that words naturally convey multiple ideas. Someone discussing blood pressure medication may also mention changes to diet and exercise, making it hard to pinpoint which of the three is actually causing the improvement. We address this issue with a new causal inference method that is able to fully isolate the effect of an intervention expressed in language, ensuring conclusions are scientifically sound."
Poster,Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs,https://ICML.cc//virtual/2025/poster/44735,"Yinong O Wang, Nivedha Sivakumar, Falaah Arif Khan, Katherine Metcalf, Adam Golinski, Natalie Mackraz, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff","The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerf, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable benchmark for modern LLMs. Combining our metric and dataset, we provide insightful comparisons of eight open-source LLMs. For example, Mistral-8B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, this work provides a holistic framework for LLM evaluation by jointly assessing fairness and uncertainty, enabling the development of more transparent and accountable AI systems.","As large language models (LLMs) such as ChatGPT become more broadly deployed and increasingly impact many aspects of the society, any fairness issue of LLMs, e.g., gender bias, becomes an urgent problem which leads to profound harms at large scale. Existing fairness evaluation methods are based on the output results from LLMs. While this is indeed helpful, the fairness evaluation will be more accurate and informative if we also incorporate LLMs' confidence about their output, which is what the proposed UCerF metric captures. In addition, we also offer a new dataset SynthBias that is larger in scale and more suitable for recent LLMs than the precedent dataset WinoBias.UCerF integrates the confidence of LLMs (in another word, estimated model uncertainty) into fairness evaluation by introducing a continuous scale of model behavior preference. From left to right, this scale captures confidently incorrect behavior, unconfident incorrect behavior, unconfident correct behavior, and confidently correct behavior. A model's fairness is captured by its behavior difference on this scale when facing stereotypical and anti-stereotypical scenarios.With UCerF and SynthBias, we reveals that model confidence (uncertainty) actually impacts fairness evaluation significantly, i.e., some seemingly fair model is in fact not as fair because it is very confident when giving biased output. With the new metric and dataset, we offer a better way to ensure LLMs is fair before deploying and causing harm."
Poster,IT$^3$: Idempotent Test-Time Training,https://ICML.cc//virtual/2025/poster/45551,"Nikita Durasov, Assaf Shocher, Doruk Oner, Gal Chechik, Alexei Efros, EPFL Pascal Fua","Deep learning models often struggle when deployed in real-world settings due to distribution shifts between training and test data. While existing approaches like domain adaptation and test-time training (TTT) offer partial solutions, they typically require additional data or domain-specific auxiliary tasks. We present Idempotent Test-Time Training (IT3), a novel approach that enables on-the-fly adaptation to distribution shifts using only the current test instance, without any auxiliary task design. Our key insight is that enforcing idempotence---where repeated applications of a function yield the same result---can effectively replace domain-specific auxiliary tasks used in previous TTT methods. We theoretically connect idempotence to prediction confidence and demonstrate that minimizing the distance between successive applications of our model during inference leads to improved out-of-distribution performance. Extensive experiments across diverse domains (including image classification, aerodynamics prediction, and aerial segmentation) and architectures (MLPs, CNNs, GNNs) show that IT3 consistently outperforms existing approaches while being simpler and more widely applicable. Our results suggest that idempotence provides a universal principle for test-time adaptation that generalizes across domains and architectures.","Modern AI systems often fail when faced with unexpected changes in the data they are given — for example, when a camera sees an image in poor lighting or a sensor collects slightly different data than what the system was trained on. Traditionally, researchers have tried to fix this by designing special training tricks or feeding the system more examples. But these solutions don’t always work and often require extra data or task-specific adjustments.Our work introduces a simple and universal alternative called Idempotent Test-Time Training (IT³). Instead of crafting custom solutions, we teach the model a general principle: its predictions should stay the same if we run them through the model again. If they change too much, that’s a sign the input is unusual or corrupted. So during inference, we slightly update the model on-the-fly to make its predictions consistent — without needing new data or prior knowledge.This approach works well across very different tasks — from classifying images to predicting aerodynamic properties and mapping roads in aerial images — and helps models stay reliable even in unfamiliar conditions."
Poster,ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks,https://ICML.cc//virtual/2025/poster/44303,"Saurabh Jha, Rohan Arora, Yuji Watanabe, Takumi Yanagawa, Yinfang Chen, Jackson Clark, Bhavya Bhavya, Mudit Verma, Harshit Kumar, Hirokuni Kitahara, Noah Zheutlin, Saki Takano, Divya Pathak, Felix George, Xinbo Wu, Bekir Turkkan, Gerard Vanloo, Michael Nidd, Ting Dai, Oishik Chatterjee, Pranjal Gupta, Suranjana Samanta, Pooja Aggarwal, Rong Lee, Jae-wook Ahn, Debanjana Kar, Amit Paradkar, Yu Deng, Pratibha Moogi, Prateeti Mohapatra, Naoki Abe, Chandrasekhar Narayanaswami, Tianyin Xu, Lav Varshney, Ruchi Mahindru, Anca Sailer, Laura Shwartz, Daby Sow, Nicholas Fuller, Ruchir Puri","Realizing the vision of using AI agents to automate critical IT tasks depends on the ability to measure and understand effectiveness of proposed solutions. We introduce ITBench, a framework that offers a systematic methodology for benchmarking AI agents to address real-world IT automation tasks. Our initial release targets three key areas: Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The design enables AI researchers to understand the challenges and opportunities of AI agents for IT automation with push-button workflows and interpretable metrics. IT-Bench includes an initial set of 102 real-world scenarios, which can be easily extended by community contributions. Our results show that agents powered by state-of-the-art models resolve only 11.4% of SRE scenarios, 25.2% of CISO scenarios, and 25.8% of FinOps scenarios (excluding anomaly detection). For FinOps-specific anomaly detection (AD) scenarios, AI agents achieve an F1 score of 0.35. We expect ITBench to be a key enabler of AI-driven IT automation that is correct, safe, and fast. IT-Bench, along with a leaderboard and sample agent implementations, is available at https://github.com/ibm/itbench.","Imagine using smart computer programs, called AI agents, to automatically handle complex IT tasks like keeping IT systems running smoothly (Site Reliability Engineering - SRE), ensuring security and compliance (Compliance and Security Operations - CISO), or managing technology spending (Financial Operations - FinOps). For this to become a reality, we need a reliable way to check if these AI agents are actually good at these jobs. Our work (IT-Bench) solves this challenge by providing the first comprehensive testing framework for IT automation agents. Think of it as a standardized assessment that evaluates AI performance across 102 real-world IT scenarios. This benchmark enables researchers to objectively compare different AI systems and measure their capabilities with precision. The initial tests using IT-Bench revealed that even the most advanced AI agents still have a long way to go. They were only able to successfully solve about 11.4% of the site reliability problems, about 25.2% of the security problems, and 25.8% of the financial management problems. By providing this testing framework, IT-Bench aims to help researchers and developers build better AI agents that can correctly, safely, and quickly automate IT tasks, ultimately making technology more reliable and efficient."
