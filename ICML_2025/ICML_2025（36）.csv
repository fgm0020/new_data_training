type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation,https://ICML.cc//virtual/2025/poster/45308,"Zixuan Hu, Yichun Hu, Xiaotong Li, SHIXIANG TANG, LINGYU DUAN","Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to unseen domains under extreme data scarcity and multiple shifts. Previous approaches mainly focused on sample selection strategies, while overlooking the fundamental problem on underlying optimization. Initially, we critically analyze the widely-adopted entropy minimization framework in WTTA and uncover its significant limitations in noisy optimization dynamics that substantially hinder adaptation efficiency. Through our analysis, we identify region confidence as a superior alternative to traditional entropy, however, its direct optimization remains computationally prohibitive for real-time applications. In this paper, we introduce a novel region-integrated method **ReCAP** that bypasses the lengthy process. Specifically, we propose a probabilistic region modeling scheme that flexibly captures semantic changes in embedding space. Subsequently, we develop a finite-to-infinite asymptotic approximation that transforms the intractable region confidence into a tractable and upper-bounded proxy. These innovations significantly unlock the overlooked potential dynamics in local region in a concise solution. Our extensive experiments demonstrate the consistent superiority of ReCAP over existing methods across various datasets and wild scenarios. The source code will be available at https://github.com/hzcar/ReCAP.","When we train AI models to understand visual inputs, they often struggle when shown images or videos that look very different from what they saw during training — especially if only a few new examples are available. This is a major challenge in the real world, where lighting, backgrounds, or styles can change drastically.Our research focuses on making these models adapt quickly and reliably in such unpredictable situations — a setting we call Wild Test-Time Adaptation (WTTA). While past methods mostly tried to pick better samples to learn from, we looked deeper into how the model learns and found that a commonly used strategy doesn’t work well when the data is noisy or inconsistent.To address this, we propose a new method called ReCAP, which allows the model to grasp local visual patterns more effectively and adjust its internal understanding in real time. It’s faster, more robust, and more accurate than existing techniques. We’ve made the code publicly available to support further research."
Poster,Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence,https://ICML.cc//virtual/2025/poster/44940,"Gouki Minegishi, Hiroki Furuta, Shohei Taniguchi, Yusuke Iwasawa, Yutaka Matsuo","Transformer-based language models exhibit In-Context Learning (ICL), where predictions are made adaptively based on context. While prior work links induction heads to ICL through a sudden jump in accuracy, this can only account for ICL when the answer is included within the context.However, an important property of practical ICL in large language models is the ability to meta-learn how to solve tasks from context, rather than just copying answers from context; how such an ability is obtained during training is largely unexplored.In this paper, we experimentally clarify how such meta-learning ability is acquired by analyzing the dynamics of the model's circuit during training. Specifically, we extend the copy task from previous research into an In-Context Meta Learning setting, where models must infer a task from examples to answer queries.Interestingly, in this setting, we find that there are multiple phases in the process of acquiring such abilities, and that a unique circuit emerges in each phase, contrasting with the single-phases change in induction heads. The emergence of such circuits can be related to several phenomena known in large language models, and our analysis lead to a deeper understanding of the source of the transformer's ICL ability.","Transformer-based language models, such as ChatGPT, have an impressive ability called In-Context Learning (ICL), where they adaptively answer questions based on examples provided in the context. Prior research explained this behavior mainly when models directly copy answers given explicitly in the context. However, real-world scenarios often require these models to learn how to solve new tasks just by seeing a few examples, without directly copying the answers.In this study, we explored how this more general and practical form of learning emerges during the training of these models. Through careful experiments, we discovered that the learning process happens in multiple stages. At each stage, the model forms specialized internal structures or ""circuits"" that help it progressively better understand and solve new tasks.Our findings suggest that the ability of transformers to quickly adapt to unseen tasks stems from this multi-stage development of internal circuits. Understanding these phases provides valuable insight into why modern language models like ChatGPT perform so well on diverse tasks, moving us closer to fully unlocking their potential and enhancing their reliability in everyday applications."
Poster,Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds for Score-Based Generative Models in W2-distance,https://ICML.cc//virtual/2025/poster/44471,"Marta Gentiloni Silveri, Antonio Ocello","Score-based Generative Models (SGMs) aim to sample from a target distribution by learning score functions using samples perturbed by Gaussian noise. Existing convergence bounds for SGMs in the $\mathcal{W}_2$-distance rely on stringent assumptions about the data distribution. In this work, we present a novel framework for analyzing $\mathcal{W}_2$-convergence in SGMs, significantly relaxing traditional assumptions such as log-concavity and score regularity. Leveraging the regularization properties of the Ornstein-Uhlenbeck (OU) process, we show that weak log-concavity of the data distribution evolves into log-concavity over time. This transition is rigorously quantified through a PDE-based analysis of the Hamilton-Jacobi-Bellman equation governing the log-density of the forward process. Moreover, we establish that the drift of the time-reversed OU process alternates between contractive and non-contractive regimes, reflecting the dynamics of concavity.Our approach circumvents the need for stringent regularity conditions on the score function and its estimators, relying instead on milder, more practical assumptions. We demonstrate the wide applicability of this framework through explicit computations on Gaussian mixture models, illustrating its versatility and potential for broader classes of data distributions.","This work improves how machines learn to generate realistic data—such as images or simulations—by studying a class of models known as score-based generative models. Previous approaches relied on strict assumptions about the data that often don’t hold in real-world scenarios. We show that these assumptions can be relaxed by leveraging the inherent properties of the algorithm, which naturally makes complex data easier to handle over time. This results in more flexible and applicable theoretical guarantees for these generative models."
Poster,Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning,https://ICML.cc//virtual/2025/poster/44905,"Le-Trung Nguyen, Aël Quélennec, Van-Tam Nguyen, Enzo Tartaglione","On-device learning has emerged as a promising direction for AI development, particularly because of its potential to reduce latency issues and mitigate privacy risks associated with device-server communication, while improving energy efficiency. Despite these advantages, significant memory and computational constraints still represent major challenges for its deployment. Drawing on previous studies on low-rank decomposition methods that address activation memory bottlenecks in backpropagation, we propose a novel shortcut approach as an alternative. Our analysis and experiments demonstrate that our method can reduce activation memory usage, even up to $120.09\times$ compared to vanilla training, while also reducing overall training FLOPs up to $1.86\times$ when evaluated on traditional benchmarks.","The smart devices we use every day like phones, watches, and home assistants, etc are becoming more intelligent thanks to artificial intelligence (AI). However, in many cases, the AI is not actually running on the device itself. Instead, the device sends data to a server where the AI processes it and sends back a response. This setup can pose privacy risks if personal data is exposed during transmission. One way to fix this is by deploying the AI directly on the device. But teaching AI models on devices is hard because of limited memory and processing power.In our work, we build on ideas from earlier research to create a new method that reduces the size of data the AI needs to handle during learning. This helps cut down on memory use and computational cost, making it more practical to train AI directly on devices without compromising too much on performance."
Poster,Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation,https://ICML.cc//virtual/2025/poster/43502,"Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You","Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that *sparse coding* offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose **Contrastive Sparse Representation** (**CSR**), a method that specifies pre-trained embeddings into a high-dimensional but  *selectively activated* feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed—often by large margins—while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at [this URL.](https://github.com/neilwen987/CSR_Adaptive_Rep)","Modern AI systems rely on ""embeddings"" - digital representations that capture the meaning of images, text, or other data. These embeddings need to work efficiently across devices with different computing capabilities, from powerful servers to mobile phones.Our research introduces **Contrastive Sparse Representation** (**CSR**), a new technique that makes embeddings more adaptable without sacrificing quality. Unlike previous approaches that require complete retraining of AI models, CSR works with existing pre-trained embeddings and transforms them into a format where only the most important features are activated.Think of CSR like compressing a high-resolution photo: you can choose different compression levels depending on your needs, with each level preserving the most important visual information. Similarly, CSR allows AI systems to adjust embedding sizes based on available resources while maintaining accuracy.Our experiments with images, text, and combined data show that CSR outperforms previous methods in both accuracy and speed. It also requires significantly less training time, making it practical for real-world applications where both performance and efficiency matter."
Poster,Beyond Message Passing: Neural Graph Pattern Machine,https://ICML.cc//virtual/2025/poster/44748,"Zehong Wang, Zheyuan Zhang, Tianyi MA, Nitesh Chawla, Chuxu Zhang, Yanfang Ye","Graph learning tasks often hinge on identifying key substructure patterns---such as triadic closures in social networks or benzene rings in molecular graphs---that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, $k$-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. In this paper, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. Empirical evaluations across four standard tasks---node classification, link prediction, graph classification, and graph regression---demonstrate that GPM outperforms state-of-the-art baselines. Further analysis reveals that GPM exhibits strong out-of-distribution generalization, desirable scalability, and enhanced interpretability. Code and datasets are available at: https://github.com/Zehong-Wang/GPM.","We introduce a novel approach to graph representation learning that avoids the limitations of traditional message passing in graph neural networks (GNNs). Instead of iteratively aggregating information from neighboring nodes, GPM directly learns from meaningful substructures---like triangles, cliques, and cycles---that often determine key properties in graphs, such as molecular rings or social triads. The model samples these patterns using random walks, encodes them with sequential models, and identifies the most relevant ones using a transformer-based attention mechanism. This design enables GPM to capture both local and long-range dependencies more effectively than standard GNNs. Extensive experiments across node, link, and graph-level tasks show that GPM consistently outperforms state-of-the-art baselines in accuracy, robustness to distribution shifts, and scalability to large graphs. Furthermore, GPM provides enhanced interpretability by highlighting the dominant patterns driving its predictions. While the current method relies on random sampling, which may introduce inefficiencies, the framework opens the door to more expressive and pattern-aware graph learning, with potential extensions to unsupervised learning, integration with large language models, and applications in complex domains such as drug discovery and social systems."
Poster,Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity,https://ICML.cc//virtual/2025/poster/46363,"Quan Nguyen, Nishant Mehta, Cristóbal Guzmán","The minimax sample complexity of group distributionally robust optimization (GDRO) has been determined up to a $\log(K)$ factor, where $K$ is the number of groups. In this work, we venture beyond the minimax perspective via a novel notion of sparsity that we call $(\lambda, \beta)$-sparsity. In short, this condition means that at any parameter $\theta$, there is a set of at most $\beta$ groups whose risks at $\theta$ are all at least $\lambda$ larger than the risks of the other groups. To find an $\epsilon$-optimal $\theta$, we show via a novel algorithm and analysis that the $\epsilon$-dependent term in the sample complexity can swap a linear dependence on $K$ for a linear dependence on the potentially much smaller $\beta$. This improvement leverages recent progress in sleeping bandits, showing a fundamental connection between the two-player zero-sum game optimization framework for GDRO and per-action regret bounds in sleeping bandits. We next show an adaptive algorithm which, up to logarithmic factors, obtains a sample complexity bound that adapts to the best $(\lambda, \beta)$-sparsity condition that holds. We also show how to obtain a dimension-free semi-adaptive sample complexity bound with a computationally efficient method.Finally, we demonstrate the practicality of the $(\lambda, \beta)$-sparsity condition and the improved sample efficiency of our algorithms on both synthetic and real-life datasets.","If someone who already has a motorbike driving license wants to get a car driving license, they would not spend too much time on learning the common knowledge such as the signs of the road or where to get shelter under bad weather. Instead, they would focus on the specific car-driving skills that make driving a car more challenging than driving a motorbike.Similarly, for a machine that has to learn to perform well on different tasks, it is much more resource-efficient to collect and learn from the data of the most difficult tasks so that the machine can do well not only on easy tasks but also on challenging tasks. Our work presents a number of novel machine learning algorithms that efficiently discover the most difficult tasks, accurately estimate how much the tasks differ from each other, and strategically choose the data from the most difficult tasks to learn from.Through rigorous mathematical arguments, we show that these new algorithms are guaranteed to be resource-efficient and have well-balanced performances when learning a multitude of different tasks."
Poster,Beyond One-Hot Labels: Semantic Mixing for Model Calibration,https://ICML.cc//virtual/2025/poster/43489,"Haoyang Luo, Linwei Tao, Minjing Dong, Chang Xu","Model calibration seeks to ensure that models produce confidence scores that accurately reflect the true likelihood of their predictions being correct. However, existing calibration approaches are fundamentally tied to datasets of one-hot labels implicitly assuming full certainty in all the annotations. Such datasets are effective for classification but provides insufficient knowledge of uncertainty for model calibration, necessitating the curation of datasets with numerically rich ground-truth confidence values. However, due to the scarcity of uncertain visual examples, such samples are not easily available as real datasets. In this paper, we introduce calibration-aware data augmentation to create synthetic datasets of diverse samples and their ground-truth uncertainty. Specifically, we present **Calibration-aware Semantic Mixing (CSM)**, a novel framework that generates training samples with mixed class characteristics and annotates them with distinct confidence scores via diffusion models. Based on this framework, we propose calibrated reannotation to tackle the misalignment between the annotated confidence score and the mixing ratio during the diffusion reverse process. Besides, we explore the loss functions that better fit the new data representation paradigm. Experimental results demonstrate that CSM achieves superior calibration compared to the state-of-the-art calibration approaches. Our code is [available here](https://github.com/E-Galois/CSM).","Machine-learning models often give “confidence scores” alongside their predictions, but current methods for teaching models to be well-calibrated assume every training label is 100% certain, which isn’t realistic, especially when some images are genuinely ambiguous. We introduce Calibration-aware Semantic Mixing (CSM), a way to synthetically blend visual samples using diffusion models so that each mixed image comes with a precisely known “ground-truth” confidence value. We also develop a “calibrated reannotation” step to correct those scores after generation and adapt our balanced training loss to this richer form of data.By training on these varied, uncertainty-aware examples, models become significantly better at matching their reported confidence to actual accuracy, leading to more trustworthy predictions in real-world applications."
Poster,Beyond Self-Interest: How Group Strategies Reshape Content Creation in Recommendation Platforms?,https://ICML.cc//virtual/2025/poster/43971,"Yaolong Yu, Fan Yao, Sinno Jialin Pan","We employ a game-theoretic framework to study the impact of a specific strategic behavior among creators---group behavior---on recommendation platforms. In this setting, creators within a group collaborate to maximize their collective utility.We show that group behavior has a limited effect on the game's equilibrium when the group size is small. However, when the group size is large, group behavior can significantly alter content distribution and user welfare.Specifically, in a top-$K$ recommendation system with exposure-based rewards, we demonstrate that user welfare can suffer a significant loss due to group strategies, and user welfare does not necessarily increase with larger values of $K$ or more random matching, contrasting sharply with the individual creator case. Furthermore, we investigate user welfare guarantees through the lens of the Price of Anarchy (PoA). In the general case, we establish a negative result on the bound of PoA with exposure rewards, proving that it can be arbitrarily large. We then investigate a user engagement rewarding mechanism, which mitigates the issues caused by large group behavior, showing that $\text{PoA}\leq K+1$ in the general case and $\text{PoA}\leq 2$ in the binary case. Empirical results from simulations further support the effectiveness of the user engagement rewarding mechanism.","Online content platforms like Instagram, YouTube, and TikTok use recommendation systems to match users with content. While much research has focused on individual creators' strategies, many creators now work in groups to maximize collective benefits. This shift can significantly change how content is distributed, but its impact on user satisfaction and fairness is not well understood. We use a game-theoretic framework to study the effects of group behavior on recommendation systems. Our findings show that small groups have limited impact, but large groups can significantly alter content distribution, often reducing user satisfaction. We also investigate how platform parameters in a top-$K$ recommendation system influence these outcomes. We demonstrate that a rewarding mechanism based on user engagement can mitigate the negative effects of group behavior. Our work provides insights for online platforms on how to adjust recommendation systems to reduce user welfare loss caused by strategic group actions."
Poster,Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs,https://ICML.cc//virtual/2025/poster/46659,"Jie Hu, Yi-Ting Ma, Do-Young Eun","We propose a *history-driven target (HDT)* framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution $\\boldsymbol{\\mu}$. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target $\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs.","Efficiently exploring and sampling data from complex networks using random walks presents ongoing challenges. Methods like the Self-Repellent Random Walk (SRRW) aim to prevent over-exploration of areas but often come with significant computational costs.This research introduces the History-Driven Target (HDT) framework, a novel approach enhancing sampling efficiency while reducing computational demands. Instead of modifying the walker's movement rules, HDT dynamically adjusts the target distribution for any compatible MCMC sampler based on sample history, effectively guiding exploration towards under-sampled regions. This computationally lightweight design uses only local information, maintaining the same cost as the base sampler. HDT offers broad compatibility with advanced algorithms, including both reversible and non-reversible MCMC methods, and incorporates a practical ""Least Recently Used"" (LRU) caching strategy for effective use in memory-constrained scenarios with very large graphs."
