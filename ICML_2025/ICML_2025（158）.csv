type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Latent Diffusion Planning for Imitation Learning,https://ICML.cc//virtual/2025/poster/43658,"Amber Xie, Oleh Rybkin, Dorsa Sadigh, Chelsea Finn","Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations.To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data.On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data.","Imitation learning has become a recipe for success in learning robot models that can accomplish complex vision-based tasks. However, imitation methods require large datasets, particularly high-quality ones collected by experts. In order to address this limitation, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a high-level planner, that can leverage action-free demonstrations, and an inverse dynamics model, that can leverage suboptimal data, that both operate over a learned latent space. First, we learn a latent space through a model that compresses high-dimensional images into lower dimensional vectors, enabling effective computation. Then, we train the high-level and low-level model over this latent space using diffusion objectives. The planner predicts desired future states, and the low-level inverse dynamics models takes pairs of states and predicts actions. This allows LDP to leverage suboptimal and action-free data, and outperform existing methods that cannot use such data."
Poster,Latent Imputation before Prediction: A New Computational Paradigm for De Novo Peptide Sequencing,https://ICML.cc//virtual/2025/poster/43921,"Ye DU, Chen Yang, Nanxi Yu, Wanyu LIN, Qian Zhao, Shujun Wang","*De novo* peptide sequencing is a fundamental computational technique for ascertaining amino acid sequences of peptides directly from tandem mass spectrometry data, eliminating the need for reference databases. Cutting-edge models encode the observed mass spectra into latent representations from which peptides are predicted auto-regressively. However, the issue of missing fragmentation, attributable to factors such as suboptimal fragmentation efficiency and instrumental constraints, presents a formidable challenge in practical applications. To tackle this obstacle, we propose a novel computational paradigm called $\underline{\textbf{L}}$atent $\underline{\textbf{I}}$mputation before $\underline{\textbf{P}}$rediction (LIPNovo). LIPNovo is devised to compensate for missing fragmentation information within observed spectra before executing the final peptide prediction. Rather than generating raw missing data, LIPNovo performs imputation in the latent space, guided by the theoretical peak profile of the target peptide sequence. The imputation process is conceptualized as a set-prediction problem, utilizing a set of learnable peak queries to reason about the relationships among observed peaks and directly generate the latent representations of theoretical peaks through optimal bipartite matching. In this way, LIPNovo manages to supplement missing information during inference and thus boosts performance. Despite its simplicity, experiments on three benchmark datasets demonstrate that LIPNovo outperforms state-of-the-art methods by large margins. Code is available at https://github.com/usr922/LIPNovo.","Peptides, the building blocks of proteins, are crucial for understanding biological processes and developing new therapies. De novo peptide sequencing is a computational technique that determines peptide sequences directly from mass spectrometry data, without relying on existing databases. However, missing data in spectra—caused by suboptimal experimental conditions—makes sequencing challenging. To address this, we developed LIPNovo, a novel method that compensates for missing information in spectral data before predicting peptide sequences. Instead of trying to recreate missing raw data, LIPNovo uses advanced machine learning techniques to fill in the gaps within the model's inherent representations, guided by theoretical knowledge of peptides. This approach improves the quality and reliability of peptide predictions. Our experiments show that LIPNovo significantly outperforms existing methods, making peptide sequencing more accurate. This advancement has the potential to accelerate discoveries in biology, biotechnology, and medicine."
Poster,Latent Mamba Operator for Partial Differential Equations,https://ICML.cc//virtual/2025/poster/44930,"Karn Tiwari, Niladri Dutta, N M Anoop Krishnan, Prathosh AP","Neural operators have emerged as powerful data-driven frameworks for solving Partial Differential Equations (PDEs), offering significant speedups over numerical methods. However, existing neural operators struggle with scalability in high-dimensional spaces, incur high computational costs, and face challenges in capturing continuous and long-range dependencies in PDE dynamics. To address these limitations, we introduce the Latent Mamba Operator (LaMO), which integrates the efficiency of state-space models (SSMs) in latent space with the expressive power of kernel integral formulations in neural operators. We also establish a theoretical connection between state-space models (SSMs) and the kernel integral of neural operators. Extensive experiments across diverse PDE benchmarks on regular grids, structured meshes, and point clouds covering solid and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA) performance, with a 32.3\% improvement over existing baselines in solution operator approximation, highlighting its efficacy in modeling complex PDEs solution.","Introducing the Latent Mamba Operator (LaMO), a new AI-driven approach that solves the complex math behind physical processes, like airflow, heat flow, or material deformation, much faster and more accurately without knowing the underlying dynamics. Traditional simulations bog down when problems grow large or when distant interactions matter. Still, LaMO sidesteps this by compressing all the fine details into a compact “representation” and then using the sequence-learning method to predict how that representation changes. In tests across a wide range of physics problems—grids, meshes, and even scattered data—LaMO cut errors by about 32.3% and ran faster than the numerical solvers, opening the door to real-time forecasting, rapid engineering design, and other applications that once took days to compute."
Poster,Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes,https://ICML.cc//virtual/2025/poster/44849,"Zhuocheng Gong, Jian Guan, Wei Wu, Huishuai Zhang, Dongyan Zhao","Large language models (LLMs) have achieved remarkable success, yet aligning their generations with human preferences remains a critical challenge. Existing approaches to preference modeling often rely on an explicit or implicit reward function, overlooking the intricate and multifaceted nature of human preferences that may encompass conflicting factors across diverse tasks and populations. To address this limitation, we introduce Latent Preference Coding (LPC), a novel framework that models the implicit factors as well as their combinations behind holistic preferences using discrete latent codes. LPC seamlessly integrates with various offline alignment algorithms, automatically inferring the underlying factors and their importance from data without relying on pre-defined reward functions and hand-crafted combination weights. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-Instruct-8B). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment algorithms against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for responsible deployment of powerful LLMs.","We propose a method that models human preference as a combination of latent factors. The method is called Latent Preference Coding (LPC). LPC is based on discrete variational inference and can be seamlessly integrated with various offline alignment algorithms. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-Instruct-8B). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment algorithms against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for responsible deployment of powerful LLMs."
Poster,Latent Score-Based Reweighting for Robust Classification on Imbalanced Tabular Data,https://ICML.cc//virtual/2025/poster/44097,"Yunze Tong, Fengda Zhang, Zihao Tang, Kaifeng Gao, Kai Huang, Pengfei Lyu, Jun Xiao, Kun Kuang","Machine learning models often perform well on tabular data by optimizing average prediction accuracy. However, they may underperform on specific subsets due to inherent biases and spurious correlations in the training data, such as associations with non-causal features like demographic information. These biases lead to critical robustness issues as models may inherit or amplify them, resulting in poor performance where such misleading correlations do not hold. Existing mitigation methods have significant limitations: some require prior group labels, which are often unavailable, while others focus solely on the conditional distribution \(P(Y|X)\), upweighting misclassified samples without effectively balancing the overall data distribution \(P(X)\). To address these shortcomings, we propose a latent score-based reweighting framework. It leverages score-based models to capture the joint data distribution \(P(X, Y)\) without relying on additional prior information. By estimating sample density through the similarity of score vectors with neighboring data points, our method identifies underrepresented regions and upweights samples accordingly. This approach directly tackles inherent data imbalances, enhancing robustness by ensuring a more uniform dataset representation. Experiments on various tabular datasets under distribution shifts demonstrate that our method effectively improves performance on imbalanced data.","Machine learning models are great at making predictions when the data they're trained on closely resembles what they see in practice. But sometimes, these models don’t perform well for certain groups of people or situations. This happens because the models learn from patterns in the training data—even if those patterns are misleading. For example, they might wrongly assume that someone's background is tied to a particular outcome just because of how the data was collected.Our research introduces a new way to help models do better in these cases, without needing extra labels or prior knowledge about the data. We teach the model to pay more attention to data points that are rare or overlooked, by using a score-based model to provide proxy for them. This makes the overall dataset more balanced and helps the model learn fairer, more accurate predictions. We tested our approach on several datasets and found that it helps the models stay reliable, even when the data shifts or becomes more diverse."
Poster,Latent Thought Models with Variational Bayes Inference-Time Computation,https://ICML.cc//virtual/2025/poster/43587,"Deqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si, Chuan Li, Jianwen Xie, Sirui Xie, Ying Nian Wu","We propose a novel class of language models, Latent Thought Models (LTMs), which incorporate explicit latent thought vectors that follow an explicit prior model in latent space. These latent thought vectors guide the autoregressive generation of ground tokens through a Transformer decoder. Training employs a dual-rate optimization process within the classical variational Bayes framework: fast learning of local variational parameters for the posterior distribution of latent vectors (inference-time computation), and slow learning of global decoder parameters. Empirical studies reveal that LTMs possess additional scaling dimensions beyond traditional Large Language Models (LLMs), such as the number of iterations in inference-time computation and number of latent thought vectors. Higher sample efficiency can be achieved by increasing training compute per token, with further gains possible by trading model size for more inference steps. Designed based on these scaling properties, LTMs demonstrate superior sample and parameter efficiency compared to autoregressive models and discrete diffusion models. They significantly outperform these counterparts in validation perplexity and zero-shot language modeling tasks. Additionally, LTMs exhibit emergent few-shot in-context reasoning capabilities that scale with model size, and achieve competitive performance in conditional and unconditional text generation. The project page is available at [https://deqiankong.github.io/blogs/ltm](https://deqiankong.github.io/blogs/ltm).","Traditional Large language models (LLMs) can scale by increasing model size and data size. However, as model sizes grow rapidly, data availability has emerged as a critical bottleneck for continued scaling. We propose Latent Thought Models (LTMs) that add explicit ""latent thought vectors"" as internal abstract representations. Before generating text, LTMs first develop those internal thoughts, then use them to guide word-by-word generation. The model learns through a dual-rate process: fast learning that adapts thoughts for specific text and slow learning of general linguistic patterns.Compared to LLMs, LTMs achieve much better sample and computational efficiency. LTMs demonstrate in-context learning at a significantly smaller scale. Most importantly, LTMs introduce ""inference-time computation"" as a new scaling axis beyond LLMs, potentially transforming how we build efficient and generalizable AI systems."
Poster,Latent Variable Causal Discovery under Selection Bias,https://ICML.cc//virtual/2025/poster/45036,"Haoyue Dai, Yiwen Qiu, Ignavier Ng, Xinshuai Dong, Peter Spirtes, Kun Zhang","Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.","Understanding causal relations from data is a central goal in science, but it is hard when often the variables of interest are latentconstructs that cannot be directly observed. For instance, in a psychological survey, measured responses serve merely as proxies for latent personality traits. At the same time, the data is often not randomly sampled from the global population–imagine individuals with certain traits may be more willing to take a psychological survey. Ignoring such ""selection bias"" can lead to incorrect causal discovery results.This paper tackles both challenges at once: how to figure out the causal relations among both latent and observed variables when (1) we cannot observe everything, and (2) our data is biased by selection. We propose a new method that looks at how observed variables are connected–specifically, the mathematical ""rank"" of their dependencies, which are graphically informative for both latent causation and selection mechanisms–and identify the causal structure using these rank constraints.Our approach may serve as a valuable tool for researchers in social sciences, psychology, and more who want to understand the true causal relations beyond the possibly incomplete variables they observe, and behind the possibly biased data they collect. For researchers familiar with causal discovery, it is worth noting that while various tools beyond basic conditional independencies have been developed to handle latent variables since FCI, none seems to have been adapted for selection bias. This work may be a first attempt to bridge that gap."
Poster,Latent Variable Estimation in Bayesian Black-Litterman Models,https://ICML.cc//virtual/2025/poster/43691,"Thomas Y.L. Lin, Jerry Yao-Chieh Hu, Wan-Jiun Paul Chiou, Peter Lin","We revisit the Bayesian Black–Litterman (BL) portfolio model and remove its reliance on subjective investor views. Classical BL requires an investor “view”: a forecast vector $q$ and its uncertainty matrix $\Omega$ that describe how much a chosen portfolio should outperform the market.Our key idea is to treat $(q,\Omega)$ as latent variables and learn them from market data within a single Bayesian network.Consequently, the resulting posterior estimation admits closed-form expression, enabling fast inference and stable portfolio weights.Building on these, we propose two mechanisms to capture how features interact with returns: shared-latent parametrization and feature-influenced views; both recover classical BL and Markowitz portfolios as special cases.Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve Sharpe ratios by 50\% and cut turnover by 55\% relative to Markowitz and the index baselines.This work turns BL into a fully data-driven, view-free, and coherent Bayesian framework for portfolio optimization.","Many investment models ask human experts to state which assets they think will beat the market and by how much. This “investor-view” step is subjective and hard to quantify. Our work turns that step into a data problem. We treat the view and its uncertainty as hidden (latent) variables inside a single Bayesian model and let market data and asset-specific features learn them automatically. The resulting formulas stay fully analytical, so portfolio weights are quick to compute and less erratic than the classic Markowitz approach. When tested on 30 years of Dow Jones stocks and 20 years of sector-ETF data, the model raised risk-adjusted returns (Sharpe ratio) by roughly 50 percent and cut trading turnover by more than half. In short, we remove guesswork from the Black–Litterman framework and deliver a purely data-driven, coherent way to build more stable portfolios."
Poster,LAuReL: Learned Augmented Residual Layer,https://ICML.cc//virtual/2025/poster/43889,"Gaurav Menghani, Ravi Kumar, Sanjiv Kumar","One of the core pillars of efficient deep learning methods are architectural improvements, such as residual/skip connections, which have led to significantly better model convergence and quality. Since their introduction, residual connections have become ubiquitous not only in convolutional neural networks but also in transformer-based architectures, the backbone of LLMs.In this paper, we introduce the Learned Augmented Residual Layer (LAuReL) --- a novel generalization of the canonical residual connection --- designed to serve as an in-situ replacement while outperforming it in both model quality and footprint metrics.  Our experiments show that LAuReL can enhance quality for both vision and language models while adding fewer parameters and incurring less latency and memory overhead than naively increasing parameter count.For example, on the ImageNet-1K task, LAuReL achieves the same model quality improvements as naively adding an extra layer while using $2.6 \times$ fewer parameters. Similarly, when pre-training 1B and 4B parameter LLMs, LAuReL improves performance on a variety of challenging downstream evaluation tasks by 2.54\% to 20.05\%, while adding only 0.012\% and 0.1\% additional parameters, respectively.","Residual / skip connections are crucial for the strong performance of popular neural networks like CNN, Transformer, etc. These residual connections typically combine the output of a layer with the output of the preceding layer by simple addition. Residual connections help with avoiding issues such as vanishing gradients, and speed up convergence to a low loss. However, we claim that we can improve the residual connection by introducing learned lightweight linear components, such that it leads to significantly better model quality with minimal extra parameters, latency, etc. In this paper, we introduce Learned Augmented Residual Layer (LAuReL), which is a generalization of the residual connection and a drop-in replacement. LAuReL is a general framework but we provide three variants which can be used to cheaply make the residual connection adaptive instead of it being a simple summation. LAuReL variants can be combined with each other, or new variants can be constructed using the LAuReL framework.Through experiments we demonstrate that LAuReL can enhance model quality for both vision and language models while adding fewer parameters and incurring less latency and memory overhead than naively increasing parameter count by methods such as simply adding another layer to the neural network. On the ImageNet-1K task, LAuReL achieves the same model quality improvements as naively adding an extra layer while using $2.6 \times$ fewer parameters. Similarly, when pre-training 1B and 4B parameter LLMs, LAuReL improves performance on a variety of challenging downstream evaluation tasks by 2.54% to 20.05%, while adding only 0.012% and 0.1% additional parameters, respectively."
Poster,Layer by Layer: Uncovering Hidden Representations in Language Models,https://ICML.cc//virtual/2025/poster/45028,"Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, Ravid Shwartz-Ziv","From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer’s performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.","Large language models (LLMs) are made up of many layers, with each layer coming one after another. Traditionally, it's believed that the final layers are the most important because they produce the output, while earlier layers are thought to handle only simple, low-level features. However, this study finds that the middle layers often contain richer and more useful information than the final ones. We developed a new framework to measure the quality of information in each layer, using tools from information theory and geometry. After testing many models and tasks, we discovered that intermediate layers consistently provide better features for understanding text. This challenges the common assumption that only the final layers matter and suggests that tapping into middle layers could lead to more accurate and reliable AI systems."
