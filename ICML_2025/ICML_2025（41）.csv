type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling,https://ICML.cc//virtual/2025/poster/43728,"Hao Li, Yu-Hao Huang, Chang Xu, Viktor Schlegel, Renhe Jiang, Riza Batista-Navarro, Goran Nenadic, Jiang Bian","Time-series Generation (TSG) is a prominent research area with broad applications in simulations, data augmentation, and counterfactual analysis. While existing methods have shown promise in unconditional single-domain TSG, real-world applications demand for cross-domain approaches capable of controlled generation tailored to domain-specific constraints and instance-level requirements. In this paper, we argue that text can provide semantic insights, domain information and instance-specific temporal patterns, to guide and improve TSG.  We introduce ``Text-Controlled TSG'', a task focused on generating realistic time series by incorporating textual descriptions.To address data scarcity in this setting, we propose a novel LLM-based Multi-Agent framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore, we introduce Bridge, a hybrid text-controlled TSG framework that integrates semantic prototypes with text description for supporting domain-level guidance. This approach achieves state-of-the-art generation fidelity on 11 of 12 datasets, and improves controllability by up to 12% on MSE and 6% MAE compared to no text input generation, highlighting its potential for generating tailored time-series data.","Time series data—such as electricity usage, stock prices, or patient heart rates—are essential in fields like finance, energy, and healthcare. However, creating realistic synthetic time series that meet specific needs is a major challenge. Our research introduces a new method, called BRIDGE, that allows users to control time series generation using plain-language descriptions. Think of it as describing a pattern in words and receiving a matching time series in return. To do this, we designed a system where multiple AI agents work together to generate, evaluate, and refine these descriptions. We also developed a powerful model that learns both from text and from common time series patterns to generate high-quality, tailored outputs. BRIDGE not only outperforms existing methods on 11 of 12 benchmark datasets but also works well in new, unseen domains."
Poster,Bridging Fairness and Efficiency in Conformal Inference: A Surrogate-Assisted Group-Clustered Approach,https://ICML.cc//virtual/2025/poster/45471,"Chenyin Gao, Peter Gilbert, Larry Han","Standard conformal prediction ensures marginal coverage but consistently undercovers underrepresented groups, limiting its reliability for fair uncertainty quantification. Group fairness requires prediction sets to achieve a user-specified coverage level within each protected group. While group-wise conformal inference meets this requirement, it often produces excessively wide prediction sets due to limited sample sizes in underrepresented groups, highlighting a fundamental tradeoff between fairness and efficiency. To bridge this gap, we introduce Surrogate-Assisted Group-Clustered Conformal Inference (SAGCCI), a framework that improves efficiency through two key innovations: (1) clustering protected groups with similar conformal score distributions to enhance precision while maintaining fairness, and (2) deriving an efficient influence function that optimally integrates surrogate outcomes to construct tighter prediction sets. Theoretically, SAGCCI guarantees approximate group-conditional coverage in a doubly robust manner under mild convergence conditions, enabling flexible nuisance model estimation. Empirically, through simulations and an analysis of the phase 3 Moderna COVE COVID-19 vaccine trial, we demonstrate that SAGCCI outperforms existing methods, producing narrower prediction sets while maintaining valid group-conditional coverage, effectively balancing fairness and efficiency in uncertainty quantification.","How can we ensure that predictions from machine learning models are both fair and reliable — especially for underrepresented groups? A popular approach called conformal prediction provides uncertainty estimates that work well on average but often fails to give equally good results for different subgroups. In particular, it tends to underestimate uncertainty for smaller or underrepresented groups, which can lead to unfair or misleading predictions.Our work introduces a new method, called Surrogate-Assisted Group-Clustered Conformal Inference (SAGCCI), that tackles this fairness-efficiency tradeoff. It improves the quality of uncertainty estimates by clustering groups with similar patterns and borrowing information across them, so predictions can be more precise without losing fairness. It also uses auxiliary information — called “surrogates” — to sharpen predictions even when direct data is limited.We show mathematically that our method meets fairness goals under broad conditions and performs well even when models are misspecified. In both simulations and a real-world COVID-19 vaccine study, SAGCCI gave fairer and tighter prediction intervals than existing methods, suggesting a promising way forward for equitable and efficient machine learning."
Poster,Bridging Layout and RTL: Knowledge Distillation based Timing Prediction,https://ICML.cc//virtual/2025/poster/43998,"Mingjun Wang, Yihan Wen, Bin Sun, Jianan Mu, Juan Li, Xiaoyi Wang, Jing Ye, Bei Yu, Huawei Li","Accurate and efficient timing prediction at the register-transfer level (RTL) remains a fundamental challenge in electronic design automation (EDA), particularly in striking a balance between accuracy and computational efficiency. While static timing analysis (STA) provides high-fidelity results through comprehensive physical parameters, its computational overhead makes it impractical for rapid design iterations. Conversely, existing RTL-level approaches sacrifice accuracy due to the limited physical information available. We propose RTLDistil, a novel cross-stage knowledge distillation framework that bridges this gap by transferring precise physical characteristics from a layout-aware teacher model (Teacher GNN) to an efficient RTL-level student model (Student GNN), both implemented as graph neural networks (GNNs). RTLDistil efficiently predicts key timing metrics, such as arrival time (AT), and employs a multi-granularity distillation strategy that captures timing-critical features at node, subgraph, and global levels. Experimental results demonstrate that RTLDistil achieves significant improvement in RTL-level timing prediction error reduction, compared to state-of-the-art prediction models. This framework enables accurate early-stage timing prediction, advancing EDA's ``left-shift'' paradigm while maintaining computational efficiency. Our code and dataset will be publicly available at https://github.com/sklp-eda-lab/RTLDistil.","Designing computer chips is like constructing a building—problems discovered late in construction are expensive to fix. Currently, engineers can only accurately predict if a chip will run fast enough after completing most of the design work, similar to discovering structural issues only after a building is nearly complete. We developed RTLDistil, a system that uses artificial intelligence to predict chip performance much earlier in the design process. Our approach works like an experienced teacher sharing knowledge with a student: a ""teacher"" AI model learns from completed chip designs with all their physical details, then transfers this knowledge to a ""student"" AI model that can make predictions using only early-stage design sketches. The student model achieves nearly the same accuracy as the teacher while working with limited information. Our experiments show RTLDistil significantly improves early-stage predictions compared to existing methods. This advancement allows chip designers to identify and fix timing problems months earlier, potentially saving millions in development costs and accelerating the delivery of faster, more efficient computer chips that power everything from smartphones to data centers."
Poster,Bridging Protein Sequences and Microscopy Images with Unified Diffusion Models,https://ICML.cc//virtual/2025/poster/43909,"Dihan Zheng, Bo Huang","Fluorescence microscopy is ubiquitously used in cell biology research to characterize the cellular role of a protein. To help elucidate the relationship between the amino acid sequence of a protein and its cellular function, we introduce CELL-Diff, a unified diffusion model facilitating bidirectional transformations between protein sequences and their corresponding microscopy images. Utilizing reference cell morphology images and a protein sequence, CELL-Diff efficiently generates corresponding protein images. Conversely, given a protein image, the model outputs protein sequences. CELL-Diff integrates continuous and diffusion models within a unified framework and is implemented using a transformer-based network. We train CELL-Diff on the Human Protein Atlas (HPA) dataset and fine-tune it on the OpenCell dataset. Experimental results demonstrate that CELL-Diff outperforms existing methods in generating high-fidelity protein images, making it a practical tool for investigating subcellular protein localization and interactions.","Biologists often use specialized microscopes to take pictures of proteins inside cells, helping us understand what those proteins do. But creating these images is time-consuming, expensive, and usually limited to proteins we already know a lot about.We wondered if we could build a model to imagine what a protein looks like in a cell, just from its genetic sequence. We built CELL-Diff, an AI model that learns how protein sequences relate to their appearance in microscope images. Once trained, it can generate realistic images of a protein in the cell or suggest what the sequence might be if you show it a protein image.We trained CELL-Diff on large public datasets of protein images and found that it outperforms previous methods. This means scientists could use it to explore unfamiliar proteins, design new ones, or better understand how proteins interact in the cell."
Poster,Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging,https://ICML.cc//virtual/2025/poster/44093,"Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, Junxian He","Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood.In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models.  Unlike previous works that often focus on merging models of the same kind, we propose merging models **across modalities**, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a **training-free** manner.Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.","Modern AI systems that understand images and text tend to struggle with complex reasoning tasks, such as interpreting charts or solving math problems from pictures, because they lack strong reasoning skills when images are involved. We introduce a simple method that merges a reasoning-focused text AI into an image-and-text AI system, effectively transferring reasoning knowledge without any extra training. The merged AI solves visual reasoning puzzles much more accurately while retaining its ability to recognize and describe images. By studying how different parts of the merged system change, we find that image-recognition skills reside in the early layers and reasoning skills in the later layers, and that our method spreads reasoning ability across the entire network. Our work provides a straightforward way to build smarter multimodal AI and reveals how perception and reasoning interact inside these models."
Poster,BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning,https://ICML.cc//virtual/2025/poster/45505,"Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, Liwei Wang, Mingyi Hong, Zhaoran Wang","Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Our framework addresses two critical questions: (1) how to generate high-quality reasoning processes during inference automatically, and (2) how to integrate these processes into post-training. We propose the \emph{Bootstrapping Reinforced Thinking Process} (BRiTE) algorithm and demonstrate its theoretical convergence at a rate of $1/T$, where $T$ is the number of iterations. The algorithm operates in two steps. First, it generates high-quality rationales by approximating the desired posterior distribution using a reinforcement learning approach with a novel reward shaping mechanism. Second, it fine-tunes the base LLM by maximizing the joint probability of rationale generation with respect to LLM parameters. Empirical evaluation on GSM8K and MATH benchmarks demonstrates that our approach consistently improves performance across different model sizes without requiring human-annotated thinking processes, outperforming standard chain-of-thought prompting while enhancing existing post-training methods.","Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Our framework addresses two critical questions: (1) how to generate high-quality reasoning processes during inference automatically, and (2) how to integrate these processes into post-training. We propose the \emph{Bootstrapping Reinforced Thinking Process} (BRiTE) algorithm and demonstrate its theoretical convergence at a rate of $1/T$, where $T$ is the number of iterations. The algorithm operates in two steps. First, it generates high-quality rationales by approximating the desired posterior distribution using a reinforcement learning approach with a novel reward shaping mechanism. Second, it fine-tunes the base LLM by maximizing the joint probability of rationale generation with respect to LLM parameters. Empirical evaluation on GSM8K and MATH benchmarks demonstrates that our approach consistently improves performance across different model sizes without requiring human-annotated thinking processes, outperforming standard chain-of-thought prompting while enhancing existing post-training methods."
Poster,Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition,https://ICML.cc//virtual/2025/poster/45003,"Jaeheun Jung, Jaehyuk Lee, ChangHae Jung, Hanyoung Kim, Bosung Jung, Donghun Lee","Shock waves caused by earthquakes can be devastating. Generating realistic earthquake-caused ground motion waveforms help reducing losses in lives and properties, yet generative models for the task tend to generate subpar waveforms. We present High-fidelity Earthquake Groundmotion Generation System (HEGGS) and demonstrate its superior performance using earthquakes from North American, East Asian, and European regions. HEGGS exploits the intrinsic characteristics of earthquake dataset and learns the waveforms using an end-to-end differentiable generator containing conditional latent diffusion model and hi-fidelity waveform construction model. We show the learning efficiency of HEGGS by training it on a single GPU machine and validate its performance using earthquake databases from North America, East Asia, and Europe, using diverse criteria from waveform generation tasks and seismology. Once trained, HEGGS can generate three dimensional E-N-Z seismic waveforms with accurate P/S phase arrivals, envelope correlation, signal-to-noise ratio, GMPE analysis, frequency content analysis, and section plot analysis.","Can a purely data-driven method (i.e. not knowing earth science) synthesize earthquake shockwaves with seismologically plausible properties? Off-the-shelf tools failed, so we created our own that works great on earthquake recordings from North America, Far East, and Europe.We call it High-fidelity Earthquake Groundmotion Generation System (HEGGS). HEGGS exploits the fact that earthquake recordings are in fact a collection of earth-moving-sound caused by an earthquake, heard from many places. Notably, HEGGS does not require complicated earth-related measurements but only four minimal information -- earthquake location, depth, magnitude, and recording station location -- to create realistic earthquake shockwaves.Earthquake shockwaves created by HEGGS look plausible both from time series analysis and seismological perspectives. This work may help not only earthquake researchers but also the general public as a primary tool to emulate the consequences of hypothetical earthquakes."
Poster,B-score: Detecting biases in large language models using response history,https://ICML.cc//virtual/2025/poster/44236,"An Vo, Mohammad Reza Taesiri, Daeyoung Kim, Anh Nguyen","Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to ""de-bias"" themselves in a multi-turn conversation in response to questions that seek a Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases in Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: b-score.github.io.","State-of-the-art AIs have been shown to be biased against a gender (female), a race (African) or biased towards a number (number 7 or 42) or even names. This phenomenon entails severe consequences in downstream applications. We discover that AI can actually reduce its own bias when allowed to observe its own previous answers to the same question.This is similar to how a human might realize they're being unfair after reviewing their past decisions. Based on the difference in how AIs answer with or without observing their response history, we propose B-score, a metric that measures how an answer by AI may be biased (for or against a choice). For example, B-score can detect when a model heavily prefers the number 7 or “Biden” despite being asked to choose a random number or a random name between Biden or Trump. Letting AIs observe its response history has multiple effects: (1) successfully reduces bias in questions asking for a random choice (e.g., a number or a name Biden vs. Trump); (2) letting AIs think twice on the hard questions where it cannot easily generate a correct answer; and (3) reveal the real subjective opinion of a model in the questions that seek subjective preferences."
Poster,BSemiFL: Semi-supervised Federated Learning via a Bayesian Approach,https://ICML.cc//virtual/2025/poster/44504,"Haozhao Wang, Shengyu Wang, Jiaming Li, Hao Ren, Xingshuo Han, Wenchao Xu, Shangwei Guo, Tianwei Zhang, Ruixuan Li","Semi-supervised Federated Learning (SSFL) is a promising approach that allows clients to collaboratively train a global model in the absence of their local data labels. The key step of SSFL is the re-labeling where each client adopts two types of available models, namely global and local models, to re-label the local data. While various technologies such as using the global model or the average of two models have been proposed to conduct the re-labeling step, little literature delves deeply into the performance dominance and limitations of the two models. In this paper, we first theoretically and empirically demonstrate that the local model achieves higher re-labeling accuracy over local data while the global model can progressively improve the re-labeling performance by introducing the extra data knowledge of other clients. Based on these findings, we propose BSemiFL which re-labels the local data through the collaboration between the local and global model in a Bayesian approach. Specifically, to re-label any given local sample, BSemiFL first uses Bayesian inference to assess the closeness of the local/global model to the sample. Then, it applies a weighted combination of their pseudo labels, using the closeness as the weights. Theoretical analysis shows that the labeling error of our method is smaller than that of simply using the global model, the local model, or their simple average. Experimental results show that BSemiFL improves the performance by up to $9.8\%$ as compared to state-of-the-art methods.","Semi-supervised federated learning is a method that allows multiple users (we call them ""clients"") to collaborate in training an AI model. The advantage of this approach is that even if the data on each client's device is unlabeled, and without sharing the actual data, all clients can still jointly train a reasonably effective global model.A key step in this process is called ""re-labeling"". In this step, each client uses two pre-trained models —— a global model shared by all users and a local model trained specifically on their own data —— to automatically assign labels to their unlabeled data. These newly labeled data are then used to further improve the model.Past research has tried various ways to perform this labeling process —— for example, using only the global model, or taking an average of predictions from both the global and local models. However, few studies have deeply explored the strengths and limitations of these two models.In our work, through theoretical analysis and experimental validation, we observed an important phenomenon:The local model is better suited for labeling its own local data and achieves higher accuracy.The global model, although initially less accurate, gradually improves its labeling performance as it incorporates knowledge from other clients' data.Based on this finding, we propose a new method called BSemiFL. Its core idea is that instead of simply choosing one model or averaging their outputs, we use a technique called Bayesian inference to assess which model — the local or the global — is more ""similar"" to the current data sample. Based on this similarity, we compute weights for the two models and combine their predictions to generate more accurate pseudo-labels.Theoretically, we show that this method results in fewer labeling errors compared to using only the local model, only the global model, or simply averaging the two. Experimental results also demonstrate that, compared to the current state-of-the-art methods, our approach improves performance by up to 9.8%."
Poster,BSLoRA: Enhancing the Parameter Efficiency of LoRA with Intra-Layer and Inter-Layer Sharing,https://ICML.cc//virtual/2025/poster/45733,"Yuhua Zhou, Ruifeng Li, Changhai Zhou, Fei Yang, Aimin PAN","Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models (LLMs) to adapt to downstream tasks. However, in scenarios where multiple LoRA models are deployed simultaneously, standard LoRA introduces substantial trainable parameters, resulting in significant memory overhead and inference latency, particularly when supporting thousands of downstream tasks on a single server. While existing methods reduce stored parameters via parameter sharing, they fail to capture both local and global information simultaneously. To address this issue, we propose the Bi-Share LoRA (BSLoRA), which extends local LoRA with intra-LoRA and inter-LoRA parameter sharing to better capture local and global information. This approach reduces trainable parameters while maintaining or even enhancing model performance. Additionally, we design three transformation methods to improve the compatibility and collaborative efficiency of shared parameters with varying shapes, enhancing overall adaptability.Experiments on the 7B, 8B, and 13B versions of Llama show that BSLoRA, with only 44.59% of the parameters of standard LoRA,  outperforms LoRA by approximately 0.33% on commonsense reasoning and 2.08% on MMLU benchmarks. Code is available at https://github.com/yuhua-zhou/BSLoRA.git.","Large language models can be adapted to different downstream tasks using LoRA fine-tuning. However, as model parameter counts increase, the number of parameters required by LoRA fine-tuning also grows. How to effectively reduce LoRA’s parameter footprint has therefore become an important research question. We first analyze the parameters produced by LoRA fine-tuning and use an entropy-based similarity measure to evaluate the similarity between different parameter modules, discovering high entropy similarity among modules within the same layer and, likewise, among modules across different layers. Building on this insight, we introduce BSLoRA, which reduces redundancy by enabling both intra-layer and inter-layer parameter sharing. We also propose three shape-transformation strategies to overcome the challenges posed by mismatched parameter shapes when sharing. Our approach not only cuts the number of parameters needed for LoRA fine-tuning but, according to experimental results, also improves the performance of the fine-tuned model to some extent."
