type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Refined generalization analysis of the Deep Ritz Method and Physics-Informed Neural Networks,https://ICML.cc//virtual/2025/poster/44863,"Xianliang Xu, Ye Li, Zhongyi Huang","In this paper, we derive refined generalization bounds for the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). For the DRM, we focus on two prototype elliptic partial differential equations (PDEs): Poisson equation and static Schrödinger equation on the $d$-dimensional unit hypercube with the Neumann boundary condition. Furthermore, sharper generalization bounds are derived based on the localization techniques under the assumptions that the exact solutions of the PDEs lie in the Barron spaces or the general Sobolev spaces. For the PINNs, we investigate the general linear second order elliptic PDEs with Dirichlet boundary condition using the local Rademacher complexity in the multi-task learning setting. Finally, we discuss the generalization error in the setting of over-parameterization when solutions of PDEs belong to Barron space.","(1) Neural network-based methods for solving PDEs have gained popularity in scientific computing, yet their theoretical foundations remain incompletely understood.(2) Under the scenario where PDE solutions belong to Barron spaces and Sobolev spaces, we employ localization techniques to derive sharper generalization bounds for two popular methods: Physics-Informed Neural Networks (PINNs) and Deep Ritz Method (DRM).(3) This will advance the application of machine learning theory to a wide range of problems in scientific computing."
Poster,Refining Adaptive Zeroth-Order Optimization at Ease,https://ICML.cc//virtual/2025/poster/45481,"Yao Shu, Qixin Zhang, Kun He, Zhongxiang Dai","Recently, zeroth-order (ZO) optimization plays an essential role in scenarios where gradient information is inaccessible or unaffordable, such as black-box systems and resource-constrained environments. While existing adaptive methods such as ZO-AdaMM have shown promise, they are fundamentally limited by their underutilization of moment information during optimization, usually resulting in underperforming convergence. To overcome these limitations, this paper introduces *Refined Adaptive Zeroth-Order Optimization* (R-AdaZO). Specifically, we first show the untapped variance reduction effect of first moment estimate on ZO gradient estimation, which improves the accuracy and stability of ZO updates. We then refine the second moment estimate based on these variance-reduced gradient estimates to better capture the geometry of the optimization landscape, enabling a more effective scaling of ZO updates. We present rigorous theoretical analysis to show **_(a)_** *the first analysis* to the variance reduction of first moment estimate in ZO optimization, **_(b)_** *the improved second moment estimates* with a more accurate approximation of its variance-free ideal, **_(c)_** *the first variance-aware convergence framework* for adaptive ZO methods, which may be of independent interest, and **_(d)_** *the faster convergence* of R-AdaZO than existing baselines like ZO-AdaMM. Our extensive experiments, including synthetic problems, black-box adversarial attack, and memory-efficient fine-tuning of large language models (LLMs), further verify the superior convergence of R-AdaZO, indicating that R-AdaZO offers an improved solution for real-world ZO optimization challenges.","**(1) Problem:** In many real-world AI applications, we can't directly access the ""gradient"" information that tells us how to improve a model. This is common in black-box systems (where we only see inputs and outputs) or on devices with limited computing power. Existing methods for optimizing these systems, while useful, often struggle to converge quickly because they don't fully leverage the information they *do* gather about the model's behavior.**(2) Solution:** We introduce R-AdaZO, a new optimization technique designed to overcome these limitations. Our key insight is that even without direct gradients, we can significantly improve the accuracy and stability of our updates by better utilizing ""moment"" information – specifically, by showing how the first moment estimate (a kind of average direction) can reduce noise in our estimations. We then use these more accurate estimates to refine how we scale our updates, allowing the optimization process to better adapt to the problem's unique characteristics.**(3) Impact:** R-AdaZO provides a more efficient and robust way to optimize complex AI systems when traditional gradient information is unavailable. This leads to faster and more reliable training for a wide range of applications, from making AI models more resilient to attacks to efficiently fine-tuning large language models on resource-constrained devices. Our work offers a significant step forward in tackling real-world optimization challenges in black-box and resource-limited environments."
Poster,Reflection-Bench: Evaluating Epistemic Agency in Large Language Models,https://ICML.cc//virtual/2025/poster/44571,"Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, Yingchun Wang","With large language models (LLMs) increasingly deployed as cognitive engines for AI agents, the reliability and effectiveness critically hinge on their intrinsic epistemic agency, which remains understudied. Epistemic agency, the ability to flexibly construct, adapt, and monitor beliefs about dynamic environments, represents a base-model-level capacity independent of specific tools, modules, or applications. We characterize the holistic process underlying epistemic agency, which unfolds in seven interrelated dimensions: prediction, decision-making, perception, memory, counterfactual thinking, belief updating, and meta-reflection. Correspondingly, we propose Reflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven tasks with long-term relevance and minimization of data leakage. Through a comprehensive evaluation of 16 models using three prompting strategies, we identify a clear three-tier performance hierarchy and significant limitations of current LLMs, particularly in meta-reflection capabilities. While state-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our findings suggest several promising research directions, including enhancing core cognitive functions, improving cross-functional coordination, and developing adaptive processing mechanisms. Our code and data are available at https://github.com/AI45Lab/ReflectionBench.","As large language models (LLMs) become the ""brains"" of AI agents that interact with the real world, we need to understand how well LLMs can construct, adapt, and monitor their beliefs about changing environments - what we call ""epistemic agency."" This is a base-model-level characteristic that determines whether LLMs can truly serve as reliable cores of AI agents. However, current evaluations mainly focus on specific applications or isolated abilities.We proposed Reflection-Bench, a comprehensive benchmark that evaluates epistemic agency by breaking it down into seven key cognitive capabilities: predicting what will happen next, making decisions, noticing surprises, remembering past events, considering ""what if"" scenarios, updating beliefs when wrong, and reflecting on overall patterns. To evaluate these capabilities, we adapted seven cognitive tests used to study human cognition. Importantly, we designed these tests with adjustable parameters to prevent LLMs from simply memorizing answers.We tested 16 major LLMs discovered that while top models showed basic epistemic agency, all models struggled with recognizing global patterns across multiple experiences. These findings not only help us understand the capabilities and limitations of current LLMs, but also provide insights for enhancing LLMs' epistemic agency toward more reliable AI agents."
Poster,Reflection-Window Decoding: Text Generation with Selective Refinement,https://ICML.cc//virtual/2025/poster/44024,"Zeyu Tang, Zhenhao Chen, Xiangchen Song, Loka Li, Yunlong Deng, Yifan Shen, Guangyi Chen, Peter Spirtes, Kun Zhang","The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.","Current language models generate text one word at a time based on history (known as the ""autoregressive"" way), without a built-in ability to go back and fix previous mistakes. This is like writing an essay where you can never use the backspace key--once a word is written, it stays forever. Our research shows that this approach leads to suboptimal text that could have been better, if the model could revise its work as the text generation unfolds.We developed ""Reflection-Window Decoding,"" which gives language models the ability to pause and revise recent text before continuing. Using mathematical analysis, we find that the uncertainty about what to say next can serve as a signal that revision might be needed. Our approach selectively regenerates problematic sections upon reflection, allowing real-time correction without starting over completely.Our work reconsiders how language models should handle self-correction. Rather than relying solely on models' (potentially unreliable) high-level behavior to reflect on and revise complete responses after generation, our approach demonstrates the value of building correction mechanisms directly into the model itself. By enabling real-time refinement as text unfolds, we enable language models to write and revise simultaneously, just as humans do when writing."
Poster,Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens,https://ICML.cc//virtual/2025/poster/44010,"Jihwan Jeong, Xiaoyu Wang, Jingmin Wang, Scott Sanner, Pascal Poupart","Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel _doubly Bayesian_ offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.","Imagine teaching an AI to perform a task, like navigating a building, using only a fixed set of recorded examples. When faced with a new situation it hasn't seen before, the AI can become confused and make poor decisions because its knowledge is incomplete. Many existing approaches make the AI overly cautious to avoid mistakes, but this prevents it from adapting effectively.We introduce a new method called Reflect-then-Plan (RefPlan) that helps an AI reason intelligently about what it doesn't know. Our method works in two steps:* Reflect: As the AI operates, it continuously ""reflects"" on its recent experiences—the actions it took and what happened as a result—to update its understanding of the specific environment it's currently in.* Plan: When ""planning"" its next move, it doesn't rely on a single, rigid prediction of the future. Instead, it considers a range of possible scenarios based on its uncertainty, making its strategy more robust to the unexpected.Our results show that this approach significantly improves the AI's performance, making it more flexible and resilient, especially when faced with unfamiliar situations, limited data, or changing conditions."
Poster,ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding,https://ICML.cc//virtual/2025/poster/44816,"Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, Cha Zhang","Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selective attention capability. In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate ``visual thoughts'' by performing visual editing on the input image through code, shifting and refining their visual focuses. Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process. We experiment upon a wide range of structured image understanding tasks involving tables and charts. ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks. We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information. Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT.","We teach multimodal LLMs to think with images -- as an intermediate thought -- to solve structured image problems (tables and charts). Given an image and a question, LLMs are prompted to generate python code to call the provided tool functions, to directly edit on the input image. With the intermediate edit outputs, models can understand the original question much better, with 7-11% average gain. We call this method ReFocus, a visual chain-of-thought method to help models re-focus on the important parts on the input images. We then apply this method to collect a 14K training set comprising the prompted intermediate edits and final results. We show that such training data can provide better supervision compared to standard QA pairs or text-only CoT pairs, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT."
Poster,ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering,https://ICML.cc//virtual/2025/poster/45777,"Lufei Liu, Tor Aamodt","Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates.The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations.Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models.We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads.ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines.Experimental results show that we achieve 1.4$\times$ speedup on average with negligible quality loss in three real-time rendering tasks.Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/","Realistic visuals make video games and virtual reality feel more immersive and exciting, but creating these images can be slow and power-intensive. In fact, the better the image, the longer it takes. While animated movies can spend hours producing each frame, interactive experiences need to respond instantly to user input to feel smooth and believable.Our research aims to make rendering faster, so we can save power and improve image quality without slowing down the system. We notice that many frames displayed back-to-back look very similar, which inspired us to introduce a mechanism that only partially updates the neural networks involved in creating each frame. We strategically save parts of the neural network and reuse them for as long as possible before they start noticeably compromising the image.Our technique accelerates the neural networks behind the visuals, cutting energy use and making it easier for less powerful hardware to keep up without sacrificing quality."
Poster,REG: Rectified Gradient Guidance for Conditional Diffusion Models,https://ICML.cc//virtual/2025/poster/43732,"Zhengqi Gao, Kaiwen Zha, Tianyuan Zhang, Zihui Xue, Duane Boning","Guidance techniques are simple yet effective for improving conditional generation in diffusion models. Albeit their empirical success, the practical implementation of guidance diverges significantly from its theoretical motivation. In this paper, we reconcile this discrepancy by replacing the scaled marginal distribution target, which we prove theoretically invalid, with a valid scaled joint distribution objective. Additionally, we show that the established guidance implementations are approximations to the intractable optimal solution under no future foresight constraint. Building on these theoretical insights, we propose rectified gradient guidance (REG), a versatile enhancement designed to boost the performance of existing guidance methods. Experiments on 1D and 2D demonstrate that REG provides a better approximation to the optimal solution than prior guidance techniques, validating the proposed theoretical framework. Extensive experiments on class-conditional ImageNet and text-to-image generation tasks show that incorporating REG consistently improves FID and Inception/CLIP scores across various settings compared to its absence.","Diffusion models can create realistic images from random noise. To guide these models toward specific goals—like generating images of a certain class—techniques called “guidance” are used. However, current guidance methods don’t fully align with their theoretical foundations. Our work resolves this mismatch by proposing a new, more accurate theory and a method called Rectified Gradient Guidance (REG). REG improves the quality of generated images across multiple tasks while remaining compatible with existing systems, helping make diffusion models more reliable and effective."
Poster,"Regress, Don't Guess: A Regression-like Loss on Number Tokens for Language Models",https://ICML.cc//virtual/2025/poster/45082,"Jonas Zausinger, Lars Pennig, Anamarija Kozina, Sean Sdahl, Julian Sikora, Adrian Dendorfer, Timofey Kuznetsov, Mohamad Hagog, Nina Wiedemann, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Singh, Michael Danziger, Jannis Born","While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed **Number Token Loss** (NTL) comes in two flavors and minimizes either the $\mathcal{L}_p$ norm or the Wasserstein distance between the *numerical values* of the real and predicted number tokens. NTL can easily be added to any language model and extend the Cross Entropy objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope that this work can inspire LLM developers to improve their pretraining objectives.The code is available via: https://tum-ai.github.io/number-token-loss/","Large language models are great at writing documents and answering questions, but when it comes to math, they often make mistakes. A key reason is that these models do not have built-in understanding for how numbers relate to one another. For example, they treat the numbers “2” and “3” as just different words, not as digits that are close together.To address this, we developed a new way to train language models by giving the models additional feedback on numbers. Our method, called Number Token Loss (NTL), explicitly teaches models to understand that “2” and “3” are numerically close, and “2” and “9” are farther apart. It analyzes how much the model’s predictions need to shift to match the correct value, based on the numerical distance between the predicted number probabilities and true values.We tested this on math problems and found that it consistently improves performance. Importantly, our method can be used by any Language Model, is fast to compute and easy to integrate."
Poster,Regression for the Mean: Auto-Evaluation and Inference with Few Labels through Post-hoc Regression,https://ICML.cc//virtual/2025/poster/44252,"Benjamin Eyre, David Madras","The availability of machine learning systems that can effectively perform arbitrary tasks has led to synthetic labels from these systems being used in applications of statistical inference, such as data analysis or model evaluation. The Prediction Powered Inference (PPI) framework provides a way of leveraging both a large pool of pseudo-labelled data and a small sample with real, high-quality labels to produce a low-variance, unbiased estimate of the quantity being evaluated for. Most work on PPI considers a relatively sizable set of labelled samples, which can be resource intensive to obtain.  However, we find that when labelled data is scarce, the PPI++ method can perform even worse than classical inference. We analyze this phenomenon by relating PPI++ to ordinary least squares regression, which also experiences high variance with small sample sizes, and use this regression framework to better understand the efficacy of PPI. Motivated by this, we present two new PPI-based techniques that leverage robust regressors to produce even lower variance estimators in the few-label regime","Researchers often investigate problems and reach conclusions by analyzing large amounts of data. The process of generating insights from these data is known as statistical inference. Statistical inference needs a lot of data in order to be reliable, and collecting a lot of data can put a large burden on researchers. Recently, machine learning (ML) models have become so effective at answering questions that they have been used in place of humans for collecting data to save resources. Existing research has put forward ways to combine this ML labelled data with the human labelled data so that statistical inference with these combined data is both reliable and free of potential bias from the ML model. We found, however, that these existing techniques perform poorly when very little human labelled data is available. By relating this technique to a classic problem from statistics, we create two new modifications to this technique that allow for improved statistical inference even when there is not much human labelled data available."
