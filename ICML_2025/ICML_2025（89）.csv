type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Efficiently Vectorized MCMC on Modern Accelerators,https://ICML.cc//virtual/2025/poster/45532,"Hugh Dance, Pierre Glaser, Peter Orbanz, Ryan P. Adams","With the advent of automatic vectorization tools (e.g., JAX's vmap), writing multi-chain MCMC algorithms is often now as simple as invoking those tools on single-chain code. Whilst convenient, for various MCMC algorithms this results in a synchronization problem---loosely speaking, at each iteration all chains running in parallel must wait until the last chain has finished drawing its sample. In this work, we show how to design single-chain MCMC algorithms in a way that avoids synchronization overheads when vectorizing with tools like vmap, by using the framework of finite state machines (FSMs). Using a simplified model, we derive an exact theoretical form of the obtainable speed-ups using our approach, and use it to make principled recommendations for optimal algorithm design. We implement several popular MCMC algorithms as FSMs, including Elliptical Slice Sampling, HMC-NUTS, and Delayed Rejection, demonstrating speed-ups of up to an order of magnitude in experiments.","Many important scientific tasks - like estimating how a new drug will affect patients or predicting future climate patterns - rely on computing quantities of a distribution that is not known in closed form. In practice, one often uses a class of simulation techniques called ""Markov Chain Monte Carlo"" (MCMC), to draw samples from this unknown distribution and use them to estimate these quantities (e.g. its mean or mode). Modern hardware such as GPUs can run multiple MCMC simulations (often called ""chains"") in parallel to save time. Several “automatic vectorization” tools have been developed that take code for running a single chain, and transform it into code that can run a batch of chains together in lockstep. These tools have made it very convenient for researchers to leverage the power of this modern hardware. However, a limitation arises whenever each chain does a random amount of work to produce each sample. Since the chains must run in lockstep, all chains must wait at every sampling step until the slowest chain finishes returning its sample. In other words, the time taken to draw every sample is driven by the slowest chain, rather than the typical chain.In our work, we break each chain’s logic into just a few simple stages (e.g. “propose a candidate,” “compute its score,” “decide to accept or reject,” and “record the result”) and keep track of which stage each chain is in. We then pack all of these stages into a single, GPU-friendly function that can still be conveniently transformed with existing tools to run batches of simulations. At each step, the function looks at every chain’s current stage and does the required computation, allowing all chains in the batch to move forward immediately no matter what stage they are currently at. This “finite‐state machine” approach removes thousands of tiny synchronization pauses and keeps the hardware busy much more effectively. On real problems such as modeling house prices; analyzing stock price trends; and modeling predator-prey patterns, we see up to 10× more samples per second, even though the mathematical steps of each simulation remain the same. As a result, users can obtain the same statistical conclusions but in potentially a fraction of the wall‐clock time, allowing faster progress in fields like medicine, climate science, economics and engineering."
Poster,Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow,https://ICML.cc//virtual/2025/poster/46355,"Zhonglin Cao, Mario Geiger, Allan Costa, Danny Reidenbach, Karsten Kreis, Tomas Geffner, Franco Pellegrini, Guoqing Zhou, Emine Kucukbenli","Fast and accurate generation of molecular conformers is desired for downstream computational chemistry and drug discovery tasks. Currently, training and sampling state-of-the-art diffusion or flow-based models for conformer generation require significant computational resources. In this work, we build upon flow-matching and propose two mechanisms for accelerating training and inference of generative models for 3D molecular conformer generation. For fast training, we introduce the SO(3)-*Averaged Flow* training objective, which leads to faster convergence to better generation quality compared to conditional optimal transport flow or Kabsch-aligned flow. We demonstrate that models trained using SO(3)-*Averaged Flow* can reach state-of-the-art conformer generation quality. For fast inference, we show that the reflow and distillation methods of flow-based models enable few-steps or even one-step molecular conformer generation with high quality. The training techniques proposed in this work show a path towards highly efficient molecular conformer generation with flow-based models.","Molecular conformer generation is a critical task in small molecule drug-discovery pipeline. At current stage, generative models for conformer generation are too slow to be applied in industrial-scale use cases. The motivation of our work is to accelerate flow-based generative model for molecular conformer generation, while maintaining high generation quality. We firstly propose a method to training flow-based generative models, called SO(3)-*Averaged Flow*, to improve generation quality and accelerate training. We also proposed to adopt the reflow and distillation algorithms to effectively enable one-step generation of molecular conformers while maintaining high generation quality. Combining these methods, our models show promising performance in efficient conformer generation, making them more practically useful in drug-discovery applications."
Poster,Efficient Motion Prompt Learning for Robust Visual Tracking,https://ICML.cc//virtual/2025/poster/44813,"Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, Huchuan Lu","Due to the challenges of processing temporal information, most trackers depend solely on visual discriminability and overlook the unique temporal coherence of video data. In this paper, we propose a lightweight and plug-and-play motion prompt tracking method. It can be easily integrated into existing vision-based trackers to build a joint tracking framework leveraging both motion and vision cues, thereby achieving robust tracking through efficient prompt learning. A motion encoder with three different positional encodings is proposed to encode the long-term motion trajectory into the visual embedding space, while a fusion decoder and an adaptive weight mechanism are designed to dynamically fuse visual and motion features. We integrate our motion module into three different trackers with five models in total. Experiments on seven challenging tracking benchmarks demonstrate that the proposed motion module significantly improves the robustness of vision-based trackers, with minimal training costs and negligible speed sacrifice. Code is available at https://github.com/zj5559/Motion-Prompt-Tracking.","Tracking specific objects in videos is still challenging because most methods focus only on how things look, ignoring how they move over time. This makes trackers less reliable when objects undergo significant appearance changes or encounter severe distractors.We propose a simple and efficient solution: a motion prompt tracking module that helps trackers better understand object movement. Our method can be easily added to existing trackers without major changes or retraining from scratch. It uses a motion encoder to extract motion patterns from objects’ historical trajectories and combines them with visual information using a fusion decoder with an adaptive weighting mechanism.We tested this motion module on multiple tracking methods across several challenging benchmarks. Results show that it consistently improves tracking robustness with minimal impact on speed."
Poster,Efficient Multi-modal Long Context Learning for Training-free Adaptation,https://ICML.cc//virtual/2025/poster/46375,"Zehong Ma, Shiliang Zhang, Longhui Wei, Qi Tian","Traditional approaches to adapting multi-modal large language models (MLLMs) to new tasks have relied heavily on fine-tuning. This paper introduces Efficient Multi-Modal Long Context Learning (EMLoC), a novel training-free alternative that embeds demonstration examples directly into the model input. EMLoC offers a more efficient, flexible, and scalable solution for task adaptation. Because extremely lengthy inputs introduce prohibitive computational and memory overhead, EMLoC contributes a chunk-wise compression mechanism combined with layer-wise adaptive pruning. It condenses long-context multimodal inputs into compact, task-specific memory representations. By adaptively pruning tokens at each layer under a Jensen-Shannon divergence constraint, our method achieves a dramatic reduction in inference complexity without sacrificing performance. This approach is the first to seamlessly integrate compression and pruning techniques for multi-modal long-context learning, offering a scalable and efficient solution for real-world applications. Extensive experiments on diverse vision-language benchmarks demonstrate that EMLoC achieves performance on par with or superior to naive long-context approaches. Our results highlight the potential of EMLoC as a groundbreaking framework for efficient and flexible adaptation of multi-modal models in resource-constrained environments. Codes are publicly available at https://github.com/Zehong-Ma/EMLoC.","Today’s powerful AI models can understand both images and text, but adapting them to new tasks usually requires a lot of extra training, which is time-consuming and expensive. Our work introduces EMLoC, a new method that avoids extra training and instead allows the model to learn new tasks just by reading a few examples, much like how people learn from demonstrations.However, feeding large amounts of image and text data into these models creates technical challenges—it’s slow and uses a lot of memory. To solve this, we developed a way to compress and simplify the input while keeping the important information. Think of it like summarizing a long story without losing the key points. Our method smartly decides which parts of the input can be skipped or shortened at each stage of the model, keeping the results accurate while making everything run much faster.This makes it easier to use these AI systems in real-world situations where speed and limited computing resources matter, such as mobile apps or robots. You can find our code and try it out yourself at https://github.com/Zehong-Ma/EMLoC."
Poster,Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination,https://ICML.cc//virtual/2025/poster/45570,"Ilias Diakonikolas, Giannis Iakovidis, Daniel Kane, Thanasis Pittas","We study the algorithmic problem of robust mean estimation of an identity covariance Gaussian in the presence of mean-shift contamination. In this contamination model, we are given a set of points in $\mathbb{R}^d$ generated i.i.d. via the following process. For a parameter $\alpha<1/2$, the $i$-th sample $x_i$ is obtained as follows: with probability $1-\alpha$, $x_i$ is drawn from $\mathcal{N}(\mu, I)$, where $\mu \in \mathbb{R}^d$ is the target mean; and with probability $\alpha$, $x_i$ is drawn from $\mathcal{N}(z_i, I)$, where $z_i$ is unknown and potentially arbitrary. Prior work characterized the information-theoretic limits of this task. Specifically, it was shown that— in contrast to Huber contamination— in the presence of mean-shift contamination consistent estimation is possible. On the other hand, all known robust estimators in the mean-shift model have running times exponential in the dimension. Here we give the first computationally efficient algorithm for high-dimensional robust mean estimation with mean-shift contamination that can tolerate a constant fraction of outliers. In particular, our algorithm has near-optimal sample complexity, runs in sample-polynomial time, and approximates the target mean to any desired accuracy. Conceptually, our result contributes to a growing body of work that studies inference with respect to natural noise models lying in between fully adversarial and random settings.","Computing the mean over a collection of samples is a fundamental task that underlies many algorithms in both theory and practice. However, samples are often corrupted, and a substantial body of work focuses on cases where those corruptions are completely arbitrary. In such settings, estimating an accurate mean is considerably more difficult—and in fact, perfect recovery of the true mean is impossible.In our work, we assume a more structured corruption model: each corrupted sample contains some added noise rather than an arbitrary outlier. Under this assumption, we design an efficient procedure that estimates the mean with near‐perfect accuracy.We believe this approach sheds light on the challenge of computing the mean in the presence of structured (rather than arbitrary) corruption—a problem of broad practical importance."
Poster,Efficient Network Automatic Relevance Determination,https://ICML.cc//virtual/2025/poster/45268,"Hongwei Zhang, Ziqi Ye, Xinyuan Wang, Xin Guo, Zenglin Xu, Yuan Cheng, Zixin Hu, Yuan Qi","We propose Network Automatic Relevance Determination (NARD), an extension of ARD for linearly probabilistic models, to simultaneously model sparse relationships between inputs $X \in \mathbb R^{d \times N}$ and outputs $Y \in \mathbb R^{m \times N}$, while capturing the correlation structure among the $Y$. NARD employs a matrix normal prior which contains a sparsity-inducing parameter to identify and discard irrelevant features, thereby promoting sparsity in the model. Algorithmically, it iteratively updates both the precision matrix and the relationship between $Y$ and the refined inputs. To mitigate the computational inefficiencies of the $\mathcal O(m^3 + d^3)$ cost per iteration, we introduce Sequential NARD, which evaluates features sequentially, and a Surrogate Function Method, leveraging an efficient approximation of the marginal likelihood and simplifying the calculation of determinant and inverse of an intermediate matrix. Combining the Sequential update with the Surrogate Function method further reduces computational costs. The computational complexity per iteration for these three methods is reduced to $\mathcal O(m^3+p^3)$, $\mathcal O(m^3 + d^2)$, $\mathcal O(m^3+p^2)$ respectively, where $p \ll d$ is the final number of features in the model. Our methods demonstrate significant improvements in computational efficiency with comparable performance on both synthetic and real-world datasets.","Understanding how different biological features — like genes or molecular markers — are connected to physical traits or diseases is a key challenge in biomedical science. These relationships are often complex and involve many interacting factors, making it hard to find what truly matters.We developed a method called Network Automatic Relevance Determination (NARD) to help identify which features are important and how they relate to each other. Unlike traditional approaches, NARD doesn’t just focus on one-to-one links but also captures how different traits are interrelated. This is especially useful when studying biological systems where multiple traits can be influenced by shared genetic or molecular factors.To make this process more efficient, we designed optimized versions of our method that significantly reduce the time needed to analyze large datasets. This allows researchers to more quickly and effectively uncover meaningful associations in complex biological networks.Our tools offer a scalable way to explore multi-dimensional biological data, supporting discoveries that could advance disease research or personalized medicine."
Poster,Efficient Noise Calculation in Deep Learning-based MRI Reconstructions,https://ICML.cc//virtual/2025/poster/44715,"Onat Dalmaz, Arjun Desai, Reinhard Heckel, Tolga Cukur, Akshay Chaudhari, Brian Hargreaves","Accelerated MRI reconstruction involves solving an ill-posed inverse problem where noise in acquired data propagates to the reconstructed images. Noise analyses are central to MRI reconstruction for providing an explicit measure of solution fidelity and for guiding the design and deployment of novel reconstruction methods. However, deep learning (DL)-based reconstruction methods have often overlooked noise propagation due to inherent analytical and computational challenges, despite its critical importance. This work proposes a theoretically grounded, memory-efficient technique to calculate *voxel-wise variance* for quantifying uncertainty due to acquisition noise in accelerated MRI reconstructions. Our approach is based on approximating the noise covariance using the DL network's Jacobian, which is intractable to calculate. To circumvent this, we derive an *unbiased estimator* for the diagonal of this covariance matrix—voxel-wise variance—, and introduce a Jacobian sketching technique to efficiently implement it. We evaluate our method on knee and brain MRI datasets for both data-driven and physics-driven networks trained in supervised and unsupervised manners. Compared to empirical references obtained via Monte-Carlo simulations, our technique achieves near-equivalent performance while reducing computational and memory demands by an order of magnitude or more. Furthermore, our method is robust across varying input noise levels, acceleration factors, and diverse undersampling schemes, highlighting its broad applicability. Our work *reintroduces* accurate and efficient noise analysis as a central tenet of reconstruction algorithms, holding promise to reshape how we evaluate and deploy DL-based MRI.","MRI scans allow doctors to look inside the body to diagnose medical conditions, but scans typically take a long time. Researchers developed methods using artificial intelligence (AI) to speed up MRI scans, but these advanced AI methods often don't clearly show how noise (unwanted random variations) affects the images they produce. Knowing precisely where and how much noise appears in MRI scans is essential, as noise can affect diagnosis and treatment decisions.In this work, we created a new approach to measure exactly how noise from faster scans spreads through AI-based MRI reconstructions. Our method efficiently estimates how much noise exists at each point (voxel) of an image, clearly indicating areas where the image might be less reliable. This helps radiologists trust AI-generated images by identifying uncertain regions. Our technique is accurate yet much faster and uses significantly less memory compared to previous methods. It works well with different body regions, different AI models, and various scan speeds, showing it can be broadly applied.Overall, our approach brings precise and efficient noise assessment back into modern MRI, enhancing the reliability of faster, AI-powered scans and supporting better medical decisions."
Poster,Efficient Online Reinforcement Learning for Diffusion Policy,https://ICML.cc//virtual/2025/poster/46396,"Haitong Ma, Tianyi Chen, Kai Wang, Na Li, Bo Dai","Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the conventional diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy. Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and not scalable. To enable efficient training of diffusion policies in online RL, we generalize the conventional denoising score matching by reweighting the loss function. The resulting Reweighted Score Matching (RSM) preserves the optimal solution and low computational cost of denoising score matching, while eliminating the need to sample from the target distribution and allowing learning to optimize value functions. We introduce two tractable reweighted loss functions to solve two commonly used policy optimization problems, policy mirror descent and max-entropy policy, resulting in two practical algorithms named Diffusion Policy Mirror Descent (DPMD) and Soft Diffusion Actor-Critic (SDAC). We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that the proposed algorithms outperform recent diffusion-policy online RLs on most tasks, and the DPMD improves more than 120% over soft actor-critic on Humanoid and Ant.","Diffusion models are a type of AI that learn by gradually turning noise into realistic data, like images or actions, by studying many examples. They’ve recently been used to create smart decision-making systems that learn from past experiences. While they work well when the best examples are already available, it’s hard to use them in situations where the system has to learn by itself—like in online learning—because we don’t know what the “best” actions are yet. This new research introduces a smarter and more efficient training method that doesn’t need perfect examples and is much easier to run. As a result, the two new techniques they developed—called DPMD and SDAC—learn better and faster than previous methods in many robot control tasks."
Poster,Efficient Optimization with Orthogonality Constraint: a Randomized Riemannian Submanifold Method,https://ICML.cc//virtual/2025/poster/46238,"Andi Han, Pierre-Louis Poirion, Akiko Takeda","Optimization with orthogonality constraints frequently arises in various fields such as machine learning. Riemannian optimization offers a powerful framework for solving these problems by equipping the constraint set with a Riemannian manifold structure and performing optimization intrinsically on the manifold. This approach typically involves computing a search direction in the tangent space and updating variables via a retraction operation. However, as the size of the variables increases, the computational cost of the retraction can become prohibitively high, limiting the applicability of Riemannian optimization to large-scale problems.  To address this challenge and enhance scalability, we propose a novel approach that restricts each update on a random submanifold, thereby significantly reducing the per-iteration complexity. We introduce two sampling strategies for selecting the random submanifolds and theoretically analyze the convergence of the proposed methods. We provide convergence results for general nonconvex functions and functions that satisfy Riemannian Polyak–Łojasiewicz condition as well as for stochastic optimization settings. Additionally, we demonstrate how our approach can be generalized to quotient manifolds derived from the orthogonal manifold.  Extensive experiments verify the benefits of the proposed method, across a wide variety of problems.","In machine learning, we often encounter optimization problems where we need to find the best solution while satisfying orthogonality constraints. These constraints ensure that certain variables remain independent. However, as dimensionality grow larger, traditional methods for solving these problems become computationally expensive, making them impractical for large-scale applications.To address this, we propose a novel method based on Riemannian optimization, a mathematical framework that models the constraint set as a curved space called a manifold. Unlike standard approaches that require complex computations across the entire manifold, our technique reduces the workload by updating the iterates on randomly selected smaller sections, known as random submanifolds. This approach cuts down the computational complexity of each iteration significantly.We also provide theoretical guarantees of convergence, ensuring that our method reliably finds solutions. Extensive experiments confirm its effectiveness across diverse problems."
Poster,Efficient Parallel Training Methods for Spiking Neural Networks with Constant Time Complexity,https://ICML.cc//virtual/2025/poster/45776,"Wanjin Feng, Xingyu Gao, Wenqian Du, Hailong Shi, Peilin Zhao, Pengcheng Wu, Chunyan Miao","Spiking Neural Networks (SNNs) often suffer from high time complexity $O(T)$ due to the sequential processing of $T$ spikes, making training computationally expensive. In this paper, we propose a novel Fixed-point Parallel Training (FPT) method to accelerate SNN training without modifying the network architecture or introducing additional assumptions.FPT reduces the time complexity to $O(K)$, where $K$ is a small constant (usually $K=3$), by using a fixed-point iteration form of Leaky Integrate-and-Fire (LIF) neurons for all $T$ timesteps.We provide a theoretical convergence analysis of FPT and demonstrate that existing parallel spiking neurons can be viewed as special cases of our approach. Experimental results show that FPT effectively simulates the dynamics of original LIF neurons, significantly reducing computational time without sacrificing accuracy. This makes FPT a scalable and efficient solution for real-world applications, particularly for long-duration simulations.","Training brain-inspired neural networks—known as Spiking Neural Networks (SNNs)—has long been slow and computationally expensive. These models process information sequentially, like flipping through every frame of a long video, which makes training time-consuming.Our research presents a faster alternative. We developed a method called Fixed-Point Parallel Training (FPT) that replaces this frame-by-frame processing with a few carefully coordinated parallel passes. This significantly speeds up training without altering the model’s structure or requiring additional assumptions.FPT maintains the accuracy of conventional training methods while greatly reducing computational time. In our experiments, it proved especially effective for large-scale, time-intensive tasks, demonstrating its potential for real-world deployment.In short, FPT helps brain-like AI systems learn more quickly and efficiently—paving the way for practical, energy-saving applications of neural computing."
