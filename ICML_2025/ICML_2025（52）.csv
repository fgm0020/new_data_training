type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Commute Graph Neural Networks,https://ICML.cc//virtual/2025/poster/46600,"Wei Zhuo, Han Yu, Guang Tan, Xiaoxiao Li","Graph Neural Networks (GNNs) have shown remarkable success in learning from graph-structured data. However, their application to directed graphs (digraphs) presents unique challenges, primarily due to the inherent asymmetry in node relationships. Traditional GNNs are adept at capturing unidirectional relations but fall short in encoding the mutual path dependencies between nodes, such as asymmetrical shortest paths typically found in digraphs. Recognizing this gap, we introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly integrates node-wise commute time into the message passing scheme. The cornerstone of CGNN is an efficient method for computing commute time using a newly formulated digraph Laplacian. Commute time is then integrated into the neighborhood aggregation process, with neighbor contributions weighted according to their respective commute time to the central node in each layer. It enables CGNN to directly capture the mutual, asymmetric relationships in digraphs. Extensive experiments on 8 benchmarking datasets confirm the superiority of CGNN against 13 state-of-the-art methods.","Many GNNs treat directed graphs (digraphs) as collections of one-way edges, so they fail to capture the asymmetric round-trip connectivity that actually determines how strongly two nodes interact. This limitation is evident in social media, where a fan can instantly reach a celebrity, yet the return interaction rarely occurs.We introduce Commute Graph Neural Networks (CGNN) to explicitly model this asymmetry. CGNN leverages a novel digraph Laplacian (DiLap) coupled with lightweight, feature-based graph rewiring. This ensures sparsity and irreducibility, facilitating efficient computation of deterministic commute times, defined as the expected number of steps for a random walk from one node to another and back again. These commute times serve as weights for neighbor messages, allowing mutually reachable nodes to exert greater influence during aggregation.Commute time naturally captures realistic mutual interactions, such as follower-celebrity dynamics in social media or bidirectional web traffic, therefore, CGNN provides a more accurate, interpretable, and broadly applicable framework for learning from directed networks."
Poster,CommVQ: Commutative Vector Quantization for KV Cache Compression,https://ICML.cc//virtual/2025/poster/43828,"Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan","Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.","Large language models (LLMs), like those used in chatbots and document analysis, need to remember a lot of information as they read longer texts. This memory takes up a lot of space on our computers, making it hard to run these models efficiently. Our work introduces a new method, called CommVQ, that compresses this memory so it takes up much less space, without hurting the model’s performance. This allows large models to handle much longer texts on everyday computer hardware. Our approach makes these powerful models faster, cheaper, and more accessible for real-world use."
Poster,Compact Matrix Quantum Group Equivariant Neural Networks,https://ICML.cc//virtual/2025/poster/43997,Edward Pearce-Crump,"Group equivariant neural networks have proven effective in modelling a wide range of tasks where the data lives in a classical geometric space and exhibits well-defined group symmetries. However, these networks are not suitable for learning from data that lives in a non-commutative geometry, described formally by non-commutative $\mathcal{C}^{\ast}$-algebras, since the $\mathcal{C}^{\ast}$-algebra of continuous functions on a compact matrix group is commutative. To address this limitation, we derive the existence of a new type of equivariant neural network, called compact matrix quantum group equivariant neural networks, which encode symmetries that are described by compact matrix quantum groups. We characterise the weight matrices that appear in these neural networks for the easy compact matrix quantum groups, which are defined by set partitions. As a result, we obtain new characterisations of equivariant weight matrices for some compact matrix groups that have not appeared previously in the machine learning literature.","Many machine learning models improve their performance by encoding symmetries, which are typically described by groups, into their architectures. These models work well for data that lives in a classical geometric space but cannot be used to learn from data that lives in a non-commutative geometry since traditional group symmetries no longer apply.We introduce a new type of neural network that is designed to be equivariant to symmetries that are described by compact matrix quantum groups. These quantum groups generalise groups to model symmetries in certain non-commutative spaces. We prove the existence of these networks and precisely characterise the structure of their weight matrices for specific compact matrix quantum groups.Our approach makes it possible to learn from symmetries that have not been previously explored in machine learning, with potential applications in quantum physics and statistical mechanics."
Poster,Comparing Comparisons: Informative and Easy Human Feedback with Distinguishability Queries,https://ICML.cc//virtual/2025/poster/46047,"Xuening Feng, Zhaohui Jiang, Timo Kaufmann, Eyke Hüllermeier, Paul Weng, Yifei Zhu","Learning human objectives from preference feedback has significantly advanced reinforcement learning (RL) in domains where objectives are hard to formalize. However, traditional methods based on pairwise trajectory comparisons face notable challenges, including the difficulty in comparing trajectories with subtle differences and the limitation of conveying only ordinal information, limiting direct inference of preference strength. In this paper, we introduce a novel *distinguishability query*, enabling humans to express preference strength by comparing two pairs of trajectories. Labelers first indicate which of two pairs is easier to distinguish, then provide preference feedback only on the easier pair. Our proposed query type directly captures preference strength and is expected to reduce the cognitive load on the labeler. We further connect this query to cardinal utility and difference relations and develop an efficient query selection scheme to achieve a better trade-off between query informativeness and easiness. Experimental results demonstrate the potential of our method for faster, data-efficient learning and improved user-friendliness in RLHF benchmarks, particularly in classical control settings where preference strength is critical for expected utility maximization.","When training artificial intelligence (AI) systems like robots to behave the way people want, researchers often ask humans to pick their favorite between two short examples of AI behavior. But this approach has two major issues: it is difficult to compare similar behaviors, and it does not show how strongly someone prefers one over the other. This makes it harder for AI to learn effectively.We introduce a new kind of question, called a distinguishability query. Instead of comparing just one pair of behaviors, people are shown two comparisons and asked which is easier to judge. Then, they only give feedback on that easier one. This small change helps in two big ways: it gives the AI more insight into how strong a preference is, and it reduces the effort required from people.We test this method on tasks where robots learn to move or manipulate objects. Our system not only learns faster than previous methods, but also asks questions that are easier for people to answer. This approach brings us closer to building AI that can quickly and efficiently learn what humans truly value with less frustration for those providing feedback."
Poster,Comparing Few to Rank Many: Active Human Preference Learning Using Randomized Frank-Wolfe Method,https://ICML.cc//virtual/2025/poster/44684,"Kiran Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton","We study learning human preferences from limited comparison feedback, a core machine learning problem that is at the center of reinforcement learning from human feedback (RLHF). We formulate the problem as learning a Plackett-Luce (PL) model from a limited number of $K$-subset comparisons over a universe of $N$ items, where typically $K \ll N$. Our objective is to select the $K$-subsets such that all items can be ranked with minimal mistakes within the budget. We solve the problem using the D-optimal design, which minimizes the worst-case ranking loss under the estimated PL model. All known algorithms for this problem are computationally infeasible in our setting because we consider exponentially many subsets in $K$. To address this challenge, we propose a randomized Frank-Wolfe algorithm with memoization and sparse updates that has a low $O(N^2 + K^2)$ per-iteration complexity. We analyze it and demonstrate its empirical superiority on synthetic and open-source NLP datasets.","Take any collection of objects, such as movies, books, or music. Suppose that you want to learn the preference of a person over these objects, but you can ask them only to compare a smaller subset of the objects. How would you do that in the minimum number of questions? We propose, analyze, and empirically evaluate a method that computes these questions."
Poster,Compelling ReLU Networks to Exhibit Exponentially Many Linear Regions at Initialization and During Training,https://ICML.cc//virtual/2025/poster/46437,"Max Milkert, David Hyde, Forrest Laine","In a neural network with ReLU activations, the number of piecewise linear regions in the output can grow exponentially with depth.However, this is highly unlikely to happen when the initial parameters are sampled randomly, which therefore often leads to the use of networks that are unnecessarily large.To address this problem, we introduce a novel parameterization of the network that restricts its weights so that a depth $d$ network produces exactly $2^d$ linear regions at initialization and maintains those regions throughout training under the parameterization.This approach allows us to learn approximations of convex, one-dimensional functions that are several orders of magnitude more accurate than their randomly initialized counterparts.We further demonstrate a preliminary extension of our construction to multidimensional and non-convex functions, allowing the technique to replace traditional dense layers in various architectures.","An artificial neural network is often compared to a brain, where a brain has neurons and synapses, an artificial neural network has parameters - numbers that govern its behavior. It is common practice for these parameters to be set randomly, and then updated to maximize the network's performance on a task, as if the network is learning. Setting the parameters completely at random causes those located deeper in the network to be used inefficiently, and this is hard to correct through learning alone. The method we develop in this paper constrains parameter values both when they are initialized, and throughout the training process, guiding the network to a solution that uses deep parameters effectively.Extending these ideas will hopefully enable dramatic reductions in the size, energy, and computational cost of neural networks."
Poster,Competing Bandits in Matching Markets via Super Stability,https://ICML.cc//virtual/2025/poster/44153,Soumya Basu,"We study bandit learning in matching markets with two-sided reward uncertainty, extending prior research primarily focused on single-sided uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the standard GS algorithm in achieving true stable matchings under incomplete information. By employing the Extended GS algorithm, our centralized algorithm attains a logarithmic pessimal stable regret dependent on an instance-dependent admissible gap parameter.  This algorithm is further adapted to a decentralized setting with a constant regret increase.  Finally, we establish a novel centralized instance-dependent lower bound for binary stable regret, elucidating the roles of the admissible gap and super-stable matching in characterizing the complexity of stable matching with bandit feedback.","Imagine trying to set up fair and lasting partnerships, like matching people for jobs or dates online, where neither side initially knows exactly what they want or how good a match will truly be. This research presents a smarter way to find these ""stable"" partnerships, especially in complex scenarios where both sides are learning as they go. By using a more advanced matching algorithm this paper shows how systems can quickly learn to make excellent matches with very few mistakes over time, whether they operate through a central system or in a decentralized way. This work also sets a new limits for understanding how difficult it is to achieve truly stable partnerships when information is limited."
Poster,Competitively Consistent Clustering,https://ICML.cc//virtual/2025/poster/43596,"Niv Buchbinder, Roie Levin, Yue Yang","In *fully-dynamic consistent clustering*, we are given a finite metric space $(M,d)$, and a set $F\subseteq M$ of possible locations for opening centers. Data points arrive and depart, and the goal is to maintain an approximately optimal clustering solution at all times while minimizing the *recourse*, the total number of additions/deletions of centers over time. Specifically, we study fully dynamic versions of the classical $k$-center, facility location, and $k$-median problems. We design algorithms that, given a parameter $\beta\geq 1$, maintain an $O(\beta)$-approximate solution at all times, and whose total recourse is bounded by $O(\log |F| \log \Delta) \cdot OPT_{rec}^{\beta}$. Here $OPT_{rec}^{\beta}$ is the minimal recourse of an offline algorithm that maintains a $\beta$-approximate solution at all times, and $\Delta$ is the metric aspect ratio. We obtain our results via a reduction to the recently proposed *Positive Body Chasing* framework of [Bhattacharya Buchbinder Levin Saranurak, FOCS 2023], which we show gives fractional solutions to our clustering problems online. Our contribution is to round these fractional solutions while preserving the approximation and recourse guarantees. We complement our positive results with logarithmic lower bounds which show that our bounds are nearly tight.","Clustering is a basic primitive of data science where the goal is to summarize a dataset by a small number of representative points. We give new algorithms for clustering that are robust to perturbations of the data over time: in other words, if the data changes only slightly with time, then our data summaries change only slightly with time. Furthermore, we prove that for any dataset and any sequence of updates, our clustering is (almost) as stable as any clustering of the same quality. We are the first to obtain quality/stability tradeoffs of this form."
Poster,Complete-Tree Space Favors Data-Efficient Link Prediction,https://ICML.cc//virtual/2025/poster/46661,"Chi Gao, Lukai Li, Yancheng Zhou, Shangqi Guo","Link prediction is a fundamental problem for network-structured data. However, the prevalent research paradigm tends to assume abundant observed links, overlooking more challenging scenarios with a scarcity of observed links, which results in insufficient sample sizes. In real-world networks, hierarchical modularity, characterized by structured and nested connections, remains robust even with sparsely observed links. To address the challenge of limited link samples, we propose leveraging hierarchical modularity as a prior structure. We introduce complete-tree (CT) space, a discrete metric space with latent complete-tree structures, to formalize hierarchical modularity with an emphasis on its hierarchical permutation symmetry. Utilizing the group theory to quantize and compare permutation symmetries of different spaces, we prove that the CT space provides a significantly lower bound on sample complexity than the commonly used Euclidean space. We develop leaf matching, a data-efficient network embedding that maps nodes onto the CT space and conducts discrete optimization by reducing it to decentralized search. Experiments verify the data efficiency of CT space over other spaces. Moreover, leaf matching outperforms the state-of-the-art graph transformer in data-scarce scenarios while exhibiting excellent scalability. The code is available at: **https://github.com/KevinGao7/LeafMatching**.","Connections are everywhere. While predicting missing connections is a well-studied fundamental problem, we question whether such predictions are feasible when only a minority of connections is known.Recognizing that connections typically form a hierarchical structure, we conclude that it is indeed possible. Specifically, we develop the complete-tree space, which incorporates latent hierarchical structures. Our theoretical and experimental results demonstrate that once we map the known connections to this space, the prediction becomes straightforward. For example, a naively constructed network embedding in the complete-tree space can even outperform the state-of-the-art neural network in protein interaction prediction when only 2% connections are known.Given that hierarchy is a common property of real-world data, we foresee potential extensions of our complete-tree space to a wide range of domains, including natural language processing, visual modeling, hierarchical planning, or even the functional understanding of hierarchically arranged grid cells in the brain."
Poster,Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for Semantic Segmentation,https://ICML.cc//virtual/2025/poster/44329,Renhao Lu,"Recent advancements in deep neural networks have significantly enhanced the performance of semantic segmentation. However, class imbalance and instance imbalance remain persistent challenges, where smaller instances and thin boundaries are often overshadowed by larger structures. To address the multiscale nature of segmented objects, various models have incorporated mechanisms such as spatial attention and feature pyramid networks. Despite these advancements, most loss functions are still primarily pixel-wise, while regional and boundary-focused loss functions often incur high computational costs or are restricted to small-scale regions. To address this limitation, we propose the complex wavelet mutual information (CWMI) loss, a novel loss function that leverages mutual information from subband images decomposed by a complex steerable pyramid. The complex steerable pyramid captures features across multiple orientations and preserves structural similarity across scales. Meanwhile, mutual information is well-suited to capturing high-dimensional directional features and offers greater noise robustness. Extensive experiments on diverse segmentation datasets demonstrate that CWMI loss achieves significant improvements in both pixel-wise accuracy and topological metrics compared to state-of-the-art methods, while introducing minimal computational overhead. Our code is available at https://github.com/lurenhaothu/CWMI","Deep learning has transformed how computers understand images, with applications ranging from medical diagnosis to self-driving cars. A key task in this field is semantic segmentation—teaching a computer to label every pixel in an image, like identifying the boundaries of organs in a medical scan or roads in satellite images.Our research introduces a new mathematical tool called the Complex Wavelet Mutual Information (CWMI) loss, which helps train deep learning models to more accurately and robustly perform this task. CWMI works by comparing images at multiple levels of detail, similar to how a person might zoom in and out to understand both the big picture and fine details. It also uses a concept from information theory—mutual information—to measure how well the model’s predictions capture the meaningful parts of the image.We tested our method on several benchmark datasets and found that it improves both accuracy and reliability, especially around tricky areas like edges and thin structures. Importantly, it achieves this without adding significant computational cost.This work shows that combining ideas from signal processing and information theory can lead to smarter, more effective AI systems that better understand the visual world."
