type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Improving the Continuity of Goal-Achievement Ability via Policy Self-Regularization for Goal-Conditioned Reinforcement Learning,https://ICML.cc//virtual/2025/poster/43583,"Xudong Gong, Sen Yang, Feng Dawei, Kele Xu, Bo Ding, Huaimin Wang, Yong Dou","This paper addresses the challenge of discontinuity in goal-achievement capabilities observed in Goal-conditioned Reinforcement Learning (GCRL) algorithms. Through a theoretical analysis, we identify that the reuse of successful trajectories or policies during training can aid in achieving adjacent goals of achievable goals. However, the policy discrepancy between achievable and adjacent goals must be carefully managed to avoid both overly trivial and excessively large differences, which can respectively hinder policy performance. To tackle this issue, we propose a margin-based policy self-regularization approach that optimizes the policy discrepancies between adjacent desired goals to a minimal acceptable threshold. This method can be integrated into popular GCRL algorithms, such as GC-SAC, HER, and GC-PPO. Systematic evaluations across two robotic arm control tasks and a complex fixed-wing aircraft control task demonstrate that our approach significantly improves the continuity of goal-achievement abilities of GCRL algorithms, thereby enhancing their overall performance.","Training RL agents to achieve different goals, like turning a aircraft by specific angles, often leads to unexpected gaps in performance. For example, an agent might succeed at a 30-degree turn but fail at 30.1 degrees—even though the goals are nearly identical. This inconsistency suggests that current methods don’t fully leverage past successes to handle similar, slightly different goals.Our research identifies why this happens and introduces a simple but effective solution: we ensure the RL’s policy for one goal is smoothly adjusted for nearby goals, avoiding both overly rigid and overly drastic changes. This approach, called Margin-Based Policy Self-Regularization (MSR), improves performance across tasks like robotic arm control and aircraft maneuvering, making RL agents more reliable and adaptable.By integrating MSR into existing algorithms, we demonstrate consistent success rates and better overall performance, bridging the gaps that previously hindered goal-conditioned RL algorithms."
Poster,Improving the Diffusability of Autoencoders,https://ICML.cc//virtual/2025/poster/46572,"Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, Aliaksandr Siarohin","Latent diffusion models have emerged as the leading approach for generating high-quality images and videos, utilizing compressed latent representations to reduce the computational burden of the diffusion process. While recent advancements have primarily focused on scaling diffusion backbones and improving autoencoder reconstruction quality, the interaction between these components has received comparatively less attention. In this work, we perform a spectral analysis of modern autoencoders and identify inordinate high-frequency components in their latent spaces, which are especially pronounced in the autoencoders with a large bottleneck channel size. We hypothesize that this high-frequency component interferes with the coarse-to-fine nature of the diffusion synthesis process and hinders the generation quality. To mitigate the issue, we propose scale equivariance: a simple regularization strategy that aligns latent and RGB spaces across frequencies by enforcing scale equivariance in the decoder. It requires minimal code changes and only up to $20$K autoencoder fine-tuning steps, yet significantly improves generation quality, reducing FID by 19% for image generation on ImageNet-1K 256x256 and FVD by at least 44% for video generation on Kinetics-700 17x256x256. The source code is available at https://github.com/snap-research/diffusability.","In recent years, image 🖼️ and video 📹 generation models have rapidly advanced 🚀, with both industry 🏢 and academia 🎓 investing heavily 💸. Most of these models follow the latent diffusion 🌬️ approach: an autoencoder 🤖 first compresses images or videos into a smaller latent space 🌀, and then a diffusion model is trained 🏋️ to generate samples in that space 🧪.So far, most work has focused on improving 🔧 the autoencoder’s reconstruction quality 🔍 and compression rate 📦. But our work shows 💡 that the choice of autoencoder has a deeper effect—it shapes 🧩 how well a diffusion model can generate realistic outputs 🎨. We call this diffusability ✨: how easy 😌 it is for a diffusion model to learn 📚 to generate in a given representation space 📈.Diffusion models build 🧱 images by gradually refining noise 🌫️, starting from a blurry outline and adding details ✏️ step by step 🔄. This process tends to struggle 😖 with high-frequency details 📶 (like textures 🧵 or fine edges ✂️), where errors ❌ can accumulate. Normally, the human eye 👁️ is less sensitive 🧘♂️ to these errors in pixel space 🧷. But we found 🧠 that some autoencoders place more emphasis 📣 on high frequencies in their latent space—more than RGB images do 🌈. As a result ⚠️, critical image structures 🏗️ get encoded in unstable 💥 high-frequency components, making them harder 😵 for the diffusion model to learn and sample correctly 🎯.To address this 🛠️, we introduce a simple training technique 📏: during autoencoder training, we downsample ⬇️ the latent representation and require the decoder to still produce a meaningful reconstruction 🛠️➡️🖼️. This encourages 🙌 the autoencoder to store important information ℹ️ in more robust 💪, low-frequency components 🧊.We show 🧪 that this small change 🔧 leads to large improvements 📈. It makes latent spaces more suitable ✅ for diffusion models, improving both image 🖼️ and video 📹 generation quality 🎯 on benchmarks like ImageNet 🧠 and Kinetics 🏃♂️."
Poster,Improving the Effective Receptive Field of Message-Passing Neural Networks,https://ICML.cc//virtual/2025/poster/45345,"Shahaf E. Finder, Ron Shapira Weber, Moshe Eliasof, Oren Freifeld, Eran Treister","Message-Passing Neural Networks (MPNNs) have become a cornerstone for processing and analyzing graph-structured data. However, their effectiveness is often hindered by phenomena such as over-squashing, where long-range dependencies or interactions are inadequately captured and expressed in the MPNN output. This limitation mirrors the challenges of the Effective Receptive Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical receptive field is underutilized in practice. In this work, we show and theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired by recent advances in ERF augmentation for CNNs, we propose an Interleaved Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address these problems in MPNNs. Our method incorporates a hierarchical coarsening of the graph, enabling message-passing across multiscale representations and facilitating long-range interactions without excessive depth or parameterization. Through extensive evaluations on benchmarks such as the Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over baseline MPNNs in capturing long-range dependencies while maintaining computational efficiency.","Message-Passing Neural Networks (MPNNs) struggle to capture information from distant nodes due to a limited Effective Receptive Field (ERF), the region of the graph influencing a node’s representation. In this paper, we formalize the concept of ERF in GNNs and propose Interleaved Multiscale Message-Passing Neural Networks (IM-MPNNs). Our method processes graphs at multiple scales, efficiently expanding the ERF to improve long-range information integration without significantly increasing computational cost. Experiments show IM-MPNNs significantly outperform traditional GNNs in tasks requiring distant interactions, including molecular property prediction and image segmentation."
Poster,Improving the Scaling Laws of Synthetic Data with Deliberate Practice,https://ICML.cc//virtual/2025/poster/46689,"Reyhane Askari Hemmat, Mohammad Pezeshki, Elvis Dohmatob, Florian Bordes, Pietro Astolfi, Melissa Hall, Jakob Verbeek, Michal Drozdzal, Adriana Romero-Soriano","Inspired by the principle of deliberate practice in human learning, we propose Deliberate Practice for Synthetic Data Generation (DP), a novel framework that improves sample efficiency through dynamic synthetic data generation. Prior work has shown that scaling synthetic data is inherently challenging, as naively adding new data leads to diminishing returns. To address this, pruning has been identified as a key mechanism for improving scaling, enabling models to focus on the most informative synthetic samples. Rather than generating a large dataset and pruning it afterward, DP efficiently approximates the direct generation of informative samples. We theoretically show how training on challenging, informative examples improves scaling laws and empirically validate that DP achieves better scaling performance with significantly fewer training samples and iterations. On ImageNet-100, DP generates 3.4x fewer samples and requires six times fewer iterations, while on ImageNet-1k, it generates 8x fewer samples with a 30% reduction in iterations, all while achieving superior performance compared to prior work.","Modern machine learning models often rely on vast amounts of labeled data, which can be costly or unavailable. A promising alternative is to generate synthetic data using powerful generative models. However, not all synthetic data points are equally useful for training. Our paper introduces a method inspired by how humans learn more effectively from challenging tasks (the principle of deliberate practice). Instead of generating a large synthetic dataset all at once, we iteratively generate only informative and challenging examples based on the model's own uncertainty. This process helps the model to learn more efficiently. We show that this approach improves performance using less data, and we provide theoretical and empirical evidence supporting its benefits."
Poster,Improving the Statistical Efficiency of Cross-Conformal Prediction,https://ICML.cc//virtual/2025/poster/44129,"Matteo Gasparin, Aaditya Ramdas","Vovk (2015) introduced cross-conformal prediction, a modification of split conformal designed to improve the width of prediction sets. The method, when trained with a miscoverage rate equal to $\alpha$  and $n \gg K$, ensures a marginal coverage of at least $1 - 2\alpha - 2(1-\alpha)(K-1)/(n+K)$, where $n$ is the number of observations and $K$ denotes the number of folds. A simple modification of the method achieves coverage of at least $1-2\alpha$. In this work, we propose new variants of both methods that yield smaller prediction sets without compromising the latter theoretical guarantees. The proposed methods are based on recent results deriving more statistically efficient combination of p-values that leverage exchangeability and randomization. Simulations confirm the theoretical findings and bring out some important tradeoffs.","Imagine a tool that not only makes predictions but also tells you how confident it is in those predictions. Conformal prediction methods do exactly that. Instead of providing a single outcome, they return a set of possible values that is guaranteed to contain the correct answer with a specified level of confidence. This allows users to better understand the uncertainty behind a machine learning model’s output in a clear and reliable way.This research focuses on enhancing a particular version of this approach known as cross-conformal prediction. While this method already produces prediction intervals that meet the desired coverage --- meaning they include the true value with high probability --- it can sometimes be inefficient, resulting in prediction sets that are unnecessarily wide. We introduce new variants that preserve the same level of reliability while producing narrower and more informative sets. These improvements result from more efficient methods for combining p-values, which are a fundamental tool in statistical inference.In practical terms, these advancements make cross-conformal prediction more usable by providing tighter prediction sets. This can be especially valuable in real-world applications, such as industrial settings, where thousands of predictions are made each day and decision-making depends on accurate information."
Poster,Improving the Variance of Differentially Private Randomized Experiments through Clustering,https://ICML.cc//virtual/2025/poster/43512,"Adel Javanmard, Vahab Mirrokni, Jean Pouget-Abadie","Estimating causal effects from randomized experiments is only possible if participants are willing to disclose their potentially sensitive responses. Differential privacy, a widely used framework for ensuring an algorithm’s privacy guarantees, can encourage participants to share their responses without the risk of de-anonymization. However, many mechanisms achieve differential privacy by adding noise to the original dataset, which reduces the precision of causal effect estimation. This introduces a fundamental trade-off between privacy and variance when performing causal analyses on differentially private data.In this work, we propose a new differentially private mechanism, \textsc{Cluster-DP}, which leverages a given cluster structure in the data to improve the privacy-variance trade-off. While our results apply toany clustering, we demonstrate that selecting higher-quality clusters—according to a quality metric we introduce—can decrease the variance penalty without compromising privacy guarantees. Finally, we evaluate the theoretical and empirical performance of our \textsc{Cluster-DP} algorithm on both real and simulated data, comparing it to common baselines, including two special cases of our algorithm: its unclustered version and a uniform-prior version.","Our research addresses a significant challenge in contemporary online services: the necessity to experimentally assess user responses to new features or interventions while safeguarding the confidentiality of potentially sensitive outcome data. To resolve this issue, we introduce a novel framework for estimating causal effects using differentially private outcomes. In particular, in scenarios where users or items can be naturally clustered according to non-private attributes—such as product categories or demographic characteristics—our algorithm applies carefully calibrated statistical noise. For each individual, rather than using their true outcome, the method randomly substitutes it with an outcome sampled from the privatized distribution specific to their cluster. The statistical estimator we propose is shown to more accurately determine the causal effect of an intervention compared to alternative methods that do not leverage such structural information, while maintaining rigorous individual privacy standards."
Poster,Improving Transformer World Models for Data-Efficient RL,https://ICML.cc//virtual/2025/poster/45728,"Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, J Swaroop Guntupalli, Wolfgang Lehrach, Miguel Lazaro-Gredilla, Kevin Murphy","We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities---such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 69.66% after only 1M environment steps, significantly outperforming DreamerV3, which achieves $53.2\%$, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs.We then add three improvements to the standard MBRL setup: (a) ""Dyna with warmup"", which trains the policy on real and imaginary data, (b) ""nearest neighbor tokenizer"" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) ""block teacher forcing"", which allows the TWM to reason jointly about the future tokens of the next timestep.","Learning how to rapidly learn new skills from limited data is critical for building general AI systems. As a step towards this goal, we propose a new method for training AI agents for playing 2D video games, including Craftax-classic (a version of Crafter) and Minitar (a simplified version of Atari). Our method is the first to beat human performance on Craftax-classic using limited training data.  We achieve this by adding three key enhancements to existing reinforcement learning methods: a better way to preprocess images,  a new technique  to accurately predict the next frame of the game (as part of the agent's world model), and an improved method for learning the agent strategy  from both  real data (from the environment) combined with ""imaginary data"" (generated by the agent's model)."
Poster,Improving Value Estimation Critically Enhances Vanilla Policy Gradient,https://ICML.cc//virtual/2025/poster/45480,"Tao Wang, Ruipeng Zhang, Sicun Gao","Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla policy gradient in many RL tasks. Questioning the common belief that enforcing approximate trust regions leads to steady policy improvement in practice, we show that the more critical factor is the enhanced value estimation accuracy from more value update steps in each iteration. To demonstrate, we show that by simply increasing the number of value update steps per iteration, vanilla policy gradient itself can achieve performance comparable to or better than PPO in all the standard continuous control benchmark environments. Importantly, this simple change to vanilla policy gradient is significantly more robust to hyperparameter choices, opening up the possibility that RL algorithms may still become more effective and easier to use.","Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla policy gradient in many RL tasks. Questioning the common belief that enforcing approximate trust regions leads to steady policy improvement in practice, we show that the more critical factor is value estimation. To demonstrate this, we show that by simply increasing the number of value update steps per iteration, vanilla policy gradient itself can achieve performance comparable to or better than PPO in all the standard continuous control benchmark environments. We also show that vanilla policy gradient is significantly more robust to hyperparameter choices, has the potential to serve as a robust and effective alternative to PPO in various robot learning tasks."
Poster,Improving Your Model Ranking on Chatbot Arena by Vote Rigging,https://ICML.cc//virtual/2025/poster/46419,"Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin","Chatbot Arena is an open platform for evaluating LLMs by pairwise battles, in which users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be *rigged* to improve (or decrease) the ranking of a target model $m\_{t}$. We first introduce a straightforward **target-only rigging** strategy that focuses on new battles involving $m\_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m\_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about 1% of new battles will involve $m\_{t}$. To overcome this, we propose an **omnipresent rigging** strategy, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m\_{t}$, even if $m\_{t}$ is not directly involved in the battle. We conduct experiments on around *1.7 million* historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategy can improve model rankings by rigging only *hundreds of* new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. [**Code**](https://github.com/sail-sg/Rigging-ChatbotArena) is publicly available to reproduce all experiments.","Chatbot Arena is a popular leaderboard for large models, where users vote for their preferred response from two randomly sampled anonymous models. With millions of crowdsourced votes, Chatbot Arena is often regarded as the community's definitive leaderboard. However, is Chatbot Arena truly reliable?We systematically investigate this question, and our findings highlight that Chatbot Arena can be manipulated even with hundreds of rigged votes. We propose two rigging strategies: the target-only strategy and the omnipresent strategy, both aimed at improving our target model’s ranking. Notably, our omnipresent rigging can effectively use any new votes for ranking promotion, even if the target model is not directly sampled for voting. This increases the efficiency of our vote rigging while also making it more difficult to detect against various defense mechanisms. To support future research on this problem, we provide a general-purpose rigging framework and have open-sourced all our implementations. We hope our paper will spark broader discussions and encourage the community to focus on developing more robust defense mechanisms to mitigate the rigging vulnerabilities."
Poster,Improving Zero-Shot Adversarial Robustness in Vision-Language Models by Closed-form Alignment of Adversarial Path Simplices,https://ICML.cc//virtual/2025/poster/45018,"Junhao Dong, Piotr Koniusz, Yifei Zhang, Hao Zhu, Weiming Liu, Xinghua Qu, Yew Soon ONG","Vision-Language Models (VLMs) such as CLIP excel at zero-shot classification due to large-scale pre-training but are vulnerable to adversarial examples. Adversarial fine-tuning robustifies zero-shot models by aligning prediction scores of individual adversaries with their clean counterparts, which typically overlooks intermediate adversarial samples along the adversarial trajectory crossing the decision boundary. Such intermediate adversaries and their vicinity produce informative representations capturing the decision boundary in detail. They can be improved by sampling adversarial candidates from simplices formed by joining two consecutive vertices on the adversarial trajectory and their clean counterpart. However, sampling simplices for adversaries is very costly. To train robust VLM, we overcome these limitations by Taylor expansion and formulating an upper-bound of alignment loss that depends on the Jacobian/Hessian obtained at clean samples. As regions between clean and intermediate adversarial samples capture a larger decision landscape, we robustify VLM by plausible adversaries from simplices by our closed-form formulation equivalent to infinite uniform sampling of the simplex. We obtain state-of-the-art robustness across 15 datasets and diverse vision-language tasks.","Adversarial examples pose a security threat to AI systems by perturbing their decisions. To counteract this type of attack, we generate adversarial samples and encapsulate them into an adversarial simplex, which is used to robustify VLM against adversarial attacks."
