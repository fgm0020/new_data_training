type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Gradient-based Explanations for Deep Learning Survival Models,https://ICML.cc//virtual/2025/poster/45404,"Sophie Hanna Langbein, Niklas Koenen, Marvin N. Wright","Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their ""black box"" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics.","Deep learning survival models can help predict how long it takes for events (e.g., a disease) to occur, but their ""black box"" nature makes them hard to understand and trust. We developed new tools to explain how these models make time-based predictions for individual patients, showing which factors matter and how their influence changes over time. Our method, GradSHAP(t), gives faster and more accurate explanations than existing approaches. Tests on synthetic and real medical data show it helps to uncover important features and their time dynamics, making deep learning survival models more transparent and usable in healthcare."
Poster,Gradient Boosting Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45118,"Benjamin Fuhrer, Chen Tessler, Gal Dalal","We present Gradient Boosting Reinforcement Learning (GBRL), a framework that adapts the strengths of Gradient Boosting Trees (GBT) to reinforcement learning (RL) tasks. While neural networks (NNs) have become the de facto choice for RL, they face significant challenges with structured and categorical features and tend to generalize poorly to out-of-distribution samples. These are challenges for which GBTs have traditionally excelled in supervised learning. However, GBT's application in RL has been limited. The design of traditional GBT libraries is optimized for static datasets with fixed labels, making them incompatible with RL's dynamic nature, where both state distributions and reward signals evolve during training. GBRL overcomes this limitation by continuously interleaving tree construction with environment interaction. Through extensive experiments, we demonstrate that GBRL outperforms NNs in domains with structured observations and categorical features, while maintaining competitive performance on standard continuous control benchmarks. Like its supervised learning counterpart, GBRL demonstrates superior robustness to out-of-distribution samples and better handles irregular state-action relationships.","Imagine you're teaching a computer to make decisions by learning from trial and error, like learning to play a video game or manage a business. Most artificial intelligence systems today use neural networks, but these struggle with structured data that has clear patterns, like spreadsheets with categories and numbers. Decision trees, which work like ""if-then"" questions, excel at handling this type of data in traditional machine learning.Our work adapts decision trees for AI agents that learn through trial and error. Instead of neural networks, our system builds an ensemble of decision trees that collectively learn optimal strategies. Testing across various tasks, we found our approach matches neural networks in standard scenarios but significantly outperforms them with structured data. Most importantly, our method proved much more robust when faced with unexpected situations, noisy data, or misleading information—critical advantages for real-world applications where conditions are unpredictable."
Poster,Gradient Descent Converges Arbitrarily Fast for Logistic Regression via Large and Adaptive Stepsizes,https://ICML.cc//virtual/2025/poster/44660,"Ruiqi Zhang, Jingfeng Wu, Peter Bartlett","We analyze the convergence of gradient descent (GD) with large, adaptive stepsizes for logistic regression on linearly separable data. The stepsize adapts to the current risk, scaled by a fixed base stepsize \eta. We prove that once the number of iterates t surpasses a margin-dependent threshold, the averaged GD iterate achieves a risk upper bound of \exp(-\Theta(\eta t)), where \eta can be chosen arbitrarily large. This implies that GD attains \emph{arbitrarily fast} convergence rates via large stepsizes, although the risk evolution might not be monotonic. In contrast, prior adaptive stepsize GD analyses require a monotonic risk decrease, limiting their rates to \exp(-\Theta(t)). We further establish a margin-dependent lower bound on the iteration complexity for any first-order method to attain a small risk, justifying the necessity of the burn-in phase in our analysis. Our results generalize to a broad class of loss functions and two-layer networks under additional assumptions.","We study how gradient descent (GD) performs with large, adaptive stepsizes when training logistic regression models on linearly separable data. The stepsize changes based on the current loss, scaled by a fixed base stepsize \eta. We prove that after the number of iterations t exceeds a certain threshold, the averaged iterate from GD achieves a small error bounded by \exp(−\Theta(\eta t)) for an arbitrary \eta. This implies that GD attains \emph{arbitrarily fast} convergence rates via large and adaptive stepsizes. Additionally, we show that a minimum number of iterations is necessary for any first-order optimization method to reach a small risk. Our findings also extend to many other loss functions and two-layer neural networks under extra conditions."
Poster,Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs,https://ICML.cc//virtual/2025/poster/43554,"Hancheng Min, Rene Vidal","Deep learning-based classifiers are known to be vulnerable to adversarial attacks. Existing methods for defending against such attacks require adding a defense mechanism or modifying the learning procedure (e.g., by adding adversarial examples). This paper shows that for certain data distributions one can learn a provably robust classifier using standard learning methods and without adding a defense mechanism. More specifically, this paper addresses the problem of finding a robust classifier for a binary classification problem in which the data comes from an isotropic mixture of Gaussians with orthonormal cluster centers. First, we characterize the largest $\ell_2$-attack any classifier can defend against while maintaining high accuracy, and show the existence of optimal robust classifiers achieving this maximum $\ell_2$-robustness. Next, we show that given data from the orthonormal Gaussian mixture model, gradient flow on a two-layer network with a polynomial ReLU activation and without adversarial examples provably finds an optimal robust classifier.",Standard neural network training paradigm often produce networks that are susceptible to malicious attacks that try to manipulate the network outputs by injecting human imperceptible perturbations to the network inputs. We use an ideal mathematical model to explain this vulnerability of neural networks and provide insights into how we can address this issue.
Poster,Gradient Inversion of Multimodal Models,https://ICML.cc//virtual/2025/poster/44322,"Omri Ben Hemo, Alon Zolfi, Oryan Yehezkel, Omer Hofman, Roman Vainshtein, Hisashi Kojima, Yuval Elovici, Asaf Shabtai","Federated learning (FL) enables privacy-preserving distributed machine learning by sharing gradients instead of raw data. However, FL remains vulnerable to gradient inversion attacks, in which shared gradients can reveal sensitive training data. Prior research has mainly concentrated on unimodal tasks, particularly image classification, examining the reconstruction of single-modality data, and analyzing privacy vulnerabilities in these relatively simple scenarios. As multimodal models are increasingly used to address complex vision-language tasks, it becomes essential to assess the privacy risks inherent in these architectures. In this paper, we explore gradient inversion attacks targeting multimodal vision-language Document Visual Question Answering (DQA) models and propose GI-DQA, a novel method that reconstructs private document content from gradients. Through extensive evaluation on state-of-the-art DQA models, our approach exposes critical privacy vulnerabilities and highlights the urgent need for robust defenses to secure multimodal FL systems.","Federated learning is a method that lets many users train shared artificial intelligence models without revealing their private data. Instead of sending their data, users send updates to the model. This approach is designed to protect privacy, but there's a catch. It's possible for attackers to reverse-engineer those updates to uncover sensitive information. Most past research on this problem has looked at simple image-based systems. However, modern AI systems often handle more complex tasks, like answering questions about documents that include both text and images. In this work, we show that even these advanced systems are not safe. We introduce a new technique that can reconstruct private parts of a user’s document just from the updates they share. Our findings reveal serious privacy risks and show why stronger protections are needed for these more complex AI systems."
Poster,GradPS: Resolving Futile Neurons in Parameter Sharing Network for Multi-Agent Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45651,"Haoyuan Qin, Zhengzhu Liu, Chenxing Lin, Chennan Ma, Songzhu Mei, Siqi Shen, Cheng Wang","Parameter-sharing (PS) techniques have been widely adopted in cooperative Multi-Agent Reinforcement Learning (MARL). In PS, all the agents share a policy network with identical parameters, which enjoys good sample efficiency. However, PS could lead to homogeneous policies that limit MARL performance. We tackle this problem from the angle of gradient conflict among agents. We find that the existence of futile neurons whose update is canceled out by gradient conflicts among agents leads to poor learning efficiency and diversity. To address this deficiency, we propose GradPS, a gradient-based PS method. It dynamically creates multiple clones for each futile neuron. For each clone, a group of agents with low gradient-conflict shares the neuron's parameters.Our method can enjoy good sample efficiency by sharing the gradients among agents of the same clone neuron. Moreover, it can encourage diverse behaviors through independently updating an exclusive clone neuron. Through extensive experiments, we show that GradPS can learn diverse policies with promising performance. The source code for GradPS is available in \url{https://github.com/xmu-rl-3dv/GradPS}.","We studied how parameter sharing influences multiple AI agents working together. When all agents use the same network, it might limit their ability to develop different strategies.We analyzed the issue by examining how network updates (gradients) work. We found that some neurons become ""futile"" because different agents try to update them in conflicting ways.Our paper introduces GradPS, a new method that improves performance by fixing these ""futile"" neurons. This provides a fresh way to optimize other parameter-sharing techniques."
Poster,Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44800,"Motoki Omura, Kazuki Ota, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada","For continuous action spaces, actor-critic methods are widely used in online reinforcement learning (RL). However, unlike RL algorithms for discrete actions, which generally model the optimal value function using the Bellman optimality operator, RL algorithms for continuous actions typically model Q-values for the current policy using the Bellman operator. These algorithms for continuous actions rely exclusively on policy updates for improvement, which often results in low sample efficiency. This study examines the effectiveness of incorporating the Bellman optimality operator into actor-critic frameworks. Experiments in a simple environment show that modeling optimal values accelerates learning but leads to overestimation bias. To address this, we propose an annealing approach that gradually transitions from the Bellman optimality operator to the Bellman operator, thereby accelerating learning while mitigating bias. Our method, combined with TD3 and SAC, significantly outperforms existing approaches across various locomotion and manipulation tasks, demonstrating improved performance and robustness to hyperparameters related to optimality.","This study proposes a new approach to improve the learning efficiency of agents in environments with continuous action spaces, such as robotic control tasks. Traditional methods either accelerate learning at the risk of overestimating outcomes, or ensure stability but with slower progress. To balance these trade-offs, we introduce a method that gradually transitions from a fast-learning strategy to a more stable one over time. This is achieved using a technique called expectile loss, which enables a smooth interpolation between the two learning strategies. The proposed method, Annealed Q-learning, integrates seamlessly with existing reinforcement learning algorithms. Experimental results across a range of control tasks demonstrate that agents trained with this method learn more effectively than those using standard approaches. Overall, our method promotes faster and more reliable learning in continuous action domains"
Poster,GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code,https://ICML.cc//virtual/2025/poster/45469,"Samidha Verma, Arushi Goyal, Ananya Mathur, Ankit Anand, Sayan Ranu","Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a *program* that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.","Imagine you have a collection of known drug molecules and want to find which one is most similar to a newly discovered compound. To do this, scientists represent molecules as network diagrams (called graphs) where atoms are dots and chemical bonds are lines connecting them. They then measure how different two molecules are by counting the minimum number of changes needed to transform one molecular structure into another - this is called ""Graph Edit Distance."" The problem is that calculating this distance accurately is extremely time-consuming. For small molecules with 30-50 atoms, it can take hours or days. For larger molecules, it could take years using current computers. This makes it impractical for real-world drug discovery where scientists need to compare thousands of molecules quickly. Traditional computer methods tried to solve this by creating scoring tables that assign costs to different types of structural changes (like adding or removing atoms and bonds), then finding the lowest-cost way to transform one molecule into another. However, these methods weren't very accurate. More recent approaches used artificial neural networks, which are much more accurate but have a major drawback: they need expensive ""correct answer"" data to learn from. Since getting these correct answers requires the same slow calculations mentioned earlier, training these systems becomes extremely expensive and time-consuming. Our solution, called GRAIL, takes a completely different approach. Instead of training a neural network to predict the distance directly, we use Large Language Models (like GPT or Gemini) to write small computer programs that create better scoring tables. We then automatically select the best combination of these programs to minimize errors across our molecules. The key breakthrough is that GRAIL doesn't need those expensive ""correct answers"" for training. It can learn to make accurate predictions without requiring the time-consuming exact calculations that other methods depend on. This makes it both faster to develop and more practical for real-world use, while maintaining high accuracy comparable to neural network methods. This approach is particularly valuable for drug discovery, where researchers need to quickly identify promising molecular candidates from vast chemical databases. GRAIL can also be applied in areas like genetics, for comparing DNA structures and grouping similar viruses, as well as in computer science, for detecting similarities in source code to identify potential plagiarism."
Poster,GRAM: A Generative Foundation Reward Model for Reward Generalization,https://ICML.cc//virtual/2025/poster/43858,"Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, MuRun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu","In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data.  In this paper, we explore methods that train reward models using both unlabeled and labeled data.  Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning.  We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss.  This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives.  The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort.  Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.","Reward models are essential for aligning large language models (LLMs) with human preferences. Traditionally, these models are trained using labeled data, which can limit their potential. In this study, we propose a new method that combines both labeled and unlabeled data for training reward models. We introduce a generative reward model that first learns from a large amount of unlabeled data and is then fine-tuned with supervised data. Additionally, we demonstrate that using label smoothing during training improves performance by optimizing a regularized ranking loss. This approach bridges generative and discriminative models, offering a new perspective on training reward models. Our model can be easily applied to various tasks without the need for extensive fine-tuning. This means that when aligning LLMs, there is no longer a need to train a reward model from scratch with large amounts of task-specific labeled data. Instead, you can directly apply our reward model or adapt it to align your LLM."
Poster,Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs,https://ICML.cc//virtual/2025/poster/44027,"William English, Dominic Simon, Sumit Jha, Rickard Ewetz","Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a grounding of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate grounding, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the grounding and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.","Temporal logic is a mathematical language used to express or describe the behavior of a system over time. It isn't terribly easy for human beings to write complex temporal logic expressions, so we would benefit from technology that helps humans translate what they are saying in English (or whatever their own native language happens to be) into temporal logic. While some have used expensive large language models, such as Chat-GPT, we adopt a cheaper and more structured approach using a combination of two much smaller language models. We use BERT to simplify the input, that is a natural language expression, into what we call ""lifted"" natural language. This is just the same sentence, but with some groups of words replaces by small variable names. By using the lifted natural language, we can produce much more accurate temporal logic translations. Additionally, we are using a sequence-to-sequence model to perform translation, rather than a next-token-prediction or ""generative"" model. This means we can enforce rules about the entire output sequence, very efficiently. We hope that our contribution advances the field of natural language formalization."
