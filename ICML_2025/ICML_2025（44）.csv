type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Can Large Language Models Understand Intermediate Representations in Compilers?,https://ICML.cc//virtual/2025/poster/43488,"Hailong Jiang, Jianfeng Zhu, Yao Wan, Bo Fang, Hongyu Zhang, Ruoming Jin, Qiang Guan","Intermediate Representations (IRs) play a critical role in compiler design and program analysis, yet their comprehension by *Large Language Models* (LLMs) remains underexplored.  In this paper, we present an explorative empirical study evaluating the capabilities of six state-of-the-art LLMs—GPT-4, GPT-3, DeepSeek, Gemma 2, Llama 3, and Code Llama—in understanding IRs.  Specifically, we assess model performance across four core tasks: *control flow graph reconstruction*, *decompilation*, *code summarization*, and *execution reasoning*.  While LLMs exhibit competence in parsing IR syntax and identifying high-level structures, they consistently struggle with instruction-level reasoning, especially in control flow reasoning, loop handling, and dynamic execution.  Common failure modes include misinterpreting branching instructions, omitting critical operations, and relying on heuristic reasoning rather than on precise instruction-level logic.  Our findings highlight the need for IR-specific enhancements in LLM design. We recommend fine-tuning on structured IR datasets and integrating control-flow-sensitive architectures to improve the models’ effectiveness on IR-related tasks.  All the experimental data and source code are publicly available at [https://github.com/hjiang13/LLM4IR](https://github.com/hjiang13/LLM4IR).","Intermediate Representations (IRs) are a key component of modern compilers, enabling deep program optimization and analysis. At the same time, Large Language Models (LLMs) like have shown remarkable capabilities in understanding and generating high-level code. Motivated by this progress, we asked: **can these powerful models also understand IRs and be used for IR-level tasks?**To answer this, we designed four evaluation tasks — control flow reconstruction, IR decompilation, function summarization, and execution reasoning — and tested six leading LLMs on hundreds of real-world IR programs generated from C++ code.Our study reveals that while LLMs can handle surface-level syntax and recognize basic structures, they struggle with instruction-level reasoning, control flow logic, and program simulation. Models often rely on pattern matching instead of truly understanding the IR semantics.This matters because it highlights a key gap in applying LLMs to compiler technologies and low-level software analysis. By identifying where current models fall short, we offer guidance for future development, such as IR-specific training, structural prompting, and new evaluation strategies. Our work takes an important first step toward building trustworthy, LLM-powered tools that can assist with the low-level foundations of modern software systems."
Poster,CAN: Leveraging Clients As Navigators for Generative Replay in Federated Continual Learning,https://ICML.cc//virtual/2025/poster/44183,"Xuankun Rong, Jianshu Zhang, Kun He, Mang Ye","Generative replay (GR) has been extensively validated in continual learning as a mechanism to synthesize data and replay past knowledge to mitigate forgetting. By leveraging synthetic rather than real data for the replay, GR has been adopted in some federated continual learning (FCL) approaches to ensure the privacy of client-side data. While existing GR-based FCL approaches have introduced improvements, none of their enhancements specifically take into account the unique characteristics of federated learning settings. Beyond privacy constraints, what other fundamental aspects of federated learning should be explored in the context of FCL?In this work, we explore the potential benefits that come from emphasizing the role of clients throughout the process.We begin by highlighting two key observations: (a) Client Expertise Superiority, where clients, rather than the server, act as domain experts, and (b) Client Forgetting Variance, where heterogeneous data distributions across clients lead to varying levels of forgetting.Building on these insights, we propose CAN (Clients As Navigators), highlighting the pivotal role of clients in both data synthesis and data replay. Extensive evaluations demonstrate that this client-centric approach achieves state-of-the-art performance. Notably, it requires a smaller buffer size, reducing storage overhead and enhancing computational efficiency.","Many real-world AI applications, like personalized assistants or healthcare devices, collect data from different users over time. One key problem is that AI models can forget earlier knowledge when learning from new data, much like a person forgetting old lessons when cramming for a new test. Our research proposes a new way to tackle this issue by making better use of the devices (or “clients”) that hold the data. Instead of relying only on a central server, our method called CAN (Clients As Navigators) treats each client as a local expert. These clients help generate training examples and decide which past knowledge should be reviewed, based on what they are likely to forget. This results in better learning for each client while keeping their data private. Our method helps AI systems remember better, even when the data is very different across users. It also reduces memory and computational costs, making it more practical for everyday use."
Poster,Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark,https://ICML.cc//virtual/2025/poster/43702,"Yunzhuo Hao, Jiawei Gu, Huichen Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Yu Cheng","The ability to organically reason over and with both text and images is a pillar of human intelligence, yet the ability of Multimodal Large Language Models (MLLMs) to perform such multimodal reasoning remains under-explored. Existing benchmarks often emphasize text-dominant reasoning or rely on shallow visual cues, failing to adequately assess integrated visual and textual reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark targeting organic multimodal reasoning across mathematics, physics, chemistry, and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be addressed by reasoning independently in each modality, offering an enhanced test suite for MLLMs' reasoning capabilities. Our evaluation of state-of-the-art MLLMs on EMMA reveals significant limitations in handling complex multimodal and multi-step reasoning tasks, even with advanced techniques like Chain-of-Thought prompting and test-time compute scaling underperforming. These findings underscore the need for improved multimodal architectures and training paradigms to close the gap between human and model reasoning in multimodality.","Understanding the world through both words and images is a key part of human intelligence. However, today’s powerful AI models still struggle to reason effectively when they need to combine information from both text and images. Many existing tests only check how these models handle mostly text-based tasks or simple image recognition, missing the kind of deep, natural reasoning people perform when integrating text and visuals.To address this, we created a new benchmark called EMMA that challenges AI systems to solve complex problems in subjects like math, physics, chemistry, and coding using both images and text together. These problems are designed to require real cross-modal reasoning—something current models can’t just solve by looking at text and images separately.When we tested some of the best available AI models on EMMA, we found that they often failed at these harder, mixed-format tasks, even when using the latest reasoning techniques. Our results highlight the need to build better AI systems that can think more like humans when combining visual and written information.You can learn more at: https://emma-benchmark.github.io/"
Poster,Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs,https://ICML.cc//virtual/2025/poster/46387,"Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang","Despite the remarkable performance of Large Language Models (\textbf{LLMs}), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, \textbf{ICRT}, inspired by heuristics and biases in human cognition. Leveraging the \textit{simplicity effect}, we employ \textit{cognitive decomposition} to reduce the complexity of malicious prompts. Simultaneously, \textit{relevance bias} is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream \textbf{LLMs}' safety mechanisms and generates high-risk content.","Large language models remain vulnerable to jailbreak attacks that can bypass their safety mechanisms. To address this, we propose the ICRT framework, which draws on human cognitive biases—specifically the simplicity effect and relevance bias—to decompose malicious prompts into simpler components and then reorganize them for stronger semantic alignment, making it easier for the model to accept harmful instructions. In addition, we design a ranking-based harmfulness evaluation method that moves beyond a simple success-or-failure metric by using ranking aggregation algorithms (such as Elo, HodgeRank, and Rank Centrality) to quantify the severity of generated content. Experimental results demonstrate that ICRT consistently evades the safety filters of mainstream models and produces high-risk content."
Poster,Canonical Rank Adaptation: An Efficient Fine-Tuning Strategy for Vision Transformers,https://ICML.cc//virtual/2025/poster/43662,"Lokesh Veeramacheneni, Moritz Wolter, Hilde Kuehne, Juergen Gall","Modern methods for fine-tuning a Vision Transformer (ViT) like Low-Rank Adaptation (LoRA) and its variants demonstrate impressive performance. However, these methods ignore the high-dimensional nature of Multi-Head Attention (MHA) weight tensors. To address this limitation, we propose Canonical Rank Adaptation (CaRA). CaRA leverages tensor mathematics, first by tensorising the transformer into two different tensors; one for projection layers in MHA and the other for feed-forward layers. Second, the tensorised formulation is fine-tuned using the low-rank adaptation in Canonical-Polyadic Decomposition (CPD) form. Employing CaRA efficiently minimizes the number of trainable parameters. Experimentally, CaRA outperforms existing Parameter-Efficient Fine-Tuning (PEFT) methods in visual classification  benchmarks such as Visual Task Adaptation Benchmark (VTAB)-1k and Fine-Grained Visual Categorization (FGVC).","To fine-tune the Vision Transformer (ViT), retraining the entire network has proven to be resource-intensive regarding computations and memory. To address this limitation, a method called LoRA introduced a more efficient approach by training only low-rank parts of the model. While resource-effective, its design limits the effectiveness in performance to two-dimensional matrices. It doesn’t fully capture the complex, multi-dimensional nature of ViT. The multi-dimensional nature of ViT stems from the parallel attention computation blocks in the design. This work proposes Canonical Rank Adaptation (CaRA) to cater to the higher-dimensional nature of ViT. It first tensorises the attention and feed-forward layers in ViT into two multi-dimensional tensors. Then, CaRA fine-tunes them using Canonical Polyadic Decomposition (CPD), enabling low-rank updates across multiple dimensions. Experiments show that CaRA outperforms existing fine-tuning methods, including LoRA, on challenging benchmarks like the Visual Task Adaptation Benchmark-1k and Fine-Grained Vision Classification. Interestingly, it does so with significantly fewer trainable parameters, further helping to reduce the environmental cost of adapting vision models."
Poster,Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective,https://ICML.cc//virtual/2025/poster/46257,"Jiawei Huang, Bingcong Li, Christoph Dann, Niao He","Sample efficiency is critical for online Reinforcement Learning from Human Feedback (RLHF). While existing works investigate sample-efficient online exploration strategies, the potential of utilizing misspecified yet relevant reward models to accelerate learning remains underexplored. This paper studies how to transfer knowledge from those imperfect reward models in online RLHF. We start by identifying a novel property due to KL-regularization in the RLHF objective: \emph{a policy's coverability of the optimal policy is captured by its sub-optimality}. Building on this insight, we propose novel transfer learning principles and a theoretical algorithm---**T**ransfer **P**olicy **O**ptimization (**TPO**)---with provable benefits compared to standard online learning. Empirically, inspired by our theoretical findings, we develop a win-rate-based transfer policy selection strategy with improved computational efficiency. Moreover, our empirical transfer learning technique is modular and can be integrated with various policy optimization methods, such as DPO, IPO and XPO, to further enhance their performance. We validate the effectiveness of our method through experiments on summarization tasks.","Reinforcement Learning from Human Feedback (RLHF) is a key step in fine-tuning large language models (LLMs), but collecting human feedback is expensive. This makes improving sample efficiency—learning from fewer annotations—an essential goal.While most works focus on better exploration or modeling techniques, we take a different approach: **can we speed up learning by transferring knowledge from any reward models available, even if they’re imperfect**? We introduce **Transfer Policy Optimization (TPO)**, an algorithm with novel transfer learning strategies and provable benefits. Inspired by our theoretical findings, we also propose an empirical version of TPO, a scalable algorithm template that can leverage even flawed reward models to reduce the need for human feedback.Our work highlights an under-explored direction in RLHF: extracting and making use of information from imperfect signals to improve learning efficiency. This opens new possibilities for faster, cheaper, and more flexible training of LLMs."
Poster,Can Transformers Learn Full Bayesian Inference in Context?,https://ICML.cc//virtual/2025/poster/46240,"Arik Reuter, Tim G. J. Rudner, Vincent Fortuin, David Rügamer","Transformers have emerged as the dominant architecture in the field of deep learning, with a broad range of applications and remarkable in-context learning (ICL) capabilities. While not yet fully understood, ICL has already proved to be an intriguing phenomenon, allowing transformers to learn in context—without requiring further training. In this paper, we further advance the understanding of ICL by demonstrating that transformers can perform full Bayesian inference for commonly used statistical models in context. More specifically, we introduce a general framework that builds on ideas from prior fitted networks and continuous normalizing flows and enables us to infer complex posterior distributions for models such as generalized linear models and latent factor models. Extensive experiments on real-world datasets demonstrate that our ICL approach yields posterior samples that are similar in quality to state-of-the-art MCMC or variational inference methods that do not operate in context. The source code for this paper is available at https://github.com/ArikReuter/ICL_for_Full_Bayesian_Inference","Large Language Models (LLMs), such as the one behind ChatGPT, have become widely used and commercially successful. A key reason for their success is their ability to perform in-context learning (ICL): given only a few examples or instructions in the input, they can solve complex tasks without needing to change their internal parameters. In this work, we explore whether the abstract principle of ICL—learning directly from context can also be applied to a very different challenge: performing full Bayesian inference, a core task in statistics and machine learning. Traditionally, full Bayesian inference requires either very costly computations or relies on approximations that may compromise accuracy. We show that for three widely used statistical models, an ICL-based approach can achieve results comparable to expensive, exact methods while outperforming commonly used approximations. In summary, our results validate that ICL is a meaningful principle for full Bayesian inference and might therefore become a general and promising approach for solving difficult inference problems in science and engineering."
Poster,Can Transformers Reason Logically? A Study in SAT Solving,https://ICML.cc//virtual/2025/poster/46444,"Leyan Pan, Vijay Ganesh, Jacob Abernethy, Chris Esposo, Wenke Lee","We formally study the logical reasoning capabilities of decoder-only Transformers in the context of the boolean satisfiability (SAT) problem. First, we prove by construction that decoder-only Transformers can decide 3-SAT, in a non-uniform model of computation, using backtracking and deduction via Chain-of-Thought (CoT).Second, we implement our construction as a PyTorch model with a tool (PARAT) that we designed to empirically demonstrate its correctness and investigate its properties.Third, rather than \textit{programming} a transformer to reason, we evaluate empirically whether it can be \textit{trained} to do so by learning directly from algorithmic traces (``reasoning paths'') from our theoretical construction. The trained models demonstrate strong out-of-distribution generalization on problem sizes seen during training but has limited length generalization, which is consistent with the implications of our theoretical result.","Can Large Language Models Really “Think” Logically?Problem. Transformer language models can write fluent text, yet we still don’t know if they can follow a formal line of reasoning. That gap matters whenever we want AI systems that explain or verify their own steps.Approach. We focused on a classic logic puzzle called 3-SAT, where one must set every variable true / false so the whole formula holds. First, we proved (on paper) that a decoder-only Transformer with built-in “chain-of-thought’’ prompts can solve any 3-SAT instance up to a chosen size by guessing, deducing, and backtracking—exactly like a human doing trial-and-error. Next, we compiled those proof constructions into real model weights with our tool PARAT, and the resulting network solved every test case. Finally, we asked a standard Transformer to learn that reasoning just from example traces.Discovery. The trained model handled fresh logical puzzles of the same length, confirming that Transformers possess the inherent capabilites for logical deduction, but performs unreliably for larger puzzles. Closing that “length-generalization’’ gap is the next step toward safer, provably reliable AI."
Poster,Can We Predict Performance of Large Models across Vision-Language Tasks?,https://ICML.cc//virtual/2025/poster/46185,"Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould","Evaluating large vision-language models (LVLMs) is very expensive, due to high computational cost and the wide variety of tasks. The good news is that if we already have some observed performance scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix $\boldsymbol{R}$, where each entry $R_{mn}$ represents the performance score of the $m$-th model on the $n$-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, i.e., predict unknown scores. Additionally, we estimate the uncertainty of performance prediction based on MCMC. Practitioners can evaluate their models on untested tasks with higher uncertainty first, which quickly reduces the prediction errors. We further introduce several improvements to enhance PMF for scenarios with sparse observed performance scores. Our experiments demonstrate the accuracy of PMF in predicting unknown scores, the reliability of uncertainty estimates in ordering evaluations, and the effectiveness of our enhancements for handling sparse data. Our code is available at https://github.com/Qinyu-Allen-Zhao/CrossPred-LVLM.","Evaluating how well large vision-language models (LVLMs) perform on a wide range of tasks, like answering questions about images or describing scenes, can be extremely expensive. Each test on these large-scale models requires time, money, and computing resources. But do we really need to test every model-task pair? In our study, we show that if we already know how a model performs on some tasks, we can predict how it might perform on others, using a mathematical technique called probabilistic matrix factorization. Even better, our method can also estimate how confident it is in its predictions. For example, if our method is uncertain about GPT-4's performance on 3D understanding but confident about LLaVA's performance on object recognition, we can prioritize evaluating GPT-4 on the 3D task when our resources are limited. We hope our framework can help to develop and improve LVLMs more efficiently. You can explore our code here: https://github.com/Qinyu-Allen-Zhao/CrossPred-LVLM."
Poster,Cape: Context-Aware Prompt Perturbation Mechanism with Differential Privacy,https://ICML.cc//virtual/2025/poster/44690,"Haoqi Wu, Wei Dai, Wang Li, Qiang Yan","Large Language Models (LLMs) have gained significant popularity due to their remarkable capabilities in text understanding and generation. However, despite their widespread deployment in inference services such as ChatGPT, concerns about the potential leakage of sensitive user data have arisen. Existing solutions primarily rely on privacy-enhancing technologies to mitigate such risks, facing the trade-off among efficiency, privacy, and utility. To narrow this gap, we propose Cape, a context-aware prompt perturbation mechanism based on differential privacy, to enable efficient inference with an improved privacy-utility trade-off. Concretely, we introduce a hybrid utility function that better captures the token similarity. Additionally, we propose a bucketized sampling mechanism to handle large sampling space, which might lead to long-tail phenomenons. Extensive experiments across multiple datasets, along with ablation studies, demonstrate that Cape achieves a better privacy-utility trade-off compared to prior state-of-the-art works.","Despite the widespread deployment of large language models (LLMs)  in inference services such as ChatGPT, concerns about the potential leakage of sensitive user data in prompts have arisen. We want to answer the question ""Can we guarantee prompt privacy with a good privacy-utility trade-off?""To safeguard sensitive user information during inference, we leverage differential privacy (DP), which provides quantifiable privacy protection. We investigated how incorporating contextual information can improve the utility of DP-based perturbation. In addition, we examined the long-tail distribution problem prevalent in large-vocabulary settings. Interestingly, partitioning the vocabulary (i.e., the sampling space) into multiple buckets significantly alleviates this issue.The technique introduced can be integrated into LLM serving stacks, offering immediate improvements to prompt-level privacy with minimal changes to existing APIs or service infrastructure. As a result, it brings private LLM inference closer to practical, scalable deployment in real-world applications."
