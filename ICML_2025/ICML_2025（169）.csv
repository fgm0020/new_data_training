type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,LEMoN: Label Error Detection using Multimodal Neighbors,https://ICML.cc//virtual/2025/poster/44525,"Haoran Zhang, Aparna Balagopalan, Nassim Oufattole, Hyewon Jeong, Yan Wu, Jiacheng Zhu, Marzyeh Ghassemi","Large repositories of image-caption pairs are essential for the development of vision-language models. However, these datasets are often extracted from noisy data scraped from the web, and contain many mislabeled instances. In order to improve the reliability of downstream models, it is important to identify and filter images with incorrect captions. However, beyond filtering based on image-caption embedding similarity, no prior works have proposed other methods to filter noisy multimodal data, or concretely assessed the impact of noisy captioning data on downstream training. In this work, we propose, theoretically justify, and empirically validate LEMoN, a method to identify label errors in image-caption datasets. Our method leverages the multimodal neighborhood of image-caption pairs in the latent space of contrastively pretrained multimodal models to automatically identify label errors. Through empirical evaluations across eight datasets and twelve baselines, we find that LEMoN outperforms the baselines by over 3% in label error detection, and that training on datasets filtered using our method improves downstream captioning performance by more than 2 BLEU points over noisy training.","Billions of image-caption pairs scraped from the web fuel today’s vision‑language AI. However, many of these samples are wrong, and the captions do not match the accompanying image. Models trained on these mislabeled instances may then underperform or learn harmful associations, which is especially worrying in fields like medicine. Our work introduces LEMoN, a method for automatically detecting these mismatched pairs. It looks not only at how well each image matches its own caption, but also how both compare with their closest ""neighbors"" in a shared vision‑language space. If the neighbors disagree, the pair is probably mislabeled. We test our method on eight diverse datasets, from everyday photos to chest‑X‑rays, and find that LEMoN does better at detecting mislabeled examples than the best prior tools.By helping researchers clean their data, or surface suspicious samples for expert review, LEMoN paves the way for more reliable and trustworthy AI systems."
Poster,LensLLM: Unveiling Fine-Tuning Dynamics for LLM Selection,https://ICML.cc//virtual/2025/poster/44045,"Xinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, Dawei Zhou","The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: *how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks?* In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a *PAC-Bayesian Generalization Bound* that unveils fine-tuning dynamics of LLMs and then introduce *LensLLM*, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed *LensLLM* model and corresponding results at [LensLLM.io](https://github.com/Susan571/LENSLLM.git).","Today, many powerful AI language models are freely available to the public. These models can answer questions, summarize texts, and translate languages—but not all models perform equally well on every task. Choosing the right model for the job is tricky, especially since testing all of them thoroughly can be extremely time-consuming and expensive.Our work introduces a new tool, called LensLLM, that helps researchers and engineers quickly and accurately choose the best model for their needs—without having to test each one exhaustively. We found that language models go through two stages as they learn: a slow early stage and a faster, more predictable stage once enough training data is seen. Using this insight, we built a mathematical model that predicts how well a language model will do on a task with much less computation.Our tool was tested on several large benchmarks and outperformed five leading methods. Not only does LensLLM make smarter choices, but it also cuts down the computational cost by up to 88%. This can make developing AI tools faster, cheaper, and more accessible to everyone."
Poster,Less is More: Federated Graph Learning with Alleviating Topology Heterogeneity from A Causal Perspective,https://ICML.cc//virtual/2025/poster/43607,"Lele Fu, Bowen Deng, Sheng Huang, Tianchi Liao, Shirui Pan, Chuan Chen","Federated graph learning (FGL) aims to collaboratively train a global graph neural network (GNN) on multiple private graphs with preserving the local data privacy. Besides the common cases of data heterogeneity in conventional federated learning, FGL faces the unique challenge of topology heterogeneity. Most of existing FGL methods alleviate the negative impact of heterogeneity by introducing global signals.However, the manners of creating increments might not be effective and significantly increase the computation amount. In light of this, we propose the FedATH, an FGL method with Alleviating Topology Heterogeneity from a causal perspective. Inspired by the causal theory, we argue that not all edges in a topology are necessary for the training objective, less topology information might make more sense.With the aid of edge evaluator, the local graphs are divided into causal and biased subgraphs. A dual-GNN architecture is used to encode the two subgraphs into corresponding representations. Thus, the causal representations are drawn closer to the training objective while the biased representations are pulled away from it. Further, the Hilbert-Schmidt Independence Criterion is employed to strengthen the separability of the two subgraphs. Extensive experiments on six real-world graph datasets are conducted to demonstrate the superiority of the proposed FedATH over the compared approaches.","Currently, many organizations, like banks and companies, have their own network data, such as user connections or transaction links. But because of privacy concerns, they can’t just share this data with others. Our research focuses on how multiple organizations can work together to train a smart system that learns from their network data without ever sharing the raw data itself. This is difficult because each organization’s data is structured differently, like having different kinds of connections or network shapes. Most current solutions try to smooth out these differences, but that often requires a lot of computing power and doesn't always work well. We propose a new method called FedATH. The key idea is: not every connection in a network is useful, some may actually be distracting. Then, we break each local network into two parts: one with meaningful connections and one with less helpful or biased ones. We then treat them differently during training. Our system also includes a way to make sure these two parts stay separate, which helps the model focus on what really matters. We tested our method on six real-world datasets, and it consistently outperformed other approaches, showing it’s both more effective and more efficient."
Poster,Let LLM Tell What to Prune and How Much to Prune,https://ICML.cc//virtual/2025/poster/43485,"Mingzhe Yang, Sihao Lin, Changlin Li, Xiaojun Chang","Large language models (LLMs) have revolutionized various AI applications. However, their billions of parameters pose significant challenges for practical deployment. Structured pruning is a hardware-friendly compression technique and receives widespread attention. Nonetheless, existing literature typically targets a single structure of LLMs. We observe that the structure units of LLMs differ in terms of inference cost and functionality. Therefore, pruning a single structure unit in isolation often results in an imbalance between performance and efficiency. In addition, previous works mainly employ a prescribed pruning ratio. Since the significance of LLM modules may vary, it is ideal to distribute the pruning load to a specific structure unit according to its role within LLMs. To address the two issues, we propose a pruning method that targets multiple LLM modules with dynamic pruning ratios. Specifically, we find the intrinsic properties of LLMs can guide us to determine the importance of each module and thus distribute the pruning load on demand, i.e., what to prune and how much to prune. This is achieved by quantifying the complex interactions within LLMs. Extensive experiments on multiple benchmarks and LLM variants demonstrate that our method effectively balances the trade-off between efficiency and performance.","Large language models (LLMs) have revolutionized various AI applications, but their billions of parameters pose significant challenges for practical deployment. A common solution is to prune unimportant parts of the LLM to reduce its size and improve efficiency. However, most existing pruning methods focus on a single structure of LLMs, which can upset the balance between performance and efficiency. We observe that different modules of an LLM serve different roles. In particular, we find the intrinsic  properties of LLMs can help us determine the importance of each module and accordingly allocate pruning ratios more effectively. This is achieved by quantifying the complex interactions within LLMs. Therefore, in this work, we propose a pruning method that targets multiple LLM modules, with pruning ratios dynamically assigned according to the relative importance of each module. Extensive experiments demonstrate that our method effectively achieves a better trade-off between efficiency and performance."
Poster,LETS Forecast: Learning Embedology for Time Series Forecasting,https://ICML.cc//virtual/2025/poster/45595,"Abrar Majeedi, Viswanatha Reddy Gajjala, Satya Sai Srinath Namburi GNVV, Nada Elkordi, Yin Li","Real-world time series are often governed by complex nonlinear dynamics. Understanding these underlying dynamics is crucial for precise future prediction. While deep learning has achieved major success in time series forecasting, many existing approaches do not explicitly model the dynamics. To bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear dynamical systems modeling with deep neural networks. Inspired by empirical dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel deep model that learns a latent space from time-delayed embeddings, and employs kernel regression to approximate the underlying dynamics, while leveraging efficient implementation of softmax attention and allowing for accurate prediction of future time steps. To evaluate our method, we conduct comprehensive experiments on synthetic data of nonlinear dynamical systems as well as real-world time series across domains. Our results show that DeepEDM is robust to input noise, and outperforms state-of-the-art methods in forecasting accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.","Many real world time series, like stock prices, weather, or traffic, are shaped by complex systems with hidden variables. For example, while we can observe the price of a stock, we do not directly see the many interacting forces behind it, such as investor sentiment, global events, or economic signals. Most deep learning models try to predict future values by spotting patterns in the observed data, without understanding the system that generates it. Our method, DeepEDM, takes a different approach. Inspired by a technique called Empirical Dynamic Modeling, it reconstructs the hidden dynamics by using time delayed snapshots of the data and learning how the system evolves over time. By combining this with modern deep learning, DeepEDM makes accurate predictions even when the data is noisy or limited. It not only forecasts more reliably but also captures the underlying structure of the system, leading to better and more interpretable results on both synthetic and real world datasets."
Poster,Leveraging Diffusion Model as Pseudo-Anomalous Graph Generator for Graph-Level Anomaly Detection,https://ICML.cc//virtual/2025/poster/44832,"Jinyu Cai, Yunhe Zhang, Fusheng Liu, See-Kiong Ng","A fundamental challenge in graph-level anomaly detection (GLAD) is the scarcity of anomalous graph data, as the training dataset typically contains only normal graphs or very few anomalies. This imbalance hinders the development of robust detection models. In this paper, we propose **A**nomalous **G**raph **Diff**usion (AGDiff), a framework that explores the potential of diffusion models in generating pseudo-anomalous graphs for GLAD. Unlike existing diffusion-based methods that focus on modeling data normality, AGDiff leverages the latent diffusion framework to incorporate subtle perturbations into graph representations, thereby generating pseudo-anomalous graphs that closely resemble normal ones. By jointly training a classifier to distinguish these generated graph anomalies from normal graphs, AGDiff learns more discriminative decision boundaries. The shift from solely modeling normality to explicitly generating and learning from pseudo graph anomalies enables AGDiff to effectively identify complex anomalous patterns that other approaches might overlook. Comprehensive experimental results demonstrate that the proposed AGDiff significantly outperforms several state-of-the-art GLAD baselines.","Graph anomaly detection aims to identify graph-structured individuals that deviate from common patterns observed in normal cases. However, this task is particularly challenging due to the scarcity or even complete absence of anomalous examples in the training set. For instance, in early-stage cancer, a few abnormal cells may be hidden among many healthy ones and show no symptoms, yet early detection is vital for effective treatment. Hence, this paper introduces a method that leverages diffusion models to generate synthetic graph anomalies. Rather than learning only from normal data, it generates slightly perturbed graph structures that resemble anomalies and trains a model to distinguish them from normal graphs. This approach enhances the model’s ability to detect subtle and complex anomalies."
Poster,Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models,https://ICML.cc//virtual/2025/poster/45788,"Xiaoyu Wu, Jiaru Zhang, Steven Wu","Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small set of images to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the potential risks of data leakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning.  In this paper, we ask: ""Can training data be extracted from these fine-tuned DMs shared online?"" A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data.  Our method approximates fine-tuning as a gradual shift in the model's learned distribution---from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets such as WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting approximately 20% of fine-tuning data in most cases, significantly surpassing baseline performance. The code is available.","Diffusion Models (DMs) are powerful tools for generating images. Many people use these models to create personalized art by fine-tuning them on a small set of images, such as pictures of specific objects or artistic styles. They often share these fine-tuned models online through platforms like Civitai or HuggingFace, but few realize this could unintentionally leak the images used for fine-tuning. This raises privacy concerns and risks of copyright violations if the original data wasn’t meant to be shared.In this paper, we explore whether it’s possible to extract the original training data from these publicly shared fine-tuned models. We introduce a method called FineXtract, which works by guiding the generation process using the differences between the fine-tuned model and the original model. This helps us recover images that likely resemble the original fine-tuning data. Our approach can recover around 20% of the original fine-tuning images in many cases, raising important questions about privacy and copyright in generative AI."
Poster,Leveraging Offline Data in Linear Latent Contextual Bandits,https://ICML.cc//virtual/2025/poster/44306,"Chinmaya Kausik, Kevin Tan, Ambuj Tewari","Leveraging offline data is an attractive way to accelerate online sequential decision-making. However, it is crucial to account for latent states in users or environments in the offline data, and latent bandits form a compelling model for doing so. In this light, we design end-to-end latent bandit algorithms capable of handing uncountably many latent states. We focus on a linear latent contextual bandit &mdash; a linear bandit where each user has its own high-dimensional reward parameter in $\mathbb{R}^{d_A}$, but reward parameters across users lie in a low-rank latent subspace of dimension $d_K \ll d_A$. First, we provide an offline algorithm to learn this subspace with provable guarantees. We then present two online algorithms that utilize the output of this offline algorithm to accelerate online learning. The first enjoys $\tilde O(\min(d_A\sqrt{T}, d_K\sqrt{T}(1+\sqrt{d_AT/d_KN})))$ regret guarantees, so that the effective dimension is lower when the size $N$ of the offline dataset is larger. We prove a matching lower bound on regret, showing that our algorithm is minimax optimal. The second is a practical algorithm that enjoys only a slightly weaker guarantee, but is computationally efficient. We also establish the efficacy of our methods using experiments on both synthetic data and real-life movie recommendation data from MovieLens. Finally, we theoretically establish the generality of the latent bandit model by proving a de Finetti theorem for stateless decision processes.","Many real-world systems—like recommendation engines or clinical decision aids—learn better when they can combine past data with new interactions. But when past data comes from a mix of different types of users or conditions, this can confuse standard learning methods. Our work addresses this by designing algorithms that can handle these hidden differences. Specifically, we focus on settings where each user behaves differently, but these differences lie in a shared low-dimensional structure. First, we show how to use existing pre-collected data to uncover this shared structure, even when there are infinitely many user types. Then, we introduce two new learning algorithms that use this knowledge to improve decision-making with new users. One algorithm is provably optimal, and the other runs faster and is more practical. We test these methods on synthetic data and real movie recommendation data and show strong improvements. Finally, we prove that this framework of hidden differences, or latent structure, captures a larger and more general notion of reasonable models of decision-making without memory -- highlighting its generality for future applications."
Poster,Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation,https://ICML.cc//virtual/2025/poster/44681,"Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, Renjie Liao","Advances in Large Language Models (LLMs) have sparked interest in their ability to solve Olympiad-level math problems. However, the training and evaluation of these models are constrained by the limited size and quality of available datasets, as creating large-scale data for such advanced problems requires extensive effort from human experts.In addition, current benchmarks are prone to contamination, leading to unreliable evaluations.In this paper, we present an automated pipeline that leverages the rich resources of the Art of Problem Solving (AoPS) forum, which predominantly features Olympiad-level problems and community-driven solutions.Using open-source LLMs, we develop a method to extract question-answer pairs from the forum, resulting in **AoPS-Instruct**, a dataset of more than 600,000 high-quality QA pairs.Our experiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their reasoning abilities across various benchmarks. Moreover, we build an automatic pipeline that introduces **LiveAoPSBench**, an evolving evaluation set with timestamps, derived from the latest forum data, providing a contamination-resistant benchmark for assessing LLM performance.Notably, we observe a significant decline in LLM performance over time, suggesting their success on older examples may stem from pre-training exposure rather than true reasoning ability. Our work presents a scalable approach to creating and maintaining large-scale, high-quality datasets for advanced math reasoning, offering valuable insights into the capabilities and limitations of LLMs in this domain.","Most existing LLMs struggle with advanced math problems because there is very little high‑quality training data for Olympiad‑level questions, and existing benchmarks often include problems the models have already seen during pre‑training, making evaluations unreliable. To address this, we built an automated pipeline that mines the Art of Problem Solving forum for genuine competition‑level problems and community‑provided solutions, then uses open‑source LLMs to extract and clean more than 600,000 question–answer pairs, creating the AoPS‑Instruct dataset. We also developed LiveAoPSBench, an evolving evaluation set drawn from the latest forum posts, which filters out any overlap with earlier data to avoid contamination. By fine‑tuning various LLMs on AoPS‑Instruct, we observed marked improvements in their ability to solve challenging math problems. Furthermore, tracking performance over time on LiveAoPSBench revealed that many models perform worse on newer questions, indicating that past successes often stemmed from having seen similar problems during pre‑training rather than genuine reasoning skills. This work offers a scalable way to generate and maintain large, reliable datasets for advanced mathematical reasoning, helping researchers better understand and push the true capabilities of LLMs in this domain."
Poster,Leveraging Per-Instance Privacy for Machine Unlearning,https://ICML.cc//virtual/2025/poster/46697,"Naz Sepahvand, Anvith Thudi, Berivan Isik, Ashmita Bhattacharyya, Nicolas Papernot, Eleni Triantafillou, Daniel Roy, Gintare Karolina Dziugaite","We present a principled, per-instance approach to quantifying the difficulty of unlearning via fine-tuning. We begin by sharpening an analysis of noisy gradient descent for unlearning (Chien et al., 2024), obtaining a better utility–unlearning trade-off by replacing worst-case privacy loss bounds with per-instance privacy losses (Thudi et al., 2024), each of which bounds the (R ´enyi) divergence to retraining without an individual datapoint. To demonstrate the practical applicability of our theory, we present empirical results showing that our theoretical predictions are born out both for Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard fine-tuning without explicit noise. We further demonstrate that per-instance privacy losses correlate well with several existing data difficulty metrics, while alsoidentifying harder groups of data points, and introduce novel evaluation methods based on loss barriers. All together, our findings provide a foundation for more efficient and adaptive unlearning strategies tailored to the unique properties of individual data points.","In scenarios including following legislation, or corrupted training data, a model trainer is required to ""forget"" some part of their training dataset. We make the connection that a metric derived from statistics collected during training can be predictive of how hard it will be to forget a datapoint. Theoretically we prove that this metric provides an upper bound on how many steps of gradient descent are required to forget a datapoint. Empirically we find across training setups, this metric accurately ranks datapoints by how many gradient descent steps they require to be forgotten. Moreover, we find our proposed metrics discovers harder to forget datapoints, compared to past approaches to identifying difficult data points."
