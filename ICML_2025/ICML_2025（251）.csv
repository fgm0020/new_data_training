type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Reliable Algorithm Selection for Machine Learning-Guided Design,https://ICML.cc//virtual/2025/poster/44164,"Clara Fannjiang, Ji Won Park","Algorithms for machine learning-guided design, or *design algorithms*, use machine learning-based predictions to propose novel objects with desired property values. Given a new design task—for example, to design novel proteins with high binding affinity to a therapeutic target—one must choose a design algorithm and specify any hyperparameters and predictive and/or generative models involved. How can these decisions be made such that the resulting designs are successful? This paper proposes a method for *design algorithm selection*, which aims to select design algorithms that will produce a distribution of design labels satisfying a user-specified success criterion—for example, that at least ten percent of designs’ labels exceed a threshold. It does so by combining designs’ predicted property values with held-out labeled data to reliably forecast characteristics of the label distributions produced by different design algorithms, building upon techniques from prediction-powered inference (Angelopoulos et al., 2023). The method is guaranteed with high probability to return design algorithms that yield successful label distributions (or the null set if none exist), if the density ratios between the design and labeled data distributions are known. We demonstrate the method’s effectiveness in simulated protein and RNA design tasks, in settings with either known or estimated density ratios.","New molecules, biological sequences, and materials believed to have useful properties are being computationally designed by algorithms. Given the cost of validating these new objects, or *designs*, in the laboratory, how can scientists reliably choose algorithms that will produce successful designs?We propose a method to help scientists select algorithms that are guaranteed to generate successful pools of designs (or indicate if no algorithm can do so). To assess whether the designs produced by an algorithm will be successful, one could imagine getting predictions of how the designs will behave from a machine-learning system. However, predictions about designs can be fraught with errors, because designs can behave quite differently from the data that such systems are trained on. To overcome this, our approach uses additional data to characterize how prediction error distorts our assessments of whether designs are successful. It then uses statistical tools to undo this distortion and reliably forecast which algorithms will produce successful designs.Our work offers a framework for how to design novel molecules, biological sequences, or materials in a way that is guided by machine-learning systems, yet not led astray by their errors. As such, it can help scientists increase the probability of success in ambitious design projects."
Poster,Reliable and Efficient Amortized Model-based Evaluation,https://ICML.cc//virtual/2025/poster/45790,"Sang Truong, Yuheng Tu, Percy Liang, Bo Li, Sanmi Koyejo","Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models are thought to possess numerous capabilities as well as safety risks. The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 183 LMs show that this approach is more reliable and efficient compared to the current common practice.","Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models are thought to possess numerous capabilities as well as safety risks. The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 183 LMs show that this approach is more reliable and efficient compared to the current common practice."
Poster,Rényi Neural Processes,https://ICML.cc//virtual/2025/poster/43943,"Xuesong Wang, He Zhao, Edwin V. Bonilla","Neural Processes (NPs) are deep probabilistic models that represent stochastic processes by conditioning their prior distributions on a set of context points. Despite their advantages in uncertainty estimation for complex distributions,  NPs enforce parameterization coupling between the conditional prior model and the posterior model. We show that this coupling amounts to prior misspecification and revisit the NP objective to address this issue.  More specifically, we propose Rényi Neural Processes (RNP), a method that replaces the standard KL divergence with the Rényi divergence, dampening the effects of the misspecified prior during posterior updates. We validate our approach across multiple benchmarks including regression and image inpainting tasks, and show significant performance improvements of RNPs in real-world problems. Our extensive experiments show consistently better log-likelihoods over state-of-the-art NP models.","How to predict the probability of a function? We wanted to answer this question using a deep learning method called Renyi neural processes. The general idea is to encode the function with a prior model that is conditioned on some observations and later decode that information for predictions.We identified a problem of prior misspecification in existing neural processes where the prior model can be overconfident due to some parameter coupling mechanism. In light of this, we proposed a new objective using Renyi divergence that mitigates the effects of a misspecified prior. We show consistent likelihood improvements over tasks including time series regression and image inpainting.Our method has implications in predictive distributions for functions, which covers a wide range of tasks in vision, language such as super resolution generation and missing feature imputation. Our findings can also be beneficial for transfer learning such as fine tuning tasks where the prior knowledge might be overconfident."
Poster,RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers,https://ICML.cc//virtual/2025/poster/44361,"Xuwei Xu, Yang Li, Yudong Chen, Jiajun LIU, Sen Wang","We reveal that feedforward network (FFN) layers, rather than attention layers, are the primary contributors to Vision Transformer (ViT) inference latency, with their impact signifying as model size increases. This finding highlights a critical opportunity for optimizing the efficiency of large-scale ViTs by focusing on FFN layers. In this work, we propose a novel channel idle mechanism that facilitates post-training structural reparameterization for efficient FFN layers during testing. Specifically, a set of feature channels remains idle and bypasses the nonlinear activation function in each FFN layer, thereby forming a linear pathway that enables structural reparameterization during inference. This mechanism results in a family of **RePa**rameterizable **Vi**sion **T**ransformers (RePaViTs), which achieve remarkable latency reductions with acceptable sacrifices (sometimes gains) in accuracy across various ViTs. The benefits of our method scale consistently with model sizes, demonstrating greater speed improvements and progressively narrowing accuracy gaps or even higher accuracies on larger models. In particular, RePa-ViT-Large and RePa-ViT-Huge enjoy **66.8%** and **68.7%** speed-ups with **+1.7%** and **+1.1%** higher top-1 accuracies under the same training strategy, respectively. RePaViT is the first to employ structural reparameterization on FFN layers to expedite ViTs to our best knowledge, and we believe that it represents an auspicious direction for efficient ViTs. Source code is available at https://github.com/Ackesnal/RePaViT.","Vision Transformers (ViTs) are powerful AI models used for image recognition and form the backbone of many computer vision tasks. However, as ViTs become larger to achieve higher accuracy, they also become slower and more computationally demanding. This makes them difficult to deploy on resource-limited devices like drones, mobile phones, or embedded systems. As a result, improving the efficiency of existing ViT models has become a key focus in AI research.While many researchers have blamed the attention mechanism for causing most of the slowdown in ViTs, our study reveals that the feedforward network (FFN) layers are actually the main bottleneck, especially in large models. To address this, we propose a new method called RePaViT (short for ReParameterizable Vision Transformers), which lets some parts of the model skip nonlinear activation calculation in FFN layers. The linear pathway in our design facilitates structural reparameterization of FFN layers during inference, making ViT models smaller and significantly faster.Experiments show that RePaViT can reduce processing time by nearly 70% for large models, while maintaining or even improving accuracy. This is the first time such a technique has been applied to the FFN layers in ViTs, and we believe it opens up exciting possibilities for building faster, smarter AI systems."
Poster,RepLoRA: Reparameterizing Low-rank Adaptation via the Perspective of Mixture of Experts,https://ICML.cc//virtual/2025/poster/45214,"Tuan Truong, Chau Nguyen, Huy Nguyen, Minh Le, Trung Le, Nhat Ho","Low-rank Adaptation (LoRA) has emerged as a powerful and efficient method for fine-tuning large-scale foundation models. Despite its popularity, the theoretical understanding of LoRA has remained underexplored. In this paper, we present a theoretical analysis of LoRA by examining its connection to the Mixture of Experts models. Under this framework, we show that a simple technique, reparameterizing LoRA matrices, can notably accelerate the low-rank matrix estimation process. In particular, we prove that reparameterization can reduce the data needed to achieve a desired estimation error from an exponential to a polynomial scale. Motivated by this insight, we propose **Rep**arameterized **Lo**w-**R**ank **A**daptation (RepLoRA), incorporating a lightweight MLP to reparameterize the LoRA matrices. Extensive experiments across multiple domains demonstrate that RepLoRA consistently outperforms vanilla LoRA. With limited data, RepLoRA surpasses LoRA by a substantial margin of up to **40.0%** and achieves LoRA's performance using only **30.0%** of the training data, highlighting the theoretical and empirical robustness of our PEFT method.","Fine-tuning large AI models for specific tasks can be resource-intensive. Low-Rank Adaptation (LoRA) offers a solution by adjusting only a small portion of the model's parameters, making the process more efficient. However, the theoretical understanding of LoRA has been limited.In this paper, we developed a more efficient parameter-efficient fine-tuning (PEFT) method that builds upon Low-rank Adaptation (LoRA). To do so, we first provide a theoretical analysis of LoRA's sample efficiency from the perspective of a Mixture of Experts (MoE). This perspective reveals that reparameterizing LoRA matrices—essentially making the low-rank matrices the outputs of two small non-linear MLPs instead of optimizing them directly—can significantly accelerate the low-rank matrix estimation process. Specifically, we demonstrate that this reparameterization reduces the amount of data needed to achieve a certain level of estimation error from an exponential to a polynomial scale.Building on this insight, we introduce RepLoRA (Reparameterized Low-Rank Adaptation), which incorporates lightweight neural networks to reparameterize the LoRA matrices. Our extensive experiments across various domains show that RepLoRA consistently outperforms the standard LoRA approach. Notably, with limited data, RepLoRA achieves up to 40% better performance and matches LoRA's results using only 30% of the training data. Hence, this work not only provides a deeper theoretical understanding of LoRA but also offers a practical method to make AI fine-tuning more data-efficient and effective."
Poster,RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing,https://ICML.cc//virtual/2025/poster/45170,"Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang","Code auditing is the process of reviewing code with the aim of identifying bugs. Large Language Models (LLMs) have demonstrated promising capabilities for this task without requiring compilation, while also supporting user-friendly customization. However, auditing a code repository with LLMs poses significant challenges: limited context windows and hallucinations can degrade the quality of bug reports, and analyzing large-scale repositories incurs substantial time and token costs, hindering efficiency and scalability.This work introduces an LLM-based agent, RepoAudit, designed to perform autonomous repository-level code auditing. Equipped with agent memory, RepoAudit explores the codebase on demand by analyzing data-flow facts along feasible program paths within individual functions. It further incorporates a validator module to mitigate hallucinations by verifying data-flow facts and checking the satisfiability of path conditions associated with potential bugs, thereby reducing false positives. RepoAudit detects 40 true bugs across 15 real-world benchmark projects with a precision of 78.43%, requiring on average only 0.44 hours and $2.54 per project. Also, it detects 185 new bugs in high-profile projects, among which 174 have been confirmed or fixed. We have open-sourced RepoAudit at https://github.com/PurCL/RepoAudit.","Modern software often contains subtle bugs that can crash systems or create security vulnerabilities. Manually reviewing millions of lines of code is difficult, and current AI tools often struggle with large files or generate false alarms.We developed RepoAudit, an AI assistant that analyzes codebases by tracking how values move through different execution paths. It builds a memory of these paths and uses that context to ask, “Could this value cause a problem?” Before reporting a potential issue, it runs a built-in check to reduce false positives.In evaluations on 15 widely used open-source projects, RepoAudit identified 40 real bugs with nearly 80% precision, at an average cost of less than a cup of coffee per project. On larger, high-profile codebases, it reported 185 new issues, 174 of which have already been confirmed or fixed by developers. By catching issues early, RepoAudit helps save time and reduce the cost and risk of bugs. It's free, open-source, and ready to use."
Poster,Representation Preserving Multiclass Agnostic to Realizable Reduction,https://ICML.cc//virtual/2025/poster/43531,"Steve Hanneke, Qinglin Meng, Amirreza Shaeiri","We study the problem of multiclass classification when the number of labels can be unbounded within the PAC learning framework. Our main contribution is a theory that demonstrates a *simple* and *elegant* agnostic to realizable reduction for this framework. This resolves an open problem raised by the recent work of (Hopkins et al., 2022). Notably, our result is the first *representation preserving* multiclass agnostic to realizable reduction, in contrast with the compression based approach of the work of (David et al., 2017). Furthermore, our main theorem is stated in an abstract framework, called ``Unified PAC Learning'', which encompasses a range of frameworks, including multiclass PAC learning, list PAC learning, and multilabel PAC learning. In addition, we explore representation preserving reductions to the realizable setting for two noise models, namely Massart noise and Tsybakov noise, in the multiclass PAC learning framework. We believe our technique may find other applications in ensuing studies of theoretical machine learning.","We theoretically study the problem of multiclass classification when the number of labels can be unbounded. This setting is well-motivated both theoretically and empirically. For instance, from a practical standpoint, many critical machine learning tasks require classification into very large label spaces. Our main contribution is a theory that demonstrates a *simple* and *elegant* reduction, showing that a provable guarantee in a setup with a distributional assumption implies a provable guarantee in a setup without that assumption. This resolves an open problem raised by the recent work of Hopkins et al. (2022). Notably, our result is the first *representation preserving* reduction, meaning that if the theoretical result in the more restrictive setup has properties such as geometric margin, the theoretical result in the general setup retains them as well."
Poster,Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing,https://ICML.cc//virtual/2025/poster/46119,"Kento Nishi, Rahul Ramesh, Maya Okawa, Mikail Khona, Hidenori Tanaka, Ekdeep Singh Lubana","Knowledge Editing (KE) algorithms alter models' weights to perform targeted updates to incorrect, outdated, or otherwise unwanted factual associations. However, recent work has shown that applying KE can adversely affect models' broader factual recall accuracy and diminish their reasoning abilities. Although these studies give insights into the potential harms of KE algorithms, e.g., performance evaluations on benchmarks, little is understood about why such destructive failures occur. Motivated by this, we define a novel synthetic task in which a Transformer is trained from scratch to internalize a ""structured"" knowledge graph. The structure enforces relationships between entities of the graph, such that editing a factual association has ""trickling effects"" on other entities (e.g., altering X's parent is Y to Z affects who X's siblings' parent is). Through evaluations of edited models on this task, we show that KE inadvertently affects representations of entities beyond the targeted one, distorting relevant structures that allow a model to infer unseen knowledge about an entity. We call this phenomenon representation shattering and demonstrate that it degrades models' factual recall and reasoning performance. We further corroborate our findings in naturalistic settings with pre-trained Llama and Mamba models as well. Overall, our work yields a precise mechanistic hypothesis to explain why KE has adverse effects on model abilities.","When a large language model is patched after training to alter some memorized factual knowledge, the change often degrades its broader knowledge and reasoning capabilities. In the past, the cause behind this phenomenon has been unclear. We study this by building a small synthetic world of linked facts, training a Transformer, and then editing facts while tracking how the network's internal representations change.We show that knowledge editing systematically fractures the neat geometric structure that stores information. We call this ""Representation Shattering."" We show that the degree of shattering predicts how much the model's overall accuracy drops, and we verify the same effect in real models such as Llama-3 and Mamba.By revealing this hidden failure mode, our work offers a practical warning signal for risky edits and a direction for gentler, more reliable knowledge-updating techniques. Understanding and mitigating representation shattering will help future language models stay accurate, consistent, and trustworthy as they are regularly updated."
Poster,Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions,https://ICML.cc//virtual/2025/poster/43511,"Yihao Xue, Jiping Li, Baharan Mirzasoleiman","Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one,  serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs.","As AI continues to improve, a key question is how weaker models—or even humans—can guide the learning of more powerful systems. Surprisingly, recent research has shown that a strong AI model can learn from a weaker one and still outperform it, a phenomenon known as Weak-to-Strong Generalization (W2SG). However, we still don’t fully understand how or when this works.Our research offers a theoretical explanation for W2SG by analyzing how AI models internally represent information. We show that weak and strong models have distinct ways of representing data, and we derive a mathematical quantity that captures what the strong model can learn that the weak one cannot. This quantity determines how much performance gain is possible under W2SG. Our theory also explains when and how the strong model can correct the weak model’s errors—even when it is trained directly on those mistakes.This insight has practical value: it allows us to estimate W2SG performance without requiring labeled data. We validate our approach on real-world tasks in chemistry and natural language processing, using 52 different language models. Our work could help make future AI systems more reliable and better aligned with human intent."
Poster,Representation Surgery in Model Merging with Probabilistic Modeling,https://ICML.cc//virtual/2025/poster/44821,"Qi Wei, Shuo He, Enneng Yang, Tingcong Liu, Haobo Wang, Lei Feng, Bo An","Model merging aims to achieve multitask performance by merging multiple expert models without the need to access the raw training data.Recent research identified the \textit{representation bias} of model merging, characterized by a discrepancy in the representation distribution between the merged and individual models, hindering the performance of model merging methods. To mitigate the representation bias, a task-specific MLP, Surgery, was built to model the bias that is subsequently decreased on the merged representation. However, this strategy is still suboptimal due to the limited modeling capability within the deterministic manner. To address this issue, we present ProbSurgery, a probabilistic module specifically designed to accurately model the representation bias.This module generates an embedding distribution for each sample and outputs the representation bias through a sampling process.ProbSurgery offers superior representational capacity by naturally handling the uncertainty resulting from parameter interference of merging multiple models. Besides, we provide a theoretical analysis to reveal the advance of the probabilistic manner and propose an extension of ProSurgery for adapting to the task-sharing setting. Extensive experiments verify the effectiveness of ProbSurgery for representation surgery while maintaining generalization capabilities in real-world scenarios, including out-of-distribution and domain shift challenges.","When we merge multiple machine learning models to solve several tasks at once, we usually hope to get the benefits of all the individual models — without needing the original data used to train them. But this process often runs into a hidden issue: the combined model may ""see"" the world differently than the originals, due to something called representation bias. This bias causes the merged model’s internal understanding (its “representations”) to shift in a way that harms performance.Previous work tried to fix this using a simple correction layer, but its effectiveness was limited because it treated the bias in a rigid, fixed way. Our new method, ProbSurgery, tackles the problem differently: it treats the bias from uncertainty and models it using probabilities. This allows our method to better handle the messiness that arises when many models are combined.We also provide a mathematical explanation of why this probabilistic method works better, and we show that it can be extended to more collaborative learning settings. Experiments confirm that ProbSurgery consistently improves merged models across a range of real-world tasks."
