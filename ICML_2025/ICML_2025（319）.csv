type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Understanding the Unfairness in Network Quantization,https://ICML.cc//virtual/2025/poster/43689,"Bing Liu, wenjun Miao, Boyu Zhang, Qiankun Zhang, Bin Yuan, Wang, Shenghao Liu, Xianjun Deng","Network quantization, one of the most widely studied model compression methods, effectively quantizes a floating-point model to obtain a fixed-point one with negligible accuracy loss. Although great success was achieved in reducing the model size, it may exacerbate the unfairness in model accuracy across different groups of datasets.This paper considers two widely used algorithms: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), with an attempt to understand how they cause this critical issue.Theoretical analysis with empirical verifications reveals two responsible factors, as well as how they influence a metric of fairness in depth.A comparison between PTQ and QAT is then made, explaining an observation that QAT behaves even worse than PTQ in fairness, although it often preserves a higher accuracy at lower bit-widths in quantization.Finally, the paper finds out that several simple data augmentation methods can be adopted to alleviate the disparate impacts of quantization, based on a further observation that class imbalance produces distinct values of the aforementioned factors among different attribute classes. We experiment on either imbalanced (UTK-Face and FER2013) or balanced (CIFAR-10 and MNIST) datasets using ResNet and VGG models for empirical evaluation.","Compressing deep neural networks by converting 32-bit weights to 4-bit numbers makes them efficient to run on phones and cameras, but this shortcut can cause some demographic groups to suffer far larger errors than others, widening accuracy gaps.We examined two popular compression schemes—post-training quantization (PTQ) and quantization-aware training (QAT)—and found that the accuracy gaps hinges on two factors: each group’s gradient norm and the local curvature (Hessian trace) of the loss. Both factors spike when a group has less training data, and reducing bit-width widens the accuracy gaps; QAT, although usually more accurate overall, amplifies this disparity even more than PTQ. Fortunately, balancing the training data with simple augmentations—like rotating pictures or masking random patches—significantly reduces accuracy gaps without sacrificing overall performance.Our findings provide the first theoretical and empirical guide for building quantized yet fair models, helping practitioners deploy efficient AI on phones, cameras, and other resource-limited devices while safeguarding fairness."
Poster,UnHiPPO: Uncertainty-aware Initialization for State Space Models,https://ICML.cc//virtual/2025/poster/45136,"Marten Lienen, Abdullah Saydemir, Stephan Günnemann","State space models are emerging as a dominant model class for sequence problems with many relying on the HiPPO framework to initialize their dynamics. However, HiPPO fundamentally assumes data to be noise-free; an assumption often violated in practice. We extend the HiPPO theory with measurement noise and derive an uncertainty-aware initialization for state space model dynamics. In our analysis, we interpret HiPPO as a linear stochastic control problem where the data enters as a noise-free control signal. We then reformulate the problem so that the data become noisy outputs of a latent system and arrive at an alternative dynamics initialization that infers the posterior of this latent system from the data without increasing runtime. Our experiments show that our initialization improves the resistance of state-space models to noise both at training and inference time.","Many modern AI systems that understand time series—like audio or sensor data—rely on mathematical tools called state space models (SSMs). These models condense long sequences of data into a compact summary, which makes them fast and efficient. A popular approach, known as HiPPO, lets SSMs efficiently track patterns in data. However, HiPPO assumes that the input data is perfectly clean, without any random “noise” - which is rarely true for real-world data.Our work extends HiPPO to handle noisy, imperfect data. By rethinking the math that underlies HiPPO, we create a new version called UnHiPPO that automatically filters out noise when summarizing data sequences. This makes state space models more robust and reliable—for example, when processing speech or sensor readings with background disturbances.We show that this new approach improves how well models work on noisy data, both during training and when making predictions, on the example of LSSL. We hope our findings make powerful sequence models more useful and reliable for real-world applications where data is never perfectly clean."
Poster,UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control,https://ICML.cc//virtual/2025/poster/43713,"Kaizhen Zhu, Mokai Pan, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi","Recent advances in diffusion bridge models leverage Doob’s $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches frequently produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified framework for diffusion bridges based on Stochastic Optimal Control (SOC). UniDB formulates the problem through an SOC-based optimization and derives a closed-form solution for the optimal controller, thereby unifying and generalizing existing diffusion bridge models. We demonstrate that existing diffusion bridges employing Doob’s $h$-transform constitute a special case of our framework, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. Notably, UniDB seamlessly integrates with existing diffusion bridge models, requiring only minimal code modifications. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework. Our code is available at https://github.com/UniDB-SOC/UniDB/.","Diffusion bridges are advanced models used for tasks like image restoration and generation, leveraging diffusion processes to transition between two arbitrary distributions. A key technique in this area is Doob's $h$-transform, which modifies the diffusion process to ensure it reaches a specific endpoint. However, while effective, Doob's $h$-transform frequently produces blurred or excessively smoothed image details and lacks a comprehensive theoretical foundation to explain these shortcomings.UniDB, our proposed framework, addresses this issue by formulating diffusion bridges with Stochastic Optimal Control (SOC) and deriving the closed-form solution for this SOC problem. Unlike Doob's $h$-transform, which focuses solely on endpoint accuracy, UniDB balances control costs and terminal penalties through a tunable parameter. This balance allows UniDB to produce images with better detail preservation and perceptual quality, avoiding the blurring artifacts often seen with Doob's $h$-transform.UniDB's strength lies in its flexibility and adaptability. Notably, UniDB seamlessly integrates with existing diffusion bridge models, which means UniDB achieves superior performance with minimal code modifications to existing models, making it a powerful and practical solution for enhancing diffusion-based image restoration and generation tasks."
Poster,Unifews: You Need Fewer Operations for Efficient Graph Neural Networks,https://ICML.cc//virtual/2025/poster/45740,"Ningyi Liao, Zihao Yu, Ruixiao Zeng, Siqiang Luo","Graph Neural Networks (GNNs) have shown promising performance, but at the cost of resource-intensive operations on graph-scale matrices. To reduce computational overhead, previous studies attempt to sparsify the graph or network parameters, but with limited flexibility and precision boundaries. In this work, we propose Unifews, a joint sparsification technique to unify graph and weight matrix operations and enhance GNN learning efficiency. The Unifews design enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectures with on-the-fly simplification. Theoretically, we establish a novel framework to characterize sparsified GNN learning in view of the graph optimization process, showing that Unifews effectively approximates the learning objective with bounded error and reduced computational overhead. Extensive experiments demonstrate that Unifews achieves efficiency improvements with comparable or better accuracy, including 10-20x matrix operation reduction and up to 100x acceleration on graphs up to billion-edge scale.","Artificial Intelligence models can understand complex network-structured data, like the web of friendships on a social media platform. These systems, known as Graph Neural Networks (GNNs), are powerful but require enormous computational resources, making them slow and costly, especially for the massive networks common in real-world scenarios. In our research, we aimed to make these GNNs more efficient. We developed Unifews, a technique that simplifies the data structure and the model parameters at the same time. We developed a theory showing that these two components can be computed in a unified way, effectively trimming the unnecessary details while focusing on critical information.Tests show that Unifews dramatically speeds up computation by up to 100 times. This advancement allows sophisticated GNNs to analyze much larger, more complex networks, or deliver results faster using simpler and cheaper hardware, like a single GPU."
Poster,Unified Analysis of Continuous Weak Features Learning with Applications to Learning from Missing Data,https://ICML.cc//virtual/2025/poster/44597,"Kosuke Sugiyama, Masato Uchida","This paper addresses weak features learning (WFL), focusing on learning scenarios characterized by low-quality input features (weak features; WFs) that arise due to missingness, measurement errors, or ambiguous observations. We present a theoretical formalization and error analysis of WFL for continuous WFs (continuous WFL), which has been insufficiently explored in existing literature. A previous study established formalization and error analysis for WFL with discrete WFs (discrete WFL); however, this analysis does not extend to continuous WFs due to the inherent constraints of discreteness. To address this, we propose a theoretical framework specifically designed for continuous WFL, systematically capturing the interactions between feature estimation models for WFs and label prediction models for downstream tasks. Furthermore, we derive the theoretical conditions necessary for both sequential and iterative learning methods to achieve consistency. By integrating the findings of this study on continuous WFL with the existing theory of discrete WFL, we demonstrate that the WFL framework is universally applicable, providing a robust theoretical foundation for learning with low-quality features across diverse application domains.","We are interested in understanding how the quality of input information affects the performance of predictive models trained using machine learning. While this question has been explored to some extent for categorical inputs, the impact of input quality has not been sufficiently discussed in the context of continuous-valued inputs. We focus on this underexplored aspect, and analyze the relationship between the quality of continuous input features and the predictive accuracy of trained models. Our study reveals how variations in input quality influence model performance. Our findings highlight the importance of improving input data quality and lay the groundwork for theoretical analysis of its effects on predictive modeling."
Poster,Unified Breakdown Analysis for Byzantine Robust Gossip,https://ICML.cc//virtual/2025/poster/45446,"Renaud Gaucher, Aymeric Dieuleveut, Hadrien Hendrikx","In decentralized machine learning, different devices communicate in a peer-to-peer manner to collaboratively learn from each other's data. Such approaches are vulnerable to misbehaving (or Byzantine) devices. We introduce F-RG, a general framework for building robust decentralized algorithms with guarantees arising from robust-sum-like aggregation rules F. We then investigate the notion of *breakdown point*, and show an upper bound on the number of adversaries that decentralized algorithms can tolerate. We introduce a practical robust aggregation rule, coined CS+, such that CS+-RG has a near-optimal breakdown. Other choices of aggregation rules lead to existing algorithms such as ClippedGossip or NNA. We give experimental evidence to validate the effectiveness of CS+-RG and highlight the gap with NNA, in particular against a novel attack tailored to decentralized communications.","Can many computers train a model together, when some send wrong information? Is it harder when computers communicate with only a small number of the other ones? We investigate these challenges in a setting where computers communicate directly, without relying on a central server.Our approach builds on a simple idea: each computer can treat its information as a trustworthy reference and consider messages that differ too much as suspicious. We show that it can be combined with many robust averaging options, and the resulting algorithm is highly resilient: the performance remains strong when the number of misbehaving computers doesn’t exceed a threshold. For an averaging option we introduce, this threshold is optimal up to a factor of 2! Experimental results in machine learning support our theory.This work opens the way to more secure distributed systems, for collaborative machine learning, and beyond!"
Poster,Unified K-Means Clustering with Label-Guided Manifold Learning,https://ICML.cc//virtual/2025/poster/44925,"Qianqian Wang, Mengping Jiang, Zhengming Ding, Quanxue Gao","K-Means clustering is a classical and effective unsupervised learning method attributed to its simplicity and efficiency. However, it faces notable challenges, including sensitivity to random initial centroid selection, a limited ability to discover the intrinsic manifold structures within nonlinear datasets, and difficulty in achieving balanced clustering in practical scenarios. To overcome these weaknesses, we introduce a novel framework for K-Means that leverages manifold learning. This approach eliminates the need for centroid calculation and utilizes a cluster indicator matrix to align the manifold structures, thereby enhancing clustering accuracy. Beyond the traditional Euclidean distance, our model incorporates Gaussian kernel distance, K-nearest neighbor distance, and low-pass filtering distance to effectively manage data that is not linearly separable. Furthermore, we introduce a balanced regularizer to achieve balanced clustering results. The detailed experimental results demonstrate the efficacy of our proposed methodology.","K-Means clustering is a classical and effective unsupervised learning method attributed to its simplicity and efficiency. However, it faces notable challenges, including sensitivity to random initial centroid selection, a limited ability to discover the intrinsic manifold structures within nonlinear datasets, and difficulty in achieving balanced clustering in practical scenarios. To overcome these weaknesses, we introduce a novel framework for K-Means that leverages manifold learning. This approach eliminates the need for centroid calculation and utilizes a cluster indicator matrix to align the manifold structures. The method also introduces different distance metrics to effectively manage data that is not linearly separable. It also achieves balanced clustering with a balanced regularizer, which further improves clustering performance."
Poster,Unified Screening for Multiple Diseases,https://ICML.cc//virtual/2025/poster/43498,"Yiğit Narter, Alihan Hüyük, Mihaela van der Schaar, Cem Tekin","Current screening programs that focus on improving patient health while minimizing screening costs are tailored for individual diseases. Designing unified screening programs for multiple diseases requires carefully balancing competing disease risks, which is an open problem. In this work, we address this problem by casting unified screening as a referral problem, in which we choose to activate a subset of screening policies for individual diseases by accounting for competing risks that influence patient outcomes. We introduce a novel optimization framework that incorporates disease risks, budget constraints, and diagnostic error limits and characterize the structural properties of the optimal referral policy. For the unified screening of two diseases, we show that the optimal activation threshold for the screening of one disease depends on the risk of the other, resulting in decision boundaries with distinct risk-dependent profiles. We compare our unified model with independent screening programs that apply isolated activation thresholds for screening of each disease. Our approach optimizes screening decisions collectively, improving overall survival outcomes, particularly for patients with high disease risks.","Most medical screening programs are designed for just one disease at a time. But in real life, people can face risks for several diseases at once, and choosing how and when to screen for each one can be tricky, especially when there are limited resources like time, money, and testing capacity.Our study explores a better way to handle this: instead of looking at each disease separately, we treat screening as a joint decision-making problem. We built a computer model that helps decide which disease screenings should be done together and when, based on a patient’s specific risk for each disease. The model takes into account how diseases can affect each other’s outcomes, and aims to improve survival while staying within realistic limits for cost and accuracy.We found that this unified approach leads to better health outcomes, especially for high-risk patients, compared to treating each disease in isolation."
Poster,Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means,https://ICML.cc//virtual/2025/poster/46413,"Mikael Møller Høgsgaard, Andrea Paudice","The Median of Means (MoM) is a mean estimator that has gained popularity in the context of heavy-tailed data. In this work, we analyze its performance in the task of simultaneously estimating the mean of each function in a class $\mathcal{F}$ when the data distribution possesses only the first $p$ moments for $p \in (1,2]$. We prove a new sample complexity bound using a novel symmetrization technique that may be of independent interest. Additionally, we present applications of our result to $k$-means clustering with unbounded inputs and linear regression with general losses, improving upon existing works.","In this paper, we consider the classic problem of mean estimation when the dataset contains extreme values-for instance, estimating the mean earthquake magnitude, where a few very large earthquakes can skew the results. To address the influence of extreme values, we partition the data into small buckets, compute the mean within each bucket, and take the center of these means. Since extreme values are common in modern datasets, it is important to develop robust methods for estimating the mean."
Poster,Unifying 2D and 3D Vision-Language Understanding,https://ICML.cc//virtual/2025/poster/45879,"Ayush Jain, Alexander Swerdlow, Yuzhou Wang, Sergio Arnaud, Ada Martin, Alexander Sax, Franziska Meier, Katerina Fragkiadaki","Progress in 3D vision-language learning has been hindered by the scarcity of large-scale 3D datasets. We introduce UniVLG, a unified architecture for 2D and 3D vision-language understanding that bridges the gap between existing 2D-centric models and the rich 3D sensory data available in embodied systems. Our approach initializes most model weights from pre-trained 2D models and trains on both 2D and 3D vision-language data. We propose a novel language-conditioned  mask decoder shared across 2D and 3D modalities to ground objects effectively in both RGB and RGB-D images, outperforming box-based approaches. To further reduce the domain gap between 2D and 3D, we incorporate 2D-to-3D lifting strategies, enabling UniVLG to utilize 2D data to enhance 3D performance. With these innovations, our model achieves state-of-the-art performance across multiple 3D vision-language grounding tasks, demonstrating the potential of transferring advances from 2D vision-language learning to the data-constrained 3D domain. Furthermore, co-training on both 2D and 3D data enhances performance across modalities without sacrificing 2D capabilities. By removing the reliance on 3D mesh reconstruction and ground-truth object proposals, UniVLG sets a new standard for realistic, embodied-aligned evaluation. Code and additional visualizations are available at https://univlg.github.io.","Most real-world robots use 3D sensors but still rely on models trained only on 2D images, missing out on the full benefits of 3D perception. This is mainly because high-quality 3D training data is scarce and expensive. We introduce \model{}, a new vision-language model that combines 2D and 3D data to bridge this gap. \model{} uses powerful pre-trained 2D models and learns to understand 3D scenes by aligning 2D and 3D inputs. A key innovation is a language-guided mask decoder that accurately grounds objects in 3D space. Our model outperforms previous methods on major benchmarks while working in more realistic, sensor-based settings. This shows that leveraging 2D data is a practical and effective way to boost 3D understanding in embodied AI systems."
