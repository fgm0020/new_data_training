type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Unpaired Point Cloud Completion via Unbalanced Optimal Transport,https://ICML.cc//virtual/2025/poster/44328,"Taekyung Lee, Jaemoo Choi, Jaewoong Choi, Myungjoo Kang","Unpaired point cloud completion is crucial for real-world applications, where ground-truth data for complete point clouds are often unavailable. By learning a completion map from unpaired incomplete and complete point cloud data, this task avoids the reliance on paired datasets. In this paper, we propose the \textit{Unbalanced Optimal Transport Map for Unpaired Point Cloud Completion (\textbf{UOT-UPC})} model, which formulates the unpaired completion task as the (Unbalanced) Optimal Transport (OT) problem. Our method employs a Neural OT model learning the UOT map using neural networks. Our model is the first attempt to leverage UOT for unpaired point cloud completion, achieving competitive or superior performance on both single-category and multi-category benchmarks. In particular, our approach is especially robust under the class imbalance problem, which is frequently encountered in real-world unpaired point cloud completion scenarios.","Most 3D scanning technologies capture only partial views of objects, making it difficult to reconstruct their complete shapes. Traditional methods for completing these “incomplete” 3D point clouds rely on paired data—matched examples of partial and full shapes—which are expensive and often unavailable in real-world settings.We propose a new method that completes 3D point clouds without requiring such paired training data. Our approach formulates the task as an “unbalanced optimal transport” problem— a mathematical framework for mapping one distribution (incomplete point clouds) to another (complete point clouds), even when their structures differ significantly, such as class imbalance.Our model, UOT-UPC, is the first to apply unbalanced optimal transport to unpaired point cloud completion. Our model not only outperforms previous methods on standard benchmarks but also shows strong robustness when the distributions of object types differ between incomplete and complete point clouds. This method enables more accurate 3D shape understanding, with broad applications in robotics, autonomous driving, and AR/VR systems."
Poster,Unraveling the Interplay between Carryover Effects and Reward Autocorrelations in Switchback Experiments,https://ICML.cc//virtual/2025/poster/44823,"Qianglin Wen, Chengchun Shi, Ying Yang, Niansheng Tang, Hongtu Zhu","A/B testing has become the gold standard for modern technological industries for policy evaluation. Motivated by the widespread use of switchback experiments in A/B testing, this paper conducts a comprehensive comparative analysis of various switchback designs in Markovian environments. Unlike many existing works which derive the optimal design based on specific and relatively simple estimators, our analysis covers a range of state-of-the-art estimators developed in the reinforcement learning literature. It reveals that the effectiveness of different switchback designs depends crucially on (i) the size of the carryover effect and (ii) the autocorrelations among reward errors over time. Meanwhile, these findings are estimator-agnostic, i.e., they apply to all the aforementioned estimators.  Based on these insights, we provide a workflow to offer guidelines for practitioners on designing switchback experiments in A/B testing.","A/B testing has become the gold standard for policy evaluation in modern technological industries. This paper is motivated by the widespread use of switchback experiments—where a baseline and a new policy alternate at fixed intervals—and presents a comprehensive comparative analysis of various switchback designs in Markovian environments. Unlike many existing studies that derive optimal designs based on specific and relatively simple estimators, our analysis incorporates a range of advanced estimators developed in the reinforcement learning (RL) literature. We show that the effectiveness of different switchback designs is highly dependent on two key factors: (i) the size of the carryover effect—the influence of previous treatments on future outcomes, and (ii) the autocorrelations among reward errors over time. Notably, these findings are estimator-agnostic, meaning they apply to most RL estimators. Building on these insights, we propose a workflow that provides practical guidelines for practitioners on designing switchback experiments in A/B testing."
Poster,Unsupervised Learning for Class Distribution Mismatch,https://ICML.cc//virtual/2025/poster/44666,"Pan Du, Zhao, Xinai Lu, Nian Liu, Zhikai Li, Chaoyu Gong, Suyun Zhao, Hong Chen, Cuiping Li, Kai Wang, Yang You","Class distribution mismatch (CDM) refers to the discrepancy between class distributions in training data and target tasks. Previous methods address this by designing classifiers to categorize classes known during training, while grouping unknown or new classes into an ""other"" category. However, they focus on semi-supervised scenarios and heavily rely on labeled data, limiting their applicability and performance. To address this, we propose Unsupervised Learning for Class Distribution Mismatch (UCDM), which constructs positive-negative pairs from unlabeled data for classifier training. Our approach randomly samples images and uses a diffusion model to add or erase semantic classes, synthesizing diverse training pairs. Additionally, we introduce a confidence-based labeling mechanism that iteratively assigns pseudo-labels to valuable real-world data and incorporates them into the training process. Extensive experiments on three datasets demonstrate UCDM’s superiority over previous semi-supervised methods. Specifically, with a 60\% mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%, and 72.5% in classifying known, unknown, and new classes.","Machine learning models often assume that the training data has the same class distribution as the real-world data they’ll encounter, but this is rarely true. When the classes seen during training don’t match those in real-world tasks, models struggle, especially if labeled data is limited. Existing semi-supervised learning methods try to handle this mismatch using a small amount of labeled data, which limits their usefulness.To solve this, we developed UCDM, a new method that doesn’t rely on labeled data. Instead, it learns by generating training examples from unlabeled images. Using the diffusion model, we add or remove visual content from images to create diverse pairs of examples. We also introduce a way to assign pseudo-labels to real images to train the model automatically. Our approach outperforms previous methods on several challenging datasets, showing that learning from unlabeled, mismatched data is both possible and effective."
Poster,"Unveiling AI's Blind Spots: An Oracle for In-Domain, Out-of-Domain, and Adversarial Errors",https://ICML.cc//virtual/2025/poster/44793,"Shuangpeng Han, Mengmi Zhang","AI models make mistakes when recognizing images—whether in-domain, out-of-domain, or adversarial. Predicting these errors is critical for improving system reliability, reducing costly mistakes, and enabling proactive corrections in real-world applications such as healthcare, finance, and autonomous systems. However, understanding what mistakes AI models make, why they occur, and how to predict them remains an open challenge. Here, we conduct comprehensive empirical evaluations using a ""mentor"" model—a deep neural network designed to predict another ""mentee"" model’s errors. Our findings show that the mentor excels at learning from a mentee's mistakes on adversarial images with small perturbations and generalizes effectively to predict in-domain and out-of-domain errors of the mentee. Additionally, transformer-based mentor models excel at predicting errors across various mentee architectures. Subsequently, we draw insights from these observations and develop an ""oracle"" mentor model, dubbed SuperMentor, that can outperform baseline mentors in predicting errors across different error types from the ImageNet-1K dataset. Our framework paves the way for future research on anticipating and correcting AI model behaviors, ultimately increasing trust in AI systems. Our data and code are available at [here](https://github.com/ZhangLab-DeepNeuroCogLab/UnveilAIBlindSpot).","As AI becomes increasingly embedded in our daily life and high-stakes fields such as healthcare, finance, and autonomous driving, ensuring its safety and reliability is critical. Our research focuses on predicting mistakes made by AI in image recognition tasks. Specifically, we explore the idea of using one AI model, referred to as the “mentor”, to predict the errors of another AI model called the “mentee”. We investigate which types of mistakes made by the mentee serve as the most effective training sources for the mentor to learn the mentee’s error patterns. This framework offers a promising direction for anticipating and correcting AI behavior, ultimately helping to build more trustworthy AI systems."
Poster,Unveiling Markov heads in Pretrained Language Models for Offline Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45860,"Wenhao Zhao, Qiushui Xu, Linjie Xu, Lei Song, Jinyu Wang, Chunlai Zhou, Jiang Bian","Recently, incorporating knowledge from pretrained language models (PLMs) into decision transformers (DTs) has generated significant attention in offline reinforcement learning (RL). These PLMs perform well in RL tasks, raising an intriguing question: what kind of knowledge from PLMs has been transferred to RL to achieve such good results? This work first dives into this problem by analyzing each head quantitatively and points out Markov head, a crucial component that exists in the attention heads of PLMs. It leads to extreme attention on the last-input token and performs well only in short-term environments. Furthermore, we prove that this extreme attention cannot be changed by re-training embedding layer or fine-tuning. Inspired by our analysis, we propose a general method GPT-DTMA, which equips a pretrained DT with Mixture of Attention (MoA), to enable adaptive learning and accommodate diverse attention requirements during fine-tuning. Extensive experiments demonstrate the effectiveness of GPT-DTMA: it achieves superior performance in short-term environments compared to baselines, significantly reduces the performance gap of PLMs in long-term scenarios, and the experimental results also validate our theorems.","There's been a lot of interest in integrating pretrained language model knowledge into decision transformers for offline reinforcement learning. These models excel at RL tasks, leading to questions about which PLM knowledge enhances RL performance. Key findings highlight the ""Markov head"" in pretrained language models, focusing intensely on the last input token, effective in a specific type of environments. This focus can't be changed through fine-tuning. Based on these insights, the proposed GPT-DTMA method equips DTs with Mixture of Attention, enabling adaptive learning.Our research highlights the limitations of different models in two representative types of environments and sheds light on how to design a model that is applicable in any scenario."
Poster,Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities,https://ICML.cc//virtual/2025/poster/45817,"Ruchika Chavhan, Abhinav Mehrotra, Malcolm Chadwick, Alberto Gil Couto Pimentel Ramos, Luca Morreale, Mehdi Noroozi, Sourav Bhattacharya","Text-to-image synthesis has witnessed remarkable advancements in recent years. Many attempts have been made to adopt text-to-image models to support multiple tasks. However, existing approaches typically require resource-intensive re-training or additional parameters to accommodate for the new tasks, which makes the model inefficient for on-device deployment. We propose *Multi-Task Upcycling* (MTU), a simple yet effective recipe that extends the capabilities of a pre-trained text-to-image diffusion model to support a variety of image-to-image generation tasks. MTU replaces Feed-Forward Network (FFN) layers in the diffusion model with smaller FFNs, referred to as *experts*, and combines them with a dynamic routing mechanism. To the best of our knowledge, MTU is the first multi-task diffusion modeling approach that seamlessly blends multi-tasking with on-device compatibility, by mitigating the issue of parameter inflation. We show that the performance of MTU is on par with the single-task fine-tuned diffusion models across several tasks including *image editing, super-resolution*, and *inpainting*, while maintaining similar latency and computational load (GFLOPs) as the single-task fine-tuned models.","Imagine having a powerful, personalised AI assistant right on your phone—one that not only chats with you but can also edit, enhance, or fill in parts of your photos. The problem is, today’s image generation models are huge and too heavy to run on a phone, which has limited computing power. Shrinking these models without losing their abilities is a tough challenge. So, we asked: how can we take a compact, on-device model and train it to handle multiple image generation tasks? Inspired by how the brain assigns jobs to different areas, we explored pre-trained image models and found a key component responsible for solving image generation tasks. We split this part into smaller, specialised units called ""experts."" Each expert focuses on certain types of image tasks, and depending on what the model is doing, the most relevant experts get more say in the final result. With this setup, we turned a single lightweight model into a flexible multitasker—capable of doing four different image generation jobs—all while staying efficient enough to run on your phone."
Poster,Update Your Transformer to the Latest Release: Re-Basin of Task Vectors,https://ICML.cc//virtual/2025/poster/43843,"Filippo Rinaldi, Giacomo Capitani, Lorenzo Bonicelli, Donato Crisostomi, Federico Bolelli, ELISA FICARRA, Emanuele Rodola, Simone Calderara, Angelo Porrello","Foundation models serve as the backbone for numerous specialized models developed through fine-tuning. However, when the underlying pretrained model is updated or retrained (e.g., on larger and more curated datasets), the fine-tuned model becomes obsolete, losing its utility and requiring retraining. This raises the question: is it possible to transfer fine-tuning to a new release of the model? In this work, we investigate how to transfer fine-tuning to a new checkpoint without having to re-train, in a data-free manner. To do so, we draw principles from model re-basin and provide a recipe based on weight permutations to re-base the modifications made to the original base model, often called task vector. In particular, our approach tailors model re-basin for Transformer models, taking into account the challenges of residual connections and multi-head attention layers. Specifically, we propose a two-level method rooted in spectral theory, initially permuting the attention heads and subsequently adjusting parameters within select pairs of heads. Through extensive experiments on visual and textual tasks, we achieve the seamless transfer of fine-tuned knowledge to new pre-trained backbones without relying on a single training step or datapoint. Code is available at https://github.com/aimagelab/TransFusion.","In artificial intelligence, foundational models are large systems pre-trained on extensive datasets and often fine-tuned for specific tasks. When these models are updated, fine-tuned versions become outdated and require expensive retraining, which can be impractical if the original training data is inaccessible.We present ""TransFusion,"" a novel approach that enables the seamless transfer of fine-tuning from older Transformer model versions to updated ones without needing additional training or data. By realigning specialized knowledge within the model's parameters using weight permutations, our method effectively ""re-bases"" the fine-tuning.This two-stage alignment process addresses the complexities of Transformer models, preserving their original functionality while enhancing performance on targeted tasks. Our experimental results confirm that TransFusion successfully transfers specialized knowledge, reducing costs and making advanced AI solutions more accessible for real-world applications."
Poster,UP-VLA:  A Unified Understanding and Prediction Model for Embodied Agent,https://ICML.cc//virtual/2025/poster/45080,"Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen","Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities.VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus onhigh-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics.These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms.In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33\% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.","Recent research on VLA has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed visual and spatial information. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33\% improvement on the Calvin ABC-D benchmark and demonstrates improved success rates in real-world manipulation tasks."
Poster,Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting,https://ICML.cc//virtual/2025/poster/46655,"Sunny Sanyal, Hayden Prairie, Rudrajit Das, Ali Kavis, Sujay Sanghavi","Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as ""catastrophic forgetting"". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a *sample weighting scheme for the fine-tuning data* solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace, which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a $0.8$% drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving $5.4$% more accuracy on the pre-training datasets.","Pretrained AI models are powerful, but fine-tuning them on new tasks often causes them to forget what they originally learned — a problem known as catastrophic forgetting. This is especially challenging when the original training data is unavailable, as is often the case with publicly released models.To address this, we propose a simple method called FLOW. Unlike traditional approaches focusing on difficult examples during fine-tuning, FLOW does the opposite: it upweights “easy” examples — cases that the model already handles well. This helps preserve the model’s original capabilities while still allowing it to adapt to the new task.FLOW is easy to use, requires no extra parameters, and relies only on a basic calculation using the model’s pre-trained performance. Despite its simplicity, it consistently outperforms standard fine-tuning and other relevant baselines across both vision and language tasks. For example, when fine-tuning for math reasoning, FLOW achieved similar accuracy on new math problems while retaining 5% more accuracy on the original general knowledge tasks than standard fine-tuning.Because FLOW doesn’t require access to original training data, it offers a practical solution for retaining valuable model capabilities while fine-tuning on new tasks."
Poster,Validating Mechanistic Interpretations: An Axiomatic Approach,https://ICML.cc//virtual/2025/poster/43956,"Nils Palumbo, Ravi Mangal, Zifan Wang, Saranya Vijayakumar, Corina Pasareanu, Somesh Jha","Mechanistic interpretability aims to reverse engineer the computation performed by a neural network in terms of its internal components. Although there is a growing body of research on mechanistic interpretation of neural networks, the notion of a *mechanistic interpretation* itself is often ad-hoc. Inspired by the notion of abstract interpretation from the program analysis literature that aims to develop approximate semantics for programs, we give a set of axioms that formally characterize a mechanistic interpretation as a description that approximately captures the semantics of the neural network under analysis in a compositional manner. We demonstrate the applicability of these axioms for validating mechanistic interpretations on an existing, well-known interpretability study as well as on a new case study involving a Transformer-based model trained to solve the well-known 2-SAT problem.","Machine learning systems based on large and opaque neural networks are increasingly becoming both more capable and more heavily used in important real-world applications. However, we still don't understand how these models make their decisions.Mechanistic interpretations are a way to explain a model's decisions with simple, easily understood, programs which describe the meaning behind a model's internal computations. However, validating that a claimed interpretation actually describes the behavior of a real model is challenging.We present an approach for validating mechanistic interpretations which translates between the values that the original model and the interpretation operate on, to ensure that the models represent the same concepts, and to ensure that pieces of the model can be exchanged with their interpretations without affecting the model’s behavior.We apply our approach to two case studies, an original mechanistic interpretation of a model trained to perform a simple logic task, and a well-known analysis of a model trained on modular addition."
