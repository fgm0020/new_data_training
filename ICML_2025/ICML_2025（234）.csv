type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Prediction models that learn to avoid missing values,https://ICML.cc//virtual/2025/poster/43980,"Lena Stempfle, Anton Matsson, Newton Mwai, Fredrik Johansson","Handling missing values at test time is challenging for machine learning models, especially when aiming for both high accuracy and interpretability. Established approaches often add bias through imputation or excessive model complexity via missingness indicators. Moreover, either method can obscure interpretability, making it harder to understand how the model utilizes the observed variables in predictions. We propose *missingness-avoiding* (MA) machine learning, a general framework for training models to rarely require the values of missing (or imputed) features at test time. We create tailored MA learning algorithms for decision trees, tree ensembles, and sparse linear models by incorporating classifier-specific regularization terms in their learning objectives. The tree-based models leverage contextual missingness by reducing reliance on missing values based on the observed context. Experiments on real-world datasets demonstrate that **MA-DT, MA-LASSO, MA-RF**, and **MA-GBT** effectively reduce the reliance on features with missing values while maintaining predictive performance competitive with their unregularized counterparts. This shows that our framework gives practitioners a powerful tool to maintain interpretability in predictions with test-time missing values.","Machine learning models often struggle when values are missing at test time—a common issue in real-world applications like healthcare or finance. Existing solutions either fill in missing values (which can introduce bias) or make models more complex (which can reduce interpretablity). This makes it harder to understand and trust model decisions.We tackle this problem by developing missingness-avoiding (MA) machine learning—a new framework that encourages models to minimize reliance on missing features during prediction. We designed custom versions of this method for several models, including decision trees, random forests, gradient boosting, and sparse linear models. Our approach includes tailored regularization that encourages models to base decisions on observed inputs rather than imputed or missing data.In experiments across several real-world datasets, our models remained accurate while becoming significantly less dependent on missing features. This work matters because it allows practitioners to make accurate, interpretable predictions, even when some values are missing, offering safer, more trustworthy ML tools for critical domains."
Poster,Prediction-Powered Adaptive Shrinkage Estimation,https://ICML.cc//virtual/2025/poster/46514,"Sida Li, Nikolaos Ignatiadis","Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI’s benefits for individual statistical problems, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage ($\texttt{PAS}$), a method that bridges PPI with empirical Bayes shrinkage to improve estimation of multiple means. $\texttt{PAS}$ debiases noisy ML predictions $\textit{within}$ each task and then borrows strength $\textit{across}$ tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that $\texttt{PAS}$ adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.","Scientists and data analysts often face a common challenge: they have a lot of data features (like images of galaxies or product details) but only a small amount of reliable, ""gold-standard"" labeled information (like which galaxies have spirals or actual user ratings). This scarcity makes it hard to answer many related questions accurately, such as finding the fraction of spiral galaxies in different clusters or the average ratings for many different products.We developed a new statistical method called Prediction-Powered Adaptive Shrinkage (PAS). PAS cleverly combines these limited gold-standard labels with predictions from modern machine learning (ML) models. First, for each specific question (e.g., for one galaxy cluster), it uses the ML predictions to make initial estimates more precise. Then, it ""borrows strength"" across all the different questions by using these same ML predictions as a common reference point, intelligently adjusting how much to rely on them based on their estimated quality.Our method, PAS, allows researchers to get more accurate answers even when high-quality labeled data is scarce for each individual question. It automatically adapts to how good the ML predictions are, outperforming existing approaches in diverse real-world scenarios, from astronomy to analyzing customer reviews. This helps extract more reliable insights from complex datasets with many parallel questions."
Poster,Prediction-Powered E-Values,https://ICML.cc//virtual/2025/poster/43872,"Daniel Csillag, Claudio Struchiner, Guilherme Tegoni Goedert","Quality statistical inference requires a sufficient amount of data, which can be missing or hard to obtain. To this end, prediction-powered inference has risen as a promising methodology, but existing approaches are largely limited to Z-estimation problems such as inference of means and quantiles. In this paper, we apply ideas of prediction-powered inference to e-values. By doing so, we inherit all the usual benefits of e-values -- such as anytime-validity, post-hoc validity and versatile sequential inference -- as well as greatly expand the set of inferences achievable in a prediction-powered manner. In particular, we show that every inference procedure that can be framed in terms of e-values has a prediction-powered counterpart, given by our method. We showcase the effectiveness of our framework across a wide range of inference tasks, from simple hypothesis testing and confidence intervals to more involved procedures for change-point detection and causal discovery, which were out of reach of previous techniques. Our approach is modular and easily integrable into existing algorithms, making it a compelling choice for practical applications.","Quality statistical inference requires data, which can be missing or hard to obtain. It is tempting to resolve this by using predictions from powerful ML models to fill this missing data, but this can lead to biased inferences due to the model's imperfections. A recent line of work, termed 'prediction-powered inference,' seeks to design procedures to debias such inferences.In our paper, we extend prediction-powered inference to work with e-values, a modern enticing alternative to p-values. Besides inheriting the various favorable properties of e-values over p-values, doing so significantly expands the set of inferences that can be done in a prediction-powered manner; this was previously restricted to problems that fit into a 'Z-estimation' framework, which includes means, quantiles and linear regression coefficients, but not much more. With our method, on the other hand, any inference task that can be done with e-values (which is a very large class) can be done in a prediction-powered setting.We showcase the effectiveness of our method across a wide range of inference tasks, from simple hypothesis testing and confidence intervals to more involved procedures for change-point detection and causal discovery, which were out of reach of previous techniques. Our approach is modular and easily integrable into existing algorithms, making it a compelling choice for practical applications."
Poster,Prediction via Shapley Value Regression,https://ICML.cc//virtual/2025/poster/44871,"Amr Alkhatib, Roman Bresson, Henrik Boström, Michalis Vazirgiannis","Shapley values have several desirable, theoretically well-supported, properties for explaining black-box model predictions. Traditionally, Shapley values are computed post-hoc, leading to additional computational cost at inference time. To overcome this, a novel method, called ViaSHAP, is proposed, that learns a function to compute Shapley values, from which the predictions can be derived directly by summation. Two approaches to implement the proposed method are explored; one based on the universal approximation theorem and the other on the Kolmogorov-Arnold representation theorem. Results from a large-scale empirical investigation are presented, showing that ViaSHAP using Kolmogorov-Arnold Networks performs on par with state-of-the-art algorithms for tabular data. It is also shown that the explanations of ViaSHAP are significantly more accurate than the popular approximator FastSHAP on both tabular data and images.","Machine learning models can make highly accurate predictions, but often work as “black boxes”, i.e., users do not really understand why the models make certain decisions, which can be a problem when such models are used in domains like healthcare or finance, where trust and transparency are essential. One popular approach to explain a model’s decision is called the Shapley value, which helps identify which parts of the input data (like age, income, or symptoms) were most important to the model’s prediction. However, computing Shapley values is expensive.We introduce ViaSHAP, a method that learns to make predictions and generate explanations simultaneously. Unlike classic explanation methods that compute Shapley values after predictions are made, ViaSHAP takes a different approach; it first assigns Shapley values to the input components and then derives a prediction from them. Our results show that ViaSHAP performs on par with state-of-the-art models in accuracy, and provides more reliable explanations while being faster."
Poster,Predictive Data Selection: The Data That Predicts Is the Data That Teaches,https://ICML.cc//virtual/2025/poster/43781,"KaShun SHUM, Yuzhen Huang, Hongjian Zou, dingqi, YiXuan Liao, Xiaoxin Chen, Qian Liu, Junxian He","Language model pretraining involves training on extensive corpora, where data quality plays a pivotal role. In this work, we aim to directly estimate the contribution of data during pretraining and select pretraining data in an efficient manner. Specifically, we draw inspiration from recent findings showing that compression efficiency (i.e., normalized loss) of diverse models on certain text correlates strongly with their downstream performance, when the text domain aligns with the downstream benchmarks (Huang et al., 2024). Building on this observation, we hypothesize that data on which model losses are predictive of downstream abilities also contribute effectively to learning, which shares similar intuition with Thrush et al. (2024). To leverage this insight, we introduce predictive data selection (PreSelect), a lightweight and efficient data selection method that requires training and deploying only a fastText-based scorer. Through comprehensive experiments with 1B and 3B parameter models, we demonstrate that models trained on 30B tokens selected with PreSelect surpass the performance of the vanilla baseline trained on 300B tokens, achieving a 10x reduction in compute requirements. Furthermore, PreSelect significantly outperforms other competitive data selection baselines, such as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our trained data selection scorer along with the curated datasets at https://github.com/hkust-nlp/PreSelect.","Language model pretraining involves training on extensive corpora, where data quality plays a pivotal role. In this work, we aim to directly estimate the contribution of data during pretraining and select pretraining data in an efficient manner. Specifically, we draw inspiration from recent findings showing that compression efficiency (i.e., normalized loss) of diverse models on certain text correlates strongly with their downstream performance, when the text domain aligns with the downstream benchmarks (Huang et al., 2024). Building on this observation, we hypothesize that data on which model losses are predictive of downstream abilities also contribute effectively to learning, which shares similar intuition with Thrush et al. (2024). To leverage this insight, we introduce predictive data selection (PreSelect), a lightweight and efficient data selection method that requires training and deploying only a fastText-based scorer. Through comprehensive experiments with 1B and 3B parameter models, we demonstrate that models trained on 30B tokens selected with PreSelect surpass the performance of the vanilla baseline trained on 300B tokens, achieving a 10x reduction in compute requirements. Furthermore, PreSelect significantly outperforms other competitive data selection baselines, such as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our trained data selection scorer along with the curated datasets at https://github.com/hkust-nlp/PreSelect."
Poster,Predictive Performance of Deep Quantum Data Re-uploading Models,https://ICML.cc//virtual/2025/poster/46123,"Xin Wang, Hanxiao Tao, Re-Bing Wu","Quantum machine learning models incorporating data re-uploading circuits have garnered significant attention due to their exceptional expressivity and trainability. However, their ability to generate accurate predictions on unseen data, referred to as the predictive performance, remains insufficiently investigated. This study reveals a fundamental limitation in predictive performance when deep encoding layers are employed within the data re-uploading model. Concretely, we theoretically demonstrate that when processing high-dimensional data with limited-qubit data re-uploading models, their predictive performance progressively degenerates to near random-guessing levels as the number of encoding layers increases. In this context, the repeated data uploading cannot mitigate the performance degradation. These findings are validated through experiments on both synthetic linearly separable datasets and real-world datasets. Our results demonstrate that when processing high-dimensional data, the quantum data re-uploading models should be designed with wider circuit architectures rather than deeper and narrower ones.","A model architecture called data re-uploadings is currently popular in the quantum machine learning community. It is well-known because it can encode high-dimensional data into a limited-qubit quantum circuit, and the higher the dimensionality, the stronger its ability to fit data. We are curious: how good is this model's predictive performance?Our theoretical analysis reveals a surprising fact: in quantum circuits with limited qubits, once the dimensionality of encoded data becomes too high, the predictive performance of data re-uploadings models approaches random guessing. This is determined by the model structure and is independent of the optimization method. No matter how well the model performs on the training set, predictions will be approaches to random guessing for most data.Our research theoretically demonstrates the infeasibility of deep data re-uploading models and provides guidance for future quantum machine learning model research. Perhaps wider data re-uploading models would be more effective."
Poster,Preference Adaptive and Sequential Text-to-Image Generation,https://ICML.cc//virtual/2025/poster/45601,"Ofir Nabati, Guy Tennenholtz, Chih-wei Hsu, Moonkyung Ryu, Deepak Ramachandran, Yinlam Chow, Xiang Li, Craig Boutilier","We address the problem of interactive text-to-image (T2I) generation, designing a reinforcement learning (RL) agent which iteratively improves a set of generated images for a user through a sequence of prompt expansions. Using human raters, we create a novel dataset of sequential preferences, which we leverage, together with large-scale open-source (non-sequential) datasets. We construct user-preference and user-choice models using an EM strategy and identify varying user preference types. We then leverage a large multimodal language model (LMM) and a value-based RL approach to suggest an adaptive and diverse slate of prompt expansions to the user. Our Preference Adaptive and Sequential Text-to-image Agent (PASTA) extends T2I models with adaptive multi-turn capabilities, fostering collaborative co-creation and addressing uncertainty or underspecification in a user's intent. We evaluate PASTA using human raters, showing significant improvement compared to baseline methods. We also open-source our sequential rater dataset and simulated user-rater interactions to support future research in user-centric multi-turn T2I systems.","Have you ever tried to create an image using AI from a text description, only for the result to not quite match what you had in mind? It's often difficult to perfectly convey complex or evolving artistic visions with a single instruction, leading to a frustrating trial-and-error process.We've developed an AI assistant called PASTA that learns your preferences through a more conversational approach to image generation. Instead of just one attempt, PASTA shows you several image options based on your initial idea. You then pick the images you like best, and PASTA uses this feedback to refine its suggestions over several turns, guiding the image generation closer to your desired outcome. To build this, we collected new data on how people make these sequential choices and even created simulated users to help train our AI.This research makes image generation with AI a more collaborative and intuitive experience. It allows users to better express their specific ideas, helping them bring complex or abstract visions to life more effectively. Ultimately, this work aims to make AI image generation tools more satisfying and better aligned with individual user intent, and we're sharing our data to help other researchers build even more advanced creative AI."
Poster,Preference-CFR: Beyond Nash Equilibrium for Better Game Strategies,https://ICML.cc//virtual/2025/poster/44734,"Qi Ju, Thomas Tellier, Meng Sun, Zhemei Fang, YunFeng Luo","Artificial intelligence (AI) has surpassed top human players in a variety of games. In imperfect information games, these achievements have primarily been driven by Counterfactual Regret Minimization (CFR) and its variants for computing Nash equilibrium. However, most existing research has focused on maximizing payoff, while largely neglecting the importance of strategic diversity and the need for varied play styles, thereby limiting AI’s adaptability to different user preferences.To address this gap, we propose Preference-CFR (Pref-CFR), a novel method that incorporates two key parameters: preference degree and vulnerability degree. These parameters enable the AI to adjust its strategic distribution within an acceptable performance loss threshold, thereby enhancing its adaptability to a wider range of strategic demands. In our experiments with Texas Hold’em, Pref-CFR successfully trained Aggressive and Loose Passive styles that not only match original CFR-based strategies in performance but also display clearly distinct behavioral patterns. Notably, for certain hand scenarios, Pref-CFR produces strategies that diverge significantly from both conventional expert heuristics and original CFR outputs, potentially offering novel insights for professional players.","A central challenge in game theory is solving for equilibria, a problem that has been extensively studied. Researchers have achieved remarkable success in solving complex games such as Go, Texas Hold'em, and StarCraft. However, existing work primarily focuses on finding a single equilibrium strategy, overlooking the need to generate diverse playing styles that align with human preferences.Building upon the Counterfactual Regret Minimization (CFR) framework, we introduce a novel variant, Preference-CFR (Pref-CFR), which incorporates two additional parameters: preference intensity and vulnerability threshold. This approach ensures the generation of AI strategies that balance stylistic diversity with acceptable utility loss, thereby meeting user-specified playstyle requirements."
Poster,Preference Controllable Reinforcement Learning with Advanced Multi-Objective Optimization,https://ICML.cc//virtual/2025/poster/46501,"Yucheng Yang, Tianyi Zhou, Mykola Pechenizkiy, Meng Fang","Practical reinforcement learning (RL) usually requires agents to be optimized for multiple potentially conflicting criteria, e.g. speed vs. safety. Although Multi-Objective RL (MORL) algorithms have been studied in previous works, their trained agents often cover limited Pareto optimal solutions and they lack precise controllability of the delicate trade-off among multiple objectives. Hence, the resulting agent is not versatile in aligning with customized requests from different users. To bridge the gap, we develop the ``Preference controllable (PC) RL'' framework, which trains a preference-conditioned meta-policy that takes user preference as input controlling the generated trajectories within the preference region on the Pareto frontier. The PCRL framework is compatible with advanced Multi-Objective Optimization~(MOO) algorithms that are rarely seen in previous MORL approaches. We also proposed a novel preference-regularized MOO algorithm specifically for PCRL. We provide a comprehensive theoretical analysis to justify its convergence and preference controllability.We evaluate PCRL with different MOO algorithms against state-of-the-art MORL baselines in various challenging environments with up to six objectives. In these experiments, our proposed method exhibits significantly better controllability than existing approaches and can generate Pareto solutions with better diversity and utilities.","Previous mainstream Multi-Objective Reinforcement Learning (MORL) methods have overlooked recent advances in Multi-Objective Optimization (MOO). They typically rely on basic objectives such as linear scalarization, which can only discover limited optimal solutions and fails to guarantee a desired trade-off among multiple objectives. These methods also struggle with issues like conflicting gradients that result from conflicting objectives. In this work, we incorporate recent developments from the MOO literature to design a general MORL framework that can leverage advanced MOO algorithms. Furthermore, we introduce a novel MOO algorithm tailored for this framework, supported by both theoretical analysis and empirical results that demonstrate its improvements over existing MORL approaches. Our method’s memory-efficient design also makes it practical for use with larger models."
Poster,Preference Learning for AI Alignment: a Causal Perspective,https://ICML.cc//virtual/2025/poster/44331,"Katarzyna Kobalczyk, Mihaela van der Schaar","Reward modelling from preference data is a crucial step in aligning large language models (LLMs) with human values, requiring robust generalisation to novel prompt-response pairs. In this work, we propose to frame this problem in a causal paradigm, providing the rich toolbox of causality to identify the persistent challenges, such as causal misidentification, preference heterogeneity, and confounding due to user-specific factors. Inheriting from the literature of casual inference, we identify key assumptions necessary for reliable generalisation and contrast them with common data collection practices. We illustrate failure modes of naive reward models and demonstrate how causally-inspired approaches can improve model robustness. Finally, we outline desiderata for future research and practices, advocating targeted interventions to address inherent limitations of observational data.","Aligning large language models (LLMs) with human values hinges on teaching them to recognize and prefer helpful, safe, and appropriate responses—an area known as reward modeling from human preferences. Traditionally, this involves collecting comparisons between different model outputs and training a model to learn what people tend to prefer. However, this process often overlooks deeper challenges, such as why people prefer certain outputs and how those preferences might vary across users or situations.In this work, we propose viewing reward modeling through a causal lens—a framework used to distinguish true cause-effect relationships from misleading patterns in data. This shift allows us to pinpoint and address core problems in current practices, such as learning from biased data, failing to account for varying user preferences, or mistaking irrelevant patterns for meaningful signals.By applying tools from causal inference, we show how some commonly used reward models can go wrong, and how models built with causal principles can better generalise to new situations. We also provide a roadmap for future research, encouraging the AI community to rethink how we collect and use human feedback. In particular, we recommend more deliberate experimentation and data collection strategies to overcome the limitations of passive, observational data."
