type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Quantum Speedup for Hypergraph Sparsification,https://ICML.cc//virtual/2025/poster/43480,"Chenghua Liu, Minbo Gao, Zhengfeng Ji, Ying","Graph sparsification serves as a foundation for many algorithms, such as approximation algorithms for graph cuts and Laplacian system solvers. As its natural generalization, hypergraph sparsification has recently gained increasing attention, with broad applications in graph machine learning and other areas. In this work, we propose the first quantum algorithm for hypergraph sparsification, addressing an open problem proposed by Apers and de Wolf (FOCS'20). For a weighted hypergraph with $n$ vertices, $m$ hyperedges, and rank $r$, our algorithm outputs a near-linear size $\varepsilon$-spectral sparsifier in time $\widetilde O(r\sqrt{mn}/\varepsilon)$. This algorithm matches the quantum lower bound for constant $r$ and demonstrates quantum speedup when compared with the state-of-the-art $\widetilde O(mr)$-time classical algorithm. As applications, our algorithm implies quantum speedups for computing hypergraph cut sparsifiers, approximating hypergraph mincuts and hypergraph $s$-$t$ mincuts.","Hypergraphs — mathematical structures that model complex multi-way relationships, such as group collaborations in social networks or chemical bonds in molecules — are critical tools for modern algorithm design. However, efficiently simplifying them (a process called hypergraph sparsification) while retaining their essential properties remains a foundational challenge in theoretical computer science. Classical methods for this task scale linearly for large problems, and whether quantum algorithms could solve it faster has been an open question for years.In this theoretical result, we propose the first quantum algorithm for hypergraph sparsification. By harnessing quantum superposition, our method achieves a runtime that provably outperforms classical approaches for common cases. Our algorithm matches fundamental quantum speed limits (lower bounds) for constant-rank hypergraphs — a result that establishes quantum computing’s inherent advantage for this problem.This work advances our understanding of quantum algorithms for hypergraph problems, and paves the way for faster quantum techniques in optimization and machine learning—fields where hypergraphs play a crucial role."
Poster,Quantum Speedups in Regret Analysis of Infinite Horizon Average-Reward Markov Decision Processes,https://ICML.cc//virtual/2025/poster/46126,"Bhargav Ganguly, Yang Xu, Vaneet Aggarwal","This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$\footnote{$\tilde{\mathcal{O}}(\cdot)$ conceals logarithmic terms of $T$.}, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts, where $T$ is the length of the time horizon.","Reinforcement learning systems that aims to operate for unlimited periods (i.e., warehouse robots or power grid controllers) aim to maximize their long-run average reward. Classical reinforcement learning agents under infinite horizon average-reward settings accumulate regret that inevitably grows like $\tilde{O}(\sqrt{T})$.  We design Quantum-UCRL, the first algorithm that lets an agent tap a ""quantum transition oracle."" The oracle encodes all possible next-states in a single superposition; a quantum mean-estimation routine then extracts their statistics with quadratically fewer samples. A momentum-style update reuses information that would normally be destroyed by quantum measurement, and a new martingale-free analysis proves the method works. Together these ideas drive the worst-case regret down to $\tilde{O}(1)$, being only logarithmic dependent of time, beating the classical $\tilde{O}(\sqrt{T})$ lower bound by an exponential margin."
Poster,QuEst: Enhancing Estimates of Quantile-Based Distributional Measures Using Model Predictions,https://ICML.cc//virtual/2025/poster/45665,"Xinyu Yang, Tom Zollo, Benjamin Eyre, Amogh Inamdar, David Madras, Richard Zemel","As machine learning models grow increasingly competent, their predictions can supplement scarce or expensive data in various important domains. In support of this paradigm, algorithms have emerged to combine a small amount of high-fidelity observed data with a much larger set of imputed model outputs to estimate some quantity of interest. Yet current hybrid-inference tools target only means or single quantiles, limiting their applicability for many critical domains and use cases. We present QuEst, a principled framework to merge observed and imputed data to deliver point estimates and rigorous confidence intervals for a wide family of quantile-based distributional measures. QuEst covers a range of measures, from tail risk (CVaR) to population segments such as quartiles, that are central to fields such as economics, sociology, education, medicine, and more. We extend QuEst to multidimensional metrics, and introduce an additional optimization technique to further reduce variance in this and other hybrid estimators. We demonstrate the utility of our framework through experiments in economic modeling, opinion polling, and language model auto-evaluation.","We introduce QuEst, a method for combining real, observed data with machine learning model predictions to produce better estimates of important quantities.  Our framework is especially useful for enhancing experimental findings in fields such as economics, sociology, education, medicine, as well as for evaluating language models."
Poster,QuEST: Stable Training of LLMs with 1-Bit Weights and Activations,https://ICML.cc//virtual/2025/poster/45754,"Andrei Panferov, Jiale Chen, Rush Tabesh, Mahdi Nikdan, Dan Alistarh","One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment.While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by *directly training* over such representations, i.e., *Quantization-Aware Training (QAT)*, is still open: for example, a recent study put the ""optimal"" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard  FP16/BF16 precision, at 8-bits weights and activations. We advance this state-of-the-art via a new method called QuEST, for which we demonstrate optimality at 4-bits and stable convergence as low as 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new *trust gradient estimator* based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the ""true"" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at [https://github.com/IST-DASLab/QuEST](https://github.com/IST-DASLab/QuEST).","Large Language Models (LLMs) are expensive to train and use. Low-precision data types offer a way to reduce costs. We propose a novel method to train LLMs directly using these data types, which yields improved model accuracy. We demonstrate that our method achieves the best accuracy-to-cost tradeoff at around 4-bit precision and is stable for even lower precision."
Poster,QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval,https://ICML.cc//virtual/2025/poster/43476,"Jaehyun Kwak, Izaaz Inhar, Se-Young Yun, Sung-Ju Lee","Composed Image Retrieval (CIR) retrieves relevant images based on a reference image and accompanying text describing desired modifications. However, existing CIR methods only focus on retrieving the target image and disregard the relevance of other images. This limitation arises because most methods employing contrastive learning-which treats the target image as positive and all other images in the batch as negatives-can inadvertently include false negatives. This may result in retrieving irrelevant images, reducing user satisfaction even when the target image is retrieved. To address this issue, we propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which optimizes a reward model objective to reduce false negatives. Additionally, we introduce a hard negative sampling strategy that selects images positioned between two steep drops in relevance scores following the target image, to effectively filter false negatives. In order to evaluate CIR models on their alignment with human satisfaction, we create Human-Preference FashionIQ (HP-FashionIQ), a new dataset that explicitly captures user preferences beyond target retrieval. Extensive experiments demonstrate that QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting the strongest alignment with human preferences on the HP-FashionIQ dataset. The source code is available at https://github.com/jackwaky/QuRe.","Imagine uploading a photo of a shirt and typing “make it blue with short sleeves.”Today’s image search systems, Composed Image Retrieval (CIR) models are trained using a rigid rule: only the exact match is correct, and every other image is considered incorrect. This leads the model to penalize many relevant but imperfect matches during training, resulting in mixed-quality retrievals.Our method, QuRe (Query-Relevant Retrieval), takes a different approach. Instead of judging images in isolation, it teaches the model to compare: is image A more relevant than image B? We carefully select the “B” images, those that fall between two steep drops in the model’s own relevance scores, ensuring they are truly different and informative. This hard negative sampling better reflects what users actually care about.QuRe achieves state-of-the-art performance on standard benchmarks and aligns more closely with human preferences on HP-FashionIQ, a new human-annotated dataset we release. This approach enhances the accuracy and user satisfaction of visual search in e-commerce and media platforms."
Poster,QUTE: Quantifying Uncertainty in TinyML models with Early-exit-assisted ensembles for model-monitoring,https://ICML.cc//virtual/2025/poster/45956,"Nikhil Pratap Ghanathe, Steve Wilton","Uncertainty quantification (UQ) provides a resource-efficient solution for on-device monitoring of tinyML models deployed remotely without access to true labels. However, existing UQ methods impose significant memory and compute demands, making them impractical for ultra-low-power, KB-sized tinyML devices. Prior work has attempted to reduce overhead by using early-exit ensembles to quantify uncertainty in a single forward pass, but these approaches still carry prohibitive costs. To address this, we propose QUTE, a novel resource-efficient early-exit-assisted ensemble architecture optimized for tinyML models. QUTE introduces additional output blocks at the final exit of the base network, distilling early-exit knowledge into these blocks to form a diverse yet lightweight ensemble. We show that QUTE delivers superior uncertainty quality on tiny models, achieving comparable performance on larger models with 59% smaller model sizes than the closest prior work. When deployed on a microcontroller, QUTE demonstrates a 31% reduction in latency on average. In addition, we show that QUTE excels at detecting accuracy-drop events, outperforming all prior works.","Tiny AI models, known as TinyML, are being used in everything from wearable health monitors to environmental sensors. These models run directly on low-power devices — often no bigger than a coin — and help make fast decisions without needing to send data to the cloud. But there’s a problem: *how do we know when these tiny models are making a mistake*, especially when they’re deployed in remote or hard-to-reach places with no cloud access or human supervision?That’s where **QUTE** comes in. QUTE is a new technique we’ve developed to help tiny AI models recognize when they’re uncertain — in other words, when their predictions might be wrong. This ability to estimate confidence (called uncertainty quantification) is crucial for safety, reliability, and smart decision-making in the field. Unlike other methods that are too large or slow for tiny devices, QUTE is designed to work efficiently on ultra-low-power hardware.It works by adding small ""checkpoints"" inside the model and training lightweight prediction blocks that each learn something different. Together, they act like a mini-team of experts that can double-check each other’s confidence and flag when the model might be unsure — all without taking up much space or time.In real-world tests, QUTE made decisions faster (31% lower delay), used less memory (59% smaller), and was better at spotting problems compared to previous approaches. This makes QUTE a strong choice for making AI on tiny devices both faster and more trustworthy."
Poster,Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers,https://ICML.cc//virtual/2025/poster/45429,"Weilun Feng, Chuanguang Yang, Haotong Qin, Xiangqi Li, Yu Wang, Zhulin An, Libo Huang, Boyu Diao, Zixiang Zhao, Yongjun Xu, Michele Magno","Diffusion transformers (DiT) have demonstrated exceptional performance in video generation. However, their large number of parameters and high computational complexity limit their deployment on edge devices. Quantization can reduce storage requirements and accelerate inference by lowering the bit-width of model parameters.Yet, existing quantization methods for image generation models do not generalize well to video generation tasks. We identify two primary challenges: the loss of information during quantization and the misalignment between optimization objectives and the unique requirements of video generation. To address these challenges, we present **Q-VDiT**, a quantization framework specifically designed for video DiT models. From the quantization perspective, we propose the *Token aware Quantization Estimator* (TQE), which compensates for quantization errors in both the token and feature dimensions. From the optimization perspective, we introduce *Temporal Maintenance Distillation* (TMD), which preserves the spatiotemporal correlations between frames and enables the optimization of each frame with respect to the overall video context. Our W3A6 Q-VDiT achieves a scene consistency score of 23.40, setting a new benchmark and outperforming the current state-of-the-art quantization methods by **1.9$\times$**.","Diffusion transformers (DiT) have demonstrated exceptional performance in video generation. Quantization appears to be a useful acceleration method. Yet, existing quantization methods for image generation models do not generalize well to video generation tasks. We present **Q-VDiT**, a quantization framework specifically designed for video DiT models. From the quantization perspective, we propose the *Token aware Quantization Estimator* (TQE), which compensates for quantization errors. From the optimization perspective, we introduce *Temporal Maintenance Distillation* (TMD), which preserves the spatiotemporal correlations between frames. Our W3A6 Q-VDiT achieves a scene consistency score of 23.40, setting a new benchmark and outperforming the current state-of-the-art quantization methods by **1.9$\times$**."
Poster,R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts,https://ICML.cc//virtual/2025/poster/44039,"Zhongyang Li, Ziyue Li, Tianyi Zhou","In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by different downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method ''**R**e-**R**outing in **T**est-**T**ime (R2-T2)'' that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and significantly improves state-of-the-art LMMs' performance on challenging multimodal benchmarks of diverse tasks, without training any parameters in the base model. Our code can be accessed here.","Most AI assistants struggle to choose the right “tools” when answering questions about images, leading to mistakes (Problem). We solved this by letting the system look at past examples when it gets a new image-and-question task and borrow the expert “recipe” that worked before (Solution). Without changing or retraining the AI, our approach automatically picks the best experts—like a chef following a trusted recipe—to understand objects, read text, or judge spatial relationships. This means the model gives more accurate answers on the spot, even for tough questions. In practice, our method helps AI assistants become smarter about which visual skills to use each time they face a new task. People and companies will benefit because they can get better image-based insights without the cost of rebuilding or retraining large AI systems (Impact)."
Poster,R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45066,"Harsh Goel, Mohammad Omama, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Sandeep Chinchali","Multi-agent reinforcement learning (MARL) has achieved significant progress in large-scale traffic control, autonomous vehicles, and robotics. Drawing inspiration from biological systems where roles naturally emerge to enable coordination, role-based MARL methods have been proposed to enhance cooperation learning for complex tasks. However, existing methods exclusively derive roles from an agent's past experience during training, neglecting their influence on its future trajectories. This paper introduces a key insight: an agent’s role should shape its future behavior to enable effective coordination. Hence, we propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel role-based MARL framework that learns emergent roles by maximizing the mutual information between agents' roles, observed trajectories, and expected future behaviors. R3DM optimizes the proposed objective through contrastive learning on past trajectories to first derive intermediate roles that shape intrinsic rewards to promote diversity in future behaviors across different roles through a learned dynamics model. Benchmarking on SMAC and SMACv2 environments demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving multi-agent coordination to increase win rates by up to 20%. The code is available at https://github.com/UTAustin-SwarmLab/R3DM.","Multi-agent reinforcement learning (MARL) is a type of artificial intelligence where multiple agents (like robots or self-driving cars) learn to work together to achieve shared goals. Inspired by how animals and humans naturally take on different roles to cooperate, researchers have developed methods for these AI agents to learn roles too. However, most existing approaches identify the roles for agents based on what they have done in the past.This paper introduces a new idea: the roles that agents take on should influence their future behavior so they can coordinate better as a team. We present a novel MARL method R3DM, which helps agents discover and take on roles that are not only based on their past experience but also designed to shape their future behavior. R3DM encourages agents to behave differently from each other. These behavioral differences enable agents to learn a broader set of specialized behaviors that help the team succeed. Evaluations on simulated multi-player games based on StarCraft show that R3DM helped agents coordinate much better, leading upto 20% more wins compared to previous methods."
Poster,Radio: Rate–Distortion Optimization for Large Language Model Compression,https://ICML.cc//virtual/2025/poster/44344,Sean I. Young,"In recent years, the compression of large language models (LLMs) has emerged as a key problem in facilitating LLM deployment on resource-limited devices, reducing compute costs, and mitigating the environmental footprint due to large-scale AI infrastructure. Here, we establish the foundations of LLM quantization from a rate–distortion theory perspective and propose a quantization technique based on simple rate–distortion optimization. Our technique scales to models containing hundreds of billions of weight parameters and offers users the flexibility to compress models, post-training, to a model size or accuracy specified by the user.","In recent years, the compression of large language models (LLMs) has emerged as a key problem in facilitating LLM deployment on resource-limited devices, reducing compute costs, and mitigating the environmental footprint due to large-scale AI infrastructure. Here, we establish the foundations of LLM quantization from a theory perspective and propose a quantization technique based on simple optimization. Our technique scales to models containing hundreds of billions of weight parameters and offers users the flexibility to compress models, post-training, to a model size or accuracy specified by the user."
