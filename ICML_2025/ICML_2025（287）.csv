type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Strategic Planning: A Top-Down Approach to Option Generation,https://ICML.cc//virtual/2025/poster/43567,"Max Ruiz Luyten, Antonin Berthon, Mihaela van der Schaar","Real-world human decision-making often relies on strategic planning, where *high-level* goals guide the formulation of sub-goals and subsequent actions, as evidenced by domains such as healthcare, business, and urban policy. Despite notable successes in controlled settings, conventional reinforcement learning (RL) follows a *bottom-up* paradigm, which can struggle to adapt to real-world complexities such as sparse rewards and limited exploration budgets. While methods like hierarchical RL and environment shaping provide partial solutions, they frequently rely on either ad-hoc designs (e.g. choose the set of high-level actions) or purely data-driven discovery of high-level actions that still requires significant exploration. In this paper, we introduce a *top-down* framework for RL that explicitly leverages *human-like strategy* to reduce sample complexity, guide exploration, and enable high-level decision-making. We first formalize the *Strategy Problem*, which frames policy generation as finding distributions over policies that balance *specificity* and *value*. Building on this definition, we propose the *Strategist* agent—an iterative framework that leverages large language models (LLMs) to synthesize domain knowledge into a structured representation of actionable strategies and sub-goals. We further develop a *reward shaping methodology* that translates these strategies expressed in natural language into quantitative feedback for RL methods. Empirically, we demonstrate a significantly faster convergence than conventional PPO. Taken together, our findings highlight that *top-down strategic exploration* opens new avenues for enhancing RL on real-world decision problems.","Traditional artificial intelligence learns through extensive trial and error, like a child randomly pressing buttons until something works. This approach is inefficient and impractical for real-world applications like healthcare or business, where mistakes are costly and time is limited. Humans, by contrast, start with high-level strategies—like a doctor creating a treatment plan before choosing specific medications.We developed a “Strategist” system inspired by human strategic thinking that uses large language models to generate multiple high-level plans before an AI agent begins learning. Instead of random exploration, our system first identifies promising approaches, breaks these into specific steps, and guides the learning process using these strategic insights as rewards. For example, in a resource-gathering task, it might consider different methods like direct collection versus building infrastructure for long-term production.Our approach substantially improves AI learning efficiency, requiring fewer attempts to master complex tasks while discovering behaviors that traditional methods often miss entirely. This could accelerate AI deployment in critical domains like medical treatment planning or business strategy, where extensive trial-and-error learning is neither feasible nor safe, making AI more practical for real-world applications."
Poster,Strategy Coopetition Explains the Emergence and Transience of In-Context Learning,https://ICML.cc//virtual/2025/poster/44561,"Aaditya Singh, Ted Moskovitz, Sara Dragutinović, Feilx Hill, Stephanie Chan, Andrew Saxe","In-context learning (ICL) is a powerful ability that emerges in transformer models, enabling them to learn from context without weight updates. Recent work has established emergent ICL as a transient phenomenon that can sometimes disappear after long training times. In this work, we sought a mechanistic understanding of these transient dynamics. Firstly, we find that—after the disappearance of ICL—the asymptotic strategy is a remarkable hybrid between in-weights and in-context learning, which we term “context-constrained in-weights learning” (CIWL). CIWL is in competition with ICL, and eventually replaces it as the dominant strategy of the model (thus leading to ICL transience). However, we also find that the two competing strategies actually share sub-circuits, which gives rise to cooperative dynamics as well. For example, in our setup, ICL is unable to emerge quickly on its own, and can only be enabled through the simultaneous slow development of asymptotic CIWL. CIWL thus both cooperates and competes with ICL, a phenomenon we term “strategy coopetition”. Wepropose a minimal mathematical model that reproduces these key dynamics and interactions. Informed by this model, we were able to identify a setup where ICL is truly emergent and persistent.","In-context learning (ICL) is a key ability of modern large language models, allowing them to learn from context and adapt to new situations without being explicitly trained for them. Prior work has found that, in many cases of overtraining, classical in-context learning is actually transient, meaning it appears but then fades away. We reproduce this finding in a smaller scale setup, thus enabling mechanistic investigation. Through these investigations, we find that the asymptotic strategy (which emerges after ICL transience) is context-constrained in-weight learning (CIWL) -- namely, the model relies on associations learned in the past, but constrains the recall based on the current context. Competition for capacity leads CIWL to ""crowd out"" ICL, but surprisingly, cooperation between the two strategies is what leads to ICL emergence in the first place. We capture these ""coopetition"" dynamics in a toy model, which motivates an empirical setting where ICL is emergent and persistent. Overall, this work deepens our mechanistic understanding of how different strategies may tradeoff through during the course of transformer training."
Poster,Stray Intrusive Outliers-Based Feature Selection on Intra-Class Asymmetric Instance Distribution or Multiple High-Density Clusters,https://ICML.cc//virtual/2025/poster/44249,"Lixin Yuan, Yirui Wu, WENXIAO ZHANG, Minglei Yuan, Jun Liu","For data with intra-class Asymmetric instance Distribution or Multiple High-density Clusters (ADMHC), outliers are real and have specific patterns for data classification, where the class body is necessary and difficult to identify. Previous Feature Selection (FS) methods score features based on all training instances or rarely target intra-class ADMHC. In this paper, we propose a supervised FS method, Stray Intrusive Outliers-based FS (SIOFS), for data classification with intra-class ADMHC. By focusing on Stray Intrusive Outliers (SIOs), SIOFS modifies the skewness coefficient and fuses the threshold in the 3$\sigma$ principle to identify the class body, scoring features based on the intrusion degree of SIOs. In addition, the refined density-mean center is proposed to represent the general characteristics of the class body reasonably. Mathematical formulations, proofs, and logical exposition ensure the rationality and universality of the settings in the proposed SIOFS method. Extensive experiments on 16 diverse benchmark datasets demonstrate the superiority of SIOFS over 12 state-of-the-art FS methods in terms of classification accuracy, normalized mutual information, and confusion matrix. SIOFS source codes is available at https://github.com/XXXly/2025-ICML-SIOFS","In many real-world datasets, such as images or medical records, data within the same class can have complex patterns, like uneven spreads or multiple dense clusters, making it hard to distinguish between classes. Some data points, called stray outliers, which look more like another class (e.g., a resort image mistaken for a school, or handwritten digits 4 and 9 appearing similar). Traditional feature selection (FS) methods treat all data points equally, ignoring these critical outliers. This paper introduces a new FS method, SIOFS, which focuses on these stray outliers that intrude other class bodies. SIOFS identifies the main characteristic of each class using a refined statistical approach, helping identify features that best separate classes. By testing on 16 diverse datasets, SIOFS outperformed 12 existing FS methods in accuracy and reliability. This advance is particularly useful for small or complex datasets where outliers and overlapping classes are common. This paper provides an interesting way to mine the patterns of tricky data, improving automated classification in fields like healthcare or image recognition."
Poster,Stream-level Flow Matching with Gaussian Processes,https://ICML.cc//virtual/2025/poster/43928,"Ganchao Wei, Li Ma","Flow matching (FM) is a family of training algorithms for fitting continuous normalizing flows (CNFs). Conditional flow matching (CFM) exploits the fact that the marginal vector field of a CNF can be learned by fitting least-squares regression to the conditional vector field specified given one or both ends of the flow path. In this paper, we extend the CFM algorithm by defining conditional probability paths along ""streams'', instances of latent stochastic paths that connect data pairs of source and target, which are modeled with Gaussian process (GP) distributions. The unique distributional properties of GPs help preserve the ``simulation-free'' nature of CFM training. We show that this generalization of the CFM can effectively reduce the variance in the estimated marginal vector field at a moderate computational cost, thereby improving the quality of the generated samples under common metrics. Additionally, adopting the GP on the streams allows for flexibly linking multiple correlated training data points (e.g., time series). We empirically validate our claim through both simulations and applications to image and neural time series data.","Flow matching (FM) is a family of fast and powerful methods for training deep generative models that learn how to transform draws from a simple source distribution into those from a highly complex target distribution. While the effectiveness of FM has been demonstrated in a range of applications, there are a few technical limitations that can lead to a decay in sample quality and occasionally lead to generation of outliers that do not accurately reflect the characteristics of the target distribution. We recognize one of the key limitations related to how the paths connecting draws from the source to the target are constructed in existing FM algorithms, and propose an extension that incorporates Gaussian processes to model more flexible, stochastic paths. The properties of Gaussian processes make the resulting algorithm more robust, enable integration of multiple correlated distributions, all while retaining the computational efficiency of FM algorithms."
Poster,Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM,https://ICML.cc//virtual/2025/poster/44643,"Penghao Wu, Lewei Lu, Ziwei Liu","Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no information loss. Our key insight is that vision tokens from the pretrained vision encoder do not necessarily require all the heavy operations (e.g., self-attention, FFNs) in decoder-only LMMs and could be processed more lightly with proper designs. We designed a series of experiments to discover and progressively squeeze out the vision-related computation redundancy. Based on our findings, we propose ProxyV, a novel approach that utilizes proxy vision tokens to alleviate the computational burden on original vision tokens. ProxyV enhances efficiency without compromising performance and can even yield notable performance gains in scenarios with more moderate efficiency improvements. Furthermore, the flexibility of ProxyV is demonstrated through its combination with token reduction methods to boost efficiency further.","In this paper, we systematically study the computation-level redundancy on vision tokens in decoder-only LMMs and explore ways to progressively reduce it. We propose ProxyV, a novel design that introduces proxy tokens to carry out heavy computations, effectively reducing computation while ensuring performance.We extensively validate the effectiveness of ProxyV with different LLMs and show its flexibility by proposing a non-spatial variant that can be directly combined with token reduction methods."
Poster,Strengthen Out-of-Distribution Detection Capability with Progressive Self-Knowledge Distillation,https://ICML.cc//virtual/2025/poster/44372,"Yang Yang, Haonan Xu","Out-of-distribution (OOD) detection aims to ensure AI system reliability by rejecting inputs outside the training distribution. Recent work shows that memorizing atypical samples during later stages of training can hurt OOD detection, while strategies for forgetting them show promising improvements. However, directly forgetting atypical samples sacrifices ID generalization and limits the model's OOD detection capability. To address this issue, we propose Progressive Self-Knowledge Distillation (PSKD) framework, which strengthens the OOD detection capability by leveraging self-provided uncertainty-embedded targets. Specifically, PSKD adaptively selects a self-teacher model from the training history using pseudo-outliers, facilitating the learning of uncertainty knowledge via multi-level distillation applied to features and responses. As a result, PSKD achieves better ID generalization and uncertainty estimation for OOD detection. Moreover, PSKD is orthogonal to most existing methods and can be integrated as a plugin to collaborate with them. Experimental results from multiple OOD scenarios verify the effectiveness and general applicability of PSKD.","Deep learning models can make mistakes when they see out-of-distribution (OOD) data—inputs that fall outside the training distribution—but still act very confidently about those wrong answers. This overconfidence raises safety and reliability concerns. Recently, researchers have found that deep learning models tend to memorize atypical samples late in training, which makes them overly confident about these atypical samples and harder to detect when something is OOD. Some methods try to make the model forget these atypical samples, but that hurts its performance on regular tasks.In this paper, we develop a new method called Progressive Self-Knowledge Distillation (PSKD) to help models better detect OOD inputs through self-teaching. PSKD allows the model not only to learn the correct answers but also to understand how confident it should be in those answers. As a result, PSKD improves both the model’s performance on regular tasks and its ability to identify OOD data. Additionally, PSKD works well with many existing methods, making it useful for improving AI safety and reliability in various situations."
Poster,Strong and Weak Identifiability of Optimization-based Causal Discovery in Non-linear Additive Noise Models,https://ICML.cc//virtual/2025/poster/43823,"Mingjia Li, Hong Qian, Tian-Zuo Wang, ShujunLi, Min Zhang, Aimin Zhou","Causal discovery aims to identify causal relationships from observational data. Recently, optimization-based causal discovery methods have attracted extensive attention in the literature due to their efficiency in handling high-dimensional problems. However, we observe that optimization-based methods often perform well on certain problems but struggle with others. This paper identifies a specific characteristic of causal structural equations that determines the difficulty of identification in causal discovery and, in turn, the performance of optimization-based methods. We conduct an in-depth study of the additive noise model (ANM) and propose to further divide identifiable problems into strongly and weakly identifiable types based on the difficulty of identification. We also provide a sufficient condition to distinguish the two categories. Inspired by these findings, this paper further proposes GENE, a generic method for addressing strongly and weakly identifiable problems in a unified way under the ANM assumption. GENE adopts an order-based search framework that incorporates conditional independence tests into order fitness evaluation, ensuring effectiveness on weakly identifiable problems. In addition, GENE restricts the dimensionality of the effect variables to ensure \emph{scale invariance}, a property crucial for practical applications. Experiments demonstrate that GENE is uniquely effective in addressing weakly identifiable problems while also remaining competitive with state-of-the-art causal discovery algorithms for strongly identifiable problems.","Figuring out cause-and-effect relationships from data is crucial across many scientific fields, but current automated methods often struggle with complex non-linear systems, performing inconsistently. Our research identified that this is due to how ""identifiable"" the causal links are, leading us to distinguish between ""strongly"" and ""weakly"" identifiable problems. We developed GENE, a unified method that first determines a potential causal order of variables and then evaluates it by combining how well it fits the data with crucial statistical independence tests, ensuring robustness even when data scales vary. This significantly improves the discovery of correct causal links, particularly in challenging ""weakly identifiable"" scenarios where many existing methods fail, providing scientists with a more dependable tool to understand underlying mechanisms in complex systems."
Poster,Stronger Neyman Regret Guarantees for Adaptive Experimental Design,https://ICML.cc//virtual/2025/poster/44466,"Georgy Noarov, Riccardo Fogliato, Martin A Bertran, Aaron Roth","We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering *sublinear Neyman regret*, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design.Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\widetilde{O}(\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\widetilde{O}(\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual ``multigroup'' Neyman regret guarantees: Given a set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\widetilde{O}(\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments.","Consider a randomized control trial: you sequentially receive experimental units with corresponding pairs of outcomes (A/B) and you can only choose one outcome to see from each pair (as in A/B testing). A very important quantity to (unbiasedly) estimate is the average treatment effect (ATE), i.e. the average difference in rewards from A-outcomes and B-outcomes. How do you do that with estimation variance as small as possible? We first design an adaptive, sequential scheme for sampling A and B outcomes, such that it simultaneously beats every fixed design (e.g. ""choose A always with probability 50%"") up to a small difference that goes to 0 faster (in the number of experimental units T) than in prior work. Secondly, we design an even more adaptive sequential scheme that we call ""multigroup"", which takes into account the units' features to further reduce estimation variance: for instance, if units are humans, the features could be their demographics, and our scheme then beats the best fixed group-specific design on every demographic group!"
Poster,Structured Preconditioners in Adaptive Optimization: A Unified Analysis,https://ICML.cc//virtual/2025/poster/45803,"Shuo Xie, Tianhao Wang, Sashank J. Reddi, Sanjiv Kumar, Zhiyuan Li","We present a novel unified analysis for a broad class of adaptive optimization algorithms with structured (e.g., layerwise, diagonal, and kronecker-factored) preconditioners for both online regret minimization and offline convex optimization. Our analysis not only provides matching rate to several important structured preconditioned algorithms including diagonal AdaGrad, full-matrix AdaGrad, and AdaGrad-Norm, but also gives an improved convergence rate for a one-sided variant of Shampoo over that of original Shampoo. Interestingly, more structured preconditioners (e.g., diagonal Adagrad, AdaGrad-Norm which use less space and compute) are often presented as computationally efficient approximations to full-matrix Adagrad, aiming for improved optimization performance through better approximations. Our unified analysis challenges this prevailing view and reveals, perhaps surprisingly, that more structured preconditioners, despite using less space and computation per step, can outperform their less structured counterparts. To demonstrate this, we show that one-sided Shampoo, which is relatively much cheaper than full-matrix AdaGrad could outperform it both theoretically and experimentally.","When training machine learning models, especially large ones, we use tools called optimizers to help the model learn quickly and accurately. One popular optimizer is AdaGrad, which adjusts how fast each part of the model learns based on how difficult it is to train. A more powerful version called full-matrix AdaGrad learns even faster but is too expensive to run on today’s large models because it requires too much memory and computation.To solve this, researchers have designed simplified versions that use less memory by making structured approximations. Common wisdom assumes that these simpler versions are just cheaper but not as effective. In our work, we challenge that belief.We show that some of these cheaper, more structured optimizers — like AdaGrad-Norm and a version of Shampoo we call “one-sided Shampoo” — can actually perform better than the more complex ones for some specific settings, both in theory and in practice. We also provide the first unified mathematical framework that explains why this happens.This research helps us better understand how to speed up training of large models efficiently, without sacrificing performance — an important step toward making powerful AI systems more accessible and sustainable."
Poster,Structure-Guided Large Language Models for Text-to-SQL Generation,https://ICML.cc//virtual/2025/poster/44477,"Qinggang Zhang, Hao Chen, Junnan Dong, Shengyuan Chen, Feiran Huang, Xiao Huang","Recent advancements in large language models (LLMs) have shown promise in bridging the gap between natural language queries and database management systems, enabling users to interact with databases without the background of SQL. However, LLMs often struggle to fully exploit and comprehend the user intention and complex structures of databases. Decomposition-based methods have been proposed to enhance the performance of LLMs on complex tasks, but decomposing SQL generation into subtasks is non-trivial due to the declarative structure of SQL syntax and the intricate connections between query concepts and database elements. In this paper, we propose a novel Structure GUided text-to-SQL framework ( SGU-SQL) that incorporates syntax-based prompting to enhance the SQL generation capabilities of LLMs. Specifically, SGU-SQL establishes structure-aware links between user queries and database schema and recursively decomposes the complex generation task using syntax-based prompting to guide LLMs in incrementally constructing target SQLs. Extensive experiments on two benchmark datasets demonstrate that SGU-SQL consistently outperforms state-of-the-art text-to-SQL baselines.","We identify the limitations of LLM-based Text-to-SQL models and introduce SGU-SQL, which breaks down the complex generation task in a syntax-aware manner. This ensures that the generated queries maintain both semantic accuracy (correctly capturing user intentions) and syntactic correctness (following proper SQL structure).SGU-SQL proposes a graph-based structure construction to comprehend user queries and database structures and then link queries and databases with dual-graph encoding.SGU-SQL introduces tailored structure-decomposed generation strategies to decompose queries with syntax trees and then incrementally generate accurate SQL with LLM.When tested on standard benchmarks, SGU-SQL consistently outperformed existing baseline models, making it easier for non-experts to get accurate data from databases using natural language."
