type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Unifying Knowledge from Diverse Datasets to Enhance Spatial-Temporal Modeling: A Granularity-Adaptive Geographical Embedding Approach,https://ICML.cc//virtual/2025/poster/43731,"Zhigaoyuan Wang, Ying Sun, Hengshu Zhu","Spatio-temporal forecasting provides potential for discovering evolutionary patterns in geographical scientific data. However, geographical scientific datasets are often manually collected across studies, resulting in limited time spans and data scales. This hinders existing methods that rely on rich historical data for individual entities. In this paper, we argue that heterogeneous datasets from different studies can provide complementary insights into the same underlying system, helping improve predictions for geographical entities with limited historical data. To this end, we propose a Segment Quadtree Geographical Embedding Framework (SQGEF). SQGEF integrates knowledge from datasets with varied target entities, time spans, and observation variables to learn unified representations for multi-granularity entities—including those absent during training. Specifically, we propose a novel data structure, Segment Quadtree, that flexibly accommodates entities of varying granularities. SQGEF not only captures multi-level interactions from grid data but also extracts nested relationships and human-defined boundaries from diverse entities, enabling a comprehensive understanding of complex geographical structures. Experiments on real-world datasets demonstrate that SQGEF effectively represents unseen geographical entities and enhances performance for various models.","Traditionally, when we want to predict attributes of a geographical entity like a province or city, we analyze historical data from that same entity. However, especially in scientific data fields, it's often difficult to obtain historical records for the specific entity we want to predict. While we may lack data for our target entity, we frequently have access to many other datasets from the same region that contain historical records of related entities or sub-areas.We propose a method that can fuse these different types of regional datasets together and store them in a novel data structure called Segment Quadtree. This structure enables us to query information about entities that didn't appear in our original datasets, significantly improving our prediction capabilities."
Poster,Unifying Specialized Visual Encoders for Video Language Models,https://ICML.cc//virtual/2025/poster/44055,"Jihoon Chung, Tyler Zhu, Max Gonzalez Saez-Diez, Juan Carlos Niebles, Honglu Zhou, Olga Russakovsky","Recent advances in vision backbones have yielded powerful and diverse visual and video encoders. Yet, current Video Large Language Models encode visual inputs using an encoder from a single backbone family, limiting the amount and type of visual information they can process. We propose MERV, a Multi-Encoder Video Representation, which utilizes multiple encoders for a comprehensive video representation. To optimize heterogeneous features from a broad spectrum of encoders and ensure efficient and coherent feature integration, MERV first aligns encoder features spatio-temporally, then projects them into a unified structure, and finally fuses them through cross-attention. Under fair comparison, MERV achieves up to 4.62% higher accuracy than its base model, while introducing minimal extra parameters and training faster than equivalent single-encoder methods after parallelizing visual processing. Qualitative analysis shows MERV successfully captures and integrates domain knowledge from each encoder, opening new possibilities for scaling enhanced video understanding.","When researchers design AI chatbots that understand videos, they often use a single pre-existing vision-understanding module. However, we found that such modules are often specialized, e.g. for understanding colors but not movements, or vice versa. Extending their knowledge in a simple manner proved difficult. We design a method of harnessing multiple specialized vision modules together, even if each was created with different designs and goals. Our AI chatbot, MERV, answers a greater breadth of video-related questions as each module can cover for the other's weaknesses. As one of the first papers to work on multi-module chatbots, we hope this demonstrates the potential of having multiple vision-understanding modules in one system."
Poster,"UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation",https://ICML.cc//virtual/2025/poster/44980,"Wangzhi Zhan, Chen Jianpeng, Dongqi Fu, Dawei Zhou","Metamaterials are artificial materials that are designed to meet unseen properties in nature, such as ultra-stiffness and negative materials indices. In mechanical metamaterial design, three key modalities are typically involved, i.e., 3D topology, density condition, and mechanical property. Real-world complex application scenarios place the demanding requirements on machine learning models to consider all three modalities together. However, a comprehensive literature review indicates that most existing works only consider two modalities, e.g., predicting mechanical properties given the 3D topology or generating 3D topology given the required properties. Therefore, there is still a significant gap for the state-of-the-art machine learning models capturing the whole. Hence, we propose a unified model named UniMate, which consists of a modality alignment module and a synergetic diffusion generation module. Experiments indicate that UniMate outperforms the other baseline models in topology generation task, property prediction task, and condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We open-source our proposed UniMate model and corresponding results at https://github.com/wzhan24/UniMate.","Metamaterials are specially engineered materials that can achieve mechanical behaviors not commonly found in nature, such as being extremely stiff while staying lightweight. Designing these materials is complex because it involves three important parts: the 3D structure (topology), the material density, and the mechanical performance. Most existing methods can only handle two of these parts at a time.Our work introduces UniMate, a new AI model that, for the first time, can consider and connect all three parts together in a single system. UniMate can generate material structures, predict how they will perform, and determine the right conditions they need to meet specific goals.We tested UniMate on all three tasks and found that it performs better than previous methods, creating higher-quality structures, predicting properties more accurately, and confirming conditions more effectively. To support this, we also built a new dataset and evaluation tools that cover a wider range of material design problems.UniMate is a step toward making the design of mechanical metamaterials more comprehensive and effective."
Poster,UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation,https://ICML.cc//virtual/2025/poster/45772,"Qin Guo, Ailing Zeng, Dongxu Yue, Ceyuan Yang, Yang Cao, Hanzhong Guo, SHEN FEI, Wei Liu, Xihui Liu, Dan Xu","Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans (e.g., animals). Moreover, it is difficult to generate multiple overlapping humans and animals based on keypoint controls solely. These challenges arise from two main aspects: the inherent limitations of existing controllable methods and the lack of suitable datasets. First, we design a DiT-based framework, named UniMC, to explore unifying controllable multi-class image generation. UniMC integrates instance- and keypoint-level conditions into compact tokens, incorporating attributes such as class, bounding box, and keypoint coordinates. This approach overcomes the limitations of previous methods that struggled to distinguish instances and classes due to their reliance on skeleton images as conditions. Second, we propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K images with 2.9M instances. This dataset features extensive annotations such as keypoints, bounding boxes, and fine-grained captions for both humans and animals, along with rigorous manual inspection to ensure annotation accuracy. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly in heavy occlusions and multi-class scenarios.","Our approach endows text-to-image diffusion models with the ability to generate humans and animals guided by user-provided keypoints, thereby enhancing the usability of controllable generative models."
Poster,UniMoMo: Unified Generative Modeling of 3D Molecules for De Novo Binder Design,https://ICML.cc//virtual/2025/poster/45639,"Xiangzhe Kong, Zishen Zhang, Ziting Zhang, Rui Jiao, Jianzhu Ma, Wenbing Huang, Kai Liu, Yang Liu","The design of target-specific molecules such as small molecules, peptides, and antibodies is vital for biological research and drug discovery. Existing generative methods are restricted to single-domain molecules, failing to address versatile therapeutic needs or utilize cross-domain transferability to enhance model performance. In this paper, we introduce **Uni**fied generative **Mo**delingof 3D **Mo**lecules (UniMoMo), the first framework capable of designing binders of multiple molecular domains using a single model. In particular, UniMoMo unifies the representations of different molecules as graphs of blocks, where each block corresponds to either a standard amino acid or a molecular fragment. Based on these unified representations, UniMoMo utilizes a geometric latent diffusion model for 3D molecular generation, featuring an iterative full-atom autoencoder to compress blocks into latent space points, followed by an E(3)-equivariant diffusion process. Extensive benchmarks across peptides, antibodies, and small molecules demonstrate the superiority of our unified framework over existing domain-specific models, highlighting the benefits of multi-domain training.","Finding the right molecule that can stick to (or ""bind"") a disease-related protein is at the heart of making new medicines. These molecules can come in many forms, like small chemicals, short strings of amino acids (called peptides), or even large immune proteins (called antibodies). Today’s AI tools usually focus on designing just one kind of molecule at a time, with limited flexibility to take advantage of similarities across different types of drugs.In this work, we introduce **UniMoMo**, a new artificial intelligence model that can design all three major types of drug-like molecules using a single system. The model represents molecules as collections of building blocks (like amino acids or chemical fragments) and learns how these blocks interact with target proteins. We use advanced AI techniques to first compress the detailed atomic structure into a simpler form, then generate new molecules in this compressed space, and finally rebuild the full 3D atomic structure.We tested UniMoMo on a wide range of benchmarks and showed that it performs better than specialized models built for only one type of molecule. We also showed that the model can generalize—for example, designing small molecules that borrow helpful patterns from peptide or antibody interactions. This flexibility brings us a step closer to using a single tool to design many kinds of therapeutics, potentially speeding up and simplifying drug discovery."
Poster,UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules,https://ICML.cc//virtual/2025/poster/45040,"Ziyang Yu, Wenbing Huang, Yang Liu","Molecular Dynamics (MD) simulations are essential for understanding the atomic-level behavior of molecular systems, giving insights into their transitions and interactions. However, classical MD techniques are limited by the trade-off between accuracy and efficiency, while recent deep learning-based improvements have mostly focused on single-domain molecules, lacking transferability to unfamiliar molecular systems. Therefore, we propose **Uni**fied **Sim**ulator (UniSim), which leverages cross-domain knowledge to enhance the understanding of atomic interactions. First, we employ a multi-head pretraining approach to learn a unified atomic representation model from a large and diverse set of molecular data. Then, based on the stochastic interpolant framework, we learn the state transition patterns over long timesteps from MD trajectories, and introduce a force guidance module for rapidly adapting to different chemical environments. Our experiments demonstrate that UniSim achieves highly competitive performance across small molecules, peptides, and proteins.","Simulating how molecules move and interact at the atomic level is crucial for designing medicines and materials, but existing methods face a dilemma: they’re either too slow for practical use or too simplified to capture real-world complexity. Recent AI advancements only work for specific molecule types, limiting their real-world applications.We developed UniSim, a universal AI ""molecular simulator"" trained on diverse molecules—from simple chemicals to complex proteins. By learning patterns across different molecular families, UniSim predicts atomic interactions accurately while adapting quickly to new systems. Its design complies with the underlying principles of physics by mimicking ""virtual"" forces during simulation.Unlike prior tools confined to narrow domains, UniSim achieves highly competitive performance across small molecules, peptides, and proteins in our experiments. This breakthrough could accelerate discoveries in drug development by enabling realistic, efficient simulations for previously incompatible molecular systems."
Poster,Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE Solvers,https://ICML.cc//virtual/2025/poster/43911,"Hang Zhou, Yuezhou Ma, Haixu Wu, Haowen Wang, Mingsheng Long","Deep models have recently emerged as promising tools to solve partial differential equations (PDEs), known as neural PDE solvers. While neural solvers trained from either simulation data or physics-informed loss can solve PDEs reasonably well, they are mainly restricted to a few instances of PDEs, e.g. a certain equation with a limited set of coefficients. This limits their generalization to diverse PDEs, preventing them from being practical surrogate models of numerical solvers. In this paper, we present Unisolver, a novel Transformer model trained on diverse data and conditioned on diverse PDEs, aiming towards a universal neural PDE solver capable of solving a wide scope of PDEs. Instead of purely scaling up data and parameters, Unisolver stems from the theoretical analysis of the PDE-solving process. Inspired by the mathematical structure of PDEs that a PDE solution is fundamentally governed by a series of PDE components such as equation symbols and boundary conditions, we define a complete set of PDE components and flexibly embed them as domain-wise and point-wise deep conditions for Transformer PDE solvers. Integrating physical insights with recent Transformer advances, Unisolver achieves consistent state-of-the-art on three challenging large-scale benchmarks, showing impressive performance and generalizability. Code is available at [https://github.com/thuml/Unisolver](https://github.com/thuml/Unisolver).","Solving physical equations that describe natural phenomena—called partial differential equations (PDEs)—is a critical task in science and engineering. Classical numerical methods can be slow and require re-computing for each specific task. Recently, deep models have been explored as efficient surrogates for numerical solvers. However, previous methods may struggle to handle diverse types of PDEs. In this work, we propose Unisolver, a powerful AI model based on Transformer architecture capable of solving a wide scope of PDEs. Instead of just training the model on more data, we carefully design the model to incorporate different components of PDEs, like the equations, coefficients and boundary conditions, just like what a numerical solver would receive. Unisolver learns from diverse PDE samples and achieves excellent results on challenging tests, making meaningful progress towards a universal AI tool for solving physical equations efficiently."
Poster,Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems,https://ICML.cc//virtual/2025/poster/44443,"Shilong Tao, Zhe Feng, Haonan Sun, Zhanxing Zhu, Yunhuai Liu","Multi-solid systems are foundational to a wide range of real-world applications, yet modeling their complex interactions remains challenging. Existing deep learning methods predominantly rely on implicit modeling, where the factors influencing solid deformation are not explicitly represented but are instead indirectly learned. However, as the number of solids increases, these methods struggle to accurately capture intricate physical interactions. In this paper, we introduce a novel explicit modeling paradigm that incorporates factors influencing solid deformation through structured modules. Specifically, we present Unisoma, a unified and flexible Transformer-based model capable of handling variable numbers of solids. Unisoma directly captures physical interactions using contact modules and adaptive interaction allocation mechanism, and learns the deformation through a triplet relationship. Compared to implicit modeling techniques, explicit modeling is more well-suited for multi-solid systems with diverse coupling patterns, as it enables detailed treatment of each solid while preventing  information blending and confusion. Experimentally, Unisoma achieves consistent state-of-the-art performance across seven well-established datasets and two complex multi-solid tasks. Code is avaiable at [https://github.com/therontau0054/Unisoma](https://github.com/therontau0054/Unisoma).","We want to understand how multiple solid objects, like metal parts and flexible materials, interact and deform when they come into contact. This is important for many real-world applications, such as robotic gripping, metal stamping, or even medical simulations. However, existing AI (artificial intellengence) models often struggle to handle the complex interactions when many different solids are involved.To tackle this, we build a new system called Unisoma. Instead of letting the AI learn everything by trial and error (which can lead to confusion when too many objects are involved), we teach it to explicitly recognize and model key physical factors — like objects touch others and forces are applied. Our method uses a powerful type of foundational model called a Transformer to manage all this in a flexible way that works with different numbers and types of objects.We find that Unisoma is not only more accurate than existing methods, but also more efficient and better at handling unseen scenarios. This makes it a promising tool for engineers and scientists who want to simulate physical systems more reliably. We've made the code freely available to help others apply this approach to their own challenges."
Poster,Universal Approximation of Mean-Field Models via Transformers,https://ICML.cc//virtual/2025/poster/43652,"Shiba Biswal, Karthik Elamvazhuthi, Rishi Sonthalia","This paper investigates the use of transformers to approximate the mean-field dynamics of interacting particle systems exhibiting collective behavior. Such systems are fundamental in modeling phenomena across physics, biology, and engineering, including opinion formation, biological networks, and swarm robotics. The key characteristic of these systems is that the particles are indistinguishable, leading to permutation-equivariant dynamics. First, we empirically demonstrate that transformers are well-suited for approximating a variety of mean field models, including the Cucker-Smale model for flocking and milling, and the mean-field system for training two-layer neural networks. We validate our numerical experiments via mathematical theory. Specifically, we prove that if a finite-dimensional transformer effectively approximates the finite-dimensional vector field governing the particle system, then the $L_\infty$ distance between the \textit{expected transformer} and the infinite-dimensional mean-field vector field can be bounded by a function of the number of particles observed during training. Leveraging this result, we establish theoretical bounds on the distance between the true mean-field dynamics and those obtained using the transformer.","This work shows that transformer networks—the same models behind today’s language AIs—can learn to predict how large groups of identical “particles” (like birds in a flock, robots in a swarm, or neurons in a simple neural net) move together. Instead of tracking each particle, scientists often use “mean-field” equations describing the crowd’s overall behavior. Because transformers naturally handle many inputs without regard to order, they’re ideal for these indistinguishable-agent systems.The authors train transformers on two classic examples—the Cucker–Smale flocking model and a mean-field view of two-layer neural-network training—and find excellent agreement with simulated data. They then prove that if a transformer closely approximates the rules for a finite number of particles, one can mathematically bound its error when modeling infinitely many, giving a clear guarantee on how training size controls accuracy."
Poster,Universal Approximation Theorem of Deep Q-Networks,https://ICML.cc//virtual/2025/poster/45935,Qian Qi,"We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data.","Imagine teaching a computer to make smart decisions in situations that change smoothly over time, like guiding a self-driving car through flowing traffic or managing a complex power grid. Many current AI learning methods work well for situations with distinct steps, like a board game. Our research explores how a popular AI technique, called Deep Q-Networks (DQNs), can learn in these more fluid, continuous environments.We show two key things. First, these DQNs are theoretically powerful enough to learn the ""best possible moves"" (or, more technically, figure out the optimal decision-making strategy) with high accuracy, no matter how complicated the continuous task is. Second, we demonstrate that the common ways these DQNs are trained will, under the right conditions, actually lead them to correctly learn these best moves.To achieve this, we've developed a new mathematical foundation that connects these AI learning methods with established theories from control engineering. This work helps build a more solid understanding of how DQNs behave and learn in real-world scenarios that don't just jump from one step to the next, paving the way for more reliable and effective AI in dynamic systems."
