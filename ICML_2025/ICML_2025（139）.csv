type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Hypo3D: Exploring Hypothetical Reasoning in 3D,https://ICML.cc//virtual/2025/poster/46012,"Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk","The rise of vision-language foundation models marks an advancement in bridging the gap between human and machine capabilities in 3D scene reasoning. Existing 3D reasoning benchmarks assume real-time scene accessibility, which is impractical due to the high cost of frequent scene updates. To this end, we introduce *Hypothetical 3D Reasoning*, namely Hypo3D, a benchmark designed to evaluate models' ability to reason without access to real-time scene data. Models need to imagine the scene state based on a provided change description before reasoning. Hypo3D is formulated as a 3D Visual Question Answering (VQA) benchmark, comprising 7,727 context changes across 700 indoor scenes, resulting in 14,885 question-answer pairs. An anchor-based world frame is established for all scenes, ensuring consistent reference to a global frame for directional terms in context changes and QAs. Extensive experiments show that state-of-the-art foundation models struggle to reason effectively in hypothetically changed scenes. This reveals a substantial performance gap compared to humans, particularly in scenarios involving movement changes and directional reasoning. Even when the change is irrelevant to the question, models often incorrectly adjust their answers. The code and dataset are publicly available at: https://matchlab-imperial.github.io/Hypo3D.","Imagine a rescue robot navigating a collapsed building. It receives an update: a wall has fallen, opening a new path. A human can imagine this change and plan accordingly without seeing the updated scene. Today’s AI cannot. Most models rely on real time detailed 3D data to reason, which is often unavailable in dynamic unpredictable environments. To address this, we introduce Hypo3D, the first benchmark that evaluates whether AI can reason about a 3D scene using only a description of how it has changed. This mirrors human like hypothetical reasoning, mentally updating a scene before making decisions. Our dataset spans 7727 changes and 14885 question answer pairs across 700 indoor scenes, challenging models to imagine and infer. Our results reveal a striking gap: even state of the art AI models consistently fail when imagination is required. Hypo3D highlights this fundamental limitation and offers a path forward toward AI systems that reason more flexibly, safely, and in a more human like way in complex real world settings."
Poster,Hypothesis Testing for Generalized Thurstone Models,https://ICML.cc//virtual/2025/poster/45422,"Anuran Makur, Japneet Singh","In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying *generalized Thurstone model* $\mathcal{T}_F$ for a given choice function $F$. While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for $\mathcal{T}_F$ models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of $\mathcal{T}_F$ models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as $\Theta((nk)^{-1/2})$, where $n$ is the number of agents and $k$ is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets.","When people or systems compare two items—like players in a game or products in a survey—it is often assumed that these choices follow a specific kind of model based on hidden “scores” or preferences. One popular family of such models is known as the generalized Thurstone model, where each item has a score and the choice depends on how much higher one score is compared to another. Our work tackles the following fundamental question: Given only the outcomes of several such comparisons, how can we tell if the choices actually obey a generalized Thurstone model? To address this question, we propose a data-driven approach to test whether choices obey a generalized Thurstone model by using and developing tools from machine learning and statistics. We provide both rigorous mathematical guarantees and real-world experiments in our analysis. For example, we show that our approach is “optimal” in a certain sense under appropriate analytical conditions. Our results can be of utility to practitioners who either seek to test whether their modeling assumptions hold or select accurate models for comparison data."
Poster,IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck,https://ICML.cc//virtual/2025/poster/46175,"Tian Bian, Yifan Niu, Chaohao Yuan, Chengzhi Piao, Bingzhe Wu, Long-Kai Huang, Yu Rong, Tingyang Xu, Hong Cheng, Jia Li","Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to holistically identify informative circuits. In contrast to traditional causal interventions, IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.","Modern language models, like those powering chatbots and translation services, are incredibly complex and often act as ""black boxes,"" making it hard to understand how they perform specific tasks. Researchers want to uncover the hidden pathways, or ""circuits,"" inside these models that handle particular functions, such as identifying the subject in a sentence or solving math problems. However, existing methods to find these circuits are often cumbersome and inefficient because they require designing specific tests for each task.To address this, the authors of the paper introduce IBCircuit, a new method inspired by the Information Bottleneck principle—a technique that helps filter out unnecessary information while keeping the important details. IBCircuit automates the discovery of these critical circuits in a holistic way, meaning it looks at the entire model rather than isolated parts. This approach eliminates the need for manually creating task-specific tests, making the process faster and more accurate.Through experiments, IBCircuit successfully identified the essential circuits in language models that handle tasks like understanding sentence structure and performing arithmetic. Compared to previous methods, IBCircuit found circuits that are both simpler and more reliable, enhancing our ability to interpret and trust these powerful AI systems. Ultimately, this advancement helps bridge the gap between complex AI models and human understanding, paving the way for more transparent and dependable artificial intelligence."
Poster,ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks,https://ICML.cc//virtual/2025/poster/43992,"Zhiyao Ren, Siyuan Liang, Aishan Liu, Dacheng Tao","In-context learning (ICL) has demonstrated remarkable success in large language models (LLMs) due to its adaptability and parameter-free nature. However, it also introduces a critical vulnerability to backdoor attacks, where adversaries can manipulate LLM behaviors by simply poisoning a few ICL demonstrations. In this paper, we propose, for the first time, the dual-learning hypothesis, which posits that LLMs simultaneously learn both the task-relevant latent concepts and backdoor latent concepts within poisoned demonstrations, jointly influencing the probability of model outputs. Through theoretical analysis, we derive an upper bound for ICL backdoor effects, revealing that the vulnerability is dominated by the concept preference ratio between the task and the backdoor. Motivated by these findings, we propose ICLShield, a defense mechanism that dynamically adjusts the concept preference ratio. Our method encourages LLMs to select clean demonstrations during the ICL phase by leveraging confidence and similarity scores, effectively mitigating susceptibility to backdoor attacks. Extensive experiments across multiple LLMs and tasks demonstrate that our method achieves state-of-the-art defense effectiveness, significantly outperforming existing approaches (+26.02\% on average). Furthermore, our method exhibits exceptional adaptability and defensive performance even for closed-source models (*e.g.*, GPT-4).","Large language models (LLMs) like ChatGPT can learn new tasks from just a few examples shown in the prompt — this is called in-context learning (ICL). However, this flexibility creates a security vulnerability called ICL backdoor attack: attackers can manipulate the model's behavior by adding malicious examples into these demonstrations.We discovered that when processing these poisoned demonstrations, LLMs simultaneously learn two things: the task-relvant latent concepts and backdoor latent concepts. The model's final behavior depends on which learning signal is stronger — like a competition between good and bad influences. Through theoretical analysis, we found that this vulnerability depends on the balance between task-relevant and backdoor concepts. Based on this insight, we developed ICLShield, a defense method that selects and adds clean demonstrations using confidence and similarity measures.Our method achieves state-of-the-art protection, outperforming existing defenses by 26% on average, and works even with closed-source models like GPT-4, significantly improving AI safety against ICL backdoor attacks."
Poster,Identifiable Object Representations under Spatial Ambiguities,https://ICML.cc//virtual/2025/poster/44625,"Avinash Kori, Francesca Toni, Ben Glocker","Modular object-centric representations are essential for *human-like reasoning* but are challenging to obtain under spatial ambiguities,  *e.g. due to occlusions and view ambiguities*. However, addressing challenges presents both theoretical and practical difficulties. We introduce a novel multi-view probabilistic approach that aggregates view-specific slots to capture *invariant content* information while simultaneously learning disentangled global *viewpoint-level* information. Unlike prior single-view methods, our approach resolves spatial ambiguities, provides theoretical guarantees for identifiability, and requires *no viewpoint annotations*. Extensive experiments on standard benchmarks and novel complex datasets validate our method's robustness and scalability.","Object-centric learning focuses on extracting distinct representations for individual objects within a scene, as opposed to learning a single global representation for the entire scene. A key challenge arises when objects are only partially visible or the scene is viewed from oblique or obscure angles—issues collectively referred to as spatial ambiguities. In this paper, we propose a method specifically designed to address these ambiguities. Our approach involves observing a given scene from multiple viewpoints and leveraging the resulting perspectives to correlate and integrate object-specific features, thereby producing a unified, viewpoint-invariant representation for each object. We provide both theoretical justification and empirical evidence demonstrating that this multi-view correlation strategy yields more robust and reliable object representations."
Poster,Identification of Latent Confounders via Investigating the  Tensor Ranks of the Nonlinear Observations,https://ICML.cc//virtual/2025/poster/45026,"Zhengming Chen, Yewei Xia, Feng Xie, Jie Qiao, Zhifeng Hao, Ruichu Cai, Kun Zhang","We study the problem of learning discrete latent variable causal structures from mixed-type observational data. Traditional methods, such as those based on the tensor rank condition, are designed to identify discrete latent structure models and provide robust identification bounds for discrete causal models. However, when observed variables—specifically, those representing the children of latent variables—are collected at various levels with continuous data types, the tensor rank condition is not applicable, limiting further causal structure learning for latent variables. In this paper, we consider a more general case where observed variables can be either continuous or discrete, and further allow for scenarios where multiple latent parents cause the same set of observed variables. We show that, under the completeness condition, it is possible to discretize the data in a way that satisfies the full-rank assumption required by the tensor rank condition. This enables the identifiability of discrete latent structure models within mixed-type observational data. Moreover, we introduce the two-sufficient measurement condition, a more general structural assumption under which the tensor rank condition holds and the underlying latent causal structure is identifiable by a proposed two-stage identification algorithm. Extensive experiments on both simulated and real-world data validate the effectiveness of our method.","Understanding what causes certain outcomes is crucial, especially when some influencing factors are hidden or ""latent."" Imagine trying to figure out why some people get a particular disease, but you can only see their symptoms (observed data), not all the underlying biological processes (latent variables). Traditional methods for uncovering these hidden causes often rely on a mathematical tool called ""tensor rank,"" but these methods only work when all the information we observe is in a simple, categorical form (like ""yes"" or ""no"").Our research tackles a more complex and realistic scenario: what if the observed information is a mix of categories and continuous measurements (like blood pressure readings)? Or what if multiple hidden factors influence the same set of observable symptoms? We discovered a way to transform the mixed data into a form that allows the tensor rank tool to work, even with continuous measurements. This means we can now identify these hidden causal structures in a much wider range of real-world situations. We also developed a new condition and a two-step algorithm to achieve this. Our experiments show that our method is effective in identifying these hidden causes from complex, mixed-type data."
Poster,Identifying and Understanding Cross-Class Features in Adversarial Training,https://ICML.cc//virtual/2025/poster/45866,"Zeming Wei, Yiwen Guo, Yisen Wang","Adversarial training (AT) has been considered one of the most effective methods for making deep neural networks robust against adversarial attacks, while the training mechanisms and dynamics of AT remain open research problems. In this paper, we present a novel perspective on studying AT through the lens of class-wise feature attribution. Specifically, we identify the impact of a key family of features on AT that are shared by multiple classes, which we call cross-class features. These features are typically useful for robust classification, which we offer theoretical evidence to illustrate through a synthetic data model. Through systematic studies across multiple model architectures and settings, we find that during the initial stage of AT, the model tends to learn more cross-class features until the best robustness checkpoint. As AT further squeezes the training robust loss and causes robust overfitting, the model tends to make decisions based on more class-specific features.  Based on these discoveries, we further provide a unified view of two existing properties of AT, including the advantage of soft-label training and robust overfitting. Overall, these insights refine the current understanding of AT mechanisms and provide new perspectives on studying them. Our code is available at https://github.com/PKU-ML/Cross-Class-Features-AT.","Deep neural networks are vulnerable to adversarial attacks, which are slightly perturbed inputs designed to fool models. Adversarial training (AT) is a leading method to improve model robustness, but its training mechanisms and dynamics are not fully understood.In this paper, we propose a new perspective to study AT by focusing on cross-class features, which are shared by multiple classes. We find that during the initial stage of AT, models tend to learn more cross-class features, but as training progresses, they rely more on class-specific features, leading to robust overfitting. We also show that soft-label training methods can help preserve cross-class features and thus mitigate overfitting. Our findings refine the current understanding of AT mechanisms and provide new insights for studying robust generalization. By identifying the role of cross-class features in AT, our work may inspire more reliable defense strategies against adversarial attacks, improving the safety and robustness of AI systems in critical applications like autonomous driving and cybersecurity."
Poster,Identifying biological perturbation targets through causal differential networks,https://ICML.cc//virtual/2025/poster/45518,"Menghua Wu, Umesh Padia, Sean Murphy, Regina Barzilay, Tommi Jaakkola","Identifying variables responsible for changes to a biological system enables applications in  drug target discovery and cell engineering. Given a pair of observational and interventional datasets, the goal is to isolate the subset of observed variables that were the targets of the intervention. Directly applying causal discovery algorithms is challenging: the data may contain thousands of variables with as few as tens of samples per intervention, and biological systems do not adhere to classical causality assumptions. We propose a causality-inspired approach to address this practical setting. First, we infer noisy causal graphs from the observational and interventional data. Then, we learn to map the differences between these graphs, along with additional statistical features, to sets of variables that were intervened upon. Both modules are jointly trained in a supervised framework, on simulated and real data that reflect the nature of biological interventions. This approach consistently outperforms baselines for perturbation modeling on seven single-cell transcriptomics datasets. We also demonstrate significant improvements over current causal discovery methods for predicting soft and hard intervention targets across a variety of synthetic data.","Understanding the root causes of change to a biological system can uncover insights behind why drugs work, or how diseases arise. This is challenging through traditional approaches, i.e. try many interventions and see what happens, because the space of options is very large. Our approach follows the inverse direction: we first predict causal relationships that may explain each dataset, and we look for differences in these graphs. We train this model on simulated and real biological data, and the model performs well at recovering these root cases in multiple real, single-cell settings."
Poster,Identifying Causal Direction via Variational Bayesian Compression,https://ICML.cc//virtual/2025/poster/45912,"Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen","Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches.","Many scientific questions aim at answering cause-and-effect questions of what causes what—for example, does A cause B, or the other way around? Such questions are difficult to answer when we can only passively observe the data without any active interaction. Our research addresses this challenge through the principle of Occam's razor, where the simplest yet sufficiently accurate explanation of the data is likely the cause. Previous methods rely on either over-simplified or overly complex models, making a trade-off between accuracy and simplicity. Instead, we employ a more flexible approach based on Bayesian neural networks, which are tools that can capture complex patterns while still favoring simpler explanations. This provides us with a better balance between accuracy and simplicity, making it effective at telling apart the cause and effect. Results on both simulated and real-world data show that our method reliably outperforms existing techniques in answering cause-and-effect questions."
Poster,Identifying Metric Structures of Deep Latent Variable Models,https://ICML.cc//virtual/2025/poster/45898,"Stas Syrota, Yevgen Zainchkovskyy, Johnny Xi, Benjamin Bloem-Reddy, Søren Hauberg","Deep latent variable models learn condensed representations of data that, hopefully, reflect the inner workings of the studied phenomena. Unfortunately, these latent representations are not statistically identifiable, meaning they cannot be uniquely determined. Domain experts, therefore, need to tread carefully when interpreting these. Current solutions limit the lack of identifiability through additional constraints on the latent variable model, e.g. by requiring labeled training data, or by restricting the expressivity of the model. We change the goal: instead of identifying the latent variables, we identify relationships between them such as meaningful distances, angles, and volumes. We prove this is feasible under very mild model conditions and without additional labeled data. We empirically demonstrate that our theory results in more reliable latent distances, offering a principled path forward in extracting trustworthy conclusions from deep latent variable models.","Reducing complex phenomena to a smaller number of meaningful factors has always been central to scientific discovery. Today, this process is increasingly driven by machine learning models rather than individual intuition. These models are highly effective at predicting the studied phenomena while relying on some notion of a hidden structure. However, the structures the models produce are fundamentally ambiguous which makes their interpretation difficult.A key issue is that many different internal representations, ways the model “understands” the data, can perform equally well. This means that even when models appear to succeed, the specific factors they rely on might be arbitrary or misleading. As a result, two research teams using similar tools on the same dataset can reach very different conclusions based on the recovered representations. This lack of identifiability undermines trust in hidden mechanisms recovered by machine learning algorithms and limits their usefulness for scientific insight.Our work takes a different perspective. Rather than trying to make the internal representations themselves uniquely defined, we focus on relations like distances, angles, and volumes as this kinds of information os often what matters most in scientific exploration. We prove that such geometric relations between the learned representations can, in fact, be identified reliably, when their measurement respects the underlying geometry of the model."
