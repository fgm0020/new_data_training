type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,(How) Can Transformers Predict Pseudo-Random Numbers?,https://ICML.cc//virtual/2025/poster/44774,"Tao Tao, Darshil Doshi, Dayal Singh Kalra, Tianyu He, Maissam Barkeshli","Transformers excel at discovering patterns in sequential data, yet their fundamental limitations and learning mechanisms remain crucial topics of investigation. In this paper, we study the ability of Transformers to learn pseudo-random number sequences from linear congruential generators (LCGs), defined by the recurrence relation $x_{t+1} = a x_t + c \\;\mathrm{mod}\\; m$. We find that with sufficient architectural capacity and training data variety, Transformers can perform in-context prediction of LCG sequences with unseen moduli ($m$) and parameters ($a,c$). By analyzing the embedding layers and attention patterns, we uncover how Transformers develop algorithmic structures to learn these sequences in two scenarios of increasing complexity. First, we investigate how Transformers learn LCG sequences with unseen ($a, c$) but fixed modulus; and demonstrate successful learning up to $m = 2^{32}$. We find that models learn to factorize $m$ and utilize digit-wise number representations to make sequential predictions. In the second, more challenging scenario of unseen moduli, we show that Transformers can generalize to unseen moduli up to $m_{\text{test}} = 2^{16}$. In this case, the model employs a two-step strategy: first estimating the unknown modulus from the context, then utilizing prime factorizations to generate predictions. For this task, we observe a sharp transition in the accuracy at a critical depth $d= 3$. We also find that the number of in-context sequence elements needed to reach high accuracy scales sublinearly with the modulus.","We study whether a particular class of AI models, called Transformers, can learn to predict sequences of seemingly random numbers (PRNGs) that follow hidden mathematical rules. We find that when the models are complex enough and sufficient example-sequences are shown to them, they can successfully learn to predict new and unseen PRNGs by figuring out the underlying rules. These models develop their own strategies to predict PRNGs, which involve breaking the numbers into smaller prime factors and using them to simplify the sequences. Our research shows how modern AI systems can discover and apply complex mathematical rules without being explicitly programmed to do so, helping us understand both their capabilities and limitations."
Poster,How Compositional Generalization and Creativity Improve as Diffusion Models are Trained,https://ICML.cc//virtual/2025/poster/46642,"Alessandro Favero, Antonio Sclocchi, Francesco Cagnetta, Pascal Frossard, Matthieu Wyart","Natural data is often organized as a hierarchical composition of features. How many samples do generative models need in order to learn the composition rules, so as to produce a combinatorially large number of novel data? What signal in the data is exploited to learn those rules? We investigate these questions in the context of diffusion models both theoretically and empirically. Theoretically, we consider a simple probabilistic context-free grammar - a tree-like graphical model used to represent the hierarchical and compositional structure of data such as language and images. We demonstrate that diffusion models learn the grammar's composition rules with the sample complexity required for clustering features with statistically similar context, a process similar to the word2vec algorithm. However, this clustering emerges hierarchically: higher-level features associated with longer contexts require more data to be identified. This mechanism leads to a sample complexity that scales polynomially with the said context size. As a result, diffusion models trained on an intermediate dataset size generate data coherent up to a certain scale, but lacking global coherence. We test these predictions across different domains and find remarkable agreement:  both generated texts and images achieve progressively larger coherence lengths as the training time or dataset size grows. We discuss connections between the hierarchical clustering mechanism we introduce here and the renormalization group in physics.","How can AI models learn to be truly creative, composing entirely new combinations of what they’ve seen before? Humans do this all the time in language and art, but for AI models, it's not well understood how this ability develops. In our research, we investigated how powerful generative AI models, known as diffusion models, learn to combine simple building blocks into more complex patterns. We used a simplified model of grammar rules to explore this, and discovered that these AI models learn local patterns first (like the ability to produce short phrases), and only gradually build up the ability to create globally coherent outputs (like full stories or complex images) as they see more data. We confirmed these findings not only in synthetic data but also in real-world settings like image and text generation. Our work helps explain how AI creativity emerges."
Poster,How Contaminated Is Your Benchmark? Measuring Dataset Leakage in Large Language Models with Kernel Divergence,https://ICML.cc//virtual/2025/poster/43619,"Hyeong Kyu Choi, Maxim Khanov, Hongxin Wei, Sharon Li","Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Measuring dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that evaluates dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings.Code is released in https://github.com/deeplearning-wisc/kernel-divergence-score.","When we test AI models, it is important to make sure they're solving new problems, and not just memorizing things they have seen before. But sometimes, the test questions (datasets) accidentally include parts the model has already seen during its training. This makes the model look smarter than it really is. To address this issue, we introduce a new method called Kernel Divergence Score (KDS), which measures how much the model’s understanding of data changes before and after learning from a new set of examples. If the model’s view doesn’t change much, it probably already saw those examples before. We tested this method on many benchmarks, confirming that it works reliably compared to previous approaches."
Poster,How Distributed Collaboration Influences the Diffusion Model Training? A Theoretical Perspective,https://ICML.cc//virtual/2025/poster/44607,"Jing Qiao, Yu Liu, YUAN YUAN, Xiao Zhang, Zhipeng Cai, Dongxiao Yu","This paper examines the theoretical performance of distributed diffusion models in environments where computational resources and data availability vary significantly among workers. Traditional models centered on single-worker scenarios fall short in such distributed settings, particularly when some workers are resource-constrained. This discrepancy in resources and data diversity challenges the assumption of accurate score function estimation foundational to single-worker models. We establish the inaugural generation error bound for distributed diffusion models in resource-limited settings, establishing a linear relationship with the data dimension $d$ and consistency with established single-worker results. Our analysis highlights the critical role of hyperparameter selection in influencing the training dynamics, which are key to the performance of model generation. This study provides a streamlined theoretical approach to optimizing distributed diffusion models, paving the way for future research in this area.","Diffusion models are widely used to generate high-quality data, but most theories assume that the training process is based on only one powerful machine with abundant data.In practice, diffusion models often need to be trained across many devices that differ widely in speed, memory and local datasets, breaking the usual guarantee of accurate score estimation on each worker. We address this by proving the first generation-error bound for diffusion models trained in such resource-limited, heterogeneous settings. Remarkably, our bound shows a linear relationship with the data dimension—matching the best known single-machine results. We also show how key hyperparameters (such as learning rates, noise schedules and update frequencies) directly influence this bound and hence the final generation quality. By tuning these settings, we can balance workloads across devices and still achieve reliable, high-quality output."
Poster,How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction,https://ICML.cc//virtual/2025/poster/44470,"Jun Chen, Hong Chen, Yonghua Yu, Yiming Ying","In recent years, contrastive learning has achieved state-of-the-art performance in the territory of self-supervised representation learning. Many previous works have attempted to provide the theoretical understanding underlying the success of contrastive learning. Almost all of them rely on a default assumption, i.e., the label consistency assumption, which may not hold in practice (the probability of failure is called labeling error) due to the strength and randomness of common augmentation strategies, such as random resized crop (RRC). This paper investigates the theoretical impact of labeling error on the downstream classification performance of contrastive learning. We first reveal several significant negative impacts of labeling error on downstream classification risk. To mitigate these impacts, data dimensionality reduction method (e.g., singular value decomposition, SVD) is applied on original data to reduce false positive samples, and establish both theoretical and empirical evaluations. Moreover, it is also found that SVD acts as a double-edged sword, which may lead to the deterioration of downstream classification accuracy due to the reduced connectivity of the augmentation graph. Based on the above observations, we give the augmentation suggestion that we should use some moderate embedding dimension (such as $512, 1024$ in our experiments), data inflation, weak augmentation, and SVD to ensure large graph connectivity and small labeling error to improve model performance.","Contrastive learning has recently achieved state-of-the-art performance without label information. How does the label inconsistency (label error) caused by data augmentation affect contrastive learning? We wanted to theoretically answer this question from the perspective of data dimensionality reduction. This paper reveals some negative impacts of label error on the downstream prediction performance. Surprisingly, we find that traditional data dimensionality reduction methods like singular value decomposition (SVD) can mild these negative impacts based on theoretical analysis. Meanwhile, massive experiments under various settings validate our theory. Our theoretical analysis provides some useful suggestions related to contrastive training, e,g, adopting moderate embedding dimension (such as $512, 1024$ in our experiments), data inflation, weak augmentation, and SVD."
Poster,How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation,https://ICML.cc//virtual/2025/poster/45906,"Yining Pan, Qiongjie Cui, Xulei Yang, Na Zhao","LiDAR-based 3D panoptic segmentation often struggles with the inherent sparsity of data from LiDAR sensors, which makes it challenging to accurately recognize distant or small objects. Recently, a few studies have sought to overcome this challenge by integrating LiDAR inputs with camera images, leveraging the rich and dense texture information provided by the latter. While these approaches have shown promising results, they still face challenges, such as misalignment during data augmentation and the reliance on post-processing steps. To address these issues, we propose **I**mage-**A**ssists-**L**iDAR (**IAL**), a novel multi-modal 3D panoptic segmentation framework.In IAL, we first introduce a modality-synchronized data augmentation strategy, PieAug, to ensure alignment between LiDAR and image inputs from the start. Next, we adopt a transformer decoder to directly predict panoptic segmentation results. To effectively fuse LiDAR and image features into tokens for the decoder, we design a Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the complementary strengths of each modality as priors for query initialization through a Prior-based Query Generation (PQG) module, enhancing the decoder’s ability to generate accurate instance masks. Our IAL framework achieves state-of-the-art performance compared to previous multi-modal 3D panoptic segmentation methods on two widely used benchmarks. Code and models are publicly available at https://github.com/IMPL-Lab/IAL.git.","Autonomous vehicles, such as self-driving cars, need to understand everything around them to drive safely. This research helps these vehicles perceive their environment more clearly by combining camera images with LiDAR, a laser-based 3D sensor. Cameras provide rich details and color information, while LiDAR offers precise distance measurements and object outlines. Leveraging the complementary strengths of these two sensors, we show how images can be aligned with and enhance LiDAR data to identify all objects and surfaces in a scene—a task known as 3D panoptic segmentation.We developed a system called **Image-Assists-LiDAR (IAL)** that integrates information from both sensors in a top-down manner—starting with data alignment, followed by feature fusion, and finally mask generation.First, we ensure that camera and LiDAR data are well-matched and diverse during training. Second, we combine them into a unified representation that is both accurate and comprehensive. Finally, the system uses clues from both sensors to better locate objects, helping it detect more objects that one sensor may fail to catch.This dual-perspective approach leads to safer, more reliable perception in real-world applications, since the machine is far less likely to misidentify or overlook important details."
Poster,(How) Do Language Models Track State?,https://ICML.cc//virtual/2025/poster/46279,"Belinda Li, Carl Guo, Jacob Andreas","Transformer language models (LMs) exhibit behaviors—from storytelling to code generation—that seem to require tracking the unobserved state of an evolving world. How do they do this? We study state tracking in LMs trained or fine-tuned to compose permutations (i.e., to compute the order of a set of objects after a sequence of swaps). Despite the simple algebraic structure of this problem, many other tasks (e.g., simulation of finite automata and evaluation of boolean expressions) can be reduced to permutation composition, making it a natural model for state tracking in general. We show that LMs consistently learn one of two state tracking mechanisms for this task. The first closely resembles the “associative scan” construction used in recent theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second uses an easy-to-compute feature (permutation parity) to partially prune the space of outputs, and then refines this with an associative scan. LMs that learn the former algorithm tend to generalize better and converge faster, and we show how to steer LMs toward one or the other with intermediate training tasks that encourage or suppress the heuristics. Our results demonstrate that transformer LMs, whether pre-trained or fine-tuned, can learn to implement efficient and interpretable state-tracking mechanisms, and the emergence of these mechanisms can be predicted and controlled. Code and data are available at https://github.com/belindal/state-tracking","Language models (like ChatGPT) often seem to ""understand"" what's happening in a story or a game by keeping track of how things change over time. But how do they actually do this?To study this, we trained models on puzzles that involve rearranging objects—like shuffling cups on a table—and asked them to figure out where everything ends up. These puzzles are simple but reflect the kind of memory and tracking needed in tasks like reasoning about code or playing games.We found that models learn one of two strategies. One method combines chunks of information in a tree-like way—layer by layer—not processing each step in order, but more efficiently in parallel. The other strategy starts by using a shortcut: it rules out many possible answers using a quick heuristic, then fills in the rest of the details using the first, chunking-based method. The first strategy tends to be more reliable, especially for longer problems.We also found that factors like model architecture and training setup influence which strategy a model ends up using. Understanding these strategies helps us better interpret how language models reason—and how to shape them to be more accurate and trustworthy."
Poster,How Do Large Language Monkeys Get Their Power (Laws)?,https://ICML.cc//virtual/2025/poster/45319,"Rylan Schaeffer, Joshua Kazdan, John Hughes, Jordan Juravsky, Sara Price, Aengus Lynch, Erik Jones, Robert Kirk, Azalia Mirhoseini, Sanmi Koyejo","Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts.In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts.We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge?We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own.We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\sim}2-4$ orders of magnitude less inference compute.Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.","Recent research has shown a curious pattern: when language AIs are given multiple tries at a set of tasks, their overall success improves according to a ""power law""—a predictable, but not overly fast, curve. This was puzzling because, for any single task, more tries should make success much more likely, very quickly (exponentially). Our work solves this by showing that while individual tasks do follow this rapid improvement, the overall power law emerges due to how task difficulties are spread. Specifically, a small number of extremely hard tasks, where the AI has a tiny chance of success on any single attempt, collectively slow down the average improvement to a power law, even as each problem is still being tackled exponentially faster with more tries. Understanding this allows us to explain why some AI models or tasks don't follow this power law (they lack enough super-hard problems) and, more importantly, lets us predict this scaling behavior much more efficiently, using far less computational power, simply by looking at the initial success rates, especially on those toughest challenges."
Poster,How Do Transformers Learn Variable Binding in Symbolic Programs?,https://ICML.cc//virtual/2025/poster/44247,"Yiwei Wu, Atticus Geiger, Raphaël Millière","Variable binding---the ability to associate variables with values---is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains.Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches.","Variable binding is a fundamental operation in cognition that involves associating abstract placeholders with specific values, like linking $x$ to $5$ in a math problem. Classical computers can do this by storing variables and their values in explicit memory, but it's unclear how modern neural networks, which lack built-in symbolic memory, achieve this. We investigated whether the popular neural network architecture Transformer can learn to bind variables and values independently.We trained a Transformer to solve synthetic puzzles requiring tracking chains of variable assignments. For example, given $x = 5, y = x, z = y$, what is $z$?"", the model must follow the chain to determine $z$ equals $5$.Our experiments reveal that the model learns in three stages: first guessing randomly, then using simple shortcuts, and finally learning a systematic method. By analyzing its inner mechanisms, we found it repurposes parts of its structure as memory, dynamically passing information through specialized pathways.This finding shows that neural networks can spontaneously develop structured operations similar to classical symbolic systems, offering valuable insights into how advanced AI models acquire complex problem-solving skills. To help researchers explore these findings, we developed Variable Scope, an interactive web platform showcasing our experimental results."
Poster,How Effective Can Dropout Be in Multiple Instance Learning ?,https://ICML.cc//virtual/2025/poster/43917,"Wenhui Zhu, Peijie Qiu, Xiwen Chen, Zhangsihao Yang, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang","Multiple Instance Learning (MIL) is a popular weakly-supervised method for various applications, with a particular interest in histological whole slide image (WSI) classification. Due to the gigapixel resolution of WSI, applications of MIL in WSI typically necessitate a two-stage training scheme: first, extract features from the pre-trained backbone and then perform MIL aggregation. However, it is well-known that this suboptimal training scheme suffers from ""noisy"" feature embeddings from the backbone and inherent weak supervision, hindering MIL from learning rich and generalizable features. However, the most commonly used technique (i.e., dropout) for mitigating this issue has yet to be explored in MIL. In this paper, we empirically explore how effective the dropout can be in MIL. Interestingly, we observe that dropping the top-k most important instances within a bag leads to better performance and generalization even under noise attack. Based on this key observation, we propose a novel MIL-specific dropout method, termed MIL-Dropout, which systematically determines which instances to drop. Experiments on five MIL benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the performance of current MIL methods with a negligible computational cost. The code is available at \url{https://github.com/ChongQingNoSubway/MILDropout}.","Can dropout improve Multiple Instance Learning (MIL) for complex tasks like whole slide image (WSI) classification? We explored this by testing how different dropout strategies affect MIL performance.MIL is often used in weakly-supervised settings, where labels are given for groups of data points (bags), not individual instances. In digital pathology, this means assigning a diagnosis to an entire WSI made up of many smaller image tiles. Current MIL models use a two-stage process: extract features with a pre-trained model, then aggregate them. But this setup often leads to noisy features and overfitting.We found that dropping the top-k most important instances in each bag—not just dropping randomly—leads to better performance and generalization. Based on this, we propose MIL-Dropout, a simple yet effective method that learns which instances to drop. Our experiments on benchmark and medical datasets show that MIL-Dropout improves existing models with almost no extra cost."
