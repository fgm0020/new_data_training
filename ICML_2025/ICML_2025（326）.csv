type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models,https://ICML.cc//virtual/2025/poster/46129,"Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu","Recent advancements in multimodal large language models have driven breakthroughs in visual question answering. Yet, a critical gap persists, `conceptualization'—the ability to recognize and reason about the same concept despite variations in visual form, a basic ability of human reasoning. To address this challenge, we introduce the Visual Graph Arena (VGA), a dataset featuring six graph-based tasks designed to evaluate and improve AI systems’ capacity for visual abstraction. VGA uses diverse graph layouts (e.g., Kamada-Kawai vs. planar) to test reasoning independent of visual form. Experiments with state-of-the-art vision models and multimodal LLMs reveal a striking divide: humans achieved near-perfect accuracy across tasks, while models totally failed on isomorphism detection and showed limited success in path/cycle tasks. We further identify behavioral anomalies suggesting pseudo-intelligent pattern matching rather than genuine understanding. These findings underscore fundamental limitations in current AI models for visual understanding. By isolating the challenge of representation-invariant reasoning, the VGA provides a framework to drive progress toward human-like conceptualization in AI visual models. The Visual Graph Arena is available at: \href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}.","Imagine you are shown two triangles: one drawn with thick black lines, another made of colorful dots. You instantly recognize both as triangles. This ability to see past surface appearances and grasp underlying concepts is called ""conceptualization,"" and it's fundamental to human intelligence.While AI systems have become remarkably good at answering questions about images, they struggle with this basic skill. We wondered: can AI truly understand concepts, or does it just memorize visual patterns?To find out, we created the Visual Graph Arena (VGA), a test featuring six challenges involving networks of connected points. We presented these graphs in different visual styles to both humans and cutting-edge AI systems, testing whether they could reason about structure regardless of appearance.The results were striking. Humans achieved near-perfect scores across all tasks, while AI models failed dramatically. They couldn't even recognize when two graphs had identical structures but different visual presentations, and showed only limited success at basic tasks like finding paths. Even worse, we found evidence of superficial pattern matching rather than genuine understanding.Our findings reveal critical gaps in current AI vision systems and provide a framework for building more human-like artificial intelligence."
Poster,ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy,https://ICML.cc//virtual/2025/poster/43892,"Kian Kenyon-Dean, Zitong Jerry Wang, John Urbanik, Konstantin Donhauser, Jason Hartford, Saber Saberian, Nil Sahin, Ihab Bendidi, Safiye Celik, Juan Vera, Marta Fay, Imran Haque, Oren Kraus","Deriving insights from experimentally generated datasets requires methods that can account for random and systematic measurement errors and remove them in order to accurately represent the underlying effects of the conditions being tested. Here we present a framework for pretraining on large-scale microscopy datasets that includes three steps: (1) curating a set of diverse and self-consistent training samples, (2) scaling training of an appropriate foundation model architecture on this dataset, (3) evaluating intermediate layers of the trained model to identify the best representation for downstream tasks. Using this strategy, we present the largest foundation model for cell microscopy data to our knowledge, a new 1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a 60\% improvement in linear separability of genetic perturbations and obtains the best overall performance on whole-genome relationship recall, batch correction replicate consistency, and compound-gene activity prediction benchmarks.","Scientists use microscopes to generate vast numbers of images showing how cells react to drugs or genetic modifications. Extracting reliable biological insights from this massive and often noisy image data is a major challenge, slowing down our ability to understand diseases and discover new treatments. We developed a new three-step framework to train powerful AI models on this complex cell microscopy data. First, we carefully selected only the most informative images to create a high-quality, diverse training dataset. Second, we trained a very large AI model (with 1.9 billion parameters) on this refined dataset. Third, we discovered that using information from an intermediate processing stage of this AI, rather than its final output, provides a more accurate understanding of the cells' responses. Our method significantly improves how well AI can interpret these microscopy images. The new model is much better at identifying similar biological effects from different experiments, providing more consistent results, and predicting how potential drugs will interact with genes. This approach can accelerate biological discovery and help speed up the development of new medicines by making better sense of large-scale cellular imaging experiments."
Poster,Volume-Aware Distance for Robust Similarity Learning,https://ICML.cc//virtual/2025/poster/44852,"Shuo Chen, Chen Gong, Jun Li, Jian Yang","Measuring the similarity between data points plays a vital role in lots of popular representation learning tasks such as metric learning and contrastive learning. Most existing approaches utilize point-level distances to learn the point-to-point similarity between pairwise instances. However, since the finite number of training data points cannot fully cover the whole sample space consisting of an infinite number of points, the generalizability of the learned distance is usually limited by the sample size. In this paper, we thus extend the conventional form of data point to the new form of data ball with a predictable volume, so that we can naturally generalize the existing point-level distance to a new volume-aware distance (VAD) which measures the field-to-field geometric similarity. The learned VAD not only takes into account the relationship between observed instances but also uncovers the similarity among those unsampled neighbors surrounding the training data. This practice significantly enriches the coverage of sample space and thus improves the model generalizability. Theoretically, we prove that VAD tightens the error bound of traditional similarity learning and preserves crucial topological properties. Experiments on multi-domain data demonstrate the superiority of VAD over existing approaches in both supervised and unsupervised tasks.","In many machine learning tasks like image classification or data retrieval, measuring how similar data points are is crucial. Traditional methods focus on distances between individual data points, but this has a limitation: real-world data spaces are vast, and we can’t sample every possible point, so models often struggle with new, unseen data. This paper introduces a new approach that treats each data point not as a single dot but as a ""data ball"" with a predictable volume, like expanding a point into a small region. By calculating the geometric similarity between these regions (instead of just points), the model can better capture relationships between both observed and unobserved data around them. This improves how well the model generalizes to new data, making its decisions more robust and consistent across different scenarios. Theoretical analysis shows this method tightens error bounds and preserves key data structures, while experiments on various datasets—including images, text, and graphs—demonstrate that it outperforms existing techniques in both supervised and unsupervised learning tasks. Essentially, by thinking in terms of data regions rather than isolated points, we create more reliable and adaptable machine learning models."
Poster,Volume Optimality in Conformal Prediction with Structured Prediction Sets,https://ICML.cc//virtual/2025/poster/44066,"Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan","Conformal Prediction is a widely studied technique to construct prediction sets of future observations. Most conformal prediction methods focus on achieving the necessary coverage guarantees, but do not provide formal guarantees on the size (volume) of the prediction sets. We first prove the impossibility of volume optimality where any distribution-free method can only find a trivial solution. We then introduce a new notion of volume optimality by restricting the prediction sets to belong to a set family (of finite VC-dimension), specifically a union of $k$-intervals. Our main contribution is an efficient distribution-free algorithm based on dynamic programming (DP) to find a union of $k$-intervals that is guaranteed for any distribution to have near-optimal volume among all unions of $k$-intervals satisfying the desired coverage property. By adopting the framework of distributional conformal prediction (Chernozhukov et al., 2021), the new DP based conformity score can also be applied to achieve approximate conditional coverage and conditional restricted volume optimality, as long as a reasonable estimator of the conditional CDF is available. While the theoretical results already establish volume-optimality guarantees, they are complemented by experiments that demonstrate that our method can significantly outperform existing methods in many settings.","Imagine trying to predict tomorrow’s weather, but instead of just saying “it will be 75°F,” you want to give a range of temperatures that is guaranteed to include the true value, say between 70°F and 80°F. This is the idea behind conformal prediction, a powerful technique that wraps predictions in a safety net of uncertainty. It ensures these ranges (or prediction sets) are statistically reliable, meaning they include the correct answer most of the time.Many existing methods provide safe prediction sets, but not necessarily efficient ones. For example, one could always predict the whole possible range of values (like “anywhere from 0°F to 100°F”), which is technically always right, but not very useful.Our work tackles this inefficiency. We first prove a fundamental limitation: any method that provides reliable prediction sets for all situations can only find a trivial solution. To get around this, we focus on structured prediction sets, specifically, using only a small number of intervals. This makes the prediction sets more concise and interpretable.We then design a new algorithm that guarantees these structured sets are as small as possible while still being statistically valid. It works even when we know very little about the data, and performs especially well when the data has multiple clusters or modes. Our experiments show that this method produces much tighter (and still reliable) prediction ranges than existing approaches."
Poster,Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning,https://ICML.cc//virtual/2025/poster/44394,"Mengmeng Chen, Xiaohu Wu, QIQI LIU, Tiantian He, Yew Soon ONG, Yaochu Jin, Qicheng Lao, Han Yu","Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available athttps://github.com/buptcmm/phnhvvs.","Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field."
Poster,VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians,https://ICML.cc//virtual/2025/poster/43654,"Pengchong Hu, Zhizhong Han","Jointly estimating camera poses and mapping scenes from RGBD images is a fundamental task in simultaneous localization and mapping (SLAM). State-of-the-art methods employ 3D Gaussians to represent a scene, and render these Gaussians through splatting for higher efficiency and better rendering. However, these methods cannot scale up to extremely large scenes, due to the inefficient tracking and mapping strategies that need to optimize all 3D Gaussians in the limited GPU memories throughout the training to maintain the geometry and color consistency to previous RGBD observations. To resolve this issue, we propose novel tracking and mapping strategies to work with a novel 3D representation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied 3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels, without needing to learn locations, rotations, and multi-dimensional variances. Tying Gaussians to views not only significantly saves storage but also allows us to employ many more Gaussians to represent local details in the limited GPU memory. Moreover, our strategies remove the need of maintaining all Gaussians learnable throughout the training, while improving rendering quality, and tracking accuracy. We justify the effectiveness of these designs, and report better performance over the latest methods on the widely used benchmarks in terms of rendering and tracking accuracy and scalability. Please see our project page for code and videos at https://machineperceptionlab.github.io/VTGaussian-SLAM-Project.","Jointly estimating camera poses and mapping scenes from RGBD images is a fundamental task in simultaneous localization and mapping (SLAM). Recent state-of-the-art SLAM methods employ 3D Gaussians to represent a scene, but they face scalability challenges in large scenes. This is primarily due to the need to optimize all Gaussians in the limited GPU memories throughout training to maintain the geometry and color consistency to previous RGBD observations.We introduce novel tracking and mapping strategies to work with a novel 3D representation, which we term view-tied 3D Gaussians, for SLAM systems. Unlike recent approaches that learn the position and shape of each 3D Gaussian, our method ties these Gaussians directly to depth pixels from the camera. This design greatly reduces memory usage and enables the use of many more Gaussians to capture fine scene details.Our method significantly enhances rendering quality, tracking accuracy, and scalability. It achieves better performance than state-of-the-art systems on widely used benchmarks. Please see our project page for code and videos at https://machineperceptionlab.github.io/VTGaussian-SLAM-Project."
Poster,Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning,https://ICML.cc//virtual/2025/poster/45951,"Liang CHEN, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong","Harmful fine-tuning (HFT), performed directly on open-source LLMs or through Fine-tuning-as-a-Service, breaks safety alignment and poses significant threats. Existing methods aim to mitigate HFT risks by learning robust representation on alignment data or making harmful data unlearnable, but they treat each data sample equally, leaving data vulnerability patterns understudied. In this work, we reveal that certain subsets of alignment data are consistently more prone to forgetting during HFT across different fine-tuning tasks and exhibit lower robustness compared to other subsets. Inspired by these findings, we propose Vulnerability-Aware Alignment (VAA), which calculates data vulnerability, partitions data into ""vulnerable"" and ""invulnerable"" groups, and encourages balanced learning using a group distributionally robust optimization (Group DRO) framework. Specifically, VAA learns an adversarial sampler that samples examples from the currently underperforming group and then applies group-dependent adversarial perturbations to the data during training, aiming to encourage a balanced learning process across groups. Experiments across four fine-tuning tasks demonstrate that VAA significantly reduces harmful scores while preserving downstream task performance, outperforming state-of-the-art baselines.","Large language models (LLMs) are powerful tools that can be adapted to many tasks. However, when people fine-tune these models — especially using harmful or unsafe data — the model can “forget” how to behave safely. This process, called harmful fine-tuning, is a growing concern, especially as open-source models and fine-tuning services become more widely available.In our research, we found that some parts of the original safety training data are easier for the model to forget than others. These “easy-to-forget” examples are often the most important for teaching the model safe behavior. Yet, most current methods treat all data equally, which can leave models more vulnerable to harmful fine-tuning.We propose a new method called Vulnerability-Aware Alignment (VAA). It identifies which data is most likely to be forgotten and gives it more attention during training. By grouping the data and adjusting how the model learns from each group, our method helps the model stay safer, even when fine-tuned later.This work offers a more targeted way to improve model safety and helps build more trustworthy AI systems."
Poster,Wait-Less Offline Tuning and Re-solving for Online Decision Making,https://ICML.cc//virtual/2025/poster/46114,"Jingruo Sun, Wenzhi Gao, Ellen Vitercik, Yinyu Ye","Online linear programming (OLP) has found broad applications in revenue management and resource allocation. State-of-the-art OLP algorithms achieve low regret by repeatedly solving linear programming (LP) subproblems that incorporate updated resource information. However, LP-based methods are computationally expensive and often inefficient for large-scale applications. By contrast, recent first-order OLP algorithms are more computationally efficient but typically suffer from weaker regret guarantees. To address these shortcomings, we propose a new algorithm that combines the strengths of LP-based and first-order OLP algorithms. Our algorithm re-solves the LP subproblems periodically at a predefined frequency $f$ and uses the latest dual prices to guide online decision-making. In parallel, a first-order method runs during each interval between LP re-solves and smooths resource consumption. Our algorithm achieves $\mathcal{O}(\log (T/f) + \sqrt{f})$ regret and delivers a ""wait-less"" online decision-making process that balances computational efficiency and regret guarantees. Extensive experiments demonstrate at least $10$-fold improvements in regret over first-order methods and $100$-fold improvements in runtime over LP-based methods.","Many industries today, such as airlines, retail, and energy, must make rapid decisions to allocate limited resources when customer requests arrive one by one. This challenge can be modeled as an Online Linear Programming (OLP) problem. Existing OLP algorithms either make high-quality decisions at the cost of slow computation or provide faster solutions with reduced accuracy. Our research bridges this trade-off by combining both approaches: we periodically use the accurate but slow method to guide decisions, and in between, we apply a faster method to make quick updates. This hybrid design shifts most of the heavy computation offline, enabling real-time decisions with minimal delay. In this way, our algorithm balances decision quality and efficiency. Experiments demonstrate this result by showing over $10$-fold improvement in decision quality compared to the fast method and over $100$-fold computation time with almost the same decision quality compared to the accurate method. This work provides a new methodology that enables real-world systems to balance speed and decision quality effectively."
Poster,Wasserstein Flow Matching: Generative Modeling Over Families of Distributions,https://ICML.cc//virtual/2025/poster/45541,"Doron Haviv, Aram-Alexandre Pooladian, Dana Pe&#x27;er, Brandon Amos","Generative modeling typically concerns transporting a single source distribution to a target distribution via simple probability flows. However, in fields like computer graphics and single-cell genomics, samples themselves can be viewed as distributions, where standard flow matching ignores their inherent geometry. We propose Wasserstein flow matching (WFM), which lifts flow matching onto families of distributions using  the Wasserstein geometry. Notably, WFM is the first algorithm capable of generating distributions in high dimensions, whether represented analytically (as Gaussians) or empirically (as point-clouds). Our theoretical analysis establishes that Wasserstein geodesics constitute proper conditional flows over the space of distributions, making for a valid FM objective. Our algorithm leverages optimal transport theory and the attention mechanism, demonstrating versatility across computational regimes: exploiting closed-form optimal transport paths for Gaussian families, while using entropic estimates on point-clouds for general distributions. WFM successfully generates both 2D \& 3D shapes and high-dimensional cellular microenvironments from spatial transcriptomics data. Code is available at  [WassersteinFlowMatching](https://github.com/WassersteinFlowMatching/WassersteinFlowMatching/).","Generative models seek to transform unstructured noise into coherent data. Contemporary methods, including Diffusion and Flow Matching, have demonstrated remarkable success in generating high-resolution images, realistic videos, and well-formed text. These approaches thus produce single examples from a data distribution formed by individual examples, and have not yet been applied to generate complex, multi-component data points such as 3D shapes or biological cell structures.  Instead, these cases are themselves complex distributions with both internal geometry (i.e. relationships between the components) and external relationships (i.e. similarity of two sets of components). Our approach, Wasserstein Flow Matching (WFM), addresses this by directly learning how to transform these entire collections of points, or distributions, respecting both their internal geometries and their relationships in higher-order Wasserstein space. Whereas standard Flow Matching learns the transformation via straight lines from individual noise points to real data points, WFM is designed to imitate the most efficient pathways in the space of distributions, which are mathematically defined by the Wasserstein optimal transport maps. Applying this method, we have successfully generated complex 2D and 3D shapes, as well as detailed high-dimensional models of cellular environments from biological data."
Poster,Wasserstein Policy Optimization,https://ICML.cc//virtual/2025/poster/44075,"David Pfau, Ian Davies, Diana Borsa, João Madeira Araujo, Brendan Tracey, Hado van Hasselt","We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm for reinforcement learning in continuous action spaces. WPO can be derived as an approximation to Wasserstein gradient flow over the space of all policies projected into a finite-dimensional parameter space (e.g., the weights of a neural network), leading to a simple and completely general closed-form update. The resulting algorithm combines many properties of deterministic and classic policy gradient methods. Like deterministic policy gradients, it exploits knowledge of the *gradient* of the action-value function with respect to the action. Like classic policy gradients, it can be applied to stochastic policies with arbitrary distributions over actions -- without using the reparameterization trick. We show results on the DeepMind Control Suite and a magnetic confinement fusion task which compare favorably with state-of-the-art continuous control methods.","One of the most important problems in AI is how to learn to make sequences of good decisions (or 'actions') to optimize a desired objective. Many important applications require continuous actions. Such actions consist of smooth parameters (rather than, say, discrete symbols), each of which needs to be set appropriately and adaptively, and often quite precisely, to get the desired behaviour. Consider, for example, controlling robotic limbs or using magnets to control plasma in a nuclear fusion reactor.We present a new reinforcement learning algorithm, that can learn directly from experience in such settings, and which works especially well in “high-dimensional” problems with many continuous action components.  This makes it a promising algorithm for applications such as robotic control or nuclear fusion.  However, the algorithm is not specific to these tasks and we show it is effective on several continuous control problems.The new algorithm is derived from a mathematical concept called a ""Wasserstein gradient flow"".  This has inspired earlier algorithms, but a practical and efficient algorithm had not previously been derived directly.  The resulting algorithm is intuitive and elegant, and we discuss how it relates to, and improves upon, previous algorithms."
