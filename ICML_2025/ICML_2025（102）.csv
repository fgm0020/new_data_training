type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Exploring Vision Semantic Prompt for Efficient Point Cloud Understanding,https://ICML.cc//virtual/2025/poster/46672,"Yixin Zha, Chuxin Wang, Wenfei Yang, Tianzhu Zhang, Feng Wu","A series of pre-trained models have demonstrated promising results in point cloud understanding tasks and are widely applied to downstream tasks through fine-tuning. However, full fine-tuning leads to the forgetting of pretrained knowledge and substantial storage costs on edge devices. To address these issues, Parameter-Efficient Transfer Learning (PETL) methods have been proposed. According to our analysis, we find that existing 3D PETL methods cannot adequately align with semantic relationships of features required by downstream tasks, resulting in suboptimal performance. To ensure parameter efficiency while introducing rich semantic cues, we propose a novel fine-tuning paradigm for 3D pre-trained models. We utilize frozen 2D pre-trained models to provide vision semantic prompts and design a new Hybrid Attention Adapter to efficiently fuse 2D semantic cues into 3D representations with minimal trainable parameters(1.8M). Extensive experiments conducted on datasets including ScanObjectNN, ModelNet40, and ShapeNetPart demonstrate the effectiveness of our proposed paradigm. In particular, our method achieves 95.6% accuracy on ModelNet40 and attains 90.09% performance on the most challenging classification split ScanObjectNN(PB-T50-RS).","3D point clouds are crucial for applications like robotics and autonomous driving, but current methods face challenges in efficiently transferring pre-trained knowledge across tasks without losing important information or requiring large computational resources.We introduce a novel approach that combines 2D image semantics with 3D point clouds to enhance model performance while minimizing the need for additional resources. By using pre-trained 2D models to generate ""semantic prompts,"" our method helps 3D models generalize better to new tasks with parameter-efficient fine-tuning.Our experiments show that this approach significantly improves performance in tasks like object classification and part segmentation, achieving state-of-the-art results with fewer resources. This solution not only enhances accuracy but also makes it easier to deploy powerful models on devices with limited computational capabilities."
Poster,Exponential Family Variational Flow Matching for Tabular Data Generation,https://ICML.cc//virtual/2025/poster/44239,"Andres Guzman Cordero, Floor Eijkelboom, Jan-Willem van de Meent","While denoising diffusion and flow matching have driven major advances in generative modeling, their application to tabular data remains limited, despite its ubiquity in real-world applications. To this end, we develop *TabbyFlow*, a variational Flow Matching (VFM) method for tabular data generation. To apply VFM to data with mixed continuous and discrete features, we introduce **Exponential Family Variational Flow Matching (EF-VFM)**, which represents heterogeneous data types using a general exponential family distribution. We hereby obtain an efficient, data-driven objective based on moment matching, enabling principled learning of probability paths over mixed continuous and discrete variables. We also establish a connection between variational flow matching and generalized flow matching objectives based on Bregman divergences. Evaluation on tabular data benchmarks demonstrates state-of-the-art performance compared to baselines.","Recent techniques for creating realistic artificial data have greatly improved image and text generation. However, generating realistic tabular data, for example, patient records, is still challenging, even though this type of data is everywhere in practical applications.To address this, we introduce TabbyFlow, a new method specifically designed to generate realistic tabular data. Tabular data often includes different kinds of information: numbers, categories, yes/no answers, etc. Our approach, called **Exponential Family Variational Flow Matching (EF-VFM)**, can handle all these different data types smoothly. It cleverly combines and models numerical and categorical data, ensuring the generated data closely matches real data.Our approach simplifies and improves the way artificial tabular data is generated, making it more accurate and realistic. Tests show that *TabbyFlow* outperforms other leading methods, producing data that better matches real-world tabular information."
Poster,ExpProof : Operationalizing Explanations for Confidential Models with ZKPs,https://ICML.cc//virtual/2025/poster/44593,"Chhavi Yadav, Evan Laufer, Dan Boneh, Kamalika Chaudhuri","In principle, explanations are intended as a way to increase trust in machine learning models and are often obligated by regulations. However, many circumstances where these are demanded are adversarial in nature, meaning the involved parties have misaligned interests and are incentivized to manipulate explanations for their purpose. As a result, explainability methods fail to be operational in such settings despite the demand. In this paper, we take a step towards operationalizing explanations in adversarial scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive. Specifically we explore ZKP-amenable versions of the popular explainability algorithm LIME and evaluate their performance on Neural Networks and Random Forests. Our code is publicly available at : \url{https://github.com/emlaufer/ExpProof}.","**ExpProof – Building Trust in AI Explanations while keeping Model Confidential**In many real-world scenarios—such as loan applications or hiring decisions—organizations use machine learning (ML) models to make predictions. Regulations such as GDPR's Right to Explanation mandate that people affected by these decisions get an explanation for the ML decisions. However, this scenario involves misaligned incentives , in the sense that model developers are incentivized to give *incontestable* explanations rather than reveal the true workings of its model. Additionally, if the model is confidential and the explanation generation is opaque, how can we be sure the explanation is correct?Our research introduces *ExpProof*, a system that lets organizations prove that their explanations are accurate—without revealing the model weights. We use a cryptographic technique called Zero-Knowledge Proofs (ZKPs), which allows someone to prove they correctly computed a value without revealing its private information.We adapted a popular explanation method called LIME to work with ZKPs, ensuring that explanations are verifiable. We tested *ExpProof* on neural networks and decision trees, and found that it can generate proofs in under two minutes, with verification taking just fractions of a second.*ExpProof* helps build trust in AI systems by ensuring that explanations are truthful while keeping the model confidential—an important step toward more transparent and accountable machine learning."
Poster,Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs,https://ICML.cc//virtual/2025/poster/43882,"Ziang Chen, Xiaohan Chen, Jialin Liu, Xinshang Wang, Wotao Yin","Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.","Quadratic programming (QP) is widely used for decision-making in real-world applications. Traditional QP solvers can be slow, especially for large problems. This paper studies how graph neural networks (GNNs)—machine learning models built for graph-structured data—can be used to speed up QP solutions.The authors prove that for convex QPs with continuous variables and linear constraints, GNNs can reliably predict their feasibility, optimal objective value, and one of the optimal solutions. For more complex cases involving integer variables (mixed-integer QPs), GNNs face limitations. Still, the paper identifies specific cases where GNNs work well and offers practical criteria to check this.These results explain why GNNs often perform well in practice and provide theoretical foundations for using them in optimization."
Poster,Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization,https://ICML.cc//virtual/2025/poster/45058,"Ziyu Gong, Jim Lim, David I. Inouye","Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse.While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train.We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions.Our key insight is that gradient-based DM training only requires the prior's score function---not its density---allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at https://github.com/inouye-lab/SAUB.","Machine learning systems often need to work fairly across different groups of people or adapt to new environments, but current methods struggle with this. For example, when training AI to recognize images, we want it to work equally well for all demographic groups or transfer knowledge from one type of data to another. Existing approaches either do not scale to large datasets, are unstable during training, or make overly restrictive assumptions that hurt performance.This paper developed a new method that learns flexible ""score-based priors"" - essentially teaching the AI system to understand the underlying patterns in data without imposing rigid assumptions by learning the ""gradient"" or directional information from the dataset. This paper also added geometry-preserving constraints that help maintain meaningful relationships in the data, like ensuring that similar images stay close together in the AI's internal representation.This approach makes AI systems more stable, computationally efficient, and better at tasks requiring fairness and adaptability. The method showed superior performance in fair classification (ensuring equal treatment across groups), domain adaptation (applying knowledge from one dataset to another), and image translation tasks. This could lead to more trustworthy AI systems that work fairly across diverse populations and adapt better to new environments."
Poster,ExtPose: Robust and Coherent Pose Estimation by Extending ViTs,https://ICML.cc//virtual/2025/poster/44396,"Glory Rongyu CHEN, Li&#x27;an Zhuo, Linlin Yang, Qi WANG, Liefeng Bo, Bang Zhang, Angela Yao","Vision Transformers (ViT) are remarkable at 3D pose estimation, yet they still encounter certain challenges. One issue is that the popular ViT architecture for pose estimation is limited to images and lacks temporal information. Another challenge is that the prediction often fails to maintain pixel alignment with the original images. To address these issues, we propose a systematic framework for 3D pose estimation, called ExtPose. ExtPose extends image ViT to the challenging scenario and video setting by taking in additional 2D pose evidence and capturing temporal information in a full attention-based manner. We use 2D human skeleton images to integrate structured 2D pose information. By sharing parameters and attending across modalities and frames, we enhance the consistency between 3D poses and 2D videos without introducing additional parameters. We achieve state-of-the-art (SOTA) performance on multiple human and hand pose estimation benchmarks with substantial improvements to 34.0mm (-23%) on 3DPW and 4.9mm (-18%) on FreiHAND in PA-MPJPE over the other ViT-based methods respectively.","Vision Transformers (ViT) excel in 3D pose estimation but face challenges with temporal information and pixel alignment. To address these, we developed ExtPose, a new framework that integrates 2D human skeleton images and frames into ViTs, enhancing temporal and spatial accuracy.ExtPose employs an attention-based mechanism to effectively capture and process temporal sequences, improving the consistency between 3D poses and 2D images without additional parameters.This integration results in state-of-the-art performance on key benchmarks, setting new standards for accuracy in pose estimation tasks."
Poster,Extracting Rare Dependence Patterns via Adaptive Sample Reweighting,https://ICML.cc//virtual/2025/poster/44365,"Yiqing Li, Yewei Xia, Xiaofei Wang, Zhengming Chen, Liuhua Peng, Mingming Gong, Kun Zhang","Discovering dependence patterns between variables from observational data is a fundamental issue in data analysis. However, existing testing methods often fail to detect subtle yet critical patterns that occur within small regions of the data distribution--patterns we term rare dependence. These rare dependencies obscure the true underlying dependence structure in variables, particularly in causal discovery tasks. To address this issue, we propose a novel testing method that combines kernel-based (conditional) independence testing with adaptive sample importance reweighting. By learning and assigning higher importance weights to data points exhibiting significant dependence, our method amplifies the patterns and can detect them successfully. Theoretically, we analyze the asymptotic distributions of the statistics in this method and show the uniform bound of the learning scheme. Furthermore, we integrate our tests into the PC algorithm, a constraint-based approach for causal discovery, equipping it to uncover causal relationships even in the presence of rare dependence. Empirical evaluation of synthetic and real-world datasets comprehensively demonstrates the efficacy of our method.","Sometimes, important patterns in data are easy to miss, especially if they only show up in a small number of cases. For example, a rare dependence between genes might only appear under specific conditions, but could be crucial for understanding a disease. Most existing statistical tools for analyzing data overlook these subtle patterns because they’re designed to focus on the “big picture.”In our work, we develop a method that helps statistical methods pay more attention to these rare but meaningful patterns. We do this by training the system to assign more weight to data points that seem to show signs of interesting behavior, essentially telling it, “look here more closely.” This makes it easier to detect subtle relationships between variables that would otherwise be hidden.We also show how our method can help discover cause-and-effect relationships, even when these are obscured by rare dependence. Our tests on both simulated and real-world datasets show that this approach leads to more accurate and insightful discoveries."
Poster,Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts,https://ICML.cc//virtual/2025/poster/45042,"Jiahai Feng, Stuart Russell, Jacob Steinhardt","Pretrained language models (LMs) can generalize to implications of facts that they are finetuned on. For example, if finetuned on ""John Doe lives in Tokyo,"" LMs correctly answer ""What language do the people in John Doe's city speak?'' with ""Japanese''. However, little is known about the mechanisms that enable this generalization or how they are learned during pretraining.We introduce extractive structures as a framework for describing how components in LMs (e.g., MLPs or attention heads) coordinate to enable this generalization. The structures consist of informative components that store training facts as weight changes, and upstream and downstream extractive components that query and process the stored information to produce the correct implication. We hypothesize that extractive structures are learned during pretraining when encountering implications of previously known facts. This yields two predictions: a data ordering effect where extractive structures can be learned only if facts precede their implications, and a weight grafting effect where extractive structures can be grafted to predict counterfactual implications.We empirically show these effects in the OLMo-7b, Llama 3-8b, Gemma 2-9b, and Qwen 2-7b models.Of independent interest, our results also indicate that fact learning can occur at both early and late layers, which lead to different forms of generalization.","Language models that underlie modern chat assistants seem knowledgeable and intelligent about great many things. We are curious about how they actually learn all this information and develop these reasoning skills.To explore this, we take pretrained language models that are already quite smart, and teach them new facts by further training on them. For example, if we teach the model that ""John Doe lives in Tokyo"", we are interested in whether the model can figure out on its own that ""The people in the city John Doe lives in speak Japanese"". If so, the model isn't just memorizing facts it sees in training; it's reasoning and making connections. Our research aims to understand how it does this. What internal mechanisms in the model allows it to reason, and how did the model learn these internal mechanisms.To answer these questions, we develop the ""Extractive Structures"" framework, where we decompose the internals of a language model into ""informative components"", which store factual information, and ""extractive components"", which extract and process the stored information. Our work helps us pinpoint these components in a language model, and even predict how well language models learn new things based on how the components are set up. Overall, this research helps us build a clearer picture of how AI learns and reasons, an important step towards developing safe and reliable AI systems."
Poster,Extreme Value Policy Optimization for Safe Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46527,"Shiqing Gao, Yihang Zhou, Shuai Shao, Haoyu Luo, Yiheng Bing, Jiaxin Ding, Luoyi Fu, Xinbing Wang","Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.","When we teach AI systems to make decisions for real-world tasks, keeping them safe is a huge challenge — especially because rare but serious mistakes can still happen. Traditional methods often miss these rare “black swan” events, focusing mostly on the average case.To solve this, we propose a new technique called EVO. EVO pays special attention to these rare but risky situations by using the statistical tool, Extreme Value Theory, which is designed to spot and learn from unusual events. This way, our AI can better recognize and avoid the kinds of extreme mistakes that could cause real harm.In our tests, EVO made learning much safer — it greatly reduced the number of serious safety violations, all while keeping the AI just as effective at its job. This helps bring us closer to using AI confidently in places where safety is critical, like large language model, robotics, or autonomous driving."
Poster,Ex-VAD: Explainable Fine-grained Video Anomaly Detection Based on Visual-Language Models,https://ICML.cc//virtual/2025/poster/43589,"Chao Huang, Yushu Shi, Jie Wen, Wei Wang, Yong Xu, Xiaochun Cao","With advancements in visual language models (VLMs) and large language models (LLMs), video anomaly detection (VAD) has progressed beyond binary classification to fine-grained categorization and multidimensional analysis. However, existing methods focus mainly on coarse-grained detection, lacking anomaly explanations. To address these challenges, we propose Ex-VAD, an Explainable Fine-grained Video Anomaly Detection approach that combines fine-grained classification with detailed explanations of anomalies. First, we use a VLM to extract frame-level captions, and an LLM converts them to video-level explanations, enhancing the model's explainability. Second, integrating textual explanations of anomalies with visual information greatly enhances the model's anomaly detection capability. Finally, we apply label-enhanced alignment to optimize feature fusion, enabling precise fine-grained detection. Extensive experimental results on the UCF-Crime and XD-Violence datasets demonstrate that Ex-VAD significantly outperforms existing State-of-The-Art methods.","Existing VAD methods focus mainly on coarse-grained detection, lacking anomaly explanations. We propose Ex-VAD, an Explainable Fine-grained Video Anomaly Detection approach that combines fine-grained classification with detailed explanations of anomalies. First, we use a VLM to extract frame-level captions, and an LLM converts them to video-level explanations, enhancing the model's explainability. Second, integrating textual explanations of anomalies with visual information greatly enhances the model's anomaly detection capability. Finally, we apply label-enhanced alignment to optimize feature fusion, enabling precise fine-grained detection.  Extensive experimental results on the public datasets demonstrate that Ex-VAD significantly outperforms existing State-of-The-Art methods."
