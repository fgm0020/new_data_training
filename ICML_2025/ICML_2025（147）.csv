type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,In-Context Reinforcement Learning From Suboptimal Historical Data,https://ICML.cc//virtual/2025/poster/44574,"Juncheng Dong, Moyang Guo, Ethan Fang, Zhuoran Yang, Vahid Tarokh","Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the *Decision Importance Transformer* (DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.","In-context reinforcement learning (ICRL) has garnered increasing attention for its ability to enable transformer models to rapidly adapt to unseen RL tasks using only a few trajectories from those tasks. However, existing ICRL approaches typically rely on large quantities of high-quality pretraining data, limiting their practical applicability. To address this, we propose a framework that leverages only suboptimal historical data—readily available in the era of big data—for pretraining transformer models for ICRL. This significantly improves the practicality and scalability of ICRL. Central to our approach is a weighted supervised pretraining objective, where the weights are derived from an in-context advantage estimator that evaluates the quality of actions in the historical dataset. Empirically, our method yields transformer models with strong ICRL capabilities across both challenging discrete navigation and complex continuous control tasks."
Poster,Incorporating Arbitrary Matrix Group Equivariance into KANs,https://ICML.cc//virtual/2025/poster/44891,"Lexiang Hu, Yisen Wang, Zhouchen Lin","Kolmogorov-Arnold Networks (KANs) have seen great success in scientific domains thanks to spline activation functions, becoming an alternative to Multi-Layer Perceptrons (MLPs). However, spline functions may not respect symmetry in tasks, which is crucial prior knowledge in machine learning. In this paper, we propose Equivariant Kolmogorov-Arnold Networks (EKAN), a method for incorporating arbitrary matrix group equivariance into KANs, aiming to broaden their applicability to more fields. We first construct gated spline basis functions, which form the EKAN layer together with equivariant linear weights, and then define a lift layer to align the input space of EKAN with the feature space of the dataset, thereby building the entire EKAN architecture. Compared with baseline models, EKAN achieves higher accuracy with smaller datasets or fewer parameters on symmetry-related tasks, such as particle scattering and the three-body problem, often reducing test MSE by several orders of magnitude. Even in non-symbolic formula scenarios, such as top quark tagging with three jet constituents, EKAN achieves comparable results with state-of-the-art equivariant architectures using fewer than $40\\%$ of the parameters, while KANs do not outperform MLPs as expected. Code and data are available at [https://github.com/hulx2002/EKAN](https://github.com/hulx2002/EKAN).","As an emerging model in the field of science, KAN was never taught how to adhere to symmetry. This lack of capability makes it prone to losing direction when information is insufficient—but we will attempt to teach it.First, we will re-stratify KAN—like slicing a cake. Then, we modify each layer individually, instructing it on the rules it should follow, with each performing its own role. Finally, we assemble them into a cohesive whole, ensuring the new architecture systematically obeys order.Symmetry guidance will empower KAN with stronger learning capabilities—enabling it to infer broader patterns from limited examples and achieve greater efficiency."
Poster,Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems,https://ICML.cc//virtual/2025/poster/45577,"Yujun Kim, Jaeyoung Cha, Chulhee Yun","Recent theoretical results demonstrate that the convergence rates of permutation-based SGD (e.g., random reshuffling SGD) are faster than uniform-sampling SGD; however, these studies focus mainly on the large epoch regime, where the number of epochs $K$ exceeds the condition number $\kappa$. In contrast, little is known when $K$ is smaller than $\kappa$, and it is still a challenging open question whether permutation-based SGD can converge faster in this small epoch regime (Safran and Shamir, 2021). As a step toward understanding this gap, we study the naive deterministic variant, Incremental Gradient Descent (IGD), on smooth and strongly convex functions. Our lower bounds reveal that for the small epoch regime, IGD can exhibit surprisingly slow convergence even when all component functions are strongly convex. Furthermore, when some component functions are allowed to be nonconvex, we prove that the optimality gap of IGD can be significantly worse throughout the small epoch regime. Our analyses reveal that the convergence properties of permutation-based SGD in the small epoch regime may vary drastically depending on the assumptions on component functions. Lastly, we supplement the paper with tight upper and lower bounds for IGD in the large epoch regime.","How quickly do practical optimization algorithms learn the solution when the training time is limited? Many machine learning models are trained using a method called stochastic gradient descent (SGD), which improves performance by gradually adjusting model parameters. A practical variation called permutation-based SGD, which processes training data in a shuffled order, is known to work faster than the uniform-sampling SGD, which selects a random sample at each step. However, these benefits typically appear only when the training time is sufficiently long.We ask what happens when training is short, which corresponds to the common case where the computational budget is limited. To explore this, we study a simple instance of permutation-based SGD called Incremental Gradient Descent, which repeatedly processes the data in the natural order given by the dataset. We found that in short training scenarios, it can actually be much slower than expected, even for simple problems. As the complexity of the problem increases, performance can degrade further.These results show that training methods that work well in long training may behave very differently depending on the problem difficulty when time is limited. This has important implications connecting the theory and practice when the computational budget is limited."
Poster,Independence Tests for Language Models,https://ICML.cc//virtual/2025/poster/44127,"Sally Zhu, Ahmed Ahmed, Rohith Kuditipudi, Percy Liang","Motivated by liability and intellectual property concerns over open-weight models we consider the following problem: given the weights of two models, can we test whether they were trained independently---i.e., from independent random initializations? We consider two settings: *constrained* and *unconstrained*. In the constrained setting, we make assumptions about model architecture and training and propose statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. We compute the p-values by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures between the original two models versus these copies. We report p-values on pairs of 21 open-weight models (210 total pairs) and find we correctly identify all pairs of non-independent models. In the unconstrained setting we make none of the prior assumptions and allow for adversarial evasion attacks that do not change model output. We thus propose a new test which matches hidden activations between two models, which is robust to these transformations and to changes in model architecture and can also identify specific non-independent components of models. Though we no longer obtain exact p-values from this test, empirically we find it reliably distinguishes non-independent models like a p-value. Notably, we can use the test to identify specific parts of one model that are derived from another (e.g., how Llama 3.1-8B was pruned to initialize Llama 3.2-3B, or shared layers between Mistral-7B and StripedHyena-7B), and it is even robust to retraining individual layers of either model from scratch.","As large language models become increasingly common it’s important to determine when one model has been adapted from another through a practice called “fine-tuning"", which is a cost-effective and popular method for specializing the models in domains such as coding or law. However, this raises intellectual property issues and concerns about potential misuse, since the costs of developing models are increasing and come with additional legal strings attached. To address this, we developed statistical tests that detect when one model is derived from another. Our approach works by comparing the internal parameters (weights) of two models. We measure how similar two models’ weights are, compared to permuted versions of the same weights that represent a baseline for identical models. If the models are indeed related, their original weights will be significantly more similar than expected by chance, allowing us to reliably test for derivative relationships. Our tests accurately identified all derivative relationships among 210 pairs of open-source models, each with 7 billion parameters. Moreover, our methods proved robust across models of various sizes and types of modifications."
Poster,"Inducing, Detecting and Characterising Neural Modules: A Pipeline for Functional Interpretability in Reinforcement Learning",https://ICML.cc//virtual/2025/poster/45389,"Anna Soligo, Pietro Ferraro, David Boyle","Interpretability is crucial for ensuring RL systems align with human values. However, it remains challenging to achieve in complex decision making domains. Existing methods frequently attempt interpretability at the level of fundamental model units, such as neurons or decision nodes: an approach which scales poorly to large models. Here, we instead propose an approach to interpretability at the level of functional modularity. We show how encouraging sparsity and locality in network weights leads to the emergence of functional modules in RL policy networks. To detect these modules, we develop an extended Louvain algorithm which uses a novel `correlation alignment' metric to overcome the limitations of standard network analysis techniques when applied to neural network architectures. Applying these methods to 2D and 3D MiniGrid environments reveals the consistent emergence of distinct navigational modules for different axes, and we further demonstrate how these functions can be validated through direct interventions on network weights prior to inference.","Artificial intelligence systems used for decision-making, particularly reinforcement learning agents, are often treated as ""black boxes"". This lack of transparency creates safety concerns, especially when these AI systems are deployed in critical areas like healthcare or autonomous vehicles. We develop a new method to make AI decision making more interpretable by encouraging neural networks to organize themselves into specialized ""modules"", which handle different aspects of decision making. We train these modular networks to solve simple game environments, then identify what each module does by seeing how the AI agent behaves when we modify it, for example by effectively ""turning off"" a module's behaviour. We also show this modular approach is useful by using it to discover that one of our agents has learnt an incorrect proxy rather than the real task it is meant to perform. This information allows us to intervene and change the agent input slightly to ensure it learns a robust policy to solve the correct task.This modular approach offers a promising way to understand and verify AI decision-making by breaking down complex behaviors into a tractable number of interpretable components, rather than trying to analyse individual neurons."
Poster,Inductive Gradient Adjustment for Spectral Bias in Implicit Neural Representations,https://ICML.cc//virtual/2025/poster/43994,"Kexuan Shi, Hai Chen, Leheng Zhang, Shuhang Gu","Implicit Neural Representations (INRs), as a versatile representation paradigm, have achieved success in various computer vision tasks. Due to the spectral bias of the vanilla multi-layer perceptrons (MLPs), existing methods focus on designing MLPs with sophisticated architectures or repurposingtraining techniques for highly accurate INRs. In this paper, we delve into the linear dynamics model of MLPs and theoretically identify the empirical Neural Tangent Kernel (eNTK) matrix as a reliable link between spectral bias and training dynamics. Based on this insight, we propose a practical **I**nductive **G**radient **A**djustment (**IGA**) method, which could purposefully improve the spectral bias via inductive generalization of eNTK-based gradient transformation matrix. Theoretical andempirical analyses validate impacts of IGA on spectral bias. Further, we evaluate our method on different INRs tasks with various INR architectures and compare to existing training techniques. The superior and consistent improvements clearly validate the advantage of our IGA. Armed with our gradient adjustment method, better INRs with more enhanced texture details and sharpened edges can be learned from data by tailored impacts on spectral bias. The codes are available at: [https://github.com/LabShuHangGU/IGA-INR](https://github.com/LabShuHangGU/IGA-INR).","Neural networks learning to represent complex data such as images or 3D shapes are referred to as **Implicit Neural Representations (INRs)**. INRs can capture global structures and offer continuous representations with arbitrary precision, but often struggle with learning sharped edges or textures due to the implicit training bias of neural networks called “spectral bias” -- a tendency to learn smooth patterns first.In our work, we characterize the training dynamics via the linear dynamics perspective and identify the empirical Neural Tangent Kernel (eNTK) matrix as a key link between spectral bias and training dynamics. Using this insight, we propose a new method -- **Inductive Gradient Adjustment (IGA)** -- that mitigates spectral bias by inductive generalization of a gradient transformation matrix derived from the eNTK matrix.Our IGA method is model-agnostic, working across different architectures and tasks, and leads to clearer representations, without changing the model structures. We hope the superior performance of our approach will inspire growing interest in training dynamics and implicit bias in neural networks—advancing model accuracy and even generalization through improved training strategies."
Poster,Inductive Moment Matching,https://ICML.cc//virtual/2025/poster/43977,"Linqi (Alex) Zhou, Stefano Ermon, Jiaming Song","Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Moment Matching Self-Distillation (MMSD), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, MMSD does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, MMSD guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. MMSD surpasses diffusion models on ImageNet-256x256 with 2.13 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 2.05 on CIFAR-10 for a model trained from scratch.","Diffusion models and Flow Matching are slow at sampling. Even if perfectly trained, they require tens and hundreds of steps to produce high-quality samples. Recent approaches focus on distilling these slow samplers into fast one-step or few-step ones as a post-training step, but this two-stage approaches often require extensive tuning, e.g. balancing training of two networks or tuned training schedules. We investigate a single-stage approach without adversarial losses to directly achieve few-step sampling during inference, and we surpass diffusion models on both quality and sampling efficiency on standard benchmarks. We term our approach Inductive Moment Matching (IMM), which learns by using Maximum Mean Discrepancy (MMD), a stable divergence metric that matches two probability distributions using samples. In addition, we incorporate a learning strategy that allows the model to learn from its own samples inspired by mathematical induction. We theoretically prove that IMM is guaranteed to converge to the data distribution and is empirically more stable than other few-step approaches such as Consistency Training while achieving better performance on ImageNet-256x256 with only 8 steps compared to diffusion models.Our work marks a step towards few-step models trained from scratch, opening up possibilities of high-quality synthesis on higher-dimensional  data and real-time capability without complex multi-stage training strategies."
Poster,InfAlign: Inference-aware language model alignment,https://ICML.cc//virtual/2025/poster/44424,"Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, Ananda Suresh, Ahmad Beirami","Language model alignment is a critical stepin training modern generative language models.Alignment targets to improve win rate of a samplefrom the aligned model against the base model.Today, we are increasingly using inference-timealgorithms (e.g., Best-of-$N$ , controlled decoding, tree search) to decode from language modelsrather than standard sampling. We show that thistrain/test mismatch makes standard RLHF framework sub-optimal in view of such inference-timemethods. To this end, we propose a framework forinference-aware alignment (InfAlign), whichaims to optimize *inference-time win rate* of thealigned policy against the base model. We provethat for any inference-time decoding procedure,the optimal aligned policy is the solution to thestandard RLHF problem with a *transformation*of the reward. This motivates us to provide thecalibrate-and-transform RL (InfAlign-CTRL)algorithm to solve this problem, which involvesa reward calibration step and a KL-regularizedreward maximization step with a transformationof the calibrated reward. For best-of-$N$ samplingand best-of-$N$ jailbreaking, we propose specifictransformations offering up to 3-8% improvementon inference-time win rates. Finally, we also showthat our proposed reward calibration method is astrong baseline for optimizing standard win rate.","Language model alignment is a critical step in training modern generative language models.Alignment targets to improve win rate of a sample from the aligned model against the base model.Today, we are increasingly using inference-time algorithms (e.g., Best-of-$N$ , controlled decoding, tree search) to decode from language models rather than standard sampling. We show that this train/test mismatch makes standard RLHF framework sub-optimal in view of such inference-timemethods. To this end, we propose a framework for inference-aware alignment (InfAlign), whichaims to optimize *inference-time win rate* of the aligned policy against the base model. We prove that for any inference-time decoding procedure, the optimal aligned policy is the solution to the standard RLHF problem with a *transformation* of the reward. This motivates us to provide the calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. For best-of-$N$ sampling and best-of-$N$ jailbreaking, we propose specific transformations offering up to 3-8% improvement on inference-time win rates. Finally, we also show that our proposed reward calibration method is a strong baseline for optimizing standard win rate."
Poster,Inference-Time Alignment of Diffusion Models with Direct Noise Optimization,https://ICML.cc//virtual/2025/poster/45672,"Zhiwei Tang, Jiangweizhi Peng, Jiasheng Tang, Mingyi Hong, Fan Wang, Tsung-Hui Chang","In this work, we focus on the alignment problem of diffusion models with a continuous reward function, which represents specific objectives for downstream tasks, such as increasing darkness or improving the aesthetics of images. The central goal of the alignment problem is to adjust the distribution learned by diffusion models such that the generated samples maximize the target reward function. We propose a novel alignment approach, named Direct Noise Optimization (DNO), that optimizes the injected noise during the sampling process of diffusion models. By design, DNO operates at inference-time, and thus is tuning-free and prompt-agnostic, with the alignment occurring in an online fashion during generation. We rigorously study the theoretical properties of DNO and also propose variants to deal with non-differentiable reward functions. Furthermore, we identify that naive implementation of DNO occasionally suffers from the out-of-distribution reward hacking problem, where optimized samples have high rewards but are no longer in the support of the pretrained distribution. To remedy this issue, we leverage classical high-dimensional statistics theory to an effective probability regularization technique. We conduct extensive experiments on several important reward functions and demonstrate that the proposed DNO approach can achieve state-of-the-art reward scores within a reasonable time budget for generation.","AI image generators, like diffusion models, often feel like playing the lottery—you enter a prompt and wait, hoping the result matches what you imagined. Many people have to try again and again with different random seeds until they get a satisfying image. This trial-and-error process can be frustrating, time-consuming, and inefficient.What if we could automate this “lottery” and make the first image good enough? Our research shows this is possible—if we have a way to measure image quality automatically. We introduce a new method, called Direct Noise Optimization (DNO), that tweaks the internal randomness during image generation to maximize a reward signal—like how beautiful, dark, or safe the image looks. Unlike other methods, ours doesn’t require retraining the model or large datasets. It runs on regular hardware and works with any prompt.This means anyone—from artists to engineers—can generate better images with fewer tries, saving time and compute. Our method helps make AI image tools more reliable and controllable, pushing them closer to everyday creative use."
Poster,Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models,https://ICML.cc//virtual/2025/poster/46477,"Patrick Leask, Neel Nanda, Noura Al Moubayed","Sparse Autoencoders (SAEs) are a popular method for decomposing Large Language Model (LLM) activations into interpretable latents, however they have a substantial training cost and SAEs learned on different models are not directly comparable. Motivated by relative representation similarity measures, we introduce Inference-Time Decomposition of Activation models (ITDAs). ITDAs are constructed by greedily sampling activations into a dictionary based on an error threshold on their matching pursuit reconstruction. ITDAs can be trained in 1% of the time of SAEs, allowing us to cheaply train them on Llama-3.1 70B and 405B. ITDA dictionaries also enable cross-model comparisons, and outperform existing methods like CKA, SVCCA, and a relative representation method on a benchmark of representation similarity. Code available at https://github.com/pleask/itda.","Understanding how large language models (LLMs) like ChatGPT work internally is essential to safely deploying and using these models. One approach to understanding their internal activations (thoughts) is using sparse autoencoders, however these are expensive to train so they don't exist for most models, especially big state-of-the-art ones. Inference-Time Decomposition of Activations (ITDA) is an alternative approach that is 100x faster to train, but come with some performance drawbacks. They can also be more readily used to compare between different models, which opens exciting avenues in model diffing."
