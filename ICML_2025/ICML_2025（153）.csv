type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Joker: Joint Optimization Framework for Lightweight Kernel Machines,https://ICML.cc//virtual/2025/poster/44417,"Junhong Zhang, Zhihui Lai","Kernel methods are powerful tools for nonlinear learning with well-established theory.  The scalability issue has been their long-standing challenge.  Despite the existing success,  there are two limitations in large-scale kernel methods:  (i) The memory overhead is too high for users to afford;  (ii) existing efforts mainly focus on kernel ridge regression (KRR), while other models lack study.  In this paper, we propose **Joker**, a joint optimization framework for diverse kernel models, including KRR, logistic regression, and support vector machines.  We design a dual block coordinate descent method with trust region (DBCD-TR) and adopt kernel approximation with randomized features,  leading to low memory costs and high efficiency in large-scale learning.  Experiments show that **Joker** saves up to 90% memory but achieves comparable training time and performance (or even better) than the state-of-the-art methods.","This work studied the kernel method, a powerful tool for machine learning.However, this tool usually requires top-tier computers in the big data era.Therefore, it is unaffordable for most users.On the other hand, the application of kernel methods is limited to a small number of machine learning models.To address these problems, we proposed a new strategy named **Joker** to improve the process of kernel methods.In short, **Joker** divides a big task into several small ones and handles each small task iteratively.Then, we can process each task with little effort and greatly reduce the burden on the computer.In practice, **Joker**  can handle a large dataset (with 5 million images) in about 2 hours using a consumer GPU.To summarize, **Joker** lowers the requirement of computer hardware, making kernel methods easy to use for the public."
Poster,Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning,https://ICML.cc//virtual/2025/poster/45159,"Mahavir Dabas, Si Chen, Charles Fleming, Ming Jin, Ruoxi Jia","Safety alignment is crucial for Large Language Models (LLMs) to resist malicious instructions but often results in over-refusals, where benign prompts are unnecessarily rejected, impairing user experience and model utility. To this end, we introduce **ACTOR** (Activation-Based Training for Over-Refusal Reduction), a robust and compute- and-data efficient training framework that mini- mizes over-refusals by utilizing internal activation patterns from diverse queries. ACTOR precisely identifies and adjusts the activation components that trigger refusals, providing stronger control over the refusal mechanism. By fine-tuning only a single model layer, ACTOR effectively reduces over-refusals across multiple benchmarks while maintaining the model’s ability to handle harmful queries and preserving overall utility.","Problem — Today’s AI chatbots often panic: they refuse innocent questions just because the wording sounds dangerous, blocking help with topics like first-aid or chemistry homework.Solution — We discovered a tell-tale “refusal signal” hidden inside the model’s internal calculations. By gently adjusting that single signal—rather than overhauling the whole network—we teach the AI to pause only when a request is truly harmful. The training needs just minutes and a small set of examples.Impact — In tests, our fix let the chatbot answer up to one-third more harmless questions while keeping its existing safety guardrails almost untouched. Because the method is quick, cheap, and leaves the rest of the model unchanged, it can be slotted into real-world systems right away, making AI assistants more helpful without making them more risky."
Poster,K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes,https://ICML.cc//virtual/2025/poster/44963,"Hideaki Kim, Tomoharu Iwata, Akinori Fujino","Kernel method-based intensity estimators, formulated within reproducing kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs) have been among the most easy-to-implement and feasible methods for estimating the intensity functions of inhomogeneous Poisson processes. While both approaches share the term ""kernel"", they are founded on distinct theoretical principles, each with its own strengths and limitations. In this paper, we propose a novel regularized kernel method for Poisson processes based on the least squares loss and show that the resulting intensity estimator involves a specialized variant of the representer theorem: it has the dual coefficient of unity and coincides with classical KIEs. This result provides new theoretical insights into the connection between classical KIEs and kernel method-based intensity estimators, while enabling us to develop an efficient KIE by leveraging advanced techniques from RKHS theory. We refer to the proposed model as the *kernel method-based kernel intensity estimator* (K$^2$IE). Through experiments on synthetic datasets, we show that K$^2$IE achieves comparable predictive performance while significantly surpassing the state-of-the-art kernel method-based estimator in computational efficiency.","Poisson processes are widely used to analyze and forecast event patterns occurring in space and time, from tweets in SNS to disease outbreaks. A key challenge in using them is estimating the intensity function, which tells us how likely events are to occur at different locations. While recent approaches based on kernel methods provide accurate estimates, they are often very slow for large datasets. In this paper, we introduce a new kernel method-based approach that replaces the commonly used likelihood function with the least squares loss, offering a major boost in computational efficiency. We show that the proposed method achieves comparable accuracy while being significantly faster than previous kernel method-based methods. Moreover, we show that it connects closely to the kernel intensity estimator, a classical method known for its simplicity. These results make our approach both scalable and theoretically sound, helping researchers apply Poisson processes to large-scale scientific data."
Poster,KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems,https://ICML.cc//virtual/2025/poster/46178,"Jusheng Zhang, Zimeng Huang, Yijia Fan, Ningyuan Liu, Mingyan Li, Zhuojie Yang, Jiawei Yao, Jian Wang, Keze Wang","As scaling large language models faces prohibitive costs, multi-agent systems emerge as a promising alternative, though challenged by static knowledge assumptions and coordination inefficiencies. We introduce Knowledge-Aware Bayesian Bandits (KABB), a novel framework that enhances multi-agent system coordination through semantic understanding and dynamic adaptation. The framework features three key innovations: a customized knowledge distance model for deep semantic understanding, a dual-adaptation mechanism for continuous expert optimization, and a knowledge-aware Thompson Sampling strategy for efficient expert selection. Extensive evaluation demonstrates KABB achieves an optimal cost-performance balance, maintaining high performance while keeping computational demands relatively low in multi-agent coordination.","Problem. Scaling large language models and static expert ensembles incurs prohibitive compute and monetary costs.Solution. We propose Knowledge-Aware Bayesian Bandits (KABB), which (1) computes a semantic distance to match tasks and experts, (2) continuously adapts expert skills, and (3) employs a Thompson Sampling strategy that dynamically routes each task to the best subset of experts.Impact. On benchmark suites, KABB achieves equal or better accuracy while cutting computational expense by up to 7× compared to fixed ensembles."
Poster,KAN-AD: Time Series Anomaly Detection with Kolmogorov–Arnold Networks,https://ICML.cc//virtual/2025/poster/45584,"Quan Zhou, Changhua Pei, Fei Sun, HanJing, Zhengwei Gao, haiming zhang, Gaogang Xie, Dan Pei, Jianhui LI","Time series anomaly detection (TSAD) underpins real-time monitoring in cloud services and web systems, allowing rapid identification of anomalies to prevent costly failures. Most TSAD methods driven by forecasting models tend to overfit by emphasizing minor fluctuations. Our analysis reveals that effective TSAD should focus on modeling ""normal"" behavior through smooth local patterns. To achieve this, we reformulate time series modeling as approximating the series with smooth univariate functions. The local smoothness of each univariate function ensures that the fitted time series remains resilient against local disturbances. However, a direct KAN implementation proves susceptible to these disturbances due to the inherently localized characteristics of B-spline functions. We thus propose KAN-AD, replacing B-splines with truncated Fourier expansions and introducing a novel lightweight learning mechanism that emphasizes global patterns while staying robust to local disturbances. On four popular TSAD benchmarks, KAN-AD achieves an average 15% improvement in detection accuracy (with peaks exceeding 27%) over state-of-the-art baselines. Remarkably, it requires fewer than 1,000 trainable parameters, resulting in a 50% faster inference speed compared to the original KAN, demonstrating the approach's efficiency and practical viability.","Spotting unusual activity (anomalies) in the continuous data streams from online services and cloud systems is crucial. If we miss these blips, it can lead to costly failures. However, many current methods get easily sidetracked by normal, minor fluctuations in the data. They try to predict every little wiggle, which means they often struggle to get a clear picture of what ""normal"" truly looks like and can miss the real warning signs.We realized that to reliably detect anomalies, a system needs to understand the smooth, underlying patterns of normal behavior, rather than getting caught up in tiny details. So, we developed a new technique called KAN-AD. It learns to represent time series data using smooth mathematical functions. Specifically, KAN-AD uses a method based on Fourier expansions—a way to describe complex patterns as a sum of simple waves—to focus on these stable, global trends, making it robust against misleading local disturbances or ""noise.""Our KAN-AD approach significantly improves the ability to catch real problems. In tests on standard benchmarks, it boosted anomaly detection accuracy by an average of 15%, with improvements sometimes exceeding 27%, compared to other leading methods. Remarkably, KAN-AD achieves this with very few adjustable parts (less than 1,000 parameters) and is 50% faster than the original advanced technique it builds upon. This makes it a highly efficient and practical tool for keeping critical online systems running smoothly and safely."
Poster,Kandinsky Conformal Prediction: Beyond Class- and Covariate-Conditional Coverage,https://ICML.cc//virtual/2025/poster/45742,"Konstantina Bairaktari, Jiayun Wu, Steven Wu","Conformal prediction is a powerful distribution-free framework for constructing prediction sets with coverage guarantees. Classical methods, such as split conformal prediction, provide marginal coverage, ensuring that the prediction set contains the label of a random test point with a target probability. However, these guarantees may not hold uniformly across different subpopulations, leading to disparities in coverage. Prior work has explored coverage guarantees conditioned on events related to the covariates and label of the test point. We present Kandinsky conformal prediction, a framework that significantly expands the scope of conditional coverage guarantees. In contrast to Mondrian conformal prediction, which restricts its coverage guarantees to disjoint groups—reminiscent of the rigid, structured grids of Piet Mondrian’s art—our framework flexibly handles overlapping and fractional group memberships defined jointly on covariates and labels, reflecting the layered, intersecting forms in Wassily Kandinsky’s compositions. Our algorithm unifies and extends existing methods, encompassing covariate-based group conditional, class conditional, and Mondrian conformal prediction as special cases, while achieving a minimax-optimal high-probability conditional coverage bound.  Finally, we demonstrate the practicality of our approach through empirical evaluation on real-world datasets.","Conformal prediction is a powerful technique for making predictions with guaranteed reliability, ensuring that a prediction set will contain the true answer a certain percentage of the time. Imagine an AI system that predicts income for loan applications. While it might cover the correct answer 90% of the time overall, what if this coverage drops to 70% for a specific racial group or gender, while being 95% for another? This creates a serious issue, especially when individuals belong to multiple, overlapping groups—for example, a person can be both ""female"" and ""of a certain ethnicity.""We've developed a new technique called ""Kandinsky Conformal Prediction,"" named after the artist Wassily Kandinsky, whose paintings feature layered and intersecting shapes. Much like his art, our method is designed to handle the complexity of overlapping groups and even ""fractional"" group memberships, where belonging to an unobserved group is a matter of probability rather than a strict yes/no. This technique ensures that the promised level of coverage holds true for these specific communities, not just for an overall average.Our method effectively ensured fair and reliable detection of toxic online comments across 16 different overlapping demographic groups, and achieved consistent income prediction coverage across U.S. states. Despite its advanced capabilities, our algorithm is computationally efficient and statistically optimal, requiring as few samples as possible in a single training pass. In essence, this practical research leads to more trustworthy and equitable AI systems."
Poster,KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search,https://ICML.cc//virtual/2025/poster/45313,"Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Anh Tuan Luu","Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration's performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo. Our code is publicly available.","In this work, we present KBQA-o1, a system that helps AI better answer complex questions using structured knowledge bases. Unlike traditional methods, it explores the knowledge step-by-step and uses a search strategy similar to AlphaGo (Monte Carlo Tree Search) to guide its decisions. This approach improves accuracy even with limited training data and works across different open-source AI models."
Poster,KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies,https://ICML.cc//virtual/2025/poster/44965,"Shih-Min Yang, Martin Magnusson, Johannes Stork, Todor Stoyanov","Soft Actor-Critic (SAC) has achieved notable success in continuous control tasks but struggles in sparse reward settings, where infrequent rewards make efficient exploration challenging. While novelty-based exploration methods address this issue by encouraging the agent to explore novel states, they are not trivial to apply to SAC. In particular, managing the interaction between novelty-based exploration and SAC’s stochastic policy can lead to inefficient exploration and redundant sample collection. In this paper, we propose KEA (Keeping Exploration Alive) which tackles the inefficiencies in balancing exploration strategies when combining SAC with novelty-based exploration. KEA integrates a novelty-augmented SAC with a standard SAC agent, proactively coordinated via a switching mechanism. This coordination allows the agent to maintain stochasticity in high-novelty regions, enhancing exploration efficiency and reducing repeated sample collection. We first analyze this potential issue in a 2D navigation task, and then evaluate KEA on the DeepSea hard-exploration benchmark as well as sparse reward control tasks from the DeepMind Control Suite. Compared to state-of-the-art novelty-based exploration baselines, our experiments show that KEA significantly improves learning efficiency and robustness in sparse reward setups.","Many AI agents learn through trial and error, using rewards to guide their behavior. But when rewards are rare, they often wander aimlessly and learn slowly. A popular fix is to reward the agent for experiencing new situations, but simply adding a ""novelty bonus"" can lead to repetitive behavior and wasted effort. Enter **KEA**, a simple yet powerful conductor of exploration. For **familiar experiences**, it reinforces effective behaviors to encourage the agent to **revisit and refine promising actions**. In **unfamiliar zones**, it deploys a stochastic explorer to **make bold, random moves**. KEA’s switching mechanism coordinates between these two agents: the novelty-based learner polishes “fresh” yet already-seen experiences, then shifts control to the standard stochastic learner to execute bold new actions whenever true novelty arises.Tested on challenging exploration tasks, KEA learns faster and performs better than state-of-the-art novelty methods. By keeping exploration alive, KEA could accelerate progress in robotics, autonomous vehicles, and **any system that must make sense of complex environments with few rewards**."
Poster,Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models,https://ICML.cc//virtual/2025/poster/46673,"Shizhan Gong, Yankai Jiang, DOU QI, Farzan Farnia","Vision-language models, such as CLIP, have achieved significant success in aligning visual and textual representations, becoming essential components of many multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo. However, numerous studies have identified CLIP's limited fine-grained perception as a critical drawback, leading to substantial failures in downstream MLLMs. In contrast, vision-centric foundation models like DINOv2 demonstrate remarkable capabilities in capturing fine details from images. In this work, we propose a novel kernel-based method to align CLIP's visual representation with that of DINOv2, ensuring that the resulting embeddings maintain compatibility with text embeddings while enhancing perceptual capabilities. Our alignment objective is designed for efficient stochastic optimization. Following this image-only alignment fine-tuning, the visual encoder retains compatibility with the frozen text encoder and exhibits significant improvements in zero-shot object recognition, fine-grained spatial reasoning, and localization. By integrating the aligned visual encoder, downstream MLLMs also demonstrate enhanced performance. The code and models are available at https://github.com/peterant330/KUEA.","Vision-language models like CLIP are effective at linking images and text but struggle with fine-grained image details, which limits their performance in complex tasks. In contrast, image-focused models like DINOv2 excel at capturing these details.This study proposes a method to improve CLIP by aligning its visual representations with DINOv2 while preserving its compatibility with text embeddings. Fine-tuning the visual encoder enhances CLIP’s ability to recognize objects, reason spatially, and localize details more accurately.When the improved visual encoder is integrated into larger multi-modal systems, these systems show better performance on tasks requiring detailed visual understanding. This approach addresses CLIP’s key limitations, making vision-language models more effective and versatile."
Poster,KernelBench: Can LLMs Write Efficient GPU Kernels?,https://ICML.cc//virtual/2025/poster/43517,"Anne Ouyang, Simon Guo, Simran Arora, Alex Zhang, William Hu, Christopher Re, Azalia Mirhoseini","Efficient GPU kernels are crucial for building performant machine learning architectures, but writing them is a time-consuming challenge that requires significant expertise; therefore, we explore using language models (LMs) to automate kernel generation. We introduce **KernelBench**, an open-source framework for evaluating LMs' ability to write fast and correct kernels on a suite of 250 carefully selected PyTorch ML workloads. KernelBench represents a real-world engineering environment and making progress on the introduced benchmark directly translates to faster practical kernels. We introduce a new evaluation metric $\text{fast}_p$, which measures the percentage of generated kernels that are functionally correct and offer a speedup greater than an adjustable threshold $p$ over baseline. Our experiments across various state-of-the-art models and test-time methods show that frontier reasoning models perform the best out of the box but still fall short overall, matching the PyTorch baseline in less than 20\% of the cases. While we show that results can improve by leveraging execution and profiling feedback during iterative refinement, KernelBench remains a challenging benchmark, with its difficulty increasing as we raise speedup threshold $p$.","Modern AI systems demand massive computational power, delivered by dedicated AI hardware such as GPUs. To effectively utilize this hardware, engineers need to write specialized programs called GPU kernels, but the process of developing kernels is extremely difficult and time-consuming due to the deep domain knowledge required.We investigate whether language models could help automatically generate these complex GPU kernels. To test this, we created KernelBench, a comprehensive benchmark of 250 real-world AI workloads that could be accelerated using performant kernels. We found that today’s models struggle significantly with this task, with the best models only matching PyTorch's performance in less than 20% of cases. Just as human expert engineers iteratively refine their code over time, we found that leveraging execution feedback could also help AI improve its generated kernels; however, improvement remains limited and writing efficient GPU kernels still poses a challenge for current AI systems.Progress on KernelBench directly translates to faster, more efficient kernels that could reduce energy consumption and accelerate AI development. Additionally, KernelBench serves as a research environment for improving language models on this challenging and performance-critical code generation task."
