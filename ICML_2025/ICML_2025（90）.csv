type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Efficient Personalized Adaptation for Physiological Signal Foundation Model,https://ICML.cc//virtual/2025/poster/46446,"Chenrui Wu, Haishuai Wang, Xiang Zhang, Chengqi Zhang, Jiajun Bu","Time series analysis is crucial across various fields like energy, environment, transportation, finance and health. Deep learning has significantly advanced this field, particularly, the Time Series Foundation Model (TSFM) excels in multiple domains due to extensive pre-training. In this work, we focus on TSFM's challenges in medical practice: limited computing resources and medical data privacy. TSFM variants include fine-tuned models and those pre-trained for rapid deployment on diverse data. There may not be enough computing resources to train physiological signals locally in hospitals, and generalized TSFM is still inferior to task-specific methods on private, imbalanced local data. To address this, we propose PhysioPFM, a framework for efficiently personalizing TSFM. Our approach involves low-rank pre-training on public datasets, generator training by trained LoRA weights, and efficient weight generation via local data. Experimental results demonstrate that integrating generated models with TSFM enhances performance, and transferability, and reduces the need for additional sensitive data training.","We teach computers to diagnose and classify by training them on a large number of labeled sensor records from medical devices. However, large-scale foundation models require a lot of training resources, which are difficult to meet in clinical practice. For privacy concerns, patient data cannot be sent to the cloud for analysis. We trained a powerful generator on massive public data. It can be personalized for the large time series foundation model, relying on private data. This will help to quickly identify medical time series data and accurately classify the patient's condition type status."
Poster,Efficient Quantification of Multimodal Interaction at Sample Level,https://ICML.cc//virtual/2025/poster/45816,"Zequn Yang, Hongfa Wang, Di Hu","Interactions between modalities—redundancy, uniqueness, and synergy—collectively determine the composition of multimodal information. Understanding these interactions is crucial for analyzing information dynamics in multimodal systems, yet their accurate sample-level quantification presents significant theoretical and computational challenges. To address this, we introduce the Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously grounded in pointwise information theory. We first develop a redundancy estimation framework, employing an appropriate pointwise information measure to quantify this most decomposable and measurable interaction.Building upon this, we propose a general interaction estimation method that employs efficient entropy estimation, specifically tailored for sample-wise estimation in continuous distributions. Extensive experiments on synthetic and real-world datasets validate LSMI's precision and efficiency. Crucially, our sample-wise approach reveals fine-grained sample- and category-level dynamics within multimodal data, enabling practical applications such as redundancy-informed sample partitioning, targeted knowledge distillation, and interaction-aware model ensembling. The code is available at https://github.com/GeWu-Lab/LSMI_Estimator.","We quantify how information is generated through multimodal interactions, efficiently distinguishing whether it originates from shared sources across modalities, is specific to a single modality, or emerges synergistically from their combined effect. This quantification offers practical insights for real-world datasets."
Poster,Efficient Robotic Policy Learning via Latent Space Backward Planning,https://ICML.cc//virtual/2025/poster/46016,"Dongxiu Liu, Haoyi Niu, Zhihao Wang, Jinliang Zheng, Yinan Zheng, Zhonghong Ou, Jianming HU, Jianxiong Li, Xianyuan Zhan","Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks?To address this, we propose a **B**ackward **P**lanning scheme in **L**atent space (**LBP**), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction.Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: [https://lbp-authors.github.io](https://lbp-authors.github.io).","Modern robots often try to imagine future scenes as a way to plan their actions. However, current methods are often slow and prone to inaccurate predictions, which can lead robots away from their intended goals.To solve this, we introduce a new planning strategy called **Latent Space Backward Planning (LBP)**. Instead of planning forward from the present, LBP starts from the final goal and works backward, efficiently setting meaningful checkpoints along the way. This strategy helps the robot stay on track and speeds up the planning process. We test this method in both simulations and with real robots on complex tasks, and it perform better than existing methods—making it a promising step towards more efficient and reliable robot control in the real world."
Poster,Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks,https://ICML.cc//virtual/2025/poster/43638,"Thomas Massena, Léo Andéol, Thibaut Boissin, Franck Mamalet, Corentin FRIEDRICH, Mathieu Serrurier, Sébastien Gerchinovitz","Conformal Prediction (CP) has proven to be an effective post-hoc method for improving the trustworthiness of neural networks by providing prediction sets with finite-sample guarantees. However, under adversarial attacks, classical conformal guarantees do not hold anymore: this problem is addressed in the field of Robust Conformal Prediction. Several methods have been proposed to provide robust CP sets with guarantees under adversarial perturbations, but, for large scale problems, these sets are either too large or the methods are too computationally demanding to be deployed in real life scenarios. In this work, we propose a new method that leverages Lipschitz-bounded networks to precisely and efficiently estimate robust CP sets. When combined with a 1-Lipschitz robust network, we demonstrate that our *lip-rcp* method outperforms state-of-the-art results in both the size of the robust CP sets and computational efficiency in medium and large-scale scenarios such as ImageNet. Taking a different angle, we also study vanilla CP under attack, and derive new worst-case coverage bounds of vanilla CP sets, which are valid simultaneously for all adversarial attack levels. Our *lip-rcp* method makes this second approach as efficient as vanilla CP while also allowing robustness guarantees.","Most AI models make pointwise predictions (e.g. a label in classification) without a reliable notion of uncertainty. Conformal Prediction replaces these pointwise predictions by sets (in classification) or intervals (regression) with a rigorous sense of uncertainty. These sets can however exhibit pathological behaviour when someone deliberately tweaks their inputs in a way that is imperceptible to the human eye. Existing fixes give overly cautious answers (conformal sets so big they’re uninformative) or are too slow to run on real‐world tasks like classifying millions of images.We introduce *lip-rcp*, a lightweight method that wraps around specially designed “smooth” neural networks, whose sensitivity to input changes is strictly enforced. By leveraging this built-in stability, our method computes trustworthy and informative “confidence sets” (the range of labels the model can safely include) much faster and with minimal overhead compared to prior approaches. We also prove new worst-case guarantees for vanilla conformal prediction, showing exactly how reliable it remains under any attack level.*lip-rcp* brings provable safety and efficiency to large-scale vision tasks without sacrificing speed, making it practical to deploy AI systems that are robust to malicious attacks—an important step toward more robust, trustworthy machine learning."
Poster,Efficient Skill Discovery via Regret-Aware Optimization,https://ICML.cc//virtual/2025/poster/46465,"He ZHANG, Ming Zhou, shaopeng zhai, Ying Sun, Hui Xiong","Unsupervised skill discovery aims to learn diverse and distinguishable behaviors in open-ended reinforcement learning.For the existing methods, they focus on improving the diversity via pure exploration, mutual information optimization and learning temporal representation. Despite they perform well on exploration, they remain limited in terms of efficiency, especially for the high-dimensional situations.In this work, we frame the skill discovery as a min-max game of skill generation and policy learning, proposing a regret-aware method on top of temporal representation learning that expands the discovered skill space along the direction of upgradable policy strength.The key insight behind the proposed method is that the skill discovery is adversarial to the policy learning, i.e., skills with weak strength should be further explored while less exploration for the skills with converged strength.As an implementation, we score the degree of strength convergence with regret, and guide the skill discovery with a learnable skill generator. To avoid degeneration, the skill generation comes from an upgradable population of skill generators.We conduct experiments on environments with varying complexities and dimension sizes.Empirical results show that our method outperforms baselines on both efficiency and diversity.Moreover, our method achieves 15\% zero-shot improvement on high-dimensional environments, compared to existing methods.","AI systems today often struggle to efficiently discover new skills without clear instructions or explicit rewards, which limits their adaptability and usefulness in real-world scenarios. Our research addresses this by introducing Regret-aware Skill Discovery (RSD), a novel method inspired by the idea of learning from past mistakes—or ""regrets."" In RSD, the AI system actively identifies and practices skills where it previously performed poorly, rather than randomly exploring or trying to maximize all information equally. By deliberately targeting weaker skills, the system rapidly improves its overall performance and skill variety. Through extensive testing, we found that RSD not only learns faster and more efficiently, but also enables the AI to perform well in new situations it has never encountered before. This approach can significantly enhance the practical applications of AI by making them more versatile and effective."
Poster,Efficient Source-free Unlearning via Energy-Guided Data Synthesis and Discrimination-Aware Multitask Optimization,https://ICML.cc//virtual/2025/poster/43757,"Xiuyuan Wang, Chaochao Chen, Weiming Liu, Xinting Liao, Fan Wang, Xiaolin Zheng","With growing privacy concerns and the enforcement of data protection regulations, machine unlearning has emerged as a promising approach for removing the influence of forget data while maintaining model performance on retain data. However, most existing unlearning methods require access to the original training data, which is often impractical due to privacy policies, storage constraints, and other limitations. This gives rise to the challenging task of source-free unlearning, where unlearning must be accomplished without accessing the original training data. Few existing source-free unlearning methods rely on knowledge distillation and model retraining, which impose substantial computational costs. In this work, we propose the Data Synthesis-based Discrimination-Aware (DSDA) unlearning framework, which enables efficient source-free unlearning in two stages: (1) Accelerated Energy-Guided Data Synthesis (AEGDS), which employs Langevin dynamics to model the training data distribution while integrating Runge–Kutta methods and momentum to enhance efficiency. (2) Discrimination-Aware Multitask Optimization (DAMO), which refines the feature distribution of retain data and mitigates the gradient conflicts among multiple unlearning objectives. Extensive experiments on three benchmark datasets demonstrate that DSDA outperforms existing unlearning methods, validating its effectiveness and efficiency in source-free unlearning.","Growing privacy laws (like GDPR) let people demand their data be deleted from AI systems. To comply, researchers propose ""machine unlearning"", techniques to remove specific data from trained models. However, most existing unlearning methods need the original training data, which is often unavailable due to privacy rules or storage limits. Even the few workarounds are too computationally expensive for real-world use.We propose DSDA, a new framework for efficient ""source-free unlearning"" (no original data needed). First, it synthesizes artificial data mimicking the original training distribution using accelerated energy-guided sampling. Second, it introduces a discrimination-aware optimizer that precisely removes the influence of the forget data while protecting retained knowledge and resolving conflicts between unlearning tasks.We conduct extensive experiments on three benchmark datasets. Results demonstrate that DSDA outperforms existing unlearning methods, validating its effectiveness and efficiency in source-free unlearning."
Poster,Efficient Time Series Processing for Transformers and State-Space Models through Token Merging,https://ICML.cc//virtual/2025/poster/44933,"Leon Götz, Marcel Kollovieh, Stephan Günnemann, Leo Schwinn","Despite recent advances in subquadratic attention mechanisms or state-space models, processing long token sequences still imposes significant computational requirements. Token merging has emerged as a solution to increase computational efficiency in computer vision architectures. In this work, we perform the first investigations of token merging in time series analysis on both transformers and state-space models. We further introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, achieving two major benefits:  a) Local merging can adjust its computational complexity from quadratic to linear based on the neighborhood size to effectively scale to long sequences; b) Local merging is the first causal merging scheme enabling token merging in transformer decoders. Further, we identify spectral properties of the input data that reliably predict the potential benefits of local merging without requiring evaluation on downstream tasks. Our comprehensive empirical evaluation demonstrates that local merging offers substantial efficiency gains with minimal impact on accuracy, achieving up to 5400% acceleration on the recently proposed Chronos foundation model.","Transformers are good at dealing with time-based sequences. They can be slow and need a lot of computing power when working with really long sequences. In computer vision (image processing), a technique called token merging has helped to speed up transformers. Token merging combines several similar data chunks (tokens) into one. We extend this idea for the first time to time-based sequences. We also invent a new method called local merging. It only merges tokens that are locally close together, not any tokens in a sequence. This makes the method more efficient for long sequences. It can also be applied in decoder models. We test our method thoroughly and find that it makes models up to 5400% faster without affecting accuracy."
Poster,EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning,https://ICML.cc//virtual/2025/poster/46272,"Dong HUANG, Guangtao Zeng, Jianbo Dai, Meng Luo, Han Weng, Yuhao QING, Heming Cui, Zhijiang Guo, Jie Zhang","As large language models (LLMs) play an increasingly important role in code generation, enhancing both correctness and efficiency has become crucial. Current methods primarily focus on correctness, often overlooking efficiency. To address this gap, we introduce SWIFTCODE to improve both aspects by fine-tuning LLMs on a high-quality dataset comprising correct and efficient code samples. Our methodology involves leveraging multiple LLMs to generate diverse candidate code solutions for various tasks across different programming languages. We then evaluate these solutions by directly measuring their execution time and memory usage through local execution. The code solution with the lowest execution time and memory consumption is selected as the final output for each task. Experimental results demonstrate significant improvements when fine-tuning with SWIFTCODE. For instance, Qwen2.5-Coder-7B-Instruct's pass@1 score increases from 44.8\% to 57.7\%, while the average execution time for correct tasks decreases by 48.4\%. SWIFTCODE offers a scalable and effective solution for advancing AI-driven code generation, benefiting both software development and computational problem-solving.","Why did LLM generate inefficient code than the human expert-written solution? Our empirical study reveals that the efficiency of LLM-generated code is strongly correlated with the efficiency of the training dataset. Based on our observation, we construct an efficient code instruction tuning dataset, EffiInstruct, to fine-tune LLMs and then improve the efficiency of LLM-generated code."
Poster,e-GAI: e-value-based Generalized $\alpha$-Investing for Online False Discovery Rate Control,https://ICML.cc//virtual/2025/poster/45378,"Yifan Zhang, Zijian Wei, Haojie Ren, Changliang Zou","Online multiple hypothesis testing has attracted a lot of attention in many applications, e.g., anomaly status detection and stock market price monitoring. The state-of-the-art generalized $\alpha$-investing (GAI) algorithms can control online false discovery rate (FDR) on p-values only under specific dependence structures, a situation that rarely occurs in practice. The e-LOND algorithm (Xu & Ramdas, 2024) utilizes e-values to achieve online FDR control under arbitrary dependence but suffers from a significant loss in power as testing levels are derived from pre-specified descent sequences. To address these limitations, we propose a novel framework on valid e-values named e-GAI. The proposed e-GAI can ensure provable online FDR control under more general dependency conditions while improving the power by dynamically allocating the testing levels. These testing levels are updated not only by relying on both the number of previous rejections and the prior costs, but also, differing from the GAI framework, by assigning less $\alpha$-wealth for each rejection from a risk aversion perspective. Within the e-GAI framework, we introduce two new online FDR procedures, e-LORD and e-SAFFRON, and provide strategies for the long-term performance to address the issue of $\alpha$-death, a common phenomenon within the GAI framework. Furthermore, we demonstrate that e-GAI can be generalized to conditionally super-uniform p-values. Both simulated and real data experiments demonstrate the advantages of both e-LORD and e-SAFFRON in FDR control and power.","When keeping an eye on unusual events or watching stock markets as they change, it's important to reduce false alerts while still being able to spot real issues effectively. Many current methods either require very specific situations to work well or sacrifice too much detection capability just to avoid false alarms.We developed a novel framework called e-GAI that dynamically allocates ``detection resources'', similar to how a skilled investor adjusts their investments in a portfolio. This new approach helps us better control false discoveries—essentially, mistakes in our findings—while also successfully identifying more of the samples we care about. Within this framework, we design two new procedures called e-LORD and e-SAFFRON. Our experiments showed that these new methods performed exceptionally well.The e-GAI framework is useful, and it can also handle both new types of data analysis (called e-values) and the more traditional approach (p-values). This makes it versatile for many different situations where real-time decisions are needed, such as checking product quality or monitoring financial activities."
Poster,EgoPrivacy: What Your First-Person Camera Says About You?,https://ICML.cc//virtual/2025/poster/44804,"Yijiang Li, Genpei Zhang, Jiacheng Cheng, Yi Li, Xiaojun Shan, Dashan Gao, Jiancheng Lyu, Yuan Li, Ning Bi, Nuno Vasconcelos","While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera wearer. This work investigates the core question: How much privacy information about the camera wearer can be inferred from their first-person view videos? We introduce EgoPrivacy, the first large-scale benchmark for the comprehensive evaluation of privacy risks in egocentric vision. EgoPrivacy covers three types of privacy (demographic, individual, and situational), defining seven tasks that aim to recover private information ranging from fine-grained (e.g., wearer's identity) to coarse-grained (e.g., age group). To further emphasize the privacy threats inherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel attack strategy that leverages ego-to-exo retrieval from an external pool of exocentric videos to boost the effectiveness of demographic privacy attacks. An extensive comparison of the different attacks possible under all threat models is presented, showing that private information of the wearer is highly susceptible to leakage. For instance, our findings indicate that foundation models can effectively compromise wearer privacy even in zero-shot settings by recovering attributes such as identity, scene, gender, and race with 70–80% accuracy. Our code and data are available at https://github.com/williamium3000/ego-privacy.","Recent wearable cameras—like those used in action cams, smart glasses, and body cams—are great at capturing what’s happening around us. But there’s another side we tend to forget: they also reveal a lot about you, the person wearing them. While most people worry about how others are seen and tracked in these videos, this paper shows that it’s actually just as risky for the person wearing the camera.We introduce EgoPrivacy, the first large-scale set of wearable-camera videos that helps us measure just how much personal information can be taken from these videos. It’s more than just faces—it includes figuring out your age, your identity, and even details about your surroundings.We also show a clever new method—called Retrieval-Augmented Attack—where a computer secretly looks through regular videos (like those from phones and webcams) to find matches and piece together who you are, just from what your camera sees.Our tests reveal a surprising truth: wearable cameras are leaking a lot of private information about their wearers—even when you think the footage is harmless. That means as these devices become more common, we all need better ways to protect you, not just the people around you."
