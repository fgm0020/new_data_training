type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Whoever Started the interference Should End It: Guiding Data-Free Model Merging via Task Vectors,https://ICML.cc//virtual/2025/poster/43580,"Runxi Cheng, Feng Xiong, Yongxian Wei, Wanyun Zhu, Chun Yuan","Model merging seeks to integrate task-specific expert models into a unified architecture while preserving multi-task generalization capabilities, yet parameter interference between constituent models frequently induces performance degradation. Although prior work has explored many merging strategies, resolving interference without additional data for retraining or test-time computation remains challenging. In this paper, we theoretically demonstrate that the task vectors of the linear layer constitute an approximate linear subspace for its corresponding input. Therefore, we can minimize interference under the guidance of task vectors. Based on this insight, we propose **WUDI-Merging** (**W**hoever started the interference sho**U**ld en**D** **I**t), a simple yet effective model merging method that eliminates interference without any additional data or rescaling coefficients. Comprehensive empirical evaluations across vision and language benchmarks demonstrate our method's superiority, achieving state-of-the-art performance in data-free model merging scenarios (average 10.9% improvement versus baseline methods)  while even outperforming mainstream test-time adaptation approaches by 3.3%, and only very few computing resources are required. The source code and implementation details are available at https://github.com/nathanielyvo/WUDI-Merging.","Model merging seeks to integrate task-specific expert models into a unified architecture while preserving multi-task generalization capabilities, yet parameter interference between constituent models frequently induces performance degradation. Although prior work has explored many merging strategies, resolving interference without additional data for retraining or test-time computation remains challenging. In this paper, we theoretically demonstrate that the task vectors of the linear layer constitute an approximate linear subspace for its corresponding input. Therefore, we can minimize interference under the guidance of task vectors. Based on this insight, we propose **WUDI-Merging** (**W**hoever started the interference sho**U**ld en**D** **I**t), a simple yet effective model merging method that eliminates interference without any additional data or rescaling coefficients. Comprehensive empirical evaluations across vision and language benchmarks demonstrate our method's superiority, achieving state-of-the-art performance in data-free model merging scenarios (average 10.9% improvement versus baseline methods)  while even outperforming mainstream test-time adaptation approaches by 3.3%, and only very few computing resources are required. The source code and implementation details are available at https://github.com/nathanielyvo/WUDI-Merging."
Poster,"""Who experiences large model decay and why?"" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift",https://ICML.cc//virtual/2025/poster/45315,"Harvineet Singh, Fan Xia, Alexej Gossmann, Andrew Chuang, Julian Hong, Jean Feng","Machine learning (ML) models frequently experience performance degradation when deployed in new contexts. Such degradation is rarely uniform: some subgroups may suffer large performance decay while others may not. Understanding where and how large differences in performance arise is critical for designing *targeted* corrective actions that mitigate decay for the most affected subgroups while minimizing any unintended effects. Current approaches do not provide such detailed insight, as they either (i) explain how *average* performance shifts arise or (ii) identify adversely affected subgroups without insight into how this occurred. To this end, we introduce a **S**ubgroup-scanning **H**ierarchical **I**nference **F**ramework for performance drif**T** (SHIFT). SHIFT first asks ""Is there any subgroup with unacceptably large performance decay due to covariate/outcome shifts?"" (*Where?*) and, if so, dives deeper to ask ""Can we explain this using more detailed variable(subset)-specific shifts?"" (*How?*). In real-world experiments, we find that SHIFT identifies interpretable subgroups affected by performance decay, and suggests targeted actions that effectively mitigate the decay.","When performance of an ML algorithm drops in a new application context, which subgroups are adversely affected and why? Answering this question is critical to catch hidden failures of the algorithm and to develop targeted fixes for the affected subgroups.Our paper presents a framework that identifies subgroups that experience overly large performance drops and explains the reasons for the drops. We present interpretable summaries of the subgroup along with how uncertain we are given the data that we have so that the users can take informed decisions on how to mitigate the drops.To help ML practitioners inspect their algorithms, we have released a free software tool called SHIFT which only requires black-box access to algorithm outputs. This can suggest targeted fixes to improve the algorithms for the affected subgroups without sacrificing its performance elsewhere, and hence ensure that the algorithms are safe to transfer across application contexts."
Poster,Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?,https://ICML.cc//virtual/2025/poster/45753,"Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, Sanmi Koyejo","Predictable behavior from scaling advanced AI systems is an extremely desirable property for engineers, companies, economists and governments alike, and while a well-established literature exists on how pretraining performance scales, predictable scaling behavior on downstream capabilities remains elusive. While many factors are certainly responsible, this paper shines a light on a significant factor that makes predicting scaling behavior on widely used multiple-choice question answering benchmarks challenging and illuminates a path towards making such downstream evaluations predictable with scale. Using five model families and twelve well-established multiple-choice benchmarks, we show that downstream performance is computed from negative log likelihoods via a sequence of transformations that progressively degrades the statistical relationship between performance and scale. We then reveal the mechanism causing this degradation: downstream metrics require comparing the correct choice against a small number of specific incorrect choices, meaning accurately predicting downstream capabilities requires predicting not just how probability mass concentrates on the correct choice with scale, but also how probability mass fluctuates on specific incorrect choices with scale. We empirically study how probability mass on the correct choice co-varies with probability mass on incorrect choices with increasing compute, suggesting that scaling laws for \textit{incorrect} choices might be achievable. Our work also explains why pretraining scaling laws are commonly regarded as more predictable than downstream capabilities and contributes towards establishing scaling-predictable evaluations of frontier AI models.","Predicting how advanced AI systems will perform on specific tasks like multiple-choice question answering as they get bigger (scale) is surprisingly difficult, even though we can predict their general training progress well. This paper reveals a key reason: current scoring methods for these tests obscure the true relationship between AI scale and performance. The core issue is that these tests require the AI to not only identify the correct answer but also to distinguish it from a few specific incorrect options. To accurately predict performance, we need to understand how the AI's confidence in both the right and specific wrong answers changes as it scales. The authors show that by analyzing these dynamics, we might be able to develop predictable ""scaling laws"" even for these tricky incorrect choices. This research helps explain why predicting downstream task performance is harder and offers a path towards making evaluations of new AI models more reliable."
Poster,Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas,https://ICML.cc//virtual/2025/poster/44272,"Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, Manling Li","Large Vision Language Models (VLMs) have long struggled with spatial reasoning tasks. Surprisingly, even simple spatial reasoning tasks, such as recognizing “under” or “behind” relationships between only two objects, pose significant challenges for current VLMs. We believe it is crucial to use the lens of mechanism interpretability, opening up the model and diving into model’s internal states to examine the interactions between image and text tokens during spatial reasoning. Our analysis of attention behaviors reveals significant differences in how VLMs allocate attention to image versus text. By tracing the areas of images that receive the highest attention scores throughout intermediate layers, we observe a notable pattern: errors often coincide with attention being misdirected towards irrelevant objects within the image. Moreover, such attention patterns exhibit substantial differences between familiar (e.g., “on the left side of ”) and unfamiliar (e.g.,“in front of ”) spatial relationships. Motivated by these findings, we propose ADAPTVIS based on inference-time confidence scores to sharpen the attention on highly relevant regions when the model exhibits high confidence, while smoothing and broadening the attention window to consider a wider context when confidence is lower. This training-free decoding method shows significant improvement (e.g., up to a 50 absolute point improvement) on spatial reasoning benchmarks such as WhatsUp and VSR with negligible additional cost.","We analyze the failure of spatial reasoning in Vision-Language Models (VLMs) through the lens of attention to assess whether they ""look"" at the correct regions. Our findings reveal a strong bias: VLMs often fail to attend to the right spatial locations and show low confidence when handling unfamiliar relationships such as in front of or behind, while performing better on familiar ones like left or right. To address this, we propose AdaptVis, a confidence-based decoding method. When the model is confident, we sharpen the attention distribution to focus more precisely on relevant image regions; when uncertain, we smooth the distribution to allow for broader contextual exploration to make the model see other places.Our experiment results show that AdaptVis could achieve up to 50 absolute points on spatial reasoning benchmarks like WhatsUp for LLaVA models, indicating the effectiveness of our decoding method."
Poster,"""Why Is There a Tumor?"": Tell Me the Reason, Show Me the Evidence",https://ICML.cc//virtual/2025/poster/43910,"Mengmeng Ma, Tang Li, Yunxiang Peng, LIN LU, Volkan Beylergil, Binsheng Zhao, Oguz Akin, Xi Peng","Medical AI models excel at tumor detection and segmentation. However, their latent representations often lack explicit ties to clinical semantics, producing outputs less trusted in clinical practice. Most of the existing models generate either segmentation masks/labels (localizing where without why) or textual justifications (explaining why without where), failing to ground clinical concepts in spatially localized evidence. To bridge this gap, we propose to develop models that can justify the segmentation or detection using clinically relevant terms and point to visual evidence. We address two core challenges: First, we curate a rationale dataset to tackle the lack of paired images, annotations, and textual rationales for training. The dataset includes 180K image-mask-rationale triples with quality evaluated by expert radiologists. Second, we design rationale-informed optimization that disentangles and localizes fine-grained clinical concepts in a self-supervised manner without requiring pixel-level concept annotations. Experiments across medical benchmarks show our model demonstrates superior performance in segmentation, detection, and beyond. The anonymous link to our code.","Artificial intelligence (AI) can identify tumors in medical scans like MRIs, but doctors often hesitate to trust these results because the AI can’t explain why it thinks a region is cancerous. Currently, AI either highlights tumor locations or gives text explanations, but rarely both at once. To solve this, we developed an AI system that can justify its cancer predictions using terms familiar to doctors and point to visual evidence. We trained it on 180,000 expert-reviewed medical scans paired with radiologists’ notes, teaching the AI to tie its explanations to specific image regions. Tests show our AI outperforms existing tools in both accuracy and interpretability."
Poster,Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg,https://ICML.cc//virtual/2025/poster/46667,"Like Jian, Dong Liu","Federated learning (FL) enables decentralized clients to train a model collaboratively without sharing local data. A key distinction between FL and centralized learning is that clients' data are non-independent and identically distributed, which poses significant challenges in training a global model that generalizes well across heterogeneous local data distributions. In this paper, we analyze the convergence of overparameterized FedAvg with gradient descent (GD). We prove that the impact of data heterogeneity diminishes as the width of neural networks increases, ultimately vanishing when the width approaches infinity. In the infinite-width regime, we further prove that both the global and local models in FedAvg behave as linear models, and that FedAvg achieves the same generalization performance as centralized learning with the same number of GD iterations. Extensive experiments validate our theoretical findings across various network architectures, loss functions, and optimization methods.","Training artificial intelligence (AI) usually requires collecting vast amounts of data in one place, raising privacy concerns. Federated learning solves this problem by allowing multiple devices to jointly train a shared AI model without ever sharing their private data. However, because data on each device can be very different, it is difficult for federated learning models to perform equally well across all devices.We studied how large neural networks handle this data heterogeneity in federated learning. Our analysis shows something surprising: as neural networks become wider, the impact of data heterogeneity shrinks significantly and eventually disappears completely for infinitely wide networks. In fact, infinitely wide neural networks trained via federated learning perform just as well as traditional AI models trained centrally.Our findings help clarify how federated learning can achieve strong performance despite challenging data conditions, paving the way for more effective and privacy-friendly AI applications."
Poster,WikiBigEdit: Understanding the Limits of Lifelong Knowledge Editing in LLMs,https://ICML.cc//virtual/2025/poster/46232,"Lukas Thede, Karsten Roth, Matthias Bethge, Zeynep Akata, Thomas Hartvigsen","Keeping large language models factually up-to-date is crucial for deployment, yet costly retraining remains a challenge. Knowledge editing offers a promising alternative, but methods are only tested on small-scale or synthetic edit benchmarks. In this work, we aim to bridge research into lifelong knowledge editing to real-world edits at practically relevant scale. We first introduce \texttt{WikiBigEdit}; a large-scale benchmark of real-world Wikidata edits, built to automatically extend lifelong for future-proof benchmarking. In its first instance, it includes over 500K question-answer pairs for knowledge editing alongside a comprehensive evaluation pipeline. Finally, we use \texttt{WikiBigEdit} to study existing knowledge editing techniques' ability to incorporate large volumes of real-world facts and contrast their capabilities to generic modification techniques such as retrieval augmentation and continual finetuning to acquire a complete picture of the practical extent of current lifelong knowledge editing.","Large language models like ChatGPT are powerful, but their knowledge can quickly become outdated because they’re trained on static snapshots of the internet. Updating them regularly is important—especially in fields like medicine, law, or education—but retraining these massive models is expensive and slow.Researchers have been exploring faster ways to update facts in a model without retraining from scratch. One idea is “knowledge editing,” where specific facts are directly inserted or changed inside the model. However, until now, these methods have only been tested on small or artificial datasets that don’t accurately reflect how knowledge changes in the real world.In this work, we introduce WikiBigEdit, a new large-scale benchmark that tracks real changes in the Wikidata knowledge graph. It includes over half a million fact-based questions and can automatically grow over time to reflect ongoing updates. Using this benchmark, we evaluate how well existing knowledge editing techniques can handle large volumes of real-world updates. We also compare these methods with other ways of keeping models current, like attaching external memory systems or gradually fine-tuning them.The findings show that many editing methods struggle to scale up effectively, and in some cases, simpler alternatives actually work better. This benchmark and analysis provide a clearer picture of what is needed to build language models that can reliably and efficiently stay up to date over time."
Poster,WildChat-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training,https://ICML.cc//virtual/2025/poster/44492,"Benjamin Feuer, Chinmay Hegde","Language model (LLM) post-training can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of conducting large-scale comparative analyses of synthetic data generating models and LLM judges. To close this gap, we introduce WildChat-50M, the largest public chat dataset to date. We extend the existing WildChat dataset to include responses not only from GPT, but from over 50 different open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an extensive comparative analysis and demonstrate the potential of this dataset by creating Re-Wild, our own public SFT mix, which outperforms the recent Tulu-3 SFT mixture from Allen AI with only 40% as many samples.","Lots of people these days are training AI on synthetic data. But what distinguishes useful synthetic data from ... the other kind?To answer this question, we generated a lot of synthetic data using a lot of different LLMs (which we called DGMs, for Data Generating Models) and compared the performance of new LLMs trained on that synthetic data, both to each other and to some state-of-the-art open-source datasets. And we used what we learned to curate a new dataset, ReWild, which was better, by some reasonable measures, than existing public SFT datasets.We made all of our data and all of our models public, so that anyone can try training their own."
Poster,WILTing Trees: Interpreting the Distance Between MPNN Embeddings,https://ICML.cc//virtual/2025/poster/44188,"Masahiro Negishi, Thomas Gärtner, Pascal Welke","We investigate the distance function learned by message passing neural networks (MPNNs) in specific tasks, aiming to capture the _functional_ distance between prediction targets that MPNNs implicitly learn. This contrasts with previous work, which links MPNN distances on arbitrary tasks to _structural_ distances on graphs that ignore task-specific information. To address this gap, we distill the distance between MPNN embeddings into an interpretable graph distance. Our method uses optimal transport on the Weisfeiler Leman Labeling Tree (WILT), where the edge weights reveal subgraphs that strongly influence the distance between embeddings. This approach generalizes two well-known graph kernels and can be computed in linear time. Through extensive experiments, we demonstrate that MPNNs define the relative position of embeddings by focusing on a small set of subgraphs that are known to be functionally important in the domain.","Message Passing Neural Networks (MPNNs) are remarkably good at making predictions for graph-structured data, such as molecules or social networks. But how exactly do these complex models achieve such high performance?We tackled this question by developing a new, human-understandable way to measure the ""distance"" or dissimilarity between graphs, called the Weisfeiler Leman Labeling Tree distance. We used this to approximate and analyze the internal ""MPNN distance""—how the MPNN model learns to distinguish between different input graphs.Our experiments revealed that MPNNs perform well because they learn to capture the task-related functional distance between graphs (e.g., differences in a molecule's properties like oil-friendliness) rather than just their structural distance (e.g., differences in their physical shape). This result indicates that it can be more useful to interpret MPNNs from a functional distance perspective, rather than a structural distance perspective as in most previous studies. We also discovered that MPNNs focus on tiny, functionally important sub-regions within the graphs to capture the functional distance. Our method offers a new way to understand the ""black-box"" MPNNs, contributing to safer and more robust machine learning."
Poster,Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale,https://ICML.cc//virtual/2025/poster/45035,"Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, Zheng Hui","Large language models (LLMs) show potential as computer agents, enhancing productivity and software accessibility in multi-modal tasks. However, measuring agent performance in sufficiently realistic and complex environments becomes increasingly challenging as: (i) most benchmarks are limited to specific modalities/domains (e.g., text-only, web navigation, Q&A) and (ii) full benchmark evaluations are slow (on order of magnitude of multiple hours/days) given the multi-step sequential nature of tasks.To address these challenges, we introduce Windows Agent Arena: a general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real OS to use the same applications and tools available to human users when performing tasks.We create 150+ diverse tasks across representative domains that require agentic abilities in planning, screen understanding, and tool usage.Our benchmark is scalable and can be seamlessly parallelized for a full benchmark evaluation in as little as $20$ minutes.Our work not only speeds up the development and evaluation cycle of multi-modal agents, but also highlights and analyzes existing shortfalls in the agentic  abilities of several multimodal LLMs as agents within the Windows computing environment---with the best achieving only a 19.5\% success rate compared to a human success rate of 74.5\%.","Large multi-modal language models that understand text, images, and more are becoming capable digital assistants and agents, helping us accomplish complex computer tasks on their own. However, effectively measuring how well these AI agents perform realistic tasks is challenging because traditional benchmarks can be complex and slow, often taking hours if not more to provide meaningful results. This slow evaluation significantly delays progress in AI development and agent improvements. Moreover, despite the widespread popularity and extensive use of the Windows operating system (OS), there is no agentic benchmark designed specifically for the Windows OS. To solve this critical bottleneck, we introduce the Windows Agent Arena, a highly efficient and scalable evaluation framework where state-of-the-art multi-modal AI agents perform tasks using the same software and tools humans rely on daily. Our approach dramatically accelerates evaluation, enabling rapid feedback and quicker improvements in AI capabilities. Even the best current AI agents pale in comparison to the 74.5% success rate by humans, highlighting significant room for advancement."
