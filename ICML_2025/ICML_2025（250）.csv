type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Reinforcement Learning for Quantum Control under Physical Constraints,https://ICML.cc//virtual/2025/poster/45921,"Jan Ole Ernst, Aniket Chatterjee, Tim Franzmeyer, Axel Kuhn","Quantum control is concerned with the realisation of desired dynamics in quantum systems, serving as a linchpin for advancing quantum technologies and fundamental research. Analytic approaches and standard optimisation algorithms do not yield satisfactory solutions for more complex quantum systems, and especially not for real world quantum systems which are open and noisy. We devise a physics-constrained Reinforcement Learning (RL) algorithm that restricts the space of possible solutions.We incorporate priors about the desired time scales of the quantum state dynamics - as well as realistic control signal limitations - as constraints to the RL algorithm. These constraints improve solution quality and enhance computational scaleability. We evaluate our method on three broadly relevant quantum systems and incorporate real-world complications, arising from dissipation and control signal perturbations. We achieve both higher fidelities - which exceed 0.999 across all systems -  and better robustness to time-dependent perturbations and experimental imperfections than previous methods. Lastly, we demonstrate that incorporating multi-step feedback can yield solutions robust even to strong perturbations. Our implementation can be found at: https://github.com/jan-o-e/RL4qcWpc.","Controlling quantum systems—the tiny building blocks of matter like electrons and photons—is essential for developing advanced technologies such as quantum computers and sensors. It’s also key to running advanced experiments in physics and chemistry. These systems are usually controlled by applying laser pulses or microwave signals, but this is challenging in real-world settings where such systems are noisy and interact with their environment.We present a new approach using Reinforcement Learning (RL), a type of machine learning that learns how to control a system by trial and error. The algorithm tries different control signals on a simulated quantum system, then checks how close the result is to the desired outcome. What makes our method unique is that it’s guided by physical priors: it learns to prefer control signals that are realistic for experiments and that change the quantum state on the right time scales. By focusing only on physically meaningful options, the algorithm becomes faster and more effective.We tested this method on three relevant quantum systems that are being actively studied in laboratories today. It consistently achieved very high control accuracy and outperformed previous methods, especially in situations where the systems were affected by noise or other real-world imperfections."
Poster,Reinforcement Learning with Adaptive Reward Modeling for Expensive-to-Evaluate Systems,https://ICML.cc//virtual/2025/poster/43786,"Hongyuan Su, Yu Zheng, Yuan Yuan, Yuming Lin, Depeng Jin, Yong Li","Training reinforcement learning (RL) agents requires extensive trials and errors, which becomes prohibitively time-consuming in systems with costly reward evaluations.To address this challenge, we propose adaptive reward modeling (AdaReMo) which accelerates RL training by decomposing the complicated reward function into multiple localized fast reward models approximating direct reward evaluation with neural networks.These models dynamically adapt to the agent’s evolving policy by fitting the currently explored subspace with the latest trajectories, ensuring accurate reward estimation throughout the entire training process while significantly reducing computational overhead.We empirically show that AdaReMo not only achieves over 1,000 times speedup but also improves the performance by 14.6% over state-of-the-art approaches across three expensive-to-evaluate systems---molecular generation, epidemic control, and spatial planning.Code and data for the project are provided at https://github.com/tsinghua-fib-lab/AdaReMo.","Training artificial intelligence (AI) systems to make decisions through trial and error—what’s called reinforcement learning (RL)—can be very slow when it's hard or expensive to measure success.For example, if an AI is learning to design new molecules or plan how to control an epidemic, figuring out whether it made a good decision might take a lot of time or computation.To solve this, we created a method called AdaReMo (Adaptive Reward Modeling). Instead of always calculating the exact measure of success, AdaReMo trains fast, local prediction models using neural networks to estimate it more efficiently.These models quickly adjust as the AI learns, always staying up-to-date with what the AI is currently doing.This means the AI can learn faster, using much less computing power.In our experiments, AdaReMo made learning over 1,000 times faster and even improved decision quality by 14.6% compared to the best existing methods. We tested this in three complex areas: designing molecules, managing disease spread, and planning in space.You can try out our code and data at: https://github.com/tsinghua-fib-lab/AdaReMo"
Poster,Reinforcement Learning with Random Time Horizons,https://ICML.cc//virtual/2025/poster/44663,"Enric Borrell, Lorenz Richter, Christof Schuette","We extend the standard reinforcement learning framework to random time horizons. While the classical setting typically assumes finite and deterministic or infinite runtimes of trajectories, we argue that multiple real-world applications naturally exhibit random (potentially trajectory-dependent) stopping times. Since those stopping times typically depend on the policy, their randomness has an effect on policy gradient formulas, which we (mostly for the first time) derive rigorously in this work both for stochastic and deterministic policies. We present two complementary perspectives, trajectory or state-space based, and establish connections to optimal control theory. Our numerical experiments demonstrate that using the proposed formulas can significantly improve optimization convergence compared to traditional approaches.","In this work, we improve reinforcement learning in situations where it is unclear how long the related task should last. In previous work, one assumes a fixed amount of time or a task that goes on forever. But in real life, tasks often end at random times - for example, a game might end early if the player loses, or a robot might stop working if its battery runs out. We show that these random endings affect how the learning process should work, especially when it comes to adjusting the system to improve over time. We carefully figure out how to change the learning process (using so-called policy gradients) when tasks end randomly, for two types of learning systems - those that make decisions randomly and those that consider deterministic decisions. We show through experiments that our method helps the learning process work faster and better than previous methods."
Poster,Reinforcement Learning with Segment Feedback,https://ICML.cc//virtual/2025/poster/46633,"Yihan Du, Anna Winnicki, Gal Dalal, Shie Mannor, R Srikant","Standard reinforcement learning (RL) assumes that an agent can observe a reward for each state-action pair. However, in practical applications, it is often difficult and costly to collect a reward for each state-action pair. While there have been several works considering RL with trajectory feedback, it is unclear if trajectory feedback is inefficient for learning when trajectories are long. In this work, we consider a model named RL with segment feedback, which offers a general paradigm filling the gap between per-state-action feedback and trajectory feedback. In this model, we consider an episodic Markov decision process (MDP), where each episode is divided into $m$ segments, and the agent observes reward feedback only at the end of each segment. Under this model, we study two popular feedback settings: binary feedback and sum feedback, where the agent observes a binary outcome and a reward sum according to the underlying reward function, respectively. To investigate the impact of the number of segments $m$ on learning performance, we design efficient algorithms and establish regret upper and lower bounds for both feedback settings. Our theoretical and experimental results show that: under binary feedback, increasing the number of segments $m$ decreases the regret at an exponential rate; in contrast, surprisingly, under sum feedback, increasing $m$ does not reduce the regret significantly.","Standard reinforcement learning (RL) assumes that we can observe a reward for each state-action pair. However, in real-world applications such as autonomous driving, it is often difficult and costly to collect a reward for each state-action pair. While there have been prior works which consider collecting a reward for a whole trajectory, it is unclear if such trajectory feedback is inefficient when trajectories are long.In this work, we consider a model called RL with segment feedback, where each trajectory is divided into multiple segments, and we collect a reward for each segment. This model offers a general paradigm filling the gap between per-state-action feedback and trajectory feedback. Under this model, an interesting and important question is how segments impact learning performance?Our theoretical and experimental results show that: under binary feedback, where we observe a binary outcome generated according to the reward function, segments help accelerate learning; surprisingly, under sum feedback, where we observe the sum of random rewards, segments do not help expedite learning much."
Poster,Rejecting Hallucinated State Targets during Planning,https://ICML.cc//virtual/2025/poster/46511,"Mingde Zhao, Tristan Sylvain, Romain Laroche, Doina Precup, Yoshua Bengio","Generative models can be used in planning to propose targets corresponding to states that agents deem either likely or advantageous to experience. However, imperfections, common in learned models, lead to infeasible hallucinated targets, which can cause delusional behaviors and thus safety concerns. This work first categorizes and investigates the properties of several kinds of infeasible targets. Then, we devise a strategy to reject infeasible targets with a generic target evaluator, which trains alongside planning agents as an add-on without the need to change the behavior nor the architectures of the agent (and the generative model) it is attached to. We highlight that, without proper design, the evaluator can produce delusional estimates, rendering the strategy futile. Thus, to learn correct evaluations of infeasible targets, we propose to use a combination of learning rule, architecture, and two assistive hindsight relabeling strategies. Our experiments validate significant reductions in delusional behaviors and performance improvements for several kinds of existing planning agents.","Computational agents tend to blindly trust their own generated contents without questioning the feasibility, leading to delusional behaviors and AI safety concerns.We identified such problem, proposed a generic rejection-based strategy compatible with many existing methods to address such issue.This work is expected to inspire creativity in how to deal with hallucinations in generative AI, especially in computational decision-making. It can potentially save energy and time for future researchers by raising awareness of hallucination-related issues to avoid designs that lead to delusional and unsafe AIs."
Poster,Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the Squared Loss,https://ICML.cc//virtual/2025/poster/43626,"Abhijeet Mulgund, Chirag Pabbaraju","The paradigm of weak-to-strong generalization constitutes the training of a strong AI model on data labeled by a weak AI model, with the goal that the strong model nevertheless outperforms its weak supervisor on the target task of interest. For the setting of real-valued regression with the squared loss, recent work quantitatively characterizes the gain in performance of the strong model over the weak model in terms of the misfit between the strong and weak model. We generalize such a characterization to learning tasks whose loss functions correspond to arbitrary Bregman divergences when the strong class is convex. This extends the misfit-based characterization of performance gain in weak-to-strong generalization to classification tasks, as the cross-entropy loss can be expressed in terms of a Bregman divergence. In most practical scenarios, however, the strong model class may not be convex. We therefore weaken this assumption and study weak-to-strong generalization for convex combinations of $k$ strong models in the strong class, in the concrete setting of classification. This allows us to obtain a similar misfit-based characterization of performance gain, up to an additional error term that vanishes as $k$ gets large. Our theoretical findings are supported by thorough experiments on synthetic as well as real-world datasets.","(1) Previous work identified an interesting phenomenon where more complex student AI models could outperform their teachers if the teachers were smaller and less complex. This phenomenon is called weak-to-strong generalization, and it is unexpected, since it is unclear how the more complex model is deciding to correctly deviate from its teacher. (2) We extended prior work that analyzed this phenomenon with geometric tools to much more general settings. This now aligns the geometric ideas with practical use cases. (3) Understanding the mechanisms that govern weak-to-strong generalization allow us to safely build superhuman AI models that still align with our core values. In that situation, humans are the less complex teachers and the superhuman AI models are the more complex learners."
Poster,Relational Conformal Prediction for Correlated Time Series,https://ICML.cc//virtual/2025/poster/43601,"Andrea Cini, Alexander Jenkins, Danilo Mandic, Cesare Alippi, Filippo Maria Bianchi","We address the problem of uncertainty quantification in time series forecasting by exploiting observations at correlated sequences. Relational deep learning methods leveraging graph representations are among the most effective tools for obtaining point estimates from spatiotemporal data and correlated time series. However, the problem of exploiting relational structures to estimate the uncertainty of such predictions has been largely overlooked in the same context. To this end, we propose a novel distribution-free approach based on the conformal prediction framework and quantile regression. Despite the recent applications of conformal prediction to sequential data, existing methods operate independently on each target time series and do not account for relationships among them when constructing the prediction interval. We fill this void by introducing a novel conformal prediction method based on graph deep learning operators. Our approach, named Conformal Relational Prediction (CoRel), does not require the relational structure (graph) to be known a priori and can be applied on top of any pre-trained predictor. Additionally, CoRel includes an adaptive component to handle non-exchangeable data and changes in the input time series. Our approach provides accurate coverage and achieves state-of-the-art uncertainty quantification in relevant benchmarks.","This paper introduces Conformal Relational Prediction (CoRel), a new framework for estimating the uncertainty of forecasts when dealing with multiple, correlated time series such as those produced by sensor networks. Indeed, besides maximizing prediction accuracy, understanding the potential range of outcomes (prediction intervals) is crucial for reliable decision-making. However, existing methods for uncertainty quantification often treat each time series independently of the others, disregarding dependencies among correlated series. CoRel addresses this by using graph deep learning, a class of deep learning methods that model relationships by relying on graph representations. CoRel accounts for dependencies between different time series, even if these connections are not known beforehand, by learning from prediction errors. This allows us to condition uncertainty estimates for one series on observations at related neighbors. A key advantage of CoRel is that it can be applied to any pre-existing forecasting model without needing any modification. Furthermore, it is ""distribution-free"", meaning it does not rely on strong assumptions about the data's underlying distribution. CoRel includes an adaptive component to adjust to changes in the time series data over time. Experiments on real-world datasets (traffic, air quality, energy consumption) demonstrate that our approach can achieve accurate uncertainty predictions and narrower, more informative prediction intervals compared to existing methods."
Poster,Relational Invariant Learning for Robust Solvation Free Energy Prediction,https://ICML.cc//virtual/2025/poster/43578,Yeyun Chen,"Predicting the solvation free energy of molecules using graph neural networks holds significant potential for advancing drug discovery and the design of novel materials. While previous methods have demonstrated success on independent and identically distributed (IID) datasets, their performance in out-of-distribution (OOD) scenarios remains largely unexplored. We propose a novel Relational Invariant Learning framework (RILOOD) to enhance OOD generalization in solvation free energy prediction. RILOOD comprises three key components: (i) a mixup-based conditional modeling module that integrates diverse environments, (ii) a novel multi-granularity refinement strategy that extends beyond core substructures to enable context-aware representation learning for capturing multi-level interactions, and (iii) an invariant learning mechanism that identifies robust patterns generalizable to unseen environments. Extensive experiments demonstrate that RILOOD significantly outperforms state-of-the-art methods across various distribution shifts, highlighting its effectiveness in improving solvation free energy prediction under diverse conditions.","We want to help computers better predict how molecules behave in different environments — a crucial step in developing new drugs and materials. One important property is solvation free energy, which affects how a molecule dissolves in a liquid. Current AI models can make accurate predictions when the training and testing environments are the same. But in real-world science, molecules often appear in new environments, where existing models tend to fail.To tackle this, we created a new method called RILOOD. It teaches the model to focus on stable, transferable patterns in molecules and their environments. We combine several ideas: mixing different environments during training, modeling both the fine-grained structure of individual molecules and their interactions with surrounding molecules, and learning invariant features that remain reliable across varied chemical settings.Our results show that RILOOD does much better than existing approaches when handling diverse and unseen environments. This brings us a step closer to building AI models that can make accurate predictions in real-world, messy chemistry problems."
Poster,Relative Error Fair Clustering in the Weak-Strong Oracle Model,https://ICML.cc//virtual/2025/poster/46348,"Vladimir Braverman, Prathamesh Dharangutte, Shaofeng Jiang, Hoai-An Nguyen, Chen Wang, Yubo Zhang, Samson Zhou","We study fair clustering problems in a setting where distance information is obtained from two sources: a strong oracle providing exact distances, but at a high cost, and a weak oracle providing potentially inaccurate distance estimates at a low cost. The goal is to produce a near-optimal fair clustering on $n$ input points with a minimum number of strong oracle queries. This models the increasingly common trade-off between accurate but expensive similarity measures (e.g., large-scale embeddings) and cheaper but inaccurate alternatives. The study of fair clustering in the model is motivated by the important quest of achieving fairness with the presence of inaccurate information. We achieve the first $(1+\varepsilon)$-coresets for fair $k$-median clustering using $\text{poly}\left(\frac{k}{\varepsilon}\cdot\log n\right)$ queries to the strong oracle. Furthermore, our results imply coresets for the standard setting (without fairness constraints), and we could in fact obtain $(1+\varepsilon)$-coresets for $(k,z)$-clustering for general $z=O(1)$ with a similar number of strong oracle queries. In contrast, previous results achieved a constant-factor $(>10)$ approximation for the standard $k$-clustering problems, and no previous work considered the fair $k$-median clustering problem.","Clustering—grouping similar data points together—is a central task in machine learning. It's widely used in real-world applications, like recommending movies, summarizing customer data, or grouping job applicants. But clustering can go wrong in two key ways: it can be unfair (for example, placing people of one gender or race disproportionately in certain clusters), and it can be expensive when relying on accurate but costly computations.This paper tackles both problems at once. The authors study how to cluster data fairly while using a mix of cheap, potentially inaccurate information (from a weak source) and expensive, reliable information (from a strong source). Think of weak information as a quick estimate and strong information as the expensive but accurate output from a powerful model.Their main contribution is an efficient method to select a small, weighted summary of the data—called a coreset — that can be clustered instead of the full dataset. Crucially, this coreset supports fair clustering (ensuring balanced representation of subgroups like gender or race), and it can be built using only a small number of expensive strong queries. This means fairness and accuracy can be achieved without a huge computational cost. In short, this work helps make fairness in AI more affordable and practical—especially in settings where resources are limited and data is messy or unreliable."
Poster,RelGNN: Composite Message Passing for Relational Deep Learning,https://ICML.cc//virtual/2025/poster/44955,"Tianlang Chen, Charilaos Kanatsoulis, Jure Leskovec","Predictive tasks on relational databases are critical in real-world applications spanning e-commerce, healthcare, and social media. To address these tasks effectively, Relational Deep Learning (RDL) encodes relational data as graphs, enabling Graph Neural Networks (GNNs) to exploit relational structures for improved predictions. However, existing RDL methods often overlook the intrinsic structural properties of the graphs built from relational databases, leading to modeling inefficiencies, particularly in handling many-to-many relationships. Here we introduce RelGNN, a novel GNN framework specifically designed to leverage the unique structural characteristics of the graphs built from relational databases. At the core of our approach is the introduction of atomic routes, which are simple paths that enable direct single-hop interactions between the source and destination nodes. Building upon these atomic routes, RelGNN designs new composite message passing and graph attention mechanisms that reduce redundancy, highlight key signals, and enhance predictive accuracy. RelGNN is evaluated on 30 diverse real-world tasks from Relbench (Fey et al., 2024), and achieves state-of-the-art performance on the vast majority of tasks, with improvements of up to 25%.","Predictive tasks in areas like e-commerce, healthcare, and social media rely on effectively understanding and modeling relational databases. Relational Deep Learning (RDL) addresses these tasks by encoding relational data as graphs, allowing Graph Neural Networks (GNNs) to exploit the relational structures for improved predictions. However, existing RDL methods often overlook the intrinsic structural properties of these graphs, leading to modeling inefficiencies.Here we introduce RelGNN, a novel GNN framework specifically designed to leverage these unique structural characteristics of graphs built from relational databases. RelGNN introduces the concept of ""atomic routes,"" which are simple paths that enable single-hop interactions between the source and destination nodes. Building upon these atomic routes, RelGNN designs new composite message passing and graph attention mechanisms that enhance predictive accuracy.RELGNN achieves state-of-the-art performance on the vast majority of 30 diverse real-world tasks from Relbench (Fey et al., 2024), with improvements of up to 25%."
