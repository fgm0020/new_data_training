type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Revisiting Unbiased Implicit Variational Inference,https://ICML.cc//virtual/2025/poster/45871,"Tobias Pielok, Bernd Bischl, David Rügamer","Recent years have witnessed growing interest in semi-implicit variational inference (SIVI) methods due to their ability to rapidly generate samples from complex distributions. However, since the likelihood of these samples is non-trivial to estimate in high dimensions, current research focuses on finding effective SIVI training routines. Although unbiased implicit variational inference (UIVI) has largely been dismissed as imprecise and computationally prohibitive because of its inner MCMC loop, we revisit this method and show that UIVI's MCMC loop can be effectively replaced via importance sampling and the optimal proposal distribution can be learned stably by minimizing an expected forward Kullback–Leibler divergence without bias. Our refined approach demonstrates superior performance or parity with state-of-the-art methods on established SIVI benchmarks.","Modern AI systems often need to estimate uncertainty — especially when working with limited or noisy data. A common way to do this is through variational inference, which approximates complex probability distributions using simpler ones. A powerful version called semi-implicit variational inference (SIVI) can represent rich and flexible distributions but is difficult to train efficiently.An earlier approach, unbiased implicit variational inference (UIVI), offered strong theoretical guarantees but was largely abandoned due to its reliance on slow simulation methods. In this work, we revisit UIVI and replace its slow component with a more scalable technique: instead of running simulations, we draw samples from a carefully chosen distribution and give each sample a weight based on how relevant it is. This allows us to approximate the same results as the original method, but far more efficiently.Our method matches or improves upon state-of-the-art techniques on benchmark problems, making uncertainty estimation in machine learning more practical and reliable."
Poster,Revolve: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization,https://ICML.cc//virtual/2025/poster/46456,"Peiyan Zhang, Haibo Jin, Leyang Hu, Xinnuo Li, Liying Kang, Man Luo, Yangqiu Song, Haohan Wang","Recent advancements in large language models (LLMs) have significantly enhanced the ability of LLM-based systems to perform complex tasks through natural language processing and tool interaction. However, optimizing these LLM-based systems for specific tasks remains challenging, often requiring manual interventions like prompt engineering and hyperparameter tuning.Existing automatic optimization methods, such as textual feedback-based techniques (*e.g.*, TextGrad), tend to focus on immediate feedback, analogous to using immediate derivatives in traditional numerical gradient descent.However, relying solely on such feedback can be limited when the adjustments made in response to this feedback are either too small or fluctuate irregularly, potentially slowing down or even stalling the optimization process. In this paper, we introduce $\textbf{REVOLVE}$, an optimization method that tracks how $\textbf{R}$esponses $\textbf{EVOLVE}$across iterations in LLM systems. By focusing on the evolution of responses over time, REVOLVE enables more stable and effective optimization by making thoughtful, progressive adjustments at each step. Experiments across three tasks demonstrate the adaptability and efficiency of our proposal. Beyond its practical contributions, REVOLVE highlights a promising direction, where the rich knowledge from established optimization principles can be leveraged to enhance LLM systems, which paves the way for further advancements in this hybrid domain. Code is available at: https://llm-revolve.netlify.app.","Getting AI models (like chatbots or multi-agent assistants) to improve consistently can be challenging. Sometimes they get stuck or their progress is unstable, especially when dealing with complex tasks. Current methods often only look at the immediate success or failure of the latest attempt, missing valuable information about the learning process.Our paper presents REVOLVE, a new method for improving these AI models. Instead of just focusing on the last response, REVOLVE analyzes how the AI's answers change over multiple tries. By understanding this ""evolution"" or trend in the responses – similar to how engineers might consider momentum or the curve of progress in physical systems – REVOLVE makes smarter decisions about how to guide the AI's next steps.REVOLVE helps AI models learn more smoothly and reliably, avoiding common pitfalls. More importantly, it shows that powerful ideas from traditional optimization (the mathematical field of finding the best solutions) can be successfully adapted for the world of AI. This opens up exciting new research directions, suggesting that we can leverage decades of optimization knowledge to significantly boost the capabilities of future AI systems."
Poster,Reward-Augmented Data Enhances Direct Preference Alignment of LLMs,https://ICML.cc//virtual/2025/poster/43860,"Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang","Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion. Our code is available at https://github.com/shenao-zhang/reward-augmented-preference.","Direct alignment of LLMs primarily focuses on relative preferences and often overlooks the qualitative aspects of responses. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning. To overcome these shortcomings, our paper introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin."
Poster,Reward-free World Models for Online Imitation Learning,https://ICML.cc//virtual/2025/poster/44035,"Shangzhe Li, Zhiao Huang, Hao Su","Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in the reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.","Teaching robots and AI systems to perform complex tasks by simply watching human experts—known as imitation learning—is a powerful idea. However, many current methods struggle when tasks involve complicated environments or high-dimensional data, like video inputs or robotic control. To address this, we developed a new technique that lets AI systems learn from demonstrations without needing predefined rewards or full reconstructions of the environment. Instead, we train a model to understand the “rules” of the environment in a compressed, abstract form. Using this model, the system can plan and act intelligently, much like a human would. We also introduced a more stable training method that avoids common pitfalls in learning how to make decisions. Our approach closely matches expert performance on challenging benchmarks involving simulated robots and manipulation tasks. This makes it a promising step toward more reliable and scalable AI training methods—especially for real-world tasks where designing reward functions or collecting large datasets is difficult."
Poster,Reward-Guided Iterative Refinement in Diffusion Models at Test-Time with Applications to Protein and DNA Design,https://ICML.cc//virtual/2025/poster/46204,"Masatoshi Uehara, su, Yulai Zhao, Xiner Li, Aviv Regev, Shuiwang Ji, Sergey Levine, Tommaso Biancalani","To fully leverage the capabilities of diffusion models, we are often interested in optimizing downstream reward functions during inference. While numerous algorithms for reward-guided generation have been recently proposed due to their significance, current approaches predominantly focus on single-shot generation, transitioning from fully noised to denoised states. We propose a novel framework for inference-time reward optimization with diffusion models. Our approach employs an iterative refinement process consisting of two steps in each iteration: noising and reward-guided denoising. This sequential refinement allows for the gradual correction of errors introduced during reward optimization. Finally, we provide a theoretical guarantee for our framework. Finally, we demonstrate its superior empirical performance in protein and DNA design.","To get the most out of diffusion models, we often want to improve certain outcomes or ""rewards"" when generating results. Many recent methods have tried to do this, but they typically rely on a one-step process that goes directly from random noise to a final output. In contrast, we introduce a new approach that improves results step by step. Each step involves adding some noise and then refining the output based on the reward we want to achieve. This gradual process helps fix mistakes along the way. We also show that our method is both theoretically sound and performs better in practice, especially for designing proteins and DNA."
Poster,Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs,https://ICML.cc//virtual/2025/poster/46064,"Ziyu Ye, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc Le, Qijun Tan, Yuan Liu","Existing reinforcement learning (RL) methods for large language models (LLMs) rely on static prompt sets, where prompts are curated a priori, and sampled in a fixed schedule for training, regardless of their usefulness to the RL process. We design `eva`, the first method that allows LLMs to prioritize and adaptively create useful prompts during RL training by reward signals. In principle, `eva` (Evolving via A symmetric Self-Play) casts language model training as a game between: (1) a creator, who samples and generates training prompts, and (2) a solver, who generates responses to the prompts. `eva` is simple, suits both offline and online RL for LLMs, and sets a new state-of-the-art on challenging benchmarks without extra human prompts: it improves gemma-2-9b-it’s win-rate on Arena-Hard from 51.6% to 60.1% by DPO and 52.6% to 62.4% by RLOO, surpassing claude-3-opus and nearing gemini-1.5-pro, both are orders of magnitude larger. Further ablation studies show `eva` can induce meaningful learning curriculum, and effectively scale RL for LLMs beyond static human prompts.","Large language models are usually trained using a fixed set of prompts. But not all prompts are equally useful, and this static setup limits how much the model can improve.Our method, called `eva`, lets the model not only respond to prompts but also create new ones based on which ones help it learn best. This turns training into a kind of game: one part of the model proposes problems, and another part tries to solve them well.By using feedback from rewards, `eva` learns which prompts are most helpful, evolves better ones, and gradually builds a smarter training path. This leads to significantly better results -- even outperforming models that are much larger or trained with more human data.We show that by letting models generate and select their own training data, they can improve more efficiently. `eva` is simple to use, works in both offline and online training settings, and is a step toward more adaptive and general-purpose AI systems."
Poster,Reward-Guided Speculative Decoding for Efficient LLM Reasoning,https://ICML.cc//virtual/2025/poster/46166,"Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong","We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness.RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4X fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios.","Large language models can solve hard reasoning problems but often need a lot of computing power and time. This paper introduces Reward-Guided Speculative Decoding, which pairs a small, fast “draft” model with the full-strength model so most of the work happens quickly in the draft. A lightweight reward checker then decides when to call in the expensive model to refine only the most promising answers. By doing so, it keeps nearly the same answer quality but uses up to 4.4× less computation, making advanced language AI much faster and cheaper to run in real-world settings."
Poster,Reward Modeling with Ordinal Feedback: Wisdom of the Crowd,https://ICML.cc//virtual/2025/poster/46590,"Shang Liu, Yu Pan, Guanting Chen, Xiaocheng Li","The canonical setup of learning a reward model (RM) from human preferences with binary feedback discards potentially useful samples (such as ""tied"" between the two responses) and loses fine-grained information  (such as ""slightly better'""). This paper proposes a framework for learning RMs under *ordinal feedback*, generalizing the binary feedback to arbitrary granularity. We first identify a marginal unbiasedness condition, which generalizes the existing assumption of the binary feedback. The condition is validated via the sociological concept called ""wisdom of the crowd"". Under this condition, we develop a natural probability model and prove the benefits of fine-grained feedback in terms of reducing the Rademacher complexity, which may be of independent interest to another problem: the bias-variance trade-off in knowledge distillation. The framework also sheds light on designing guidelines for human annotators. Our numerical experiments validate that: (1) fine-grained feedback leads to better RM learning for both in- and out-of-distribution settings; (2) incorporating a certain proportion of tied samples boosts RM learning.","Learning from human preferences is crucial to aligning the large language models with human values. The human preference data is usually collected in pairwise comparisons by asking human annotators to express how they prefer one over the other in a pair of responses generated by LLMs. This preference is called the binary feedback. However, more fine-grained feedback can also be collected by asking the annotators how much they prefer the chosen one. For example, different levels of ""better"": ""significantly better"", ""better"", and ""slightly better"", have been adopted in data collection. We aim to answer two questions: (1) How should we incorporate the fine-grained feedback into the current LLM training system? (2) What are the benefits of the fine-grained feedback? The first question is answered by turning the qualitative descriptions into quantitative descriptions: for example, turning ""slightly better"" into ""60% of preference"". As a well-known social experiment in *Vox Populi* has shown, the crowd's average quantitative description can be very accurate. The second question is answered by statistical learning theory, showing that more fine-grained feedback helps reduce the noise introduced by the feedback system design. For example, if you are facing a tied comparison but you are only given two options, ""better"" and ""worse"", then no matter which you choose, there is unnecessary noise in your feedback. But if you are given a third option named ""tied"", you can give accurate feedback now."
Poster,Reward Translation via Reward Machine in Semi-Alignable MDPs,https://ICML.cc//virtual/2025/poster/44913,"Yun Hua, Haosheng Chen, Wenhao Li, Bo Jin, Baoxiang Wang, Hongyuan Zha, Xiangfeng Wang","Addressing reward design complexities in deep reinforcement learning is facilitated by knowledge transfer across different domains. To this end, we define \textit{reward translation} to describe the cross-domain reward transfer problem. However, current methods struggle with non-pairable and non-time-alignable incompatible MDPs.This paper presents an adaptable reward translation framework \textit{neural reward translation} featuring \textit{semi-alignable MDPs}, which allows efficient reward translation under relaxed constraints while handling the intricacies of incompatible MDPs. Given the inherent difficulty of directly mapping semi-alignable MDPs and transferring rewards, we introduce an indirect mapping method through reward machines, created using limited human input or LLM-based automated learning.Graph-matching techniques establish links between reward machines from distinct environments, thus enabling cross-domain reward translation within semi-alignable MDP settings. This broadens the applicability of DRL across multiple domains. Experiments substantiate our approach's effectiveness in tasks under environments with semi-alignable MDPs.","Designing effective reward signals for artificial intelligence (AI) agents to learn complex tasks—such as robots grasping objects or navigating environments—is challenging and time-intensive. Ideally, we would reuse the reward structures already developed for one task to help train agents in different tasks. However, transferring reward signals between tasks often fails because tasks differ too greatly, making direct comparisons difficult or impossible.To solve this, we introduce a new method called neural reward translation. Our approach does not require tasks to match perfectly in terms of time or actions—tasks only need partial similarity (what we call ""semi-alignable""). Instead of directly mapping rewards between tasks, we first represent rewards using simplified structures called ""reward machines,"" which can be created with minimal human guidance or automatically through language-based AI tools.We then use graph-matching techniques—essentially, methods that find similarities between structured information—to connect these reward machines from different tasks. Tests show our method successfully transfers reward knowledge between tasks that differ substantially. This approach makes it easier and faster to train AI across many diverse, real-world scenarios."
Poster,Rhomboid Tiling for Geometric Graph Deep Learning,https://ICML.cc//virtual/2025/poster/44198,"Yipeng Zhang, Longlong Li, Kelin Xia","Graph Neural Networks (GNNs) have proven effective for learning from graph-structured data through their neighborhood-based message passing framework. Many hierarchical graph clustering pooling methods modify this framework by introducing clustering-based strategies, enabling the construction of more expressive and powerful models. However, all of these message passing framework heavily rely on the connectivity structure of graphs, limiting their ability to capture the rich geometric features inherent in geometric graphs. To address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering method based on the rhomboid tiling structure, which performs clustering by leveraging the complex geometric information of the data and effectively extracts its higher-order geometric structures. Moreover, we design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks. The proposed model demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.","Graphs are everywhere — from social networks and molecules to 3D shapes. Computers can learn useful patterns from graph data using tools called Graph Neural Networks (GNNs), which let each node gather information from its neighbors. However, for a special class of graphs known as geometric graphs — where nodes have spatial positions — these tools often ignore the rich geometric structure and focus only on connections.To address this, we introduce a new clustering method called Rhomboid Tiling (RT) clustering, inspired by a novel concept in computational geometry known as rhomboid tiling. It groups nodes by analyzing the underlying geometric layout of the graph, allowing the model to capture more complex spatial relationships.Building on this, we designed RTPool, a new graph pooling technique that leverages RT clustering to improve graph classification tasks. Our model outperforms 21 leading methods on 7 widely used datasets, showing that geometry-aware graph learning can lead to significantly better results in understanding geomatric graph data."
