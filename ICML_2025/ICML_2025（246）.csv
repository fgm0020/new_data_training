type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding,https://ICML.cc//virtual/2025/poster/46343,"Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Shieh","The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter—a draft LLM operating on shortened retrieval contexts—to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2$\times$ speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.","Large language models (LLMs) excel at answering questions from huge documents like books or reports, but processing every word makes them painfully slow. Our solution, Retrieval-Augmented Speculative Decoding (RAPID), acts like a sharp librarian and expert editor working together. RAPID swiftly pinpoints key passages relevant to the question, then another LLM drafts potential answers using only those snippets. The main LLM, like a skilled editor, verifies these drafts in parallel against the full document, quickly correcting or improving them instead of generating answers step by step. This teamwork makes RAPID over twice as fast while often producing better answers than traditional methods. Its speed and accuracy open new doors for efficiently analyzing complex texts like legal cases, scientific papers, or lengthy reports, transforming how we use powerful LLMs in real-world tasks."
Poster,Rapid Overfitting of Multi-Pass SGD in Stochastic Convex Optimization,https://ICML.cc//virtual/2025/poster/45321,"Shira Vansover-Hager, Tomer Koren, Roi Livni","We study the out-of-sample performance of multi-pass stochastic gradient descent (SGD) in the fundamental stochastic convex optimization (SCO) model. While one-pass SGD is known to achieve an optimal $\Theta(1/\sqrt{n})$ excess population loss given a sample of size $n$, much less is understood about the multi-pass version of the algorithm which is widely used in practice. Somewhat surprisingly, we show that in the general non-smooth case of SCO, just a few epochs of SGD can already hurt its out-of-sample performance significantly and lead to overfitting. In particular, using a step size $\eta = \Theta(1/\sqrt{n})$, which gives the optimal rate after one pass, can lead to population loss as large as $\Omega(1)$ after just one additional pass. More generally, we show that the population loss from the second pass onward is of the order $\Theta(1/(\eta T) + \eta \sqrt{T})$, where $T$ is the total number of steps. These results reveal a certain phase-transition in the out-of-sample behavior of SGD after the first epoch, as well as a sharp separation between the rates of overfitting in the smooth and non-smooth cases of SCO. Additionally, we extend our results to with-replacement SGD, proving that the same asymptotic bounds hold after $O(n \log n)$ steps. Finally, we also prove a lower bound of $\Omega(\eta \sqrt{n})$ on the generalization gap of one-pass SGD in dimension $d = {\widetilde O}(n)$, improving on recent results of Koren et al. (2022) and Schliserman et al. (2024).","AI models are often trained using an algorithm called stochastic gradient descent (SGD), which processes data iteratively and updates the model based on each example it sees. In its basic form, SGD makes a single pass over the training data and is known to generalize well — meaning it performs well on new, unseen examples.In practice, however, it's common to make multiple passes over the same data to improve performance. This raises a key question: What are the limits of reusing data in SGD when it comes to generalization?Our research shows that generalization can break down surprisingly quickly — even after just one additional pass. In cases where the one-pass version performs optimally, a second pass can already lead to catastrophic overfitting, where the model memorizes the training data instead of learning patterns that apply more broadly.We analyze this behavior and identify a kind of phase transition after the first pass, where generalization begins to break down. These findings reveal a gap between theory and practice, pointing to the need for new theoretical tools to understand why multi-pass training often appears to succeed in practice, despite these fundamental limitations."
Poster,Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models,https://ICML.cc//virtual/2025/poster/46452,"Ulzee An, Moonseong Jeong, Simon Lee, Aditya Gorla, Yuzhe Yang, Sriram Sankararaman","Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes.To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining rich semantic information. Extensive experiments on 10 diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3 SuPreM, +6 MISFM, +10 Merlin, +13 VoCo, and +14 SLIViT), while entirely bypassing the need for costly training. Our results highlight Raptor's effectiveness and versatility as a foundation for advancing deep learning-based methods for medical volumes (code: github.com/sriramlab/raptor).","Raptor is a new method for analyzing 3D medical scans like MRIs and CTs—without any training. Instead of using complex 3D models, Raptor slices the scans into 2D images and processes them using a powerful, off-the-shelf 2D vision model. It then compresses the information using random projections to create compact, meaningful representations of the original 3D data. This approach is fast, requires no labeled data or training, and outperforms existing methods on a wide range of medical tasks—all while using far less computation."
Poster,RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals,https://ICML.cc//virtual/2025/poster/43898,"David Reber, Sean Richardson, Todd Nief, Cristina Garbacea, Victor Veitch","Reward models are widely used as proxies for human preferences when aligning or evaluating LLMs.However, reward models are black boxes, and it is often unclear what, exactly, they are actually rewarding. In this paper we develop Rewrite-based Attribute Treatment Estimator (RATE) as an effective method for measuring the sensitivity of a reward model to high-level attributes of responses, such as sentiment, helpfulness, or complexity. Importantly, RATE measures the *causal* effect of an attribute on the reward. RATE uses LLMs to rewrite responses to produce imperfect counterfactuals examples that can be used to measure causal effects. A key challenge is that these rewrites are imperfect in a manner that can induce substantial bias in the estimated sensitivity of the reward model to the attribute. The core idea of RATE is to adjust for this imperfect-rewrite effect by rewriting *twice*. We establish the validity of the RATE procedure and show empirically that it is an effective estimator.","Reward models are used to align large language models (LLMs) with human preferences — for example, preferring responses that are more helpful, polite, or concise. But these models are often opaque: it's hard to tell what features of a response they are actually rewarding. Are they responding to helpfulness, or simply to shorter answers? We introduce RATE, which measures how much specific attributes — like sentiment or length — influence a reward model’s scores. RATE works by rewriting responses to isolate individual attributes and observing how the model's score changes. Because these rewrites are imperfect, RATE applies a correction technique based on rewriting twice, which helps reduce potential measurement errors. This way, when attempts to make an LLM helpful, polite, or concise don't work, we can narrow in on whether it's because the reward model was faulty or something else in the alignment process."
Poster,RBench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation,https://ICML.cc//virtual/2025/poster/46102,"Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, Shi-min Hu","Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (RBench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and cross-linguistic alignment, enabling the assessment to be an Olympiad-level multidisciplinary benchmark. We evaluate many models such as o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available athttps://evalmodels.github.io/rbench/",An olympiad-level multi-disciplinary benchmark for complex reasoning evaluation.
Poster,Reaction Graph: Towards Reaction-Level Modeling for Chemical Reactions with 3D Structures,https://ICML.cc//virtual/2025/poster/45081,"Yingzhao Jian, Yue Zhang, Ying Wei, Hehe Fan, Yi Yang","Accurately modeling chemical reactions using Artificial Intelligence (AI) can accelerate discovery and development, especially in fields like drug design and material science. Although AI has made remarkable advancements in single molecule recognition, such as predicting molecular properties, the study of interactions between molecules, particularly chemical reactions, has been relatively overlooked. In this paper, we introduce Reaction Graph (RG), a unified graph representation that encapsulates the 3D molecular structures within chemical reactions. RG integrates the molecular graphs of reactants and products into a cohesive framework, effectively capturing the interatomic relationships pertinent to the reaction process. Additionally, it incorporates the 3D structure information of molecules in a simple yet effective manner. We conduct experiments on a range of tasks, including chemical reaction classification, condition prediction,  and yield prediction. RG achieves the highest accuracy across six datasets, demonstrating its effectiveness. The code is available at https://github.com/Shadow-Dream/Reaction-Graph.","Chemical reactions happen when atoms rearrange themselves, turning molecules into something new, like making medicines or new materials. However, current artificial intelligence (AI) methods still are not great at guessing how reactions will turn out, including conditions, yields, or reaction types.We think this is because these AI methods can't easily see how atoms move around or clearly understand molecules' 3D shapes. To fix that, we made something called the Reaction Graph (RG). RG connects each atom before and after a reaction, clearly showing how they rearrange. It also highlights simple triangles to show the molecules' 3D shapes, which surprisingly helps a lot.When we tested RG, it consistently performed better than other methods at predicting reaction conditions, yields, and types. RG can potentially help chemists do better research and make better products."
Poster,RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning,https://ICML.cc//virtual/2025/poster/44615,"Yuanhuiyi Lyu, Xu Zheng, Lutao Jiang, Yibo Yan, Xin Zou, Huiyu Zhou, Linfeng Zhang, Xuming Hu","Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present **the first** real-object-based retrieval-augmented generation framework (**RealRAG**), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by **self-reflective contrastive learning**, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to **all types** of state-of-the-art text-to-image generative models and also delivers **remarkable** performance boosts with all of them, such as a **gain of *16.18\%* FID score** with the auto-regressive model on the Stanford Car benchmark.","Text-to-image models, like Stable Diffusion V3 and Flux, have made impressive strides in generating images from text. However, these models often struggle when asked to generate highly specific or unseen objects, leading to strange or distorted results. For instance, they may fail to accurately generate new or detailed objects, such as a Tesla Cybertruck, because they only know what they've been trained on.To address this issue, we developed a new framework called RealRAG. This framework enhances text-to-image generation by incorporating real-world images to fill in the gaps in the model's knowledge. We introduced a novel approach called self-reflective contrastive learning to ensure the model retrieves relevant real-world images, allowing it to generate more realistic and accurate images of unfamiliar objects.RealRAG can be applied to any state-of-the-art text-to-image model and improves their performance significantly. For example, it improved the realism of auto-regressive models by 16.18%, demonstrating its ability to generate high-quality images of fine-grained objects."
Poster,Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment,https://ICML.cc//virtual/2025/poster/44155,"Cheryl Li, Tianyuan Xu, Yiwen Guo","Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques.Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations."" This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence.To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions.By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit.A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution.Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability.","Large language models excel at generating text but often struggle with precise reasoning tasks, such as solving math or algorithm problems. Existing methods, like step-by-step explanations or generating code snippets, can still produce logical inconsistencies, for example, describing a correct step but implementing it incorrectly.This paper introduces RaLU, a new framework that improves reasoning accuracy by aligning natural language explanations with code logic. RaLU breaks down a problem-solving process into smaller, verifiable units, such as checking conditions or loops in code. It then engages the model in a self-checking dialogue to validate, refine, and explain each unit. This iterative process makes every step in the reasoning path more logically sound and in line with the problem requirements.Experiments demonstrate that RaLU significantly outperforms existing methods in mathematical and coding tasks. By combining the clarity of natural language with the rigor of code, RaLU makes AI reasoning more reliable and transparent, paving the way for trustworthy applications in education, programming, and decision-making."
Poster,Reasoning Limitations of Multimodal Large Language Models. A case study of Bongard Problems,https://ICML.cc//virtual/2025/poster/45515,"Mikołaj Małkiński, Szymon Pawlonka, Jacek Mańdziuk","Abstract visual reasoning (AVR) involves discovering shared concepts across images through analogy, akin to solving IQ test problems. Bongard Problems (BPs) remain a key challenge in AVR, requiring both visual reasoning and verbal description. We investigate whether multimodal large language models (MLLMs) can solve BPs by formulating a set of diverse MLLM-suited solution strategies and testing $4$ proprietary and $4$ open-access models on $3$ BP datasets featuring synthetic (classic BPs) and real-world (Bongard HOI and Bongard-OpenWorld) images. Despite some successes on real-world datasets, MLLMs struggle with synthetic BPs. To explore this gap, we introduce Bongard-RWR, a dataset representing synthetic BP concepts using real-world images. Our findings suggest that weak MLLM performance on classical BPs is not due to the domain specificity, but rather comes from their general AVR limitations. Code and dataset are available at: https://github.com/pavonism/bongard-rwr","Humans are good at recognizing abstract patterns in images, like those found in IQ tests. But can advanced AI models do the same? Our study investigates whether multimodal large language models that can process both images and text can solve Bongard Problems, a type of visual-textual reasoning challenge. We tested several advanced AI models on tasks involving synthetic and real-world images. While these models showed some success with real-world tasks, they struggled with synthetic puzzles, which are more abstract. To probe this issue, we created Bongard-RWR, a new dataset representing abstract concepts with real-world images. Our findings suggest that the AI models’ difficulties aren't just because of an unfamiliar synthetic image domain, but stem from fundamental limitations in how they understand visual concepts. This highlights the need to improve abstract reasoning capabilities of these models, a step toward more human-like reasoning."
Poster,Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation,https://ICML.cc//virtual/2025/poster/44014,"Zhuohao Yu, Weizheng Gu, Yidong Wang, Xingru Jiang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang","Large Language Models excel at code generation yet struggle with complex programming tasks that demand sophisticated reasoning. To bridge this gap, traditional process supervision relies on learned reward models requiring costly training data and suffering from reward misalignment, while outcome supervision fails for complex tasks needing coordinated intermediate steps.We introduce **O**utcome **R**efining **P**rocess **S**upervision, which unifies process and outcome supervision by leveraging executable verification: a tree-structured search framework generates strategic alternatives, profiles execution metrics, and scores candidates via self-critique mechanisms that integrate runtime feedback with reasoning.Experiments across 5 models and 3 benchmarks show consistent gains, with **26.9%** higher correctness and **42.2%** improved code efficiency. The results demonstrate that ORPS enables LLMs to overcome local optima in code generation, suggesting a promising direction for combining verifiable outcomes with structured reasoning to tackle complex challenges.","Large language models can write code, but they struggle with complex programming tasks that require careful step-by-step reasoning - like a student who can memorize formulas but struggles with multi-step word problems. Current methods either only check if the final code works (missing opportunities to improve) or require expensive training of separate AI systems to guide the reasoning process.We propose ORPS, which guides AI reasoning by actually running code at each step and using the results to explore different solution strategies - like having a programming tutor who tests your code as you write it. Instead of following one path, our system maintains multiple solution attempts simultaneously, learning from execution results to identify better algorithms.This approach improved code generation success by 27% while making solutions 42% more efficient, without requiring expensive training of guidance systems. Remarkably, smaller AI models using our method outperformed larger models, suggesting that good reasoning matters more than raw model size for complex programming tasks."
