type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data,https://ICML.cc//virtual/2025/poster/44700,"David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein","Foundation models are strong data compressors, but when accounting for their parameter size, their compression ratios are inferior to standard compression algorithms. Naively reducing the parameter count does not necessarily help as it deteriorates predictions and, accordingly, compression. We conduct a large-scale empirical study to find a sweet spot where pre-trained vanilla transformers can achieve competitive compression ratios. To this end, we train models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG-XL, FLAC) — even when accounting for parameter size. We achieve, e.g., the lowest compression ratio of 0.49 on  OOD audio data (vs. 0.54 for FLAC). We conduct extensive ablations and hyperparameter sweeps to study the impact of model- and dataset scale, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities,  but unlike large-scale foundation models, transfer to unseen modalities is generally weak.","It is a well-established fact that neural networks can be used to compress data, including text, images, and audio, much like how zip compresses files. Prior work has shown that current large language models are very effective at compression, even for data they have not encountered during training. However, they are too large to be practically useful as compressors, as one would also need to store the model parameters in the compressed output. As a result, we investigate whether much smaller neural networks of the same type can bridge this gap, i.e., whether they can be used as practical compressors and whether they also generalize to data from unseen domains. We trained small networks on large amounts of data from various sources (text, audio, images) and demonstrated that they can outperform popular compression tools such as gzip, JPEG, and FLAC. However, while these small networks can learn to compress multiple types of data, they are generally incapable of compressing previously unseen data, unlike large language models."
Poster,Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead,https://ICML.cc//virtual/2025/poster/46530,"Rickard Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem Choshen, Kristjan Greenewald, Mikhail Yurochkin, Justin Solomon","Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRAs. We propose a method for the joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 1000 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80\% of the throughput of serving a single LoRA.","Large language models can be fine-tuned for many tasks by attaching thousands of small adapters, but servers slow down as they frequently swap these adapters in and out of limited memory. By condensing the adapters’ overlapping information into a single shared core and preserving only necessary task-specific fragments, our method allows one server to efficiently deliver thousands of customized models at nearly the speed and cost of serving just one."
Poster,Compute Optimal Inference and Provable Amortisation Gap in Sparse Autoencoders,https://ICML.cc//virtual/2025/poster/46270,"Charles O&#x27;Neill, Alim Gumran, David Klindt","A recent line of work has shown promise in using sparse autoencoders (SAEs) to uncover interpretable features in neural network representations. However, the simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform accurate sparse inference. Using compressed sensing theory, we prove that an SAE encoder is inherently insufficient for accurate sparse inference, even in solvable cases. We then decouple encoding and decoding processes to empirically explore conditions where more sophisticated sparse inference methods outperform traditional SAE encoders. Our results reveal substantial performance gains with minimal compute increases in correct inference of sparse codes. We demonstrate this generalises to SAEs applied to large language models, where more expressive encoders achieve greater interpretability. This work opens new avenues for understanding neural network representations and analysing large language model activations.","Understanding how neural networks work internally is crucial as they're increasingly used in important decisions. Scientists use tools called sparse autoencoders (SAEs) to extract interpretable features from these complex models, but there's a fundamental problem: SAEs use overly simple methods that can't recover the best possible sparse representations.This paper proves mathematically that SAEs have an inherent limitation: they cannot achieve optimal sparse inference even when it's theoretically possible. The authors show that more sophisticated encoding methods, like multilayer perceptrons, significantly outperform traditional SAEs while using only slightly more computation.When tested on large language models like GPT-2, these better encoding methods actually produced more interpretable features than simpler approaches, contradicting the common belief that simpler methods are necessary for interpretability. This work opens new possibilities for better understanding how neural networks represent information internally."
Poster,Compute or Load KV Cache? Why Not Both?,https://ICML.cc//virtual/2025/poster/45020,"Shuowei Jin, Xueshen Liu, Qingzhao Zhang, Zhuoqing Morley Mao","Large Language Models (LLMs) are increasingly deployed in large-scale online services, enabling sophisticated applications. However, the computational overhead of generating key-value (KV) caches in the prefill stage presents a major bottleneck, particularly for long-context inputs. Prefix caching mitigates this issue by storing KV caches for reuse, reducing redundant computation. Despite its advantages, prefix caching suffers from high latency due to the limited I/O bandwidth of storage devices, constraining inference efficiency. To address this challenge, we introduce Cake, a novel KV cache loading system that optimally utilizes both computational and I/O resources in parallel. Cake employs a bidirectional scheduling strategy that dynamically balances KV cache computation and loading, ensuring efficient resource utilization. Additionally, Cake incorporates an adaptive scheduling mechanism that seamlessly integrates with non-prefix caching requests, improving system throughput and adapting to fluctuating resource availabilty. Through extensive evaluations across various hardware configurations, datasets, and storage conditions, Cake achieves on average 2.6× reduction in Time to First Token (TTFT) compared to compute-only and I/O-only methods. Our findings highlight Cake as an effective and practical solution for optimizing long-context LLM inference, bridging the gap between computation and I/O efficiency in large-scale AI deployments.","Large Language Models (LLMs) have become vital tools in various online applications, helping users with tasks like answering questions, summarizing documents, and chatting naturally. However, using these models with lengthy text inputs (like an entire book or a long conversation) can be slow, as they need to perform extensive calculations before providing their first response.Previous approaches typically relied solely on loading pre-calculated data from storage, but these methods alone have limitations and often fail to sufficiently reduce the latency experienced by users, especially when storage bandwidth is limited. In this work, we introduce a new approach called ""Cake,"" which smartly combines recomputing calculations and loading pre-stored data simultaneously. Cake efficiently manages resources by recomputing certain data while loading other parts from storage, significantly reducing the waiting time even under constrained storage bandwidth.By dynamically adapting to available computing power and storage bandwidth, Cake effectively handles real-world conditions, enhancing the responsiveness of LLMs. Our extensive experiments demonstrate that Cake consistently accelerates the initial response time, making large language models faster and more practical for everyday applications."
Poster,Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows,https://ICML.cc//virtual/2025/poster/45021,"Gabriele Visentin, Patrick Cheridito","We present a novel method for efficiently computing optimal transport maps and Wasserstein barycenters in high-dimensional spaces. Our approach uses conditional normalizing flows to approximate the input distributions as invertible pushforward transformations from a common latent space. This makes it possible to directly solve the primal problem using gradient-based minimization of the transport cost, unlike previous methods that rely on dual formulations and complex adversarial optimization. We show how this approach can be extended to compute Wasserstein barycenters by solving a conditional variance minimization problem. A key advantage of our conditional architecture is that it enables the computation of barycenters for hundreds of input distributions, which was computationally infeasible with previous methods. Our numerical experiments illustrate that our approach yields accurate results across various high-dimensional tasks and compares favorably with previous state-of-the-art methods.","Wasserstein barycenters provide a natural way of defining the average of a set of probability distributions and are widely used in applications such as shape interpolation, style translation, and fairness. However, computing them remains difficult, particularly in high-dimensional spaces. In this paper, we introduce a new method based on conditional normalizing flows that directly leverages the primal formulation of the problem, unlike prior approaches that rely on the dual formulation. Our approach delivers accurate results in high dimensions, compares favorably with existing state-of-the-art methods, and can efficiently compute barycenters for hundreds of input distributions, which was computationally infeasible with previous methods."
Poster,Computing Voting Rules with Improvement Feedback,https://ICML.cc//virtual/2025/poster/45074,"Evi Micha, Vasilis Varsamis","Aggregating preferences under incomplete or constrained feedback is a fundamental problem in social choice and related domains. While prior work has established strong impossibility results for pairwise comparisons, this paper extends the inquiry to improvement feedback, where voters express incremental adjustments rather than complete preferences. We provide a complete characterization of the positional scoring rules that can be computed given improvement feedback. Interestingly, while plurality is learnable under improvement feedback—unlike with pairwise feedback—strong impossibility results persist for many other positional scoring rules. Furthermore, we show that improvement feedback, unlike pairwise feedback, does not suffice for the computation of any Condorcet-consistent rule. We complement our theoretical findings with experimental results, providing further insights into the practical implications of improvement feedback for preference aggregation.","Figuring out group choices when people can’t give full opinions is a key challenge in voting and decision-making. Past research showed it’s often impossible to get good results when people only compare two options at a time. This study looks at a different kind of input, where people suggest small improvements instead of ranking everything. We identify which voting methods can still work with this kind of feedback. Interestingly, one popular method (plurality) works here, while it does not in the comparison setting. However, many other voting methods still face major limits. The study also finds that this feedback type doesn’t help with rules meant to reflect the true majority favorite. We add experiments to explore how these ideas might play out in practice."
Poster,COMRECGC: Global Graph Counterfactual Explainer through Common Recourse,https://ICML.cc//virtual/2025/poster/43939,"Gregoire Fournier, Sourav Medya","Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its blackbox nature. Explanations of the GNNs’ predictions can be categorized into two types—factual and counterfactual. Given a GNN trained on binary classification into “accept” and “reject” classes, a global counterfactual explanation consists in generating a small set of “accept” graphs relevant to all of the input “reject” graphs. The transformation of a “reject” graph into an “accept” graph is called a recourse. A common recourse explanation is a small set of recourse, from which every “reject” graph can be turned into an “accept” graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.","Understanding and Improving AI Decisions in NetworksArtificial intelligence (AI) tools called graph neural networks (GNNs) are used to make decisions in many areas, such as figuring out how people connect on social media, predicting how molecules behave in medicine, or helping recommend products to users. But even though these tools can be very accurate, they often work like a “black box”—we see what decision they made, but we don’t know why.To make these decisions more understandable, researchers are developing ways to explain them. One kind of explanation looks at what small changes would make the AI change its mind—for example, what changes would cause it to approve something it originally rejected. These are called ""counterfactual explanations.""In this paper, the authors go a step further. Instead of looking at each decision separately, they ask: Can we find a small number of helpful changes that work across many rejected cases to turn them into accepted ones? Imagine finding just a few tweaks that could improve lots of different things at once—that’s what they call ""common recourse.""The researchers create a method, called COMRECGC, to find these useful common changes. They test it on several real-world problems and show that it works better than other existing methods. This approach could be especially helpful in fields like drug development or biology, where figuring out small changes that make a big difference could save time, money, and lives."
Poster,Concentration Distribution Learning from Label Distributions,https://ICML.cc//virtual/2025/poster/44042,"Jiawei Tang, Yuheng Jia","Label distribution learning (LDL) is an effective method to predict the relative label description degree (a.k.a. label distribution) of a sample. However, the label distribution is not a complete representation of an instance because it overlooks the absolute intensity of each label. Specifically, it's impossible to obtain the total description degree of hidden labels that not in the label space, which leads to the loss of information and confusion in instances. To solve the above problem, we come up with a new concept named background concentration to serve as the absolute description degree term of the label distribution and introduce it into the LDL process, forming the improved paradigm of concentration distribution learning. Moreover, we propose a novel model by probabilistic methods and neural networks to learn label distributions and background concentrations from existing LDL datasets. Extensive experiments prove that the proposed approach is able to extract background concentrations from label distributions while producing more accurate prediction results than the state-of-the-art LDL methods. The code is available in https://github.com/seutjw/CDL-LD.","Every image has its background, and we always focus on the main components instead. In machine learning, every instances also has its background, and the label only describes its main parts. As a result, instances with the same proportion of main parts and different backgrounds will share an identical label, which is very unreasonable.To this end, we take the proportion of backgrounds in instances into the learning objective, and come up with a new paradigm, which learns labels and background proportions of instances simultaneously. We also design a corresponding learning algorithm, and prove its effectiveness in both theoretical and experimental aspects.For the convenience of subsequent researchers on this idea, we construct a real-world dataset for this paradigm. This further enhances the practical value of our research, making it a promising research direction."
Poster,ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features,https://ICML.cc//virtual/2025/poster/45272,"Alec Helbling, Tuna Han Salih Meral, Benjamin Hoover, Pinar Yanardag, Polo Chau","Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized *concept embeddings*, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention maps. ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 15 other zero-shot interpretability methods on the ImageNet-Segmentation dataset. ConceptAttention works for popular image models and even seamlessly generalizes to video generation. Our work contributes the first evidence that the representations of multi-modal DiTs are highly transferable to vision tasks like segmentation.","Recent AI models are capable of generating high quality images from text descriptions. However, it is difficult to understand the internals of these models. Our approach, called ConceptAttention, explains the inner workings of these models by creating a set of heat maps for simple text concepts like ""cat"" or ""sky"". These heat maps highlight the locations in the image where these concepts are present. Our approach gives insight into how a model ""sees"" the image that it is generating, improving the transparency of these models. Our model outperforms a variety of other existing methods at isolating the locations of textual concepts, and requires no additional training. Remarkably, despite the fact that we designed ConceptAttention to work for image generation models, we found that it works to video generation models too. Not only does our method improve the interpretability and transparency of these powerful machine learning models, it also can be applied to different applications like image editing and segmentation."
Poster,Concept-Based Unsupervised Domain Adaptation,https://ICML.cc//virtual/2025/poster/44848,"Xinyue Xu, Yueying Hu, Hui Tang, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li","Concept Bottleneck Models (CBMs) enhance interpretability by explaining predictions through human-understandable concepts but typically assume that training and test data share the same distribution. This assumption often fails under domain shifts, leading to degraded performance and poor generalization. To address these limitations and improve the robustness of CBMs, we propose the Concept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed to: (1) align concept representations across domains using adversarial training, (2) introduce a relaxation threshold to allow minor domain-specific differences in concept distributions, thereby preventing performance drop due to over-constraints of these distributions, (3) infer concepts directly in the target domain without requiring labeled concept data, enabling CBMs to adapt to diverse domains, and (4) integrate concept learning into conventional domain adaptation (DA) with theoretical guarantees, improving interpretability and establishing new benchmarks for DA. Experiments demonstrate that our approach significantly outperforms the state-of-the-art CBM and DA methods on real-world datasets.","Many deep learning models can make impressive predictions, but their decision-making process often remains a black box. Concept Bottleneck Models (CBMs) help by making their decisions from human-understandable concepts, like ""color"" or ""shape"". However, these models usually assume that the data they used during training is similar to what they encounter in the real world. When this isn’t true — for example, if a model trained on sunny-day images is tested on rainy-day ones — their performance can drop dramatically.To solve this, we developed a new method that helps CBMs stay reliable even when facing unfamiliar data. Our approach, called Concept-Based Unsupervised Domain Adaptation, makes the model to adjust for differences between training and testing situations, without needing extra labeled examples in the new domain. We also allow for small changes between the old and new domains, providing necessary flexibility when adapting a model to a new domain.Experiments show that our method significantly outperforms existing CBM and domain adaptation approaches on real-world tasks. By bridging the gap between interpretability and adaptability, our work enables AI systems to remain both understandable and effective in changing environments."
