type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Position: Theory of Mind Benchmarks are Broken for Large Language Models,https://ICML.cc//virtual/2025/poster/40168,"Matthew Riemer, Zahra Ashktorab, Djallel Bouneffouf, Payel Das, Miao Liu, Justin Weisz, Murray Campbell","Our paper argues that the majority of theory of mind benchmarks are broken because of their inability to directly test how large language models (LLMs) adapt to new partners. This problem stems from the fact that theory of mind benchmarks for LLMs are overwhelmingly inspired by the methods used to test theory of mind in humans and fall victim to a fallacy of attributing human-like qualities to AI agents. We expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs. Most theory of mind benchmarks only measure what we call *literal theory of mind*: the ability to predict the behavior of others. However, this type of metric is only informative when agents exhibit self-consistent reasoning. Thus, we introduce the concept of *functional theory of mind*: the ability to adapt to agents in-context following a rational response to their behavior. We find that many open source LLMs are capable of displaying strong literal theory of mind capabilities, but seem to struggle with functional theory of mind -- even with exceedingly simple partner policies. Simply put, strong literal theory of mind performance does not necessarily imply strong functional theory of mind performance or vice versa. Achieving functional theory of mind, particularly over long interaction horizons with a partner, is a significant challenge deserving a prominent role in any meaningful LLM theory of mind evaluation.","Our paper argues that most current benchmarks for understanding whether large language models (LLMs) display theory of mind are flawed. This is because they implicitly assume that LLMs think like humans do when this is not the case. For example, we often expect people to be consistent when answering different questions about the same situation. Today's AI models, however, don’t always reason in such consistent ways. Many of the existing benchmarks only check whether a model can guess what someone will do in a given situation. But this kind of test only works if the AI reasons in a self-consistent manner. We propose to instead evaluate LLMs by seeing whether they can adjust to the behavior of others over time, much like people do in real conversations. We call this more practical ability *functional theory of mind.* We find that while many AI models do well on existing benchmarks, they often fail to adapt their behavior to new partners, even in very simple settings. In short, being good at predicting the actions of others doesn't mean the LLMs truly understands or responds flexibly to them. Truly achieving this deeper kind of understanding is a major challenge — but one that's crucial if we want AI that can interact meaningfully with people."
Poster,Position: The Right to AI,https://ICML.cc//virtual/2025/poster/40155,"Rashid Mushkani, Hugo Berard, Allison Cohen, Shin Koseki","This position paper proposes a “Right to AI,” which asserts that individuals and communities should meaningfully participate in the development and governance of the AI systems that shape their lives. Motivated by the increasing deployment of AI in critical domains and inspired by Henri Lefebvre's concept of the “Right to the City,” we reconceptualize AI as a societal infrastructure, rather than merely a product of expert design. In this paper, we critically evaluate how generative agents, large-scale data extraction, and diverse cultural values bring new complexities to AI oversight. The paper proposes that grassroots participatory methodologies can mitigate biased outcomes and enhance social responsiveness. It asserts that data is socially produced and should be managed and owned collectively. Drawing on Sherry Arnstein’s Ladder of Citizen Participation and analyzing nine case studies, the paper develops a four-tier model for the Right to AI that situates the current paradigm and envisions an aspirational future. It proposes recommendations for inclusive data ownership, transparent design processes, and stakeholder-driven oversight. We also discuss market-led and state-centric alternatives and argue that participatory approaches offer a better balance between technical efficiency and democratic legitimacy.","Artificial intelligence (AI) systems increasingly influence decisions in everyday life. Yet most of the design and governance of these systems is handled by a small group of companies and institutions, with limited public involvement.This paper introduces the idea of a “Right to AI,” which suggests that people and communities should have opportunities to participate in decisions about how AI is developed, used, and regulated. Drawing from models in urban planning and participatory governance, we frame AI as a form of societal infrastructure—something that affects collective life and could be shaped through more inclusive processes.We propose a model that ranges from consumer-based approaches, where users are passive recipients of AI outcomes, to systems where citizens share real decision-making power. This model applies to both current practices and possible future directions.The paper examines case studies of participatory AI projects and outlines conditions that can support meaningful public involvement. While challenges remain, including questions of expertise, scale, and accountability, the goal is to identify practical steps that could help make AI governance more responsive to the communities it affects."
Poster,Position: Truly Self-Improving Agents Require Intrinsic Metacognitive Learning,https://ICML.cc//virtual/2025/poster/40177,"Tennison Liu, Mihaela van der Schaar","Self-improving agents aim to continuously acquire new capabilities with minimal supervision. However, current approaches face two key limitations: their self-improvement processes are often rigid, fail to generalize across tasks domains, and struggle to scale with increasing agent capabilities. We argue that effective self-improvement requires intrinsic metacognitive learning, defined as an agent’s $\textit{intrinsic}$ ability to actively evaluate, reflect on, and adapt its own learning processes. Drawing inspiration from human metacognition, we introduce a formal framework comprising three components: $\textit{metacognitive knowledge}$ (self-assessment of capabilities, tasks, and learning strategies), $\textit{metacognitive planning}$ (deciding what and how to learn), and $\textit{metacognitive evaluation}$ (reflecting on learning experiences to improve future learning). Analyzing existing self-improving agents, we find they rely predominantly on $\textit{extrinsic}$ metacognitive mechanisms, which are fixed, human-designed loops that limit scalability and adaptability. Examining each component, we contend that many ingredients for intrinsic metacognition are already present. Finally, we explore how to optimally distribute metacognitive responsibilities between humans and agents, and robustly evaluate and improve intrinsic metacognitive learning, key challenges that must be addressed to enable truly sustained, generalized, and aligned self-improvement.","**(1)** As AI agents become more autonomous, a major challenge is enabling them to self-improve without constant human oversight. Most current systems rely on fixed, externally designed self-improvement loops that do not adapt over time or across tasks, limiting their ability to adapt and scale in complex, changing environments.**(2)** We propose a framework for intrinsic metacognitive learning, where agents reflect on what they know, how they learn, and how well their learning strategies are working, then adapt those strategies accordingly. We analyze existing LLM-based agents and show how some already exhibit early signs of this capability, while identifying the components that remain underdeveloped.**(3)** This research matters because enabling agents to self-improve is key to long-term, general-purpose autonomy. Our work provides a roadmap for developing AI systems that can potentially exhibit sustained and robust self-improvement, but also safer and more aligned with human goals as they evolve."
Poster,Position: Trustworthy AI Agents Require the Integration of Large Language Models and Formal Methods,https://ICML.cc//virtual/2025/poster/40101,"Yedi Zhang, Yufan Cai, Xinyue Zuo, Xiaokun Luan, Kailong Wang, Zhe Hou, Yifan Zhang, Zhiyuan Wei, Meng Sun, Jun Sun, Jing Sun, Jin Song Dong","Large Language Models (LLMs) have emerged as a transformative AI paradigm, profoundly influencing broad aspects of daily life. Despite their remarkable performance, LLMs exhibit a fundamental limitation: hallucination—the tendency to produce misleading outputs that appear plausible. This inherent unreliability poses significant risks, particularly in high-stakes domains where trustworthiness is essential.On the other hand, Formal Methods (FMs), which share foundations with symbolic AI, provide mathematically rigorous techniques for modeling, specifying, reasoning, and verifying the correctness of systems. These methods have been widely employed in mission-critical domains such as aerospace, defense, and cybersecurity. However, the broader adoption of FMs remains constrained by significant challenges, including steep learning curves, limited scalability, and difficulties in adapting to the dynamic requirements of daily applications.To build trustworthy AI agents, we argue that the integration of LLMs and FMs is necessary to overcome the limitations of both paradigms. LLMs offer adaptability and human-like reasoning but lack formal guarantees of correctness and reliability. FMs provide rigor but need enhanced accessibility and automation to support broader adoption from LLMs.","Large language models (LLMs), like those behind today’s AI assistants, are powerful tools capable of writing, reasoning, and coding. Yet, they often produce confident but incorrect responses—a phenomenon known as ""hallucination""—making them unreliable in high-stakes domains such as healthcare and law.Formal methods (FMs), which provide mathematical guarantees about system behavior, have long been used in safety-critical fields like aviation. However, they are typically difficult to use, lack scalability, and struggle with dynamic real-world tasks.To build trustworthy AI agents, we argue that LLMs and FMs must be deeply integrated—not just in one direction, but both. First, we explore how LLMs can enhance FMs by improving their automation and scalability. Then, we investigate how FMs can make LLMs more accurate, consistent, and reliable. These two directions go hand-in-hand: each strengthens the other, and together they pave the way toward building verifiably trustworthy AI systems."
Poster,Position: Uncertainty Quantification Needs Reassessment for Large Language Model Agents,https://ICML.cc//virtual/2025/poster/40147,"Michael Kirchhof, Gjergji Kasneci, Enkelejda Kasneci","Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.","When Artificial Intelligence (AI) models generate text, it sometimes includes wrong information. This happens when the model is uncertain, either because it does not know a certain fact or because a question is not clear enough. Previously, there were many debates in the field about what types of uncertainties there are. We show that researchers have come to conflicting definitions of the types, and that the types of uncertainties do not function anymore in chatbots. Instead, we propose to focus more on why an AI model becomes uncertain, how it can resolve its questions by asking the user, and how it should speak about its uncertainties. We believe that research on these directions will lead to more trustworthy AI models in the future."
Poster,Position: We Can’t Understand AI Using our Existing Vocabulary,https://ICML.cc//virtual/2025/poster/40128,"John Hewitt, Robert Geirhos, Been Kim","This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we shouldstrive to develop neologisms: new words thatrepresent precise human concepts that we wantto teach machines, or machine concepts that weneed to learn. We start from the premise thathumans and machines have differing concepts.This means interpretability can be framed as acommunication problem: humans must be able toreference and control machine concepts, and communicate human concepts to machines. Creatinga shared human-machine language through developing neologisms, we believe, could solve thiscommunication problem. Successful neologismsachieve a useful amount of abstraction: not toodetailed, so they’re reusable in many contexts, andnot too high-level, so they convey precise information. As a proof of concept, we demonstrate howa “length neologism” enables controlling LLMresponse length, while a “diversity neologism” allows sampling more variable responses. Takentogether, we argue that we cannot understand AIusing our existing vocabulary, and expanding itthrough neologisms creates opportunities for bothcontrolling and understanding machines better.","Understanding AI systems is a critical problem as they become increasingly deployed in the world. We frame the problem of understanding and controlling AI systems as a communication problem, like how we try to communicate complex concepts between humans. In this opinion piece, we argue that we need to develop new words for ways in which AI systems see the world, and new words that teach AI systems how we see the world, in order to achieve this communication. Akin to how humans invent new words to discuss new or complex things, we need to do this with AI systems. We provide arguments for this position and early experiments showcasing how we might accomplish this in the future."
Poster,Position: We Need An Algorithmic Understanding of Generative AI,https://ICML.cc//virtual/2025/poster/40121,"Oliver Eberle, Thomas McGee, Hamza Giaffar, Taylor Webb, Ida Momennejad","What algorithms do LLMs actually learn and use to solve problems? Studies addressing this question are sparse, as research priorities are focused on improving performance through scale, leaving a theoretical and empirical gap in understanding emergent algorithms. This position paper proposes AlgEval: a framework for systematic research into the algorithms that LLMs learn and use. AlgEval aims to uncover algorithmic primitives, reflected in latent representations, attention, and inference-time compute, and their algorithmic composition to solve task-specific problems. We highlight potential methodological paths and a case study toward this goal, focusing on emergent search algorithms. Our case study illustrates both the formation of top-down hypotheses about candidate algorithms, and bottom-up tests of these hypotheses via circuit-level analysis of attention patterns and hidden states. The rigorous, systematic evaluation of how LLMs actually solve tasks provides an alternative to resource-intensive scaling, reorienting the field toward a principled understanding of underlying computations. Such algorithmic explanations offer a pathway to human-understandable interpretability, enabling comprehension of the model's internal reasoning performance measures. This can in turn lead to more sample-efficient methods for training and improving performance, as well as novel architectures for end-to-end and multi-agent systems.","Generative AI, specifically Large language models (LLMs), have demonstrated impressive performance, yet we currently don’t know how they solve problems. Existing research has focused on scaling performance and the interpretability of individual components, leaving a gap in  understanding the algorithms these models implicitly learn and apply.We propose AlgEval, a framework for evaluating how LLMs solve problems, and algorithmically quantifying how latent representations and attention transformations implement solutions layer by layer. AlgEval focuses on identifying algorithmic primitives and their composition by analyzing attention, hidden states, and inference-time compute. We demonstrate this through a case study or reasoning that requires planning and graph search, testing whether models implement classic algorithms like BFS or DFS using top-down hypotheses and bottom-up analysis.AlgEval proposes a research agenda for understanding AI systems and applying this understand to develop systems that are interpretable, sample-efficient, and grounded in theory. By uncovering algorithmic explanations, we move beyond black-box performance toward models that not only perform well but are also human-understandable and guide the design of more robust systems."
Poster,"Position: We Need Responsible, Application-Driven (RAD) AI Research",https://ICML.cc//virtual/2025/poster/40103,"Sarah Hartman, Cheng Soon Ong, Julia Powles, Petra Kuhnert","This position paper argues that achieving meaningful scientific and societal advances with artificial intelligence (AI) requires a responsible, application-driven approach (RAD) to AI research. As AI is increasingly integrated into society, AI researchers must engage with the specific contexts where AI is being applied. This includes being responsive to ethical and legal considerations, technical and societal constraints, and public discourse. We present the case for RAD-AI to drive research through a three-staged approach: (1) building transdisciplinary teams and people-centred studies; (2) addressing context-specific methods, ethical commitments, assumptions, and metrics; and (3) testing and sustaining efficacy through staged testbeds and a community of practice. We present a vision for the future of application-driven AI research to unlock new value through technically feasible methods that are adaptive to the contextual needs and values of the communities they ultimately serve.","AI is increasingly used across society in sectors such as healthcare, agriculture, and public broadcasting—but too often, it’s developed in isolation from the real-world contexts it is ultimately used in. This disconnect can lead to ineffective or even harmful outcomes. We propose an approach called Responsible, Application-Driven AI (RAD-AI). It combines deep technical expertise with context-specific ethical, legal, and societal considerations from the start. RAD-AI follows a three-step process: build diverse, people-centred studies; navigate complexity in context-specific methods, ethical commitments, assumptions, and metrics; and test and sustain efficacy in real-world environments.RAD-AI helps AI researchers break out of the “black box” by designing systems that are not only technically sound but also socially responsible. This approach builds a civically engaged research community, enhances public confidence in AI, and ensures AI solutions are aligned with the values and needs of the people they serve. It’s a roadmap for making AI research more adaptive and responsive to contextual needs and societal values, and it’s totally RAD!"
Poster,"Position: When Incentives Backfire, Data Stops Being Human",https://ICML.cc//virtual/2025/poster/40176,"Sebastin Santy, Prasanta Bhattacharya, Manoel Ribeiro, Kelsey Allen, Sewoong Oh","Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content -- it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations -- rather than relying solely on external incentives -- can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.","Discussions around data quality in machine learning often focus on technical indicators and definitions, overlooking the human sources that generate this data. Much of today’s data comes from user participation on online platforms. This led us to ask: can we learn something about sustaining data quality by examining how humans participate on these platforms?We examine the quantity-quality tradeoff in data generation through the lens of human motivation. Drawing from social science, we show how excessive reliance on external incentives can undermine intrinsic motivation. We propose a shift: design engaging, suitably-incentivized environments (e.g., online games) that encourage meaningful participation while producing high-quality data.Our paper highlights the motivational forces behind online data generation for AI/ML and illustrates cases of past systems that have successfully navigated the quantity-quality tradeoff to generate meaningful human data. We also emphasize key design considerations for building trustworthy data collection environments of the future that will not only generate high quality data, but also respect and support the people contributing it."
Poster,Position: You Can't Manufacture a NeRF,https://ICML.cc//virtual/2025/poster/40110,"Marta An Kimmel, Mueed Rehman, Yonatan Bisk, Gary Fedder","In this paper, we examine the manufacturability gap in state-of-the-art generative models for 3D object representations.  Many models for generating 3D assets focus on rendering virtual content and do not consider the constraints of real-world manufacturing, such as milling, casting, or injection molding. We demonstrate that existing generative models for computer-aided design representation do not generalize outside of their training datasets or to unmodified real, human-created objects. We identify limitations with the current approaches, including missing manufacturing-readable semantics, the inability to decompose complex shapes into parameterized segments appropriate for computer-aided manufacturing, and a lack of appropriate scoring metrics to assess the generated output versus the true reconstruction. The academic community could greatly impact real-world manufacturing by rallying around pathways to solve these challenges. We offer revised, more realistic datasets and baseline benchmarks as a step in targeting the challenge. In evaluating these datasets, we find that existing models are severely overfit to simpler data.","Most reconstructive and generative AI for 3D objects targets creating objects that are only fit for being rendered on the screen, they do not create files in a format that can be created in the real world via common, modern manufacturing techniques. The ones that do create files in that format (called boundary representation) can only create very simple objects. We demonstrate the limits of existing models in this paper."
