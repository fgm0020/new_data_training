type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization,https://ICML.cc//virtual/2025/poster/46254,"Phillip Guo, Aaquib Syed, Abhay Sheshadri, Aidan Ewart, Gintare Karolina Dziugaite","Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability---which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability---can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the *lookup-table mechanism* for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models.We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.","We address the problem of editing or removing specific factual knowledge from large language models without compromising overall performance. Much of the existing literature simply leverages stronger adversarial training, which is weak against basic attacks to recover the original knowledge. We wanted to approach editing differently: by understanding mechanistically where undesired knowledge exists, so that we could fully remove knowledge from the weights. We tackle this by first using mechanistic interpretability to identify precise components in the model responsible for connecting subjects in the prompt to their corresponding unwanted factual representations. Then we solely fine-tune these components to represent alternative factual representations, and find that this yields an edited model far more resistant to attempts at recovering the original facts.Having a robust factual editing method provides a way to surgically remove harmful knowledge and sensitive or private information. This enhances control over AI behavior, enabling safer language models."
Poster,MedRAX: Medical Reasoning Agent for Chest X-ray,https://ICML.cc//virtual/2025/poster/45678,"Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, BO WANG","Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art  CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX","Chest X-ray interpretation is a critical but labor-intensive task in medicine. Existing artificial intelligence (AI) tools often function as standalone applications, which restricts their integration into comprehensive clinical workflows. Moreover, current general-purpose AI models, despite their advancements, may not consistently provide the multi-step analytical capabilities or the transparent decision-making processes required in medical diagnostics.We have developed MedRAX, an AI framework designed to overcome these limitations in chest X-ray analysis. MedRAX operates by coordinating a suite of specialized AI tools, each proficient in specific tasks such as disease detection, identifying and outlining anatomical structures, or answering detailed image-based questions. The system dynamically selects and sequences these tools, integrating their outputs to address complex medical queries without requiring retraining of the core framework when tools are added or modified.This approach enables MedRAX to offer more accurate, detailed, and interpretable analyses of chest X-rays compared to existing methods, representing a significant advancement towards the practical application of AI in radiology. The system aims to improve diagnostic efficiency, reduce potential for error, and increase the clarity of AI-driven insights, thereby supporting medical professionals and potentially enhancing patient care through more robust AI assistance."
Poster,MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding,https://ICML.cc//virtual/2025/poster/45718,"Yuxin Zuo, Shang Qu, Yifei Li, Zhang-Ren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou","We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 18 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.","We began our research to address a critical gap in evaluating AI systems designed for medical applications. Existing medical AI benchmarks were either too simple or failed to reflect the complexity of real-world clinical scenarios. Most notably, they lacked questions from a wide range of specialties and did not sufficiently test expert-level reasoning.To address this, we created MedXpertQA, a new benchmark composed of 4,460 challenging medical questions, including both text-based (Text subset) and multimodal (MM subset) questions. These questions were sourced from professional medical exams and clinical cases across 17 specialties and 11 body systems. We carefully filtered and augmented the dataset using both AI and human experts, and ensured that the questions demand complex medical reasoning. We also evaluated 18 state-of-the-art models to understand their performance on these tasks.Our work matters because it provides a challenge and realistic benchmark for developing and testing medical AI systems. MedXpertQA pushes current models beyond simple pattern recognition, encouraging development toward systems capable of trustworthy and expert-level clinical decision support."
Poster,MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents,https://ICML.cc//virtual/2025/poster/44447,"Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, William Wang","Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent’s next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent’s trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs. Code is available at https://github.com/kaijiezhu11/MELON.","AI assistants that can use tools like checking emails or browsing websites are becoming common, but they have a dangerous vulnerability: malicious instructions hidden in external content can trick them into performing unauthorized actions, like transferring money to scammers instead of completing your request.Existing defenses have major flaws—they either require expensive AI retraining, block too many legitimate actions (reducing usefulness), or fail against sophisticated attacks while maintaining functionality.We developed MELON, a defense that runs two parallel processes: one handles your original request normally, while the other removes your request but keeps retrieved information. If both try to perform the same actions, it indicates the AI is following hidden malicious instructions rather than your request.Testing shows MELON prevents over 99% of attacks while maintaining the AI's ability to complete legitimate tasks—a major improvement over existing defenses that force users to choose between security and functionality. MELON makes AI assistants safer for everyday use without significantly reducing their helpfulness."
Poster,MemFreezing: A Novel Adversarial Attack on Temporal Graph Neural Networks under Limited Future Knowledge,https://ICML.cc//virtual/2025/poster/43931,"Yue Dai, Liang Liu, Xulong Tang, Youtao Zhang, Jun Yang","Temporal graph neural networks (TGNN) have achieved significant momentum in many real-world dynamic graph tasks.While most existing TGNN attack methods assume worst-case scenarios where attackers have complete knowledge of the input graph, the assumption may not always hold in real-world situations, where attackers can, at best, access information about existing nodes and edges but not future ones after the attack.However, studying adversarial attacks under these constraints is crucial, as limited future knowledge can reveal TGNN vulnerabilities overlooked in idealized settings.Nevertheless, designing effective attacks in such scenarios is challenging: the evolving graph can weaken their impact and make it hard to affect unseen nodes. To address these challenges, we introduce MemFreezing, a novel adversarial attack framework that delivers long-lasting and spreading disruptions in TGNNs without requiring post-attack knowledge of the graph. MemFreezing strategically injects fake nodes or edges to push node memories into a stable “frozen state,” reducing their responsiveness to subsequent graph changes and limiting their ability to convey meaningful information. As the graph evolves, these affected nodes maintain and propagate their frozen state through their neighbors. Experimental results show that MemFreezing persistently degrades TGNN performance across various tasks, offering a more enduring adversarial strategy under limited future knowledge.","AI models like TGNNs are highly accurate for tasks involving dynamic graphs (e.g., social networks, traffic prediction), but their reliability can be compromised by adversarial attacks—malicious manipulations of input data. Unlike traditional attacks, targeting dynamic graphs is harder because attackers have limited knowledge on future changes.We discovered a new threat: attackers can mislead TGNNs by preventing them from detecting changes in dynamic graphs. By injecting small amounts of fake data, we show how TGNNs can be tricked into ignoring real updates, leading to incorrect predictions.Our work highlights a critical vulnerability in TGNNs, emphasizing the need for defenses that ensure these models remain sensitive to real-time changes in dynamic graphs. This is key for maintaining trustworthy AI in real-world applications."
Poster,Memorization Sinks: Isolating Memorization during LLM Training,https://ICML.cc//virtual/2025/poster/43838,"Gaurav Ghosal, Pratyush Maini, Aditi Raghunathan","Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of *natural* sequences (those that resemble linguistically plausible text) become *mechanistically entangled* with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of MemSinks that promotes isolation of memorization by design. We leverage a sequence identifier to activate a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that MemSinks facilitates clean isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement MemSinks at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at http://github.com/grghosal/MemSinks.","In this paper, we examine how to train large language models (LLMs) so that it is easier to remove memorized information from them. While existing research has studied many post-hoc approaches for removing memorization, these methods often harm the model's general capabilities. We demonstrate this is particularly the case when the memorized sequences are similar to the rest of the training data. Our analysis suggests that in these cases, memorization is often very entangled in the model with its general abilities. Next, we study approaches to specifically train models to seperate memorization from their general abilities. To overcome this challenge, we introduce Memorization Sinks (MemSinks) which sets aside a specific part of the model to store memorized information. We demonstrate that Memorization Sinks both (a) achieves good general capability performance and (b) makes it significantly easier to remove memorization after training."
Poster,Memory Layers at Scale,https://ICML.cc//virtual/2025/poster/46172,"Vincent-Pierre Berges, Barlas Oğuz, Daniel HAZIZA, Scott Yih, Luke Zettlemoyer, Gargi Ghosh","Memory layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs.  Conceptually, sparsely activated memory layers complement compute-heavy dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply.  This work takes memory layers beyond proof-of-concept, proving their utility at contemporary scale.  On downstream tasks, language models augmented with our improved memory layer outperform dense models with more than twice the computation budget, as well as mixture-of-expert models when matched for both compute and parameters.  We find gains are especially pronounced for factual tasks.  We provide a fully parallelizable memory layer implementation, demonstrating scaling laws with up to 128B memory parameters, pretrained to 1 trillion tokens, comparing to base models with up to 8B parameters.","Modern language models are getting larger and more expensive to run, as they rely heavily on complex computations to generate accurate responses. We asked whether there’s a smarter way to improve performance—without making models significantly slower or more costly.We introduced a new type of model component called a memory layer, which helps the model “remember” useful information using a lookup system, similar to how a person might check notes instead of trying to recall everything from memory. These layers don’t require much extra computation and can be natively integrated with existing parts of the model.We tested memory layers at a much larger scale than before and found they worked especially well on fact-based tasks, like recalling knowledge or answering trivia. In many cases, models with memory layers outperformed others that used over twice the computational power.To help the community, we’ve released an efficient version of this memory system and show how it can scale to billions of memory entries—paving the way for faster, smarter AI systems."
Poster,MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning,https://ICML.cc//virtual/2025/poster/43793,"Suning Huang, Zheyu Zhang, Tianhai Liang, Yihan Xu, Zhehao Kou, Chenhao Lu, Guowei Xu, Zhengrong Xue, Huazhe Xu","Visual deep reinforcement learning (RL) enables robots to acquire skills from visual input for unstructured tasks. However, current algorithms suffer from low sample efficiency, limiting their practical applicability. In this work, we present MENTOR, a method that improves both the *architecture* and *optimization* of RL agents. Specifically, MENTOR replaces the standard multi-layer perceptron (MLP) with a mixture-of-experts (MoE) backbone and introduces a task-oriented perturbation mechanism. MENTOR outperforms state-of-the-art methods across three simulation benchmarks and achieves an average of 83\% success rate on three challenging real-world robotic manipulation tasks, significantly surpassing the 32% success rate of the strongest existing model-free visual RL algorithm. These results underscore the importance of sample efficiency in advancing visual RL for real-world robotics. Experimental videos are available at https://suninghuang19.github.io/mentor_page/.","(1) Teaching robots skills from raw camera images is painfully slow and data-hungry. (2) We introduce MENTOR, an AI mentor that assigns specialist mini-brains to each situation and nudges them using lessons from past successes. (3) Robots can learn complex manipulation in real world, taking a step toward  adaptable helpers in workplaces and everyday homes."
Poster,MERGE$^3$: Efficient Evolutionary Merging on Consumer-grade GPUs,https://ICML.cc//virtual/2025/poster/43950,"Tommaso Mencattini, Adrian Robert Minut, Donato Crisostomi, Andrea Santilli, Emanuele Rodola","Evolutionary model merging enables the creation of high-performing multi-task models but remains computationally prohibitive for consumer hardware. We introduce MERGE$^3$, an efficient framework that makes evolutionary merging of Large Language Models (LLMs) feasible on a single GPU by reducing fitness computation costs 50× while retaining a large fraction of the original performance. MERGE$^3$ achieves this by **E**xtracting a reduced dataset for evaluation, **E**stimating model abilities using Item Response Theory (IRT), and **E**volving optimal merges via IRT-based performance estimators. Our method enables state-of-the-art multilingual and cross-lingual merging, transferring knowledge across languages with significantly lower computational overhead. We provide theoretical guarantees and an open-source library, democratizing high-quality model merging.","Large Language Models are expensive to train, but there’s a clever shortcut: merging open-source models that are already trained. This technique, called *model merging*, allows developers to combine the strengths of existing models into new ones, without starting from scratch. It’s grown fast in popularity because it works well and runs on everyday hardware. In fact, about 30% of the models on Hugging Face’s Open LLM leaderboard are created this way.However, most merging methods rely on manual tweaking and guesswork, which often limits their effectiveness. In theory, the best approach is *evolutionary merging*, which automatically explores different ways to combine models by simulating a kind of natural selection. Unfortunately, this method is rarely used in practice, as it’s so *computationally demanding* that it’s not feasible on typical hardware, and as a result, evolutionary merges are *virtually absent from public model hubs* like Hugging Face.**MERGE$^3$** changes that. It makes evolutionary merging practical on a single consumer GPU by cutting the compute cost 50-fold. It does this by evaluating only a small, smart sample of data and using a technique from educational testing, called *Item Response Theory,* to estimate model performance. Then it evolves better merges over time, efficiently and effectively.Despite using just *2%* of the data for the fitness computation of the evolutionary algorithm, MERGE$^3$ produces models that almost match the quality of much more expensive methods. It can transfer skills, like mathematical reasoning, across languages, and create multilingual models that outperform their individual parts. Lowering the hardware and time access bar to evolutionary merging, MERGE$^3$ brings state-of-the-art model merging to everyone."
Poster,Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation,https://ICML.cc//virtual/2025/poster/44814,"Juncheol Shin, Minsang Seok, Seonggon Kim, Eunhyeok Park","Model merging has emerged as a powerful technique for combining task-specific weights, achieving superior performance in multi-target domain adaptation. However, when applied to practical scenarios, such as quantized models, new challenges arise. In practical scenarios, quantization is often applied to target-specific data, but this process restricts the domain of interest and introduces discretization effects, making model merging highly non-trivial. In this study, we analyze the impact of quantization on model merging through the lens of error barriers. Leveraging these insights, we propose a novel post-training quantization, HDRQ - Hessian and distant regularizing quantization - that is designed to consider model merging for multi-target domain adaptation. Our approach ensures that the quantization process incurs minimal deviation from the source pre-trained model while flattening the loss surface to facilitate smooth model merging. To our knowledge, this is the first study on this challenge, and extensive experiments confirm its effectiveness.","Merging models trained on different datasets can produce a single model that performs well across all data. However, in real-world scenarios, models are often quantized to low precision to meet resource constraints-especially on edge devices like mobile phones. Unfortunately, merging these quantized models typically results in a loss of performance.We explored whether improved quantization techniques could help preserve model quality when models are merged. Using a metric called the error barrier, we analyzed the behavior of quantized models during merging and found that encouraging the weights of each quantized model to remain close and converge to flatter regions of the loss landscape can lead to better merged performance. Based on these insights, we developed HDRQ (Hessian and Distance Regularization Quantization), a novel quantization algorithm that guides models toward flatter minima and keeps their weights close together.HDRQ enables effective merging of low-precision models without significant degradation in quality. This makes it possible to combine models directly on small devices without server involvement. As a result, users can generate versatile models that work across mutiple datasets in demand, all while staying within tight resource budgets"
