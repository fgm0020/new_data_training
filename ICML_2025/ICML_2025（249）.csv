type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Regret-Free Reinforcement Learning for Temporal Logic Specifications,https://ICML.cc//virtual/2025/poster/45203,"R Majumdar, Mahmoud Salamati, Sadegh Soudjani","Learning to control an unknown dynamical system with respect to high-level temporal specifications is an important problem in control theory. We present the first regret-free online algorithm for learning a controller for linear temporal logic (LTL) specifications for systems with unknown dynamics.We assume that the underlying (unknown) dynamics is modeled by a finite-state and action Markov decision process (MDPs).Our core technical result is a regret-free learning algorithm for infinite-horizon reach-avoid problems on MDPs.For general LTL specifications, we show that the synthesis problem can be reduced to a reach-avoid problem once the graph structure is known.Additionally, we provide an algorithm for learning the graph structure, assuming knowledge of a minimum transition probability, which operates independently of the main regret-free algorithm. Our LTL controller synthesis algorithm provides sharp bounds on how close we are to achieving optimal behavior after a finite number of learning episodes.In contrast, previous algorithms for LTL synthesis only provide asymptotic guarantees, which give no insight into the transient performance during the learning phase.","Many real-world systems, such as robots and autonomous vehicles, are expected to perform complex temporal tasks--such as reaching a goal while avoiding obstacles. These tasks can be described using temporal logic specifications. The standard approach is to use Reinforcement Learning (RL) such that a control policy is learned from the interactions of the agent with the environment, but existing RL methods only guarantee success after infinite time and do not give information on how close the agent is to the optimal behavior at a particular point of time. This paper provides the guarantee that the RL agent learns to perform the temporal tasks with **regret** as a clear measure of progress, which is the gap between the actual and optimal success over time. We assume the underlying dynamics are modeled as a Markov decision process with a finite set of states and actions, and that only a lower bound over the minimum transition probability is known.In short, we propose the first regret-free algorithm for policy synthesis with infinite-horizon temporal tasks. Our algorithm enables computing an upper bound on the number of learning episodes required for the learned policy to get close to the optimal one."
Poster,Regularized Langevin Dynamics for Combinatorial Optimization,https://ICML.cc//virtual/2025/poster/44737,"Shengyu Feng, Yiming Yang","This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative paradigm. However, we observe that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA), and the other one based on neural network (NN). Empirical results on three classic CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA algorithm reduces the runtime of the previous SOTA SA method by up to 80\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. Our code is available at https://github.com/Shengyu-Feng/RLD4CO.","Existing sampling-based methods typically suffer from the local-optima issue in combinatorial optimization. We propose to enforce the update magnitude between the sampled and current solutions, forcing the solutions to escape the local minima. This simple technique significantly boosts the performance of both simulated annealing-based and neural network-based solvers for combinatorial optimization, achieving state-of-the-art results on maximum independent set, maximum clique and maximum cut problems."
Poster,Reidentify: Context-Aware Identity Generation for Contextual Multi-Agent Reinforcement Learning,https://ICML.cc//virtual/2025/poster/43673,"Zhiwei XU, Kun Hu, Xin Xin, Weiliang Meng, Yiwei Shi, Hangyu Mao, Bin Zhang, dapeng Li, Jiangjin Yin","Generalizing multi-agent reinforcement learning (MARL) to accommodate variations in problem configurations remains a critical challenge in real-world applications, where even subtle differences in task setups can cause pre-trained policies to fail. To address this, we propose Context-Aware Identity Generation (CAID), a novel framework to enhance MARL performance under the Contextual MARL (CMARL) setting. CAID dynamically generates unique agent identities through the agent identity decoder built on a causal Transformer architecture. These identities provide contextualized representations that align corresponding agents across similar problem variants, facilitating policy reuse and improving sample efficiency. Furthermore, the action regulator in CAID incorporates these agent identities into the action-value space, enabling seamless adaptation to varying contexts. Extensive experiments on CMARL benchmarks demonstrate that CAID significantly outperforms existing approaches by enhancing both sample efficiency and generalization across diverse context variants.","In many real-world scenarios, teams of AI agents need to work together in changing environments—for example, robots cooperating in different room layouts or traffic lights adjusting to different intersections. However, most current AI training methods for teams of agents struggle when even small details in the environment change. Our work introduces a new approach called Context-Aware Identity Generation (CAID) that helps AI agents better adapt to such changes. CAID gives each agent a unique identity based on the situation they are in, allowing them to understand their roles more clearly and act more effectively as a team. We tested this method in several challenging simulation environments, and found that CAID helps agents learn faster and perform more reliably across a wide range of situations."
Poster,RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation,https://ICML.cc//virtual/2025/poster/45344,"Xinnuo Xu, Rachel Lawrence, Kshitij Dubey, Atharva Pandey, Risa Ueno, Fabian Falck, Aditya Nori, Rahul Sharma, Amit Sharma, Javier Gonzalez","Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true “reasoning” or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE: a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.","Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true “reasoning” or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE: a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy."
Poster,ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45523,"Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang","Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks.","Robots often learn by copying human actions, but the quality of these examples can vary a lot, which hurts performance. We introduce ReinboT, a new model that helps robots make better decisions by using ideas from reinforcement learning, where the goal is to choose actions that lead to the best long-term results. ReinboT learns to predict how useful each step is, helping it focus on good data and ignore bad ones. This makes the robot’s actions more reliable and smarter over time. In tests, ReinboT outperformed other methods on a challenging dataset with mixed-quality data and showed strong results even in new, unseen tasks. Our work helps robots become better at learning complex tasks, even from imperfect data."
Poster,"REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",https://ICML.cc//virtual/2025/poster/45336,"Simon Geisler, Tom Wollschläger, M. Hesham Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, Stephan Günnemann","To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model frequently does not complete the response in a harmful manner. Moreover, the affirmative objective is usually not adapted to model-specific preferences and essentially ignores the fact that LLMs output a distribution over responses. If low attack success under such an objective is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization problem over the population of responses. We derive a generally applicable objective via the REINFORCE policy-gradient formalism and demonstrate its efficacy with the state-of-the-art jailbreak algorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD). For example, our objective doubles the attack success rate (ASR) on Llama3 and increases the ASR from 2\% to 50\% with circuit breaker defense.","Current methods to trick Large Language Models (LLMs) into saying harmful things often don't work well. They try to make the LLM start with a specific ""bad"" phrase, but the LLM frequently stops or changes its tune before finishing a genuinely harmful response. This makes us wrongly believe LLMs are safer than they truly are because these attack methods are too simplistic and don't adapt. Thus, we've developed a smarter attack strategy. Instead of just focusing on a fixed starting phrase, our method (using a technique called REINFORCE) looks at the meaning of the LLM's entire range of possible answers and adapts the attack specifically to the model it's targeting. Our new approach significantly boosts the success of these ""jailbreak"" attacks. For instance, it doubled the attack success rate on Llama3 and made attacks on defended models jump from 2% to 50% effective. By showing how LLMs can be more effectively compromised, our research provides a truer picture of their vulnerabilities, which is crucial for building genuinely safer AI systems."
Poster,Reinforced Learning Explicit Circuit Representations for Quantum State Characterization from Local Measurements,https://ICML.cc//virtual/2025/poster/45430,"Manwen Liao, Yan Zhu, Weitian Zhang, Yuxiang Yang","Characterizing quantum states is essential for advancing many quantum technologies. Recently, deep neural networks have been applied to learn quantum states by generating compressed implicit representations. Despite their success in predicting properties of the states, these representations remain a black box, lacking insights into strategies for experimental reconstruction. In this work, we aim to open this black box by developing explicit representations through generating surrogate state preparation circuits for property estimation. We design a reinforcement learning agent equipped with a Transformer-based architecture and a local fidelity reward function. Relying solely on measurement data from a few neighboring qubits, our agent accurately recovers properties of target states. We also theoretically analyze the global fidelity the agent can achieve when it learns a good local approximation. Extensive experiments demonstrate the effectiveness of our framework in learning various states of up to 100 qubits, including those generated by shallow Instantaneous Quantum Polynomial circuits, evolved by Ising Hamiltonians, and many-body ground states. Furthermore, the learned circuit representations can be applied to Hamiltonian learning as a downstream task utilizing a simple linear model.","Characterizing and understanding quantum states is crucial for the development of quantum technologies, but doing so becomes increasingly difficult as quantum systems grow in size. Recent machine learning approaches can predict properties of quantum states, but they work like black boxes and do not provide a clear way to reconstruct the state in actual quantum experiments.We propose a new framework that makes the representation of quantum states more transparent and usable. Our framework learns an explicit circuit representation, a step-by-step recipe that can be used to physically recreate the state on a quantum computer. We achieve this by training a reinforcement learning agent that uses only experimentally available measurement settings. Our framework is validated through extensive experiments on quantum systems with up to 100 qubits.Our framework makes quantum state representations both interpretable and experimentally implementable. It enables large-scale quantum state characterization and provides a way for downstream applications such as learning the underlying physical model of a system. Our framework bridges the gap between machine-learned representations and real-world quantum hardware implementation."
Poster,Reinforced Lifelong Editing for Language Models,https://ICML.cc//virtual/2025/poster/46622,"Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, Junfeng Fang, Xiang Wang","Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and prevalent approaches leverage hypernetworks to generate these parameter updates. However, they face significant challenges in lifelong editing due to their incompatibility with LLM parameters that dynamically change during the editing process. To address this, we observed that hypernetwork-based lifelong editing aligns with reinforcement learning modeling and proposed **RLEdit**, an RL-based editing method. By treating editing losses as rewards and optimizing hypernetwork parameters at the full knowledge sequence level, we enable it to precisely capture LLM changes and generate appropriate parameter updates. Our extensive empirical evaluation across several LLMs demonstrates that RLEdit outperforms existing methods in lifelong editing with superior effectiveness and efficiency, achieving a **59.24%** improvement while requiring only **2.11%** of the time compared to most approaches.","Large Language Models (LLMs) learn vast amounts of information. However, this knowledge can become outdated or incorrect over time, and constantly retraining them from scratch is expensive and time-consuming. To address this, researchers use ""model editing"" to directly tweak LLMs' internal settings. Many current editing tools, however, struggle when making continuous updates because the LLM's internal state changes with each edit. Our research introduces a new method called **RLEdit**. We noticed that this challenge of ongoing editing is similar to how LLM can learn from rewards. RLEdit uses this reinforcement learning concept to better track the LLM's changes and make more precise updates. When tested, RLEdit was significantly more effective and much faster at these lifelong edits compared to existing methods, showing a major improvement in updating accuracy while requiring only a small fraction of the time. This helps keep LLMs more accurate and up-to-date efficiently."
Poster,Reinforce LLM Reasoning through Multi-Agent Reflection,https://ICML.cc//virtual/2025/poster/46364,"Yurun Yuan, Tengyang Xie","Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (**D**irect **P**olicy **S**earch by **D**ynamic **P**rogramming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.","Large language models (LLMs) like ChatGPT can solve complex tasks but often struggle to fix their own mistakes. This paper introduces DPSDP, a method that helps LLMs reflect, take feedback, and refine their answers—much like how students learn from reviewing errors.Instead of one model doing everything, DPSDP trains two specialized models: an actor that proposes answers and a critic that gives feedback. They go back and forth over several rounds, improving the response each time. The final answer is chosen by majority vote.By using reinforcement learning to train this collaboration, the method significantly boosts accuracy on math problems—including Olympiad-level ones—and generalizes well to new tasks. This shows how structured reflection and teamwork can make AI reason more like humans."
Poster,Reinforcement Learning Control of a Physical Robot Device for Assisted Human Walking without a Simulator,https://ICML.cc//virtual/2025/poster/43549,"junmin zhong, Emiliano Quinones Yumbla, Seyed Yousef Soltanian, Ruofan Wu, Wenlong Zhang, Jennie Si","This study presents an innovative reinforcement learning (RL) control approach to facilitate soft exosuit-assisted human walking. Our goal is to address the ongoing challenges in developing reliable RL-based methods for controlling physical devices. To overcome key obstacles—such as limited data, the absence of a simulator for human-robot interaction during walking, the need for low computational overhead in real-time deployment, and the demand for rapid adaptation to achieve personalized control while ensuring human safety—we propose an online Adaptation from an offline Imitating Expert Policy (AIP) approach. Our offline learning mimics human expert actions through real human walking demonstrations without robot assistance. The resulted policy is then used to initialize online actor-critic learning, the goal of which is to optimally personalize robot assistance. In addition to being fast and robust, our online RL method also posses important properties such as learning convergence, dynamic stability, and solution optimality. We have successfully demonstrated our simpleand robust framework for safe robot control on all five tested human participants, without selectively presenting results. The qualitative performance guarantees provided by our online RL, along with the consistent experimental validation of AIP control, represent the first demonstration of online adaptation for softsuit control personalization and serve as important evidence for the use of online RL in controlling a physical device to solve a real-life problem.","Soft robotic exosuits have been developed aiming to assist human walking with reduced effort. But some real-world challenges need to be addressed for their deployment. Unlike rigid exoskeletons, soft exosuits are made of flexible materials that are comfortable to wear but difficult to model, simulate and control. Additionally, it requires personalization for individual users in order to fully utilize the device to its advantage. For control purposes, a prominent issue is the time delay in soft actuators, which introduces additional challenge to reliable and real-time control of the device customized to individuals.This study addresses the above challenges by introducing a novel method Adaptation from an Imitating Policy (AIP), which learns to control the exosuit directly on a human user. Instead of relying on extensive simulations or a massive dataset, both of which are not available to this application, AIP creates an offline policy first and then adapt it in real time to a new user. AIP is designed to handle environmental noise, actuator delays, and inherent inter- and intra- person variability during movement.We tested our method with 5 human users. Results show reduced walking effort while the device adapting safely to different individuals. Our system offers learning stability, user safety, and improved performance without the need of a simulator—bringing soft wearable robots closer to practical use in daily life."
