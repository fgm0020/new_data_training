type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,BiMark: Unbiased Multilayer Watermarking for Large Language Models,https://ICML.cc//virtual/2025/poster/44824,"Xiaoyan Feng, He Zhang, Yanjun Zhang, Leo Yu Zhang, Shirui Pan","Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation.To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity.To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations:(1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art  multi-bit watermarking methods, BiMark achieves up to 30\% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.","How to trace AI-generated text for ensuring responsible AI applications? We want to address this issue by embedding traceable information into AI-generated text, like a digital fingerprint. The challenge is maintaining the quality of generated text while embedding as much information as possible.To achieve this goal, we developed a method called BiMark that slightly influences how AI chooses words during text generation.  Traceable information can be encoded into text generation by using a fair coin flip mechanism. The hidden information can be reliably extracted later by analyzing how candidate words are distributed.Our method enables more trustworthy AI applications by providing a way to verify the authenticity and origin of AI-generated content, helping combat misinformation and protect intellectual property."
Poster,Binary Hypothesis Testing for Softmax Models and Leverage Score Models,https://ICML.cc//virtual/2025/poster/44589,"Yuzhou Gu, Zhao Song, Junze Yin","Softmax distributions are widely used in machine learning, including Large Language Models (LLMs), where the attention unit uses softmax distributions. We abstract the attention unit as the softmax model, where given a vector input, the model produces an output drawn from the softmax distribution (which depends on the vector input). We consider the fundamental problem of binary hypothesis testing in the setting of softmax models. That is, given an unknown softmax model, which is known to be one of the two given softmax models, how many queries are needed to determine which one is the truth? We show that the sample complexity is asymptotically $O(\epsilon^{-2})$ where $\epsilon$ is a certain distance between the parameters of the models.  Furthermore, we draw an analogy between the softmax model and the leverage score model, an important tool for algorithm design in linear algebra and graph theory. The leverage score model, on a high level, is a model which, given a vector input, produces an output drawn from a distribution dependent on the input. We obtain similar results for the binary hypothesis testing problem for leverage score models.","Imagine you’re trying to figure out if a coin is fair. You flip it many times and count how often it lands heads. If it’s close to 50/50, you might say, ""Seems fair."" But if it lands heads 90% of the time, something feels off.This is the core idea behind hypothesis testing, a method for making decisions under uncertainty. We begin with a default assumption (called the null hypothesis), like ""The coin is fair,"" and then collect data to see whether that assumption holds. If the evidence strongly contradicts it, we reject the null and accept an alternative hypothesis, like ""The coin is biased."" Our theoretical work explores hypothesis testing in the context of two fundamental mathematical tools: the softmax distribution and the leverage score distribution. These tools are central to modern AI systems, scientific computing frameworks, and operations research methods, shaping technologies we rely on every day. Our results provide insights into decision-making under uncertainty, with potential applications such as determining whether two neural networks behave similarly, among many others."
Poster,BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models,https://ICML.cc//virtual/2025/poster/46249,"Susan Liang, Dejan Markovic, Israel D. Gebru, Steven Krenn, Todd Keebler, Jacob Sandakly, Frank Yu, Samuel Hassel, Chenliang Xu, Alexander Richard","Binaural rendering aims to synthesize binaural audio that mimics natural hearing based on a mono audio and the locations of the speaker and listener.  Although many methods have been proposed to solve this problem, they struggle with rendering quality and streamable inference. Synthesizing high-quality binaural audio that is indistinguishable from real-world recordings requires precise modeling of binaural cues, room reverb, and ambient sounds. Additionally, real-world applications demand streaming inference. To address these challenges, we propose a flow matching based streaming binaural speech synthesis framework called BinauralFlow. We consider binaural rendering to be a generation problem rather than a regression problem and design a conditional flow matching model to render high-quality audio. Moreover, we design a causal U-Net architecture that estimates the current audio frame solely based on past information to tailor generative models for streaming inference. Finally, we introduce a continuous inference pipeline incorporating streaming STFT/ISTFT operations, a buffer bank, a midpoint solver, and an early skip schedule to improve rendering continuity and speed. Quantitative and qualitative evaluations demonstrate the superiority of our method over SOTA approaches. A perceptual study further reveals that our model is nearly indistinguishable from real-world recordings, with a 42% confusion rate.","Imagine you’re wearing headphones and you hear someone speaking — not just the voice, but where they are in the room, and how far away they are. This 3D-like listening experience is called “binaural audio,” and it’s key to making virtual reality, games, and immersive media feel lifelike.Today, creating this kind of audio usually requires specialized equipment or software that either can’t generate high enough quality or can’t do it fast enough for live situations like gaming or live chat.Our research presents a new method called BinauralFlow that uses advanced machine learning to create high-quality, realistic binaural speech from regular (mono) audio --- on the fly, in real time. Instead of just copying sound, our system makes the sound nearly indistinguishable from real recordings.This has the potential to transform how we experience digital sound, enabling more natural-sounding virtual meetings, immersive virtual worlds, and even more convincing avatars or virtual assistants. By improving both speed and realism, BinauralFlow brings us a step closer to seamless, lifelike audio experiences in our everyday digital lives."
Poster,Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation,https://ICML.cc//virtual/2025/poster/44399,"Michal Lukasik, Lin Chen, Harikrishna Narasimhan, Aditya Menon, Wittawat Jitkrittum, Felix Xinnan Yu, Sashank J. Reddi, Thomas Fu, MohammadHossein Bateni, Sanjiv Kumar","Bipartite ranking is a fundamental supervised learning problem, with the goal of learning a ranking over instances with maximal area under the ROC curve (AUC) against a single binary target label. However, one may often observe multiple binary target labels, e.g., from distinct human annotators. How can one synthesize such labels into a single coherent ranking? In this work, we formally analyze two approaches to this problem—loss aggregation and label aggregation—by characterizing their Bayes-optimal solutions. We show that while both approaches can yield Pareto-optimal solutions, loss aggregation can exhibit label dictatorship: one can inadvertently (and undesirably) favor one label over others. This suggests that label aggregation can be preferable to loss aggregation, which we empirically verify.","Imagine you're judging a talent show with several other judges. Each judge provides a simple ""yes"" or ""no"" vote for each contestant. The challenge is, how do you combine all these individual votes to create a single, fair ranking of all the contestants? This is a common problem in AI, such as when ranking search results using different signals like ""relevance"" and ""user engagement.""Our research investigates two common ways to solve this. One method, loss aggregation, is like weighting and adding up each judge's final scorecard. The other, label aggregation, is like first creating a combined vote for each contestant from the individual ""yes/no""s, and then ranking them based on that.We discovered a critical flaw in the first method: it can lead to ""label dictatorship."" This means the final ranking might accidentally be dominated by a single judge's opinion, not because they are wiser, but simply because they are very picky or very generous (their votes are statistically ""skewed""). Our work shows that the second method, label aggregation, avoids this problem and provides a more balanced and reliable ranking. This insight helps developers build fairer and more predictable ranking systems."
Poster,Bi-perspective Splitting Defense: Achieving Clean-Seed-Free Backdoor Security,https://ICML.cc//virtual/2025/poster/45701,"Yangyang Shen, Xiao Tan, Dian Shen, Meng Wang, Beilun Wang","Backdoor attacks have seriously threatened deep neural networks (DNNs) by embedding concealed vulnerabilities through data poisoning. To counteract these attacks, training benign models from poisoned data garnered considerable interest from researchers. High-performing defenses often rely on additional clean subsets/seeds, which is untenable due to increasing privacy concerns and data scarcity. In the absence of additional clean subsets/seeds, defenders resort to complex feature extraction and analysis, resulting in excessive overhead and compromised performance. To address these challenges, we identify the key lies in sufficient utilization of both the easier-to-obtain target labels and clean hard samples. In this work, we propose a Bi-perspective Splitting Defense (BSD). BSD distinguishes clean samples using both semantic and loss statistics characteristics through open set recognition-based splitting (OSS) and altruistic model-based data splitting (ALS) respectively. Through extensive experiments on benchmark datasets and against representative attacks, we empirically demonstrate that BSD surpasses existing defenses by over 20\% in average Defense Effectiveness Rating (DER), achieving clean data-free backdoor security.","Imagine if someone could secretly tamper with an AI system, like a hacker slipping a hidden backdoor into a lock. That’s what backdoor attacks do: they quietly sneak triggers into AI training data so the model only misbehaves under specific conditions. Defending against these attacks is tough, especially when extra clean data isn’t available due to privacy concerns or limited resources.Our research offers a new solution categorized into training-time defense. We introduce BSD, a defense that uses two perspectives to better spot poisoned data and improve the training result: how the AI understands each example, and how difficult it is for the AI to learn. These clues help distinguish clean data from poisoned data, without needing additional clean samples.BSD performs well in tests across multiple datasets and attack types. And the detailed design of BSD may inspire future research."
Poster,Bivariate Causal Discovery with Proxy Variables: Integral Solving and Beyond,https://ICML.cc//virtual/2025/poster/45269,"Yong Wu, Yanwei Fu, Shouyan Wang, Xinwei Sun","Bivariate causal discovery is challenging when unmeasured confounders exist. To adjust for the bias, previous methods employed the proxy variable (*i.e.*, negative control outcome (NCO)) to test the treatment-outcome relationship through integral equations -- and assumed that violation of this equation indicates the causal relationship. Upon this, they could establish asymptotic properties for causal hypothesis testing. However, these methods either relied on parametric assumptions or required discretizing continuous variables, which may lead to information loss. Moreover, it is unclear when this underlying integral-related assumption holds, making it difficult to justify the utility in practice. To address these problems, we first consider the scenario where only NCO is available. We propose a novel non-parametric procedure, which enjoys asymptotic properties and preserves more information. Moreover, we find that when NCO affects the outcome, the above integral-related assumption may not hold, rendering the causal relation unidentifiable. Informed by this, we further consider the scenario when the negative control exposure (NCE) is also available. In this scenario, we construct another integral restriction aided by this proxy, which can discover causation when NCO affects the outcome. We demonstrate these findings and the effectiveness of our proposals through comprehensive numerical studies.","Discovering cause-and-effect relationships from data is difficult when hidden confounders exist. A popular approach in causal inference is to use negative control outcomes—proxy variables that help detect bias. Existing methods based on this idea often rely on strong assumptions or need large datasets to work well. We propose a new method that avoids these limitations: it makes fewer assumptions and works better with smaller samples. We also show that when the proxy itself affects the outcome, standard assumptions may fail, making causality impossible to identify. To address this, we introduce a new strategy that uses an additional proxy, a negative control exposure, to uncover causal effects even in these harder cases. Our method is supported by both theory and experiments."
Poster,Black-Box Adversarial Attacks on LLM-Based Code Completion,https://ICML.cc//virtual/2025/poster/44302,"Slobodan Jenko, Niels Mündler, Jingxuan He, Mark Vero, Martin Vechev","Modern code completion engines, powered by large language models (LLMs), assist millions of developers with their strong capabilities to generate functionally correct code. Due to this popularity, it is crucial to investigate the security implications of relying on LLM-based code completion. In this work, we demonstrate that state-of-the-art black-box LLM-based code completion engines can be stealthily biased by adversaries to significantly increase their rate of insecure code generation. We present the first attack, named INSEC, that achieves this goal. INSEC works by injecting an attack string as a short comment in the completion input. The attack string is crafted through a query-based optimization procedure starting from a set of carefully designed initialization schemes. We demonstrate INSEC's broad applicability and effectiveness by evaluating it on various state-of-the-art open-source models and black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a diverse set of security-critical test cases, covering 16 CWEs across 5 programming languages, INSEC increases the rate of generated insecure code by more than 50%, while maintaining the functional correctness of generated code. We consider INSEC practical - it requires low resources and costs less than 10 US dollars to develop on commodity hardware. Moreover, we showcase the attack's real-world deployability, by developing an IDE plug-in that stealthily injects INSEC into the GitHub Copilot extension.","Modern code completion tools, which use advanced AI models called large language models (LLMs), help millions of programmers by automatically suggesting code that works correctly. Because these tools are so popular, it's important to understand if they can introduce security risks.In this study, we show that attackers can secretly influence these AI-powered code completion tools to produce insecure code more frequently.  This insecure code could then be exploited by an attacker when the product based on this code is released publicly. We developed the first attack of this kind, called INSEC. INSEC works by inserting a specially crafted short comment into the code input, which tricks the AI into generating insecure code. We create this special comment using a method that optimizes it through trial-and-error queries, starting from carefully chosen initial examples.We tested INSEC on several popular open-source AI models and commercial services like OpenAI's API and GitHub Copilot. Our tests covered a wide range of security issues (16 different types of vulnerabilities) across five programming languages. We found that INSEC increased the rate of insecure code generation by more than 50%, without affecting the functionality of the generated code.INSEC is easy and inexpensive to carry out—it can be developed using common hardware for less than $10. To demonstrate its real-world impact, we even created a plugin for a popular developer tool that secretly inserts the INSEC attack into GitHub Copilot, showing how easily this attack could be used in practice."
Poster,Blink of an eye: a simple theory for feature localization in generative models,https://ICML.cc//virtual/2025/poster/45312,"Marvin Li, Aayush Karan, Sitan Chen","Large language models can exhibit unexpected behavior in the blink of an eye. In a recent computer use demo, a language model switched from coding to Googling pictures of Yellowstone, and these sudden shifts in behavior have also been observed in reasoning patterns and jailbreaks. This phenomenon is not unique to autoregressive models: in diffusion models, key features of the final output are decided in narrow ``critical windows'' of the generation process. In this work we develop a simple, unifying theory to explain this phenomenon. Using the formalism of stochastic localization for generative models, we show that it emerges generically as the generation process localizes to a sub-population of the distribution it models.  While critical windows have been studied at length in diffusion models, existing theory heavily relies on strong distributional assumptions and the particulars of Gaussian diffusion. In contrast to existing work our theory (1) applies to autoregressive and diffusion models; (2) makes very few distributional assumptions; (3) quantitatively improves previous bounds even when specialized to diffusions; and (4) requires basic mathematical tools. Finally, we validate our predictions empirically for LLMs and find that critical windows often coincide with failures in problem solving for various math and reasoning benchmarks.","Language models can change their behavior rapidly; for example, a language model might suddenly switch from coding to browsing information about national parks. These rapid changes, called ""critical windows,"" have also been observed for language models solving math problems and hacks eliciting dangerous information from language models. Surprisingly, these critical windows have also been observed in image and video generation models as well. In this work, we apply the mathematical formalism of stochastic localization to develop a simple, general, and unifying theory that explains this phenomenon across all generative models. We show that this occurs whenever the generative model specializes to a sub-population of the distribution it models. Our research has implications for the safety and reasoning capabilities of language models and could inspire new methods to make language models more robust to these types of failures."
Poster,BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference,https://ICML.cc//virtual/2025/poster/44936,"Wonsuk Jang, Thierry Tambe","The rapidly increasing size of large language models (LLMs) presents significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with hardware-supported fine-grained scaling emerging as a promising solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. We propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from a formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. To leverage this efficiently, we propose a two-stage approach for online DialectFP4 activation quantization. Importantly, DialectFP4 ensures energy efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit usage per data, while being only 5.45% (2.69%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.","Large language models (LLMs) are getting very huge, demanding lots of memory and computing power. A common solution is to ""quantize"" them—making the numbers they use smaller (e.g., 16 bits to 4 bits), often processing data in ""blocks"", pieces of matrix. However, current block-based methods sometimes don't fully grasp the unique patterns within each data block.We introduce ""BlockDialect,"" a new technique that intelligently assigns the optimal number format to each data block for more accurate representation. This uses our specially designed ""DialectFP4,"" a collection of number formats tailored for diverse data patterns. An efficient two-stage process then chooses the best format for each block as the LLM runs, all while being energy-efficient by using simple, hardware-friendly calculations.BlockDialect improves the accuracy of popular LLMs like LLaMA3 compared to existing techniques, while using even fewer bits for each data. Beyond the direct energy savings from this data reduction, our approach is also designed to work seamlessly with energy-efficient hardware units, offering a promising way to run these advanced AI models much more efficiently overall."
Poster,BoA: Attention-aware Post-training Quantization without Backpropagation,https://ICML.cc//virtual/2025/poster/45092,"Junhan Kim, Ho-young Kim, Eulrang Cho, Chungman Lee, Joonyoung Kim, Yongkweon Jeon","Post-training quantization (PTQ) is a promising solution for deploying large language models (LLMs) on resource-constrained devices. Early methods developed for small-scale networks, such as ResNet, rely on gradient-based optimization, which becomes impractical for hyper-scale LLMs with billions of parameters.While recently proposed backpropagation-free or transformation-based methods alleviate this issue, they ignore inter-layer interactions or use the naive nearest-rounding-based quantized weight assignment to save the heavy computational cost of weight optimization.In this paper, we introduce a novel backpropagation-free PTQ algorithm that optimizes quantized weights by considering inter-layer dependencies. The key innovation is the development of attention-aware Hessian matrices that capture inter-layer interactions within the attention module. Extensive experiments demonstrate that our approach not only outperforms existing weight quantization methods but also shows good synergy with conventional methods to suppress activation outliers, leading to state-of-the-art weight-activation quantization performance.The code will be available at https://github.com/SamsungLabs/BoA.","Post-training quantization (PTQ) is a promising solution for deploying large language models (LLMs) on resource-constrained devices. Early methods developed for small-scale networks, such as ResNet, rely on gradient-based optimization, which becomes impractical for hyper-scale LLMs with billions of parameters.While recently proposed methods alleviate this issue, their performance remains limited because they ignore inter-layer interactions or use the naive nearest-rounding-based quantized weight assignment to save the heavy computational cost of weight optimization.In this paper, we introduce a novel backpropagation-free quantization algorithm that optimizes quantized weights by considering inter-layer dependencies. The key innovation is the development of attention-aware Hessian matrices that capture inter-layer interactions within the attention module. Extensive experiments demonstrate that our approach not only outperforms existing weight quantization methods but also shows good synergy with conventional methods to suppress activation outliers, leading to state-of-the-art weight-activation quantization performance.Our breakthrough is particularly impactful for on-device AI, allowing real-time model inference on mobile and edge devices without the need for server access, paving the way for scalable, decentralized AI solutions."
