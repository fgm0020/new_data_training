type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,On the Alignment between Fairness and Accuracy: from the Perspective of Adversarial Robustness,https://ICML.cc//virtual/2025/poster/44670,"Junyi Chai, Taeuk Jang, Jing Gao, Xiaoqian Wang","While numerous work has been proposed to address fairness in machine learning, existing methods do not guarantee fair predictions under imperceptible feature perturbation, and a seemingly fair model can suffer from large group-wise disparities under such perturbation. Moreover, while adversarial training has been shown to be reliable in improving a model's robustness to defend against adversarial feature perturbation that deteriorates accuracy, it has not been properly studied in the context of adversarial perturbation against fairness. To tackle these challenges, in this paper, we study the problem of adversarial attack and adversarial robustness w.r.t. two terms: fairness and accuracy. From the adversarial attack perspective, we propose a unified structure for adversarial attacks against fairness which brings together common notions in group fairness, and we theoretically prove the equivalence of adversarial attacks against different fairness notions. Further, we derive the connections between adversarial attacks against fairness and those against accuracy. From the adversarial robustness perspective, we theoretically align robustness to adversarial attacks against fairness and accuracy, where robustness w.r.t. one term enhances robustness w.r.t. the other term. Our study suggests a novel way to unify adversarial training w.r.t. fairness and accuracy, and experiments show our proposed method achieves better robustness w.r.t. both terms.","Machine learning models are often trained to make fair predictions across different groups, but their fairness can quickly break down if even tiny, hard-to-detect changes are made to the data. A model that appears fair in everyday situations may actually treat groups quite differently when faced with subtle tweaks.Our research asks: Can we build machine learning models that remain both accurate and fair, when the data is changed in small, hard-to-detect ways? While it’s often believed that you have to choose between fairness and accuracy, we show that when data is maliciously modified, techniques designed to keep models accurate can also help maintain fairness under such changes—with only minor adjustments.We developed a practical method that helps models maintain fair and accurate predictions, even when someone tries to fool them with subtle changes in the data. Our experiments show that this approach results in models whose decisions remain consistent, fair, and trustworthy—even in challenging situations."
Poster,On the Benefits of Active Data Collection in Operator Learning,https://ICML.cc//virtual/2025/poster/44410,"Unique Subedi, Ambuj Tewari","We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel.  We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with thepassive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts.","In this paper, we study how to efficiently teach machines to predict the behavior of complex systems, such as those described by physical equations, using only a small number of examples. Instead of passively learning from randomly chosen data, we propose an active learning approach where the model carefully selects the most informative inputs. We show, both theoretically and through experiments, that this strategy can significantly reduce the amount of data needed to learn accurately. Our findings suggest that, especially in scientific settings where data is expensive to obtain, actively choosing what to learn from can lead to faster and more efficient modeling."
Poster,On the Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics,https://ICML.cc//virtual/2025/poster/44184,"Binghui Li, Yuanzhi Li","Similar to surprising performance in the standard deep learning, deep nets trained by adversarial training also generalize well for unseen clean data (natural data). However, despite adversarial training can achieve low robust training error, there exists a significant robust generalization gap. We call this phenomenon the Clean Generalization and Robust Overfitting (CGRO). In this work, we study the CGRO phenomenon in adversarial training from two views: representation complexity and training dynamics. Specifically, we consider a binary classification setting with $N$ separated training data points. First, we prove that, based on the assumption that we assume there is $\operatorname{poly}(D)$-size clean classifier (where $D$ is the data dimension), ReLU net with only $O(N D)$ extra parameters is able to leverages robust memorization to achieve the CGRO, while robust classifier still requires exponential representation complexity in worst case. Next, we focus on a structured-data case to analyze training dynamics, where we train a two-layer convolutional network with $O(N D)$ width against adversarial perturbation. We then show that a three-stage phase transition occurs during learning process and the network provably converges to robust memorization regime, which thereby results in the CGRO. Besides, we also empirically verify our theoretical analysis by experiments in real-image recognition datasets.","Adversarial training, similar to standard deep learning, enables deep nets to generalize well to unseen clean data. However, even though adversarial training can reduce training errors, a significant gap in robust generalization remains. We call this the Clean Generalization and Robust Overfitting (CGRO) phenomenon. In this study, we explore CGRO from two perspectives: model complexity and training dynamics. We show that a simple neural network can achieve CGRO through robust memorization, while a fully robust classifier requires much more complex representations. We also analyze the training process of a convolutional network and identify a three-stage phase transition during learning, which leads to robust memorization and explains the CGRO effect. Our theoretical analysis is supported by experiments on real-world image recognition datasets."
Poster,On The Concurrence of Layer-wise Preconditioning Methods and Provable Feature Learning,https://ICML.cc//virtual/2025/poster/44797,"Thomas T. Zhang, Behrad Moniri, Ansh Nagwekar, Faraz Rahman, Anton Xue, Hamed Hassani, Nikolai Matni","Layer-wise preconditioning methods are a family of memory-efficient optimization algorithms that introduce preconditioners per axis of each layer's weight tensors. These methods have seen a recent resurgence, demonstrating impressive performance relative to entry-wise (""diagonal"") preconditioning methods such as Adam(W) on a wide range of neural network optimization tasks. Complementary to their practical performance, we demonstrate that layer-wise preconditioning methods are provably necessary from a statistical perspective. To showcase this, we consider two prototypical models, *linear representation learning* and *single-index learning*, which are widely used to study how typical algorithms efficiently learn useful *features* to enable generalization.In these problems, we show SGD is a suboptimal feature learner when extending beyond ideal isotropic inputs $\mathbf{x} \sim \mathsf{N}(\mathbf{0}, \mathbf{I})$ and well-conditioned settings typically assumed in prior work.We demonstrate theoretically and numerically that this suboptimality is fundamental, and that layer-wise preconditioning emerges naturally as the solution. We further show that standard tools like Adam preconditioning and batch-norm only mildly mitigate these issues, supporting the unique benefits of layer-wise preconditioning.","Recently, a new family of optimization algorithms have shown great promise in making neural network training faster and more efficient in practice. These algorithms introduce new versions ""preconditioning"", which is the practice of ""re-sizing"" a problem to hopefully make it easier to find good solutions. The current standard optimizer, Adam, performs ""preconditioning"" independently on each parameter of a neural network, while these new algorithms do so in a new way that can also take into account dependencies between parameters within each layer of the network, hence ""layer-wise preconditioning"".On the other hand, theory researchers have proposed various problems to understand very clearly how neural networks can find good solutions. These works typically study the most basic optimization algorithm in stochastic gradient descent (SGD). However, we found that SGD is somehow fundamentally limited: when the data is not perfectly ""well-conditioned"" (imagine some coordinates of the data are larger than others), then these positive results about neural network training no longer hold, in theory or in practice.In finding ways to adjust SGD to work for general types of data, we found that the resulting algorithm aligns with these (practical) ""layer-wise preconditioning"" algorithms. This has implications both for theorists, where these results provide a concrete path to analyzing larger families of neural network optimization algorithms, and practitioners, where these results provide a strong mathematical motivation for why these new algorithms work."
Poster,On the Convergence of Continuous Single-timescale Actor-critic,https://ICML.cc//virtual/2025/poster/44001,"Xuyang Chen, Lin Zhao","Actor-critic algorithms have been instrumental in boosting the performance of numerous challenging applications involving continuous control, such as highly robust and agile robot motion control. However, their theoretical understanding remains largely underdeveloped. Existing analyses mostly focus on finite state-action spaces and on simplified variants of actor-critic, such as double-loop updates with i.i.d. sampling, which are often impractical for real-world applications.We consider the canonical and widely adopted single-timescale updates with Markovian sampling in continuous state-action space. Specifically, we establish finite-time convergence by introducing a novel Lyapunov analysis framework, which provides a unified convergence characterization of both the actor and the critic. Our approach is less conservative than previous methods and offers new insights into the coupled dynamics of actor-critic updates.","Actor-critic algorithms have played a pivotal role in advancing performance across a range of challenging continuous control tasks, including robust and agile robotic motion. In this work, we establish the finite-time convergence of the widely used single-timescale actor-critic algorithm with Markovian sampling under continuous state-action spaces. This result bridges the gap between practical implementations and theoretical guarantees, and offers a promising method for analyzing other single-timescale reinforcement learning algorithms."
Poster,On the Diversity of Adversarial Ensemble Learning,https://ICML.cc//virtual/2025/poster/45608,"Jun-Qi Guo, Meng-Zhang Qian, Wei Gao, Zhi-Hua Zhou","Diversity has been one of the most crucial factors on the design of adversarial ensemble methods. This work focuses on the fundamental problems: How to define the diversity for the adversarial ensemble, and how to correlate with algorithmic performance. We first show that it is an NP-Hard problem to precisely calculate the diversity of two networks in adversarial ensemble learning, which makes it different from prior  diversity analysis. We present the first diversity decomposition under the first-order approximation for the adversarial ensemble learning. Specifically, the adversarial ensemble loss can be decomposed into average of individual adversarial losses,  gradient diversity, prediction diversity and cross diversity. Hence, it is not sufficient to merely consider the gradient diversity on the characterization of diversity as in previous adversarial ensemble methods. We present diversity decomposition for classification with cross-entropy loss similarly. Based on the theoretical analysis, we develop new  ensemble method via orthogonal adversarial predictions to simultaneously improve gradient diversity and cross diversity. We finally conduct experiments to validate the effectiveness of our method.","In real-world applications, machine learning models can be easily fooled by adversarial examples—specially crafted inputs that look almost identical to normal data but lead the model to make wrong predictions. This is a serious concern in high-stakes fields like healthcare, finance, and autonomous driving.Ensemble learning is a popular strategy to improve robustness against such attacks, which combines multiple models to make better decisions. Diversity has always been one of the most crucial factors in the design of ensemble methods. However, it is challenging and still poorly understood to define and measure the diversity in adversarial ensemble learning. This work focuses on two fundamental problems: How to define the diversity for the adversarial ensemble, and how to correlate with algorithmic performance.We first prove that it is NP-Hard to precisely calculate the diversity of neural networks in adversarial ensemble learning is NP-hard, meaning it’s extremely difficult computationally. To overcome this, we take the first-order approximation and decompose the adversarial ensemble loss into four components: the average loss of individual models, prediction diversity (how differently models predict), gradient diversity (how different their gradients), cross diversity (how predictions and gradients interact). This is the first decomposition for diversity in adversarial ensemble learning, and it reveals that focusing only on gradient diversity—as many past methods do—is insufficient.Based on this analysis, we introduce a new method called AdvEOAP (Adversarial Ensemble via Orthogonal Adversarial Predictions). It trains models so that their predictions on adversarial examples are orthogonal, boosting both gradient diversity and cross diversity. Experiments on standard datasets (MNIST, Fashion-MNIST, CIFAR-10) show that AdvEOAP significantly outperforms previous methods under a wide range of adversarial attacks."
Poster,On the Duality between Gradient Transformations and Adapters,https://ICML.cc//virtual/2025/poster/43999,"Lucas Torroba Hennigen, Hunter Lang, Han Guo, Yoon Kim","We study memory-efficient optimization of neural networks (in particular language models) with *linear gradient transformations*, where the gradients are linearly mapped to a lower dimensional space than the full parameter space, thus saving memory required for gradient accumulation and optimizer state persistence. The model parameters are updated by first performing an optimization step in the lower dimensional space and then going back into the original parameter space via the linear map's transpose. We show that optimizing the model in this transformed space is equivalent to reparameterizing the original model through a *linear adapter* that additively modifies the model parameters, and then only optimizing the adapter's parameters. When the transformation is Kronecker-factored, this establishes an equivalence between GaLore and one-sided LoRA. We show that this duality between gradient transformations and adapter-based reparameterizations unifies existing approaches to memory-efficient training and suggests new techniques for improving training efficiency and memory use.","One of the many aspects that makes training large language models (LLMs) challenging is the large amount of memory that is required to train them. Many methods have been proposed to deal with memory limitations, among them low-rank adapters (e.g., LoRA) and gradient transformations (e.g., GaLore).In this paper, we show that these two methods, under certain conditions, are mathematically equivalent. This allows us to take techniques that were only applicable to one method and apply them to the other method, providing performance improvements.Overall, our findings suggest ways to reduce the amount of resources required to train large language models. For example, we found that in a decentralized and memory-constrained setting, one can train better LLMs by being deliberate about the choice of gradient transformation that is used, so that each worker trains only one “slice” of the larger model."
Poster,On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning,https://ICML.cc//virtual/2025/poster/44725,"Naram Mhaisen, George Iosifidis","We revisit the Follow the Regularized Leader (FTRL) framework for Online Convex Optimization (OCO) over compact sets, focusing on achieving dynamic regret guarantees. Prior work has highlighted the framework’s limitations in dynamic environments due to its tendency to produce ""lazy"" iterates. However, building on insights showing FTRL's ability to produce ""agile"" iterates, we show that it can indeed recover known dynamic regret bounds through optimistic composition of future costs and careful linearization of past costs, which can lead to pruning some of them. This new analysis of FTRL against dynamic comparators yields a principled way to interpolate between greedy and agile updates and offers several benefits, including refined control over regret terms, optimism without cyclic dependence, and the application of minimal recursive regularization akin to AdaFTRL. More broadly, we show that it is not the ""lazy"" projection style of FTRL that hinders (optimistic) dynamic regret, but the decoupling of the algorithm’s state (linearized history) from its iterates, allowing the state to grow arbitrarily. Instead, pruning synchronizes these two when necessary.","Machine learning systems often make sequential decisions, such as choosing investments or managing resources, but struggle when environments constantly change. Existing methods either adapt slowly or rely heavily on perfect future predictions, limiting their practical use.We revisited a classical decision-making method called ""Follow the Regularized Leader"" (FTRL), known for its simplicity but previously thought unsuitable for dynamic environments. By carefully selecting and sometimes discarding (pruning) past information, our improved approach makes smarter decisions that adapt quickly when conditions change.The analysis also shows that this enhanced FTRL algorithm achieves robust performance, even when predictions about future conditions are uncertain. It smoothly adjusts between cautious and aggressive strategies based on the reliability of the predictions so far. This provides flexibility, ensuring good performance both when future predictions are accurate and when they fail.These results help extend the practical applicability of FTRL, giving developers a flexible tool to build smarter systems that can adapt effectively in real-world, changing environments."
Poster,On the Emergence of Position Bias in Transformers,https://ICML.cc//virtual/2025/poster/44889,"Xinyi Wu, Yifei Wang, Stefanie Jegelka, Ali Jadbabaie","Recent studies have revealed various manifestations of position bias in transformer architectures, from the ""lost-in-the-middle"" phenomenon to attention sinks, yet a comprehensive theoretical understanding of how attention masks and positional encodings shape these biases remains elusive. This paper presents a graph-theoretic framework for analyzing position bias in multi-layer attention.  Modeling attention masks as directed graphs, we quantify how tokens interact with contextual information based on their sequential positions. We uncover two key insights: First, causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens. Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers—coupled with the causal mask—leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions. Through controlled numerical experiments, we not only validate our theoretical findings but also reproduce position biases observed in real-world LLMs. Our framework offers a principled foundation for understanding positional biases in transformers, shedding light on the complex interplay of attention mechanism components and guiding more informed architectural design.","Large language models (LLMs), like those behind chatbots and code assistants, often pay too much attention to the beginning or end of a passage while overlooking the middle—a phenomenon known as position bias. This behavior can reduce model reliability on tasks involving long documents, source code, or complex reasoning. In our work, we developed a new way to study this issue by modeling how information flows between words in a transformer—the backbone of most modern LLMs—as a graph. This graph-theoretic perspective revealed that two key components of transformers, the causal mask and positional encodings, push the model's attention in different directions. The causal mask steers focus toward earlier words, while positional encodings emphasize nearby words instead.Through theoretical analysis and controlled experiments, we show how these components interact to produce the position biases observed in real-world LLMs. Our framework provides a principled foundation for diagnosing, understanding, and mitigating these biases, paving the way for more balanced and reliable model behavior in future LLM designs."
Poster,On-the-Fly Adaptive Distillation of Transformer to Dual-State  Linear Attention for Long-Context LLM Serving,https://ICML.cc//virtual/2025/poster/43982,"Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang “Atlas” Wang, Aditya Akella","Large language models (LLMs) excel at capturing global token dependencies via self-attention but face prohibitive compute and memory costs on lengthy inputs. While sub-quadratic methods (e.g., linear attention) can reduce these costs, they often degrade accuracy due to overemphasizing recent tokens. In this work, we first propose *dual-state linear attention* (**DSLA**), a novel design that maintains two specialized hidden states—one for preserving historical context and one for tracking recency—thereby mitigating the short-range bias typical of linear-attention architectures. To further balance efficiency and accuracy under dynamic workload conditions, we introduce DSLA-*Serve*, an online *adaptive distillation* framework that progressively replaces Transformer layers with DSLA layers at inference time, guided by a sensitivity-based layer ordering. DSLA-*Serve* uses a chained fine-tuning strategy to ensure that each newly converted DSLA layer remains consistent with previously replaced layers, preserving the overall quality. Extensive evaluations on commonsense reasoning, long-context QA, and text summarization demonstrate that DSLA-*Serve* yields **2.3×** faster inference than Llama2-7B and **3.0×** faster than the hybrid Zamba-7B, while retaining comparable performance across downstream tasks. Our ablation studies show that DSLA’s dual states capture both global and local dependencies, addressing the historical-token underrepresentation seen in prior linear attentions.","Large language models (LLMs), like those behind chatbots and AI assistants, are powerful but slow and memory-intensive—especially when processing long texts. Some faster alternatives exist, but they often lose accuracy because they focus too much on recent words and neglect earlier parts of the input.In this work, we introduce a new method called **Dual-State Linear Attention (DSLA)**. It maintains two separate memory states—one for past context and one for recent information—allowing it to stay accurate while being more efficient.We also develop an automatic system called DSLA-*Serve*, which gradually replaces the heavier parts of the model with our lighter DSLA components during real-time use. It performs this replacement carefully to ensure the AI continues to perform well, even as it switches to more efficient alternatives.When tested on challenging tasks like reasoning, answering long questions, and summarizing text, DSLA-*Serve* ran up to 3× faster than existing models—without sacrificing performance."
