type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator,https://ICML.cc//virtual/2025/poster/44175,"YuXin Li, Felix Dangel, Derek Tam, Colin Raffel","The diagonal of a model's Fisher Information Matrix (the ""Fisher"") has frequently been used as a way to measure parameter sensitivity.Typically, the Fisher is estimated by computing the squared gradient of the model's outputs with respect to its parameters, averaged over a few hundred or thousand examples — a process which incurs nontrivial computational costs.At the same time, adaptive gradient methods like the ubiquitous Adam optimizer compute a moving average of the squared gradient over the course of training.This paper therefore explores whether an approximation of the Fisher can be obtained ""for free"" by recycling the squared gradient accumulator that has already been computed over the course of training.Through a comprehensive set of experiments covering five applications of the Fisher, we demonstrate that the ""Squisher"" (**Squ**ared gradient accumulator as an approximation of the F**isher**) consistently performs similarly to the Fisher while outperforming baseline methods.Additionally, we clarify the exact differences between the Squisher and the Fisher and provide empirical quantification of their respective impact.","Understanding which parts of a neural network are most important (i.e., which parameters matter most) can help with tasks like model merging, pruning, transfer learning, and continual learning. A popular tool for this is the diagonal of the Fisher Information Matrix, which we refer to as the Fisher. But calculating it can be expensive — it requires extra computation on hundreds or thousands of examples.In this paper, we ask if we get a good-enough version of the Fisher without paying the full price. Surprisingly, the answer is yes. During training, widely used optimizers like Adam already keep track of a similar quantity: the squared gradients of the model's parameters. This approximation, which we call Squisher (**Squ**ared gradient accumulator as an approximation of the F**isher**), requires no extra computation or memory and is readily available “for free.”Across five common applications of the Fisher, we show that Squisher produces results comparable to the original Fisher method, but with significantly lower computational cost. It saves time and resources, making it easier to apply these techniques at scale."
Poster,FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain,https://ICML.cc//virtual/2025/poster/44605,"Rohan Deb, Kiran Thekumparampil, Kousha Kalantari, Gaurush Hiranandani, Shoham Sabach, Branislav Kveton","Supervised fine-tuning (SFT) is the most common way of adapting large language models (LLMs) to a new domain. In this paper, we improve the efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed budget of training examples, which determines the computational cost of fine-tuning, we select those that maximize information gain, as measured by the Fisher information matrix of the SFT objective. We approximate it efficiently by linearization at the last layer of the LLM. Our approach is computationally efficient, analyzable, and performs well empirically. We demonstrate this on several problems, with both quantitative results and LLM-as-a-judge evaluations.","Imagine that you want to teach a large language model (like ChatGPT) to do better on a new task, but you can only give it a limited number of examples to learn from. We carefully choose the ones that teach the model the most. We do this efficiently using tools from statistics, and we show that our approach works well in both theory and practice."
Poster,Fixed-Confidence Multiple Change Point Identification under Bandit Feedback,https://ICML.cc//virtual/2025/poster/45783,"Joseph Lazzaro, Ciara Pike-Burke","Piecewise constant functions describe a variety of real-world phenomena in domains ranging from chemistry to manufacturing. In practice, it is often required to confidently identify the locations of the abrupt changes in these functions as quickly as possible. For this, we introduce a fixed-confidence piecewise constant bandit problem. Here, we sequentially query points in the domain and receive noisy evaluations of the function under bandit feedback. We provide instance-dependent lower bounds for the complexity of change point identification in this problem. These lower bounds illustrate that an optimal method should focus its sampling efforts adjacent to each of the change points, and the number of samples around each change point should be inversely proportional to the magnitude of the change.Building on this, we devise a simple and computationally efficient variant of Track-and-Stop and prove that it is asymptotically optimal in many regimes. We support our theoretical findings with experimental results in synthetic environments demonstrating the efficiency of our method.","In many settings such as chemistry or manufacturing, tiny changes in the experimental input (e.g. temperature) can lead to large changes in the output (e.g.yield). Often it is required to locate the changes in input that lead to thesebig jumps in output. Moreover, it is often expensive to run these experimentsat every possible input value. We therefore aim to find methods to locate thejumps as efficiently as possible.To tackle this problem, we construct methods to sequentially choose an input (e.g. temperature) at which we will observe a noisy output. We can useprevious observations to carefully select these points. We repeat the processuntil we can confidently stop and return the location of the set of jumps orchange points. In this paper, we mathematically upper bound the expectedtime for our proposed methods to stop. We then show that these expectedstopping times are theoretically optimal when we want to have high confidence thatour returned change point locations are correct. We complement these resultswith some experiments showing our methods are effective in practice."
Poster,Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function,https://ICML.cc//virtual/2025/poster/44912,"Christopher Subich, Syed Husain, Leo Separovic, Jing Yang","Recent advancements in data-driven weather forecasting models have delivered deterministic models that outperform the leading operational forecast systems based on traditional, physics-based models. However, these data-driven models are typically trained with a mean squared error loss function, which causes smoothing of fine scales through a ``double penalty'' effect.  We develop a simple, parameter-free modification to this loss function that avoids this problem by separating the loss attributable to decorrelation from the loss attributable to spectral amplitude errors.  Fine-tuning the GraphCast model with this new loss function results in sharp deterministic weather forecasts, an increase of the model's effective resolution from 1,250km to 160km, improvements to ensemble spread, and improvements to predictions of tropical cyclone strength and surface wind extremes.","Data-driven weather forecasting is an emerging field that may soon overtake traditional numerical weather prediction for short and medium-term forecasts.  However, deterministic data-driven models – models which are asked to provide the single best guess of future weather – tend to ""hedge their bets"" and under-predict fine-scale variation and extreme weather.  This arises because of a ""double penalty"" during training with traditional loss functions.  These loss functions punish both false positives and false negatives, so a model that correctly predicts a system like a hurricane but places it in the wrong location will be punished both for missing the ""true"" storm and for developing a ""false"" storm that doesn't match the ground truth.We address this issue by developing a modified loss function for model training.  This loss function has separate terms that separate the reward from accurate prediction of the intensity of short and long-wavelength fluctuations (spectral amplitude) from its correlation with the ground truth (spectral coherence).  For large scales that are very predictable, this encourages the same forecast behaviour as traditional error measures, but for small scales that are chaotic and unpredictable this encourages the model to still produce a realistic forecast.We fine-tune the ¼°, 13-level GraphCast model with this loss function, and the resulting model shows realistic variation to scales of 160 km (improved from 1,250 km), improves forecasts of tropical cyclone intensity, and modestly improves forecast variability in an ensemble setting."
Poster,Fixing the Loose Brake: Exponential-Tailed Stopping Time in Best Arm Identification,https://ICML.cc//virtual/2025/poster/46019,"Kapilan Balagopalan, Tuan Nguyen, Yao Zhao, Kwang-Sung Jun","The best arm identification problem requires identifying the best alternative (i.e., arm) in active experimentation using the smallest number of experiments (i.e., arm pulls), which is crucial for cost-efficient and timely decision-making processes. In the fixed confidence setting, an algorithm must stop data-dependently and return the estimated best arm with a correctness guarantee. Since this stopping time is random, we desire its distribution to have light tails. Unfortunately, many existing studies focus on high probability or in expectation bounds on the stopping time, which allow heavy tails and, for high probability bounds, even not stopping at all. We first prove that this never-stopping event can indeed happen for some popular algorithms. Motivated by this, we propose algorithms that provably enjoy an exponential-tailed stopping time, which improves upon the polynomial tail bound reported by Kalyanakrishnan et al. (2012). The first algorithm is based on a fixed budget algorithm called Sequential Halving along with a doubling trick. The second algorithm is a meta algorithm that takes in any fixed confidence algorithm with a high probability stopping guarantee and turns it into one that enjoys an exponential-tailed stopping time. Our results imply that there is much more to be desired for contemporary fixed confidence algorithms.","Imagine needing to find the best option among many choices – like the most effective ad or medical treatment – through experiments. You want to be sure you've picked the winner using the fewest trials.The problem is, some current methods for doing this can take an unpredictably long time, or might even never give you a final answer. We found this ""never stopping"" risk is real.Our research offers new ways to experiment that guarantee a much faster and more predictable stopping time. One method refines a tournament-style approach, and another acts as an upgrade for existing techniques. Essentially, our solutions make sure the process of finding the best option doesn't drag on, ensuring you get reliable answers efficiently."
Poster,FLAM: Frame-Wise Language-Audio Modeling,https://ICML.cc//virtual/2025/poster/46310,"Yusong Wu, Christos Tsirigotis, Ke Chen, Cheng-Zhi Anna Huang, Aaron Courville, Oriol Nieto, Prem Seetharaman, Justin Salamon","Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval but struggle with frame-wise audio understanding. Prior works use temporal-aware labels or unsupervised training to improve frame-wise capabilities, but they still lack fine-grained labeling capability to pinpoint when an event occurs. While traditional sound event detection models can precisely localize events, they are limited to pre-defined categories, making them ineffective for real-world scenarios with out-of-distribution events. In this work, we introduce FLAM, an open-vocabulary contrastive audio-language model capable of localizing specific sound events. FLAM employs a memory-efficient and calibrated frame-wise objective with logit adjustment to address spurious correlations, such as event dependencies and label imbalances during training. To enable frame-wise supervision, we leverage a large-scale dataset with diverse audio events, LLM-generated captions and simulation. Experimental results and case studies demonstrate that FLAM significantly improves the open-vocabulary localization capability while maintaining strong performance in global retrieval and downstream tasks.","Sound event detection—figuring out when and where certain sounds happen—can greatly enhance how we search, organize, and interact with audio data. However, existing systems are limited by predefined sound categories and struggle to accurately pinpoint the exact timing of diverse events.Our research tackles this by introducing FLAM, an innovative system that matches audio frames directly to natural language descriptions, making it possible to detect any sound described by a user, even if it wasn't part of the training set. To overcome the challenge of scarce temporal audio annotations, we generated a large, diverse dataset by combining and labeling short sound clips within various backgrounds via data augmentation. We trained FLAM with this data using a specialized objective that corrects biases and enhances precision.The result is a powerful model capable of accurately locating the timing of sound events in real-time, significantly outperforming previous methods. FLAM opens new opportunities for audio applications like smart search, accessibility tools, and multimedia content analysis, empowering users to intuitively interact with sound data."
Poster,"FlashTP: Fused, Sparsity-Aware Tensor Product for Machine Learning Interatomic Potentials",https://ICML.cc//virtual/2025/poster/43610,"Seung Lee, Hojoon Kim, Yutack Park, Dawoon Jeong, Seungwu Han, Yeonhong Park, Jae W. Lee","Machine Learning Interatomic Potentials (MLIPs) enable efficient molecular dynamics (MD) simulations with high accuracy. While equivariant MLIPs achieve state-of-the-art accuracy, they face significant computational bottlenecks centered around their Tensor-Product layer, which account for up to 75\% of training time and cause substantial memory overhead. We present FlashTP, a highly optimized tensor-product library that addresses these inefficiencies through kernel fusion, sparse computation, and path-aggregated execution. FlashTP achieves up to 41.6$\times$ and 60.8$\times$ kernel speedups over _e3nn_ and NVIDIA cuEquivariance, respectively. For SevenNet-l3i5, it delivers 4.2$\times$ and 3.5$\times$ speedup while reducing peak memory usage by 6.3$\times$ and 6.2$\times$ for inference and training, respectively. The code is available at https://github.com/SNU-ARC/flashTP.","Imagine watching a slow-motion movie of atoms as they jiggle, bump into each other, and form new structures. That’s what molecular dynamics (MD) simulations do on a computer—letting scientists see how materials behave or how proteins fold, without costly lab experimentsRecently, researchers have started using machine-learning interatomic potentials (MLIPs)—deep neural networks trained on high-precision quantum data—to make these simulations both faster and more accurate. However, MLIP-driven simulations are bottlenecked by a mathematical operation called the tensor product, which consumes approximately 75–90% of both computation time and memory.We built FlashTP, an optimized GPU library that fuses all of those slow steps into one, removing redundant data movement and cleverly skipping work that isn’t needed. On modern hardware, FlashTP lets scientists train their models more than 3.5× faster, run simulations 4.2× faster, and use over 6× less memory compared to the popular MLIP framework _e3nn_. Best of all, it plugs right into the _e3nn_ framework, so you can switch on FlashTP with almost zero code changes and start seeing the speed boost immediately."
Poster,Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape,https://ICML.cc//virtual/2025/poster/46534,"Tao Li, Zhengbao He, Yujun Li, Yasheng Wang, Lifeng Shang, Xiaolin Huang","Fine-tuning large-scale pre-trained models is prohibitively expensive in terms of computation and memory costs. Low-Rank Adaptation (LoRA), a popular Parameter-Efficient Fine-Tuning (PEFT) method, offers an efficient solution by optimizing only low-rank matrices. Despite recent progress in improving LoRA's performance, the relationship between the LoRA optimization space and the full parameter space is often overlooked. A solution that appears flat in the loss landscape of the LoRA space may still exhibit sharp directions in the full parameter space, potentially compromising generalization.We introduce Flat-LoRA, which aims to identify a low-rank adaptation situated in a flat region of the full parameter space.Instead of adopting the well-established sharpness-aware minimization approach, which incurs significant computation and memory overheads, we employ a Bayesian expectation loss objective to preserve training efficiency. Further, we design a refined strategy for generating random perturbations to enhance performance and carefully manage memory overhead using random seeds.Experiments across diverse tasks—including mathematical reasoning, coding abilities, dialogue generation, instruction following, and text-to-image generation—demonstrate that Flat-LoRA improves both in-domain and out-of-domain generalization.Code is available at https://github.com/nblt/Flat-LoRA.","A popular approach for fine-tuning large-scale pre-trained models on specific downstream tasks is to optimize only a small subset of parameters, known as Parameter-Efficient Fine-Tuning (PEFT) methods. These methods, such as those using low-rank matrices, are widely adopted for their efficiency. However, from the perspective of the loss landscape, there is a notable inconsistency between PEFT methods and full fine-tuning: a solution that is flat (which is desirable for generalization) in the reduced parameter space is not necessarily flat in the full parameter space.Traditionally, to encourage flatness across the entire parameter space, Sharpness-Aware Minimization (SAM) is employed. However, SAM doubles the training time and requires an additional copy of the model weights to compute the sharpness direction, which is often impractical due to its computational and memory overhead.Our goal is to achieve flatness in the full parameter space for PEFT methods, while maintaining efficiency in both time and memory. To address this, we propose a simple approach that introduces carefully designed random perturbations. These perturbations can be efficiently generated and stored using random seeds, ensuring that the method remains lightweight.Our approach can be easily integrated into existing efficient fine-tuning methods, enhancing generalization performance with minimal additional cost."
Poster,FlatQuant: Flatness Matters for LLM Quantization,https://ICML.cc//virtual/2025/poster/43726,"Yuxuan Sun, Ruikang Liu, Haoli Bai, Han Bao, Kang Zhao, Yuening Li, JiaxinHu, Xianzhi Yu, Lu Hou, Chun Yuan, Xin Jiang, Wulong Liu, Jun Yao","Recently, quantization has been widely used for the compression and acceleration of large language models (LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still exhibit steep and dispersed distributions. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach that enhances the flatness of weights and activations. Our approach identifies optimal affine transformations for each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead of affine transformation, we apply Kronecker product with two lightweight matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments demonstrate that FlatQuant establishes a new state-of-the-art benchmark for quantization. For example, it achieves less than 1\% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5\%. Additionally, it provides up to 2.3x prefill speedup and 1.7x decoding speedup compared to the FP16 model. Code is available at: https://github.com/ruikangliu/FlatQuant.","Low-bit quantization is a widely used technique to compress large language models (LLMs) and accelerate inference. While 8-bit quantization is common, reducing precision to 4-bit often leads to significant accuracy loss due to extreme outliers in tensors, especially in activations.We find that improving the flatness of tensors can greatly reduce this quantization loss. Our method proposes a new post-training quantization approach that uses affine transformations to smooth out outliers, enabled by algorithm–system co-design. To ensure efficiency, we construct the transformations using lightweight Kronecker-product-structured matrices, optimized for each layer's outlier distribution.Our approach achieves less than 1% accuracy drop on major LLMs like LLaMA-3 under full 4-bit quantization while maintaining strong speedups, making 4-bit quantization far more practical for real-world applications."
Poster,Fleet of Agents: Coordinated Problem Solving with Large Language Models,https://ICML.cc//virtual/2025/poster/43539,"Lars Klein, Nearchos Potamitis, Roland Aydin, Robert West, Caglar Gulcehre, Akhil Arora","While numerous frameworks have been developed to enhance the reasoning abilities of large language models (LLMs), there is a scarcity of methods that effectively balance the trade-off between cost and quality. In this paper, we introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework utilizing LLMs as agents to navigate through dynamic tree searches, employing a genetic-type particle filtering approach. FoA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation. This mechanism enables dynamic branching, adapting the exploration strategy based on discovered solutions. We conduct extensive experiments on four benchmark tasks, \``Game of 24\'', \``Mini-Crosswords\'', \``WebShop\'' and \``SciBench\'', utilizing four different LLMs, GPT-3.5, GPT-4, LLaMA3.2-11B, and LLaMA3.2-90B. On average across all tasks and LLMs, FoA obtains an absolute quality improvement of $\simeq 5\%$ while requiring only $\simeq 35\%$ of the cost of previous SOTA methods. Notably, our analyses reveal that (1) FoA achieves the best cost-quality trade-off among all benchmarked methods, and (2) FoA+ LLaMA3.2-11B surpasses the Llama3.2-90B model. FoA is publicly available at [https://github.com/au-clan/FoA](https://github.com/au-clan/FoA).","Large language models (LLMs), like GPT-4 and LLaMA, are powerful tools for solving complex problems. But making them reason well often comes with a trade-off between high costs and good quality. Our work introduces a new framework called Fleet of Agents (FoA), which cleverly balances cost and quality. Instead of relying on a single agent or blindly exploring many paths, FoA uses an approach inspired by ""genetic"" or ""evolutionary"" algorithms. FoA spawns many small agents to explore possible solutions and then selects the most promising ones to continue, much like nature, which favors the fittest.We tested FoA using various LLMs on tasks such as mathematical puzzles, crosswords, question answering, and online shopping. Across all tasks, FoA consistently resulted in better solution quality while substantially reducing the computational cost compared to existing methods.This means FoA helps AI systems reason more effectively and efficiently, making them more accessible, practical, and sustainable for a wide range of applications. We have made FoA publicly available so others can use and build on it."
