type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training,https://ICML.cc//virtual/2025/poster/45497,"Yang Luo, Zangwei Zheng, Ziheng Qin, Zirui Zhu, Yong Liu, Yang You","Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens.This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at https://github.com/NUS-HPC-AI-Lab/MERIT.","Training large AI language models efficiently is challenging because using bigger batches of data often leads to unstable or lower-quality results. Current training methods (like AdamW or LAMB) struggle with this because they can’t properly control sharp spikes in attention logits—critical parts of how these models process information. While LAMB partly fixes this, it still misses key details, like how to limit extreme values in certain weights or account for relationships between neighboring row/column values in the model’s parameters.To solve this, we developed **MERIT**, a new training method that:**Controls extreme values** by using a ""max-aware"" approach to adjust updates, preventing attention values from spiking.**Focuses on local attention patterns** in the model’s weights to make updates more precise and stable.In tests with GPT-2 models, MERIT allowed training with larger batch sizes than AdamW and LAMB without sacrificing performance. This means models can be trained faster, accelerating progress in AI development.**Why it matters:**By addressing overlooked details in how training updates are scaled, MERIT improves stability and opens the door to training larger AI models more efficiently—a critical step for advancing technologies like large language models."
Poster,MetaAgent: Automatically Constructing Multi-Agent Systems Based on Finite State Machines,https://ICML.cc//virtual/2025/poster/43677,"Yaolun Zhang, Xiaogeng Liu, Chaowei Xiao","Large Language Models (LLMs) have demonstrated the ability to solve a wide range of practical tasks within multi-agent systems. However, existing human-designed multi-agent frameworks are typically limited to a small set of pre-defined scenarios, while current automated design methods suffer from several limitations, such as the lack of tool integration, dependence on external training data, and rigid communication structures. In this paper, we propose \textbf{MetaAgent}, a  \textbf{finite state machine} based framework that can automatically generate a multi-agent system. Given a task description, MetaAgent will design a multi-agent system and polish it through an optimization algorithm. When the multi-agent system is deployed, the finite state machine will control the agent's actions and the state transitions. To evaluate our framework, we conduct experiments on both text-based tasks and practical tasks. The results indicate that the generated multi-agent system surpasses other auto-designed methods and can achieve a comparable performance with the human-designed multi-agent system, which is optimized for those specific tasks.","Decomposing a complex task, such as software development, and assigning its parts to multiple collaborating AI agents is a paradigm known as a multi-agent system. However, most existing multi-agent systems rely on manual, human design, resulting in fixed collaboration workflows that are limited to predefined scenarios. When an error from a previous step is discovered, these rigid structures often lack the ability to backtrack for corrections.Our research, MetaAgent, is a framework designed to address this challenge by automatically constructing a multi-agent system from a given task description. The core of MetaAgent is its use of a mathematical model called a ""Finite State Machine"" (FSM) to organize the agents. This can be understood as a dynamic blueprint that not only defines which agent is responsible for each stage but also specifies the precise conditions required to transition to the next state.This FSM-based structure provides two key advantages: Firstly, it supports agents that can use external tools, such as a search engine or code interpreter, to interact with the world and solve complex problems. It also enables ""State Traceback,"" a capability allowing the system to return to a previous state to fix issues. For instance, if a tester agent finds a bug in the software, the system can transition back to the programmer agent's state for refinement.Experiments show that systems generated by MetaAgent outperform other automated design methods and achieve performance comparable to—and sometimes exceeding—human-designed systems that were specifically optimized for those tasks. MetaAgent provides an efficient, automated paradigm for creating more powerful and adaptive multi-agent systems, lowering the development barrier and enhancing their ability to solve complex, real-world problems"
Poster,Meta-Black-Box-Optimization through Offline Q-function Learning,https://ICML.cc//virtual/2025/poster/46306,"Zeyuan Ma, Zhiguang Cao, Zhou Jiang, Hongshu Guo, Yue-Jiao Gong","Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated that using RL to learn a meta-level policy for dynamic algorithm configuration (DAC) over an optimization task distribution could significantly enhance the performance of the low-level BBO algorithm. However, the online learning paradigms in existing works makes the efficiency of MetaBBO problematic.To address this, we propose an offline learning-based MetaBBO framework in this paper, termed Q-Mamba, to attain both effectiveness and efficiency in MetaBBO. Specifically, we first transform DAC task into long-sequence decision process. This allows us further introduce an effective Q-function decomposition mechanism to reduce the learning difficulty within the intricate algorithm configuration space. Under this setting, we propose three novel designs to meta-learn DAC policy from offline data: we first propose a novel collection strategy for constructing offline DAC experiences dataset with balanced exploration and exploitation. We then establish a decomposition-based Q-loss that incorporates conservative Q-learning to promote stable offline learning from the offline dataset. To further improve the offline learning efficiency, we equip our work with a Mamba architecture which helps long-sequence learning effectiveness and efficiency by selective state model and hardware-aware parallel scan respectively. Through extensive benchmarking, we observe that Q-Mamba achieves competitive or even superior performance to prior online/offline baselines, while significantly improving the training efficiency of existing online baselines. We provide sourcecodes of Q-Mamba \href{https://github.com/MetaEvo/Q-Mamba}{online}.","We developed a novel way to teach computer how to automatically set hyper-parameters for optimization algorithms. Since existing methods let the computer interact with the optimization algorithms in an online paradigm, their learning efficiency is relatively low. Hence this paper discusses how to make computer capable of learning from historical experience, that is, offline learning with higher efficiency."
Poster,Metadata Conditioning Accelerates Language Model Pre-training,https://ICML.cc//virtual/2025/poster/45996,"Tianyu Gao, Alexander Wettig, Luxi He, Yihe Dong, Sadhika Malladi, Danqi Chen","The vast diversity of styles, domains, and quality levels present in language model pre-training corpora is essential in developing general model capabilities, but efficiently learning and deploying the correct behaviors exemplified in each of these heterogeneous data sources is challenging. To address this, we propose a new method, termed Metadata Conditioning then Cooldown (MeCo), to incorporate additional learning cues during pre-training. MeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the text during training and later uses a cooldown phase with only the standard text, thereby enabling the model to function normally even without metadata. MeCo significantly accelerates pre-training across different model scales (600M to 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For instance, a 1.6B language model trained with MeCo matches the downstream task performance of standard pre-training while using 33% less data. Additionally, MeCo enables us to steer language models by conditioning the inference prompt on either real or fabricated metadata that encodes the desired properties of the output: for example, prepending wikipedia.org to reduce harmful generations or factquizmaster.com (fabricated) to improve common knowledge task performance. We also demonstrate that MeCo is compatible with different types of metadata, such as model-generated topics. MeCo is remarkably simple, adds no computational overhead, and demonstrates promise in producing more capable and steerable language models.","Diversity in data is essential for training versatile language models, but it also poses unique challenges for models to efficiently learn and deploy correct behaviors. We propose a new method, termed Metadata Conditioning then Cooldown (MeCo), to incorporate additional learning cues during pre-training. MeCo first prepends metadata (e.g., URLs like en.wikipedia.org) to the text during training and later uses a cooldown phase with only the standard text, thereby enabling the model to function normally even without metadata. MeCo significantly accelerates pre-training across different model scales and training sources, by up to 33%. Additionally, MeCo enables us to steer language models by conditioning the inference prompt on either real or fabricated URLs that encode the desired properties of the output: for example, prepending factquizmaster.com (fabricated) to improve common knowledge task performance. We also demonstrate that MeCo is compatible with different types of metadata, such as model-generated topics. MeCo is remarkably simple, adds no computational overhead, and demonstrates promise in producing more capable and steerable language models."
Poster,Meta Optimality for Demographic Parity Constrained Regression via Post-Processing,https://ICML.cc//virtual/2025/poster/44330,Kazuto Fukuchi,"We address the regression problem under the constraint of demographic parity, a commonly used fairness definition. Recent studies have revealed fair minimax optimal regression algorithms, the most accurate algorithms that adhere to the fairness constraint. However, these analyses are tightly coupled with specific data generation models. In this paper, we provide meta-theorems that can be applied to various situations to validate the fair minimax optimality of the corresponding regression algorithms. Furthermore, we demonstrate that fair minimax optimal regression can be achieved through post-processing methods, allowing researchers and practitioners to focus on improving conventional regression techniques, which can then be efficiently adapted for fair regression.","Machine learning systems are increasingly used to make important decisions in areas like healthcare, finance, and criminal justice. However, these systems can sometimes treat different groups of people unfairly - for example, showing bias against certain racial groups, genders, or age groups. While researchers have developed methods to make these systems fair, the theoretical understanding of when these methods work best has been limited to specific situations.We developed a new theoretical framework that helps us understand when and how to make machine learning systems fair across a wide range of situations. Our analysis shows that a common approach called post-processing - where we first build a prediction system and then adjust its outputs to ensure fair treatment - can be theoretically proven to be the best possible approach in many more scenarios than previously thought. We also provide guidance on how to construct these post-processing methods to achieve the best possible performance.Our work provides a theoretical foundation that helps practitioners choose and implement the best fairness methods for their specific applications. This is particularly important as machine learning systems become more widespread in society, where unfair treatment can have serious consequences for individuals and communities. Our findings give practitioners more confidence in using established fairness techniques across different applications."
Poster,MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters,https://ICML.cc//virtual/2025/poster/45796,"Arsalan Sharifnassab, Saber Salehkaleybar, Rich Sutton","We address the challenge of optimizing meta-parameters (hyperparameters) in machine learning, a key factor for efficient training and high model performance. Rather than relying on expensive meta-parameter search methods, we introduce MetaOptimize: a dynamic approach that adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that considers the long-term impact of step sizes on training, through a discounted sum of future losses.  We also introduce lower-complexity variants of MetaOptimize that, in  conjunction with its adaptability to various optimization algorithms, achieve performance comparable to those of the best hand-crafted learning rate schedules across diverse machine learning tasks.","Machine learning systems rely on “meta-parameters” — such as how fast a model learns — which can greatly affect performance. Traditionally, researchers manually test many combinations of these settings before training a model, a slow and expensive process.Our research introduces **MetaOptimize**, a method that automatically adjusts these meta-parameters — especially learning rates — as the model trains. Instead of sticking to a fixed schedule, MetaOptimize learns how to update these settings in real time based on how training is going.This dynamic approach reduces the need for costly trial-and-error searches, improves training speed, and adapts better to changing environments, and works across a wide range of machine learning tasks and algorithms — all without the need for manual tuning."
Poster,Meta-Reinforcement Learning with Adaptation from Human Feedback via Preference-Order-Preserving Task Embedding,https://ICML.cc//virtual/2025/poster/45397,"Siyuan Xu, Minghui Zhu","This paper studies meta-reinforcement learning with adaptation from human feedback. It aims to pre-train a meta-model that can achieve few-shot adaptation for new tasks from human preference queries without relying on reward signals. To solve the problem, we propose the framework *adaptation via Preference-Order-preserving EMbedding* (POEM). In the meta-training, the framework learns a task encoder, which maps tasks to a preference-order-preserving task embedding space, and a decoder, which maps the embeddings to the task-specific policies. In the adaptation from human feedback, the task encoder facilitates efficient task embedding inference for new tasks from the preference queries and then obtains the task-specific policy. We provide a theoretical guarantee for the convergence of the adaptation process to the task-specific optimal policy and experimentally demonstrate its state-of-the-art performance with substantial improvement over baseline methods.","This paper introduces a new way to teach AI systems how to quickly adapt to new tasks using feedback from humans instead of complex programming or reward setups. The method helps the AI learn patterns across many training tasks, so that when it faces a new task, it can understand what to do just by comparing options that people prefer. This makes the training process much faster and more efficient, especially in situations where it’s hard to define what success looks like. The approach shows strong results in robotic simulations, performing as well or better than existing methods while using much less human input."
Poster,"Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation",https://ICML.cc//virtual/2025/poster/46591,"Juno Kim, Denny Wu, Jason Lee, Taiji Suzuki","A key paradigm to improve the reasoning capabilities of large language models (LLMs) is to allocate more inference-time compute to search against a verifier or reward model. This process can then be utilized to refine the pretrained model or distill its reasoning patterns into more efficient models. In this paper, we study inference-time computation by viewing chain-of-thought (CoT) generation as a metastable Markov process: easy reasoning steps (e.g., algebraic manipulations) form densely connected clusters, while hard reasoning steps (e.g., applying a relevant theorem) create sparse, low-probability edges between clusters, leading to phase transitions at longer timescales. Under this framework, we prove that implementing a search protocol that rewards sparse edges improves CoT by decreasing the expected number of steps to reach different clusters. In contrast, we establish a limit on reasoning capability when the model is restricted to local information of the pretrained graph. We also show that the information gained by search can be utilized to obtain a better reasoning model: (1) the pretrained model can be directly finetuned to favor sparse edges via policy gradient methods, and moreover (2) a compressed \emph{metastable representation} of the reasoning dynamics can be distilled into a smaller, more efficient model.","A promising way to improve the reasoning ability of large language models (LLMs) is to invest more computation at inference time, for instance searching for better reasoning paths among potential output sequences by receiving rewards. This kind of search can also be used to refine the original model or train smaller, faster ones that inherit its reasoning skills. In this paper, we theoretically study the benefits of inference-time search for chain-of-thought (CoT) reasoning by mathematically modeling the output of the base model as a random process known as a metastable Markov process. Under this model, easy reasoning steps form dense clusters, while harder conceptual leaps (like invoking the right theorem to solve a problem) act as rare transitions between clusters. We show that encouraging these rare transitions can improve the efficiency of CoT search by reducing the time needed to reach new insights. We also demonstrate how to leverage the information gained during this search: the original model can be fine-tuned to favor more insightful steps, and a compressed representation of the search dynamics can be distilled into a smaller model that retains strong reasoning performance."
Poster,MetricEmbedding: Accelerate Metric Nearness by Tropical Inner Product,https://ICML.cc//virtual/2025/poster/46088,"Muyang Cao, Jiajun Yu, Xin Du, Gang Pan, Wei Wang","The Metric Nearness Problem involves restoring a non-metric matrix to its closest metric-compliant form, addressing issues such as noise, missing values, and data inconsistencies. Ensuring metric properties, particularly the  $O(N^3)$ triangle inequality constraints, presents significant computational challenges, especially in large-scale scenarios where traditional methods suffer from high time and space complexity. We propose a novel solution based on the tropical inner product (max-plus operation), which we prove satisfies the triangle inequality for non-negative real matrices. By transforming the problem into a continuous optimization task, our method directly minimizes the distance to the target matrix. This approach not only restores metric properties but also generates metric-preserving embeddings, enabling real-time updates and reducing computational and storage overhead for downstream tasks. Experimental results demonstrate that our method achieves up to 60× speed improvements over state-of-the-art approaches, and efficiently scales from $1e4 \times 1e4$ to $1e5 \times 1e5$ matrices with significantly lower memory usage.","(1) Large-scale data may violate basic metric rules due to noise or missing values, which can degrade the performance of downstream tasks.(2) We propose a new method that uses tropical inner product to restore metric properties while significantly improving processing efficiency.(3) This enables faster handling of large datasets and provides a more reliable foundation for tasks like search and recommendation."
Poster,M+: Extending MemoryLLM with Scalable Long-Term Memory,https://ICML.cc//virtual/2025/poster/45431,"Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogerio Feris, Zexue He","Equipping large language models (LLMs) with latent-space memory has attracted increasing attention as they can extend the context window of existing language models. However, retaining information from the distant past remains a challenge. For example, MemoryLLM (Wang et al., 2024a), as a representative work with latent-space memory, compresses past information into hidden states across all layers, forming a memory pool of 1B parameters. While effective for sequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k tokens. In this work, we address this limitation by introducing M+, a memory-augmented model based on MemoryLLM that significantly enhances long-term information retention. M+ integrates a long-term memory mechanism with a co-trained retriever, dynamically retrieving relevant information during text generation. We evaluate M+ on diverse benchmarks, including long-context understanding and knowledge retention tasks. Experimental results show that M+ significantly outperforms MemoryLLM and recent strong baselines, extending knowledge retention from under 20k to over 160k tokens with similar GPU memory overhead.","Large language models (like ChatGPT) can normally “remember” only what fits in their immediate reading window—roughly a few thousand words. When conversations or documents get longer, older parts fall out of view and the model starts to forget. Researchers have tried to fix this with latent-space memory—special chunks of internal data that act like sticky notes for past information. A recent system called MemoryLLM was one of the first big steps: it compresses earlier text into a huge internal memory so the model can recall up to about 20 000 tokens (roughly a short novel). Our new model, M+, pushes that memory horizon far further. We add:1. Long-term memory slots that keep important facts around permanently.2. A built-in “retriever” that knows how to fetch the right memory at the right moment while the model is writing.Together these upgrades let M+ keep track of more than 160 000 tokens—eight times farther than before—without using extra GPU space. In tests that probe long-document understanding and fact recall, M+ consistently beats MemoryLLM and other state-of-the-art methods, showing it can hold on to distant details while remaining just as efficient to run."
