type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression,https://ICML.cc//virtual/2025/poster/45253,"Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov","Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400×, end-to-end speedup of up to 3.7× as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme.","Large language models (LLMs), like ChatGPT, are excellent at generating human-like text but struggle when handling very lengthy documents. Their main issue arises from the need to remember extensive past information, causing them to slow down as text generation continues. To overcome this challenge, we developed a training-free technique called RocketKV. RocketKV compresses the memory requirement in two consecutive stages. In the first stage, it permanently removes information determined to be less important, significantly reducing memory usage right away. In the second stage, RocketKV carefully selects the most relevant pieces of the remaining memory based on the current query, dynamically deciding what to prioritize at each text generation step. This two-stage strategy ensures that the model can efficiently maintain essential information without becoming overwhelmed by irrelevant details. By intelligently combining permanent pruning with dynamic selection, RocketKV dramatically reduces memory needs by up to 400 times and speeds up by 3.7 times when running on an Nvidia A100 GPU. This innovation allows LLMs to handle long conversations and extensive documents more effectively, making advanced language technologies more practical and accessible in everyday applications."
Poster,RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer,https://ICML.cc//virtual/2025/poster/45303,"Haotian Ni, Yake Wei, Hang Liu, Gong Chen, Chong Peng, Hao Lin, Di Hu","Multimodal learning faces challenges in effectively fusing information from diverse modalities, especially when modality quality varies across samples. Dynamic fusion strategies, such as attention mechanism in Transformers, aim to address such challenge by adaptively emphasizing modalities based on the characteristics of input data. However, through amounts of carefully designed experiments, we surprisingly observed that the dynamic adaptability of widely-used self-attention models diminishes. Model tends to prefer one modality regardless of data characteristics. This bias triggers a self-reinforcing cycle that progressively overemphasizes the favored modality, widening the distribution gap in attention keys across modalities and deactivating attention mechanism's dynamic properties. To revive adaptability, we propose a simple yet effective method Rolling Query (RollingQ), which balances attention allocation by rotating the query to break the self-reinforcing cycle and mitigate the key distribution gap. Extensive experiments on various multimodal scenarios validate the effectiveness of RollingQ and the restoration of cooperation dynamics is pivotal for enhancing the broader capabilities of widely deployed multimodal Transformers. The source code is available at https://github.com/GeWu-Lab/RollingQ_ICML2025.","Multimodal learning focus on combining information from different types of data, like images, text, or sound. A major challenge is making sure the system can effectively handle and combine these different types, especially when the quality of the data varies. Many models, such as those using Transformers with attention mechanisms, try to tackle this by adaptively emphasizing modalities based on the characteristics of input data.However, we discovered that these models can sometimes get stuck in a pattern, where the model starts favoring one type of data too much, regardless of data characteristics. This creates a ""self-reinforcing cycle"" where the model progressively overemphasizes the favored modality, making it less adaptable and limiting its ability to handle different inputs.To fix this issue, we proposed a new approach called Rolling Query (RollingQ). This method helps balance the model's attention across different types of data by rotating its focus, breaking the self-reinforcing cycle, and ensuring the system works better overall. We tested RollingQ across various scenarios and found that it significantly improves the model’s performance and ability to combine different data types effectively."
Poster,Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction,https://ICML.cc//virtual/2025/poster/45769,"Vaishnavh Nagarajan, Chen Wu, Charles Ding, Aditi Raghunathan","We design a suite of minimal algorithmic tasks that are a loose abstraction of _open-ended_ real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model.Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended _stochastic_ planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns  (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; multi-token approaches, namely teacherless training and diffusion models, comparatively excel in producing diverse and original output.  Secondly, to elicit randomness without hurting coherence, we find that injecting noise at the input layer (dubbed _seed-conditioning_) works surprisingly as well as (and in some conditions, better than) temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and temperature sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity","In the near future, we hope to use AI for doing science and for generating fresh data (like fresh math problems) to train other AI models. Unlike today's benchmark tasks, these are ""open-ended""  tasks where you want the model to explore new and diverse ways responding --- like a scientist would.So, how well are AI models suited for these open-ended tasks? This is very hard to answer because judging metrics like diversity and novelty is subjective! But here's how we do it: we design _simple_ open-ended tasks, where we can quantify how different factors of the tasks/the models affect these metrics -- very much like how a Galileo or a Newton would choose simple objects like spheres and pendulums to study the effect of mass, gravity, initial velocity, angle of throw, and so on. Our tasks are inspired by day-to-day creative tasks that require a random ""eureka"" moment. In these tasks, we study two types of creativity, one that requires combining pieces of knowledge (like in wordplay) and another that requires designing clever constructions (like puzzle-design or story-design). In these tasks, we play around with language models and find two different things:1. Currently, we teach models by making them predict one word after the other.  Instead, we want to teach the model to predict many words in advance so it actually learns the big picture of such creative tasks! This is what we mean by **Look before you leap**.2. Currently, under the hood, AI models first figure out many possible responses, and only then randomly select one response to provide. This is too much wasted work! Instead, we argue the model should fix its random decision(s) first, and then work out just one single response and produce it. This is what we mean by **Roll the dice before you leap**.Overall, we design simple settings that quantify the limits of language models in two distinct types of creativity. This allows future work to have greater clarity on how to evaluate open-ended thinking, and explore our algorithmic ideas and findings in real-world settings."
Poster,ROME is Forged in Adversity: Robust Distilled Datasets via Information Bottleneck,https://ICML.cc//virtual/2025/poster/44781,"Zheng Zhou, Wenquan Feng, Qiaosheng Zhang, Shuchang Lyu, Qi Zhao, Guangliang Cheng","Dataset Distillation (DD) compresses large datasets into smaller, synthetic subsets, enabling models trained on them to achieve performance comparable to those trained on the full data. However, these models remain vulnerable to adversarial attacks, limiting their use in safety-critical applications. While adversarial robustness has been extensively studied in related fields, research on improving DD robustness is still limited. To address this, we propose ROME, a novel method that enhances the adversarial RObustness of DD by leveraging the InforMation BottlenEck (IB) principle. ROME includes two components: a performance-aligned term to preserve accuracy and a robustness-aligned term to improve robustness by aligning feature distributions between synthetic and perturbed images. Furthermore, we introduce the Improved Robustness Ratio (I-RR), a refined metric to better evaluate DD robustness. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate that ROME outperforms existing DD methods in adversarial robustness, achieving maximum I-RR improvements of nearly 40% under white-box attacks and nearly 35% under black-box attacks. Our code is available at https://github.com/zhouzhengqd/ROME.","Training modern machine learning models often requires large datasets, which can be expensive and difficult to collect. A popular solution is dataset distillation, where small synthetic datasets are created to help models learn efficiently. However, models trained on these synthetic datasets are often vulnerable to small but harmful changes in input data, known as adversarial attacks. This research presents ROME, a method that improves the reliability of models trained on distilled data. ROME uses ideas from information theory to keep the most useful parts of the data while reducing irrelevant noise. This allows the model to stay accurate even when the inputs are slightly changed in a malicious way. By improving both efficiency and robustness, ROME helps make machine learning systems more practical and secure in real-world scenarios such as autonomous driving and medical diagnosis."
Poster,ROPO: Robust Preference Optimization for Large Language Models,https://ICML.cc//virtual/2025/poster/46424,"Xize Liang, Chao Chen, Shuang Qiu, Jie Wang, Yue Wu, Zhihang Fu, Hanzhu Chen, Feng Wu, Jieping Ye","The prevalent noise in the preference data unavoidably poses significant challenges to the preference alignment of large language models (LLMs). Existing efforts for this problem either marginally alleviate the impact of noise without noise reduction, or rely on external LLMs that incur substantial computational costs. To address these challenges, we propose **RO**bust **P**reference **O**ptimization (**ROPO**), an iterative alignment approach that integrates *noise-tolerance* and *noise filtering* without the aid of external models. Specifically, ROPO first formulates the training process with adaptive noise reduction as an optimization problem, which can be efficiently solved in an iterative paradigm. Then, to equip this solving process with noise-tolerance and noise-identification capabilities, we derive a robust loss that suppresses the gradients from samples with high uncertainty. We demonstrate both empirically and theoretically that the derived loss is key to the noise-tolerance and effective filtering of noisy samples. The derived loss further inspires a robustness-guided rejection sampling technique to compensate for the potential important information in discarded queries. Extensive experiments on several widely-used datasets and model architectures demonstrate that ROPO significantly outperforms all baselines under **four** practical noise settings and the random symmetric noise, with its advantage increasing as the noise rate increases.","Large language models (LLMs) often learn from human preference data, but this data is frequently noisy---containing mistakes or inconsistencies---which makes it hard to align models effectively. Existing solutions either ignore the noise or rely on expensive external models to clean it. Our research introduces ROPO, a new method that trains LLMs to handle noisy preference data efficiently and accurately, without using any external models. ROPO combines two key ideas: it actively filters out noisy data during training, and it reduces the influence of uncertain samples through a robust loss function. This loss helps the model focus on reliable signals while still preserving potentially useful information from filtered data through a guided sampling strategy. ROPO works iteratively, refining the model and the data quality at the same time. We prove that our approach is both theoretically sound and practically effective. Experiments show that ROPO consistently outperforms existing methods, especially when the noise is severe. This makes ROPO a valuable tool for training more reliable and efficient LLMs in real-world settings."
Poster,ROS: A GNN-based Relax-Optimize-and-Sample Framework for Max-$k$-Cut Problems,https://ICML.cc//virtual/2025/poster/45182,"Yeqing Qiu, Ye XUE, Akang Wang, Yiheng Wang, Qingjiang Shi, Zhiquan Luo","The Max-$k$-Cut problem is a fundamental combinatorial optimization challenge that generalizes the classic $\mathcal{NP}$-complete Max-Cut problem. While relaxation techniques are commonly employed to tackle Max-$k$-Cut, they often lack guarantees of equivalence between the solutions of the original problem and its relaxation. To address this issue, we introduce the Relax-Optimize-and-Sample (ROS) framework. In particular, we begin by relaxing the discrete constraints to the continuous probability simplex form. Next, we pre-train and fine-tune a graph neural network model to efficiently optimize the relaxed problem. Subsequently, we propose a sampling-based construction algorithm to map the continuous solution back to a high-quality Max-$k$-Cut solution. By integrating geometric landscape analysis with statistical theory, we establish the consistency of function values between the continuous solution and its mapped counterpart. Extensive experimental results on random regular graphs and the Gset benchmark demonstrate that the proposed ROS framework effectively scales to large instances with up to $20,000$ nodes in just a few seconds, outperforming state-of-the-art algorithms. Furthermore, ROS exhibits strong generalization capabilities across both in-distribution and out-of-distribution instances, underscoring its effectiveness for large-scale optimization tasks.","Dividing a network into groups to maximize the number of connections between them is a classic and hard problem in computer science, known as the Max-k-Cut problem. Solving it directly is computationally difficult, especially for large graphs.Our method, called Relax-Optimize-and-Sample (ROS), offers a new way to tackle this challenge. First, we soften the original problem into a continuous version that’s easier to handle mathematically. Then, we train a graph neural network to find a good solution to this relaxed problem. Finally, we convert the solution back into a valid grouping of the original network.We also show—using ideas from geometry and statistics—that the relaxed and original solutions are closely connected. ROS can handle graphs with tens of thousands of nodes in just seconds, beating the best existing methods. Even better, it works well on new kinds of graphs it hasn't seen before, making it a powerful tool for large-scale optimization problems in network design, clustering, and more."
Poster,RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models,https://ICML.cc//virtual/2025/poster/44438,"Quan Wei, Chung-Yiu Yau, Hoi To Wai, Yang Zhao, Dongyeop Kang, Youngsuk Park, Mingyi Hong","Supervised fine-tuning is a standard method for adapting pre-trained large language models (LLMs) to downstream tasks. Quantization has been recently studied as a post-training technique for efficient LLM deployment. To obtain quantized fine-tuned LLMs, conventional pipelines would first fine-tune the pre-trained models, followed by post-training quantization. This often yields suboptimal performance as it fails to leverage the synergy between fine-tuning and quantization. To effectively realize low-bit quantization of weights, activations and KV caches in LLMs, we propose an algorithm named Rotated Straight-Through-Estimator (RoSTE), which combines quantization-aware supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that identifies an effective rotation configuration to reduce activation outliers. We provide theoretical insights on RoSTE by analyzing its prediction error when applied to an overparameterized least square quantized training problem. Our findings reveal that the prediction error is directly proportional to the quantization error of the converged weights, which can be effectively managed through an optimized rotation configuration. Experiments on Pythia, Qwen and Llama models of different sizes demonstrate the effectiveness of RoSTE. Compared to existing post-SFT quantization baselines, our method consistently achieves superior performances across various tasks and different LLM architectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE.","Large language models like ChatGPT are powerful but require a lot of memory and computing power, making them difficult to use on smaller devices. One way to make these models more efficient is through quantization, which compresses the model by using fewer bits to store numbers. However, quantizing a model after it’s already trained often hurts its performance.Our research introduces a smarter way: we fine-tune and compress the model at the same time using a new method called RoSTE. RoSTE learns how to rotate and adjust the model’s data in a way that reduces errors caused by compression. This makes the model smaller and faster while keeping its performance high.We tested RoSTE on popular models and tasks and found that it consistently worked better than existing compression methods. This approach helps make AI models more accessible and usable on a wider range of devices."
Poster,rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,https://ICML.cc//virtual/2025/poster/46400,"Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang","We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of  OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising ``deep thinking'' through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\""ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0%, surpassing o1-preview by +4.5%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% of the brightest high school math students. Code and data are available at https://github.com/microsoft/rStar.","rStar-Math shows that small AI models can match or outperform larger ones like OpenAI’s o1 in solving challenging math problems, without learning from them directly. It uses a technique called Monte Carlo Tree Search (MCTS) to explore different solution paths before choosing the best one. rStar-Math trains two small models: one for generating step-by-step solutions and another for scoring them. It introduces three key innovations: high-quality data generation through code-augmented rollouts, a more effective scoring model without step-by-step labels, and a self-improvement loop where both models evolve together. After several rounds of this self-improvement process, rStar-Math reaches state-of-the-art results. On the difficult MATH benchmark, it reaches the same level as OpenAI’s o1. On real USA Math Olympiad (AIME) exams, it solves 8 out of 15 problems, placing it in the top 20% of top-performing high school students."
Poster,RuleAdapter: Dynamic Rules for training Safety Reward Models in RLHF,https://ICML.cc//virtual/2025/poster/43772,"Xiaomin Li, Mingye Gao, Zhiwei Zhang, Jingxuan Fan, Weiyu Li","Reinforcement Learning from Human Feedback (RLHF) is widely used to align models with human preferences, particularly to enhance the safety of responses generated by LLMs. This method traditionally relies on choosing preferred responses from response pairs. However, due to variations in human opinions and the difficulty of making an overall comparison of two responses, there is a growing shift towards a fine-grained annotation approach, assessing responses based on multiple specific metrics or rules. Selecting and applying these rules efficiently while accommodating the diversity of preference data remains a significant challenge. In this paper, we introduce a dynamic approach that adaptively selects the most critical rules for each pair of responses. We develop a mathematical framework that leverages the maximum discrepancy between each paired responses and theoretically show that this strategy optimizes the mutual information between the rule-based labeling and the hidden ground-truth preferences. We then train an 8B reward model using the adaptively labeled preference dataset and evaluate its performance on RewardBench. As of May 25, 2025, our model achieved the highest safety performance on the leaderboard, outperforming various larger models.","Training AI systems to follow human values and avoid harmful behavior is a major challenge. A popular approach is to train these systems using human preferences — for example, by showing them two answers and picking the safer one. But people often disagree, and it's hard to explain what exactly makes one answer better than another.Our research proposes a better way: instead of comparing answers directly, we judge them based on specific safety rules — like avoiding misinformation or harmful advice — and select the most relevant rules for each situation. We built a system, called the Rule Adapter, that picks the five most important rules for any given example, focusing on the biggest differences between the answers. This makes the training process more efficient and more interpretable.We used this method to train a new AI safety model, which now ranks No.1 on a public leaderboard — beating many much larger models. This approach could make it easier to train safer and more responsible AI systems, while reducing the need for expensive human input."
Poster,RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning,https://ICML.cc//virtual/2025/poster/43712,"Jason Chan, Robert Gaizauskas, Zhixue Zhao","Formal logic enables computers to reason in natural language by representing sentences in symbolic forms and applying rules to derive conclusions. However, in what our study characterizes as ""rulebreaker"" scenarios, this method can lead to conclusions that are typically not inferred or accepted by humans given their common sense and factual knowledge. Inspired by works in cognitive science, we create RULEBREAKERS, the first dataset for rigorously evaluating the ability of large language models (LLMs) to recognize and respond to rulebreakers (versus non-rulebreakers) in a knowledge-informed and human-like manner. Evaluating seven LLMs, we find that most models achieve mediocre accuracy on RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules, unlike what is expected from typical human reasoners. Further analysis suggests that this apparent failure is potentially associated with the models' poor utilization of their world knowledge and their attention distribution patterns. Whilst revealing a limitation of current LLMs, our study also provides a timely counterbalance to a growing body of recent works that propose methods relying on formal logic to improve LLMs' general reasoning capabilities, highlighting their risk of further increasing divergence between LLMs and human-like reasoning.","Humans use common sense and real-world knowledge in everyday reasoning. This flexibility sets us apart from systems that rely strictly on formal logic. For example, suppose we are told that ""Anne is either in Stockholm or somewhere in Sweden"" but we find out in fact that ""Anne is not in Sweden”. Common sense would tell us that she is not in Stockholm either, because Stockholm is in Sweden. By contrast, simply applying a logical rule, “A or B; not B; therefore, A”, would lead to a counterintuitive conclusion that “Anne is in Stockholm”. We call these situations “rulebreakers”, where rigidly applying logic leads to conclusions that contradict common sense and real-world knowledge. Do large language models (LLMs) reason flexibly like humans in these situations? To find out, we create a large dataset of rulebreakers to test these models. We find that most LLMs struggle to do so: they tend to over-rigidly apply logical rules and accept conclusions that contradict common sense and real-world knowledge. This highlights not only a shortcoming of current LLMs, but also an important pitfall in guiding them to reason more “logically”, as it could further undermine their ability to reason in a flexible and human-like manner."
