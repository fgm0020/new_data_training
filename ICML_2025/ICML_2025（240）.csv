type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Provable and Practical Online Learning Rate Adaptation with Hypergradient Descent,https://ICML.cc//virtual/2025/poster/45486,"Ya-Chi Chu, Wenzhi Gao, Yinyu Ye, Madeleine Udell","This paper investigates the convergence properties of the hypergradient descent method ($\texttt{HDM}$), a 25-year-old heuristic originally proposed for adaptive stepsize selection in stochastic first-order methods. We provide the first rigorous convergence analysis of $\texttt{HDM}$ using the online learning framework and apply this analysis to develop a new state-of-the-art adaptive gradient methods with empirical and theoretical support. Notably, $\texttt{HDM}$ automatically identifies the optimal stepsize for the local optimization landscape and achieves local superlinear convergence. Our analysis explains the instability of $\texttt{HDM}$ reported in the literature and proposes efficient strategies to address it. We also develop two $\texttt{HDM}$ variants with heavy-ball and Nesterov momentum. Experiments on deterministic convex problems show  $\texttt{HDM}$ with heavy-ball momentum ($\texttt{HDM-HB}$) exhibits robust performance and significantly outperforms other adaptive first-order methods. Moreover, $\texttt{HDM-HB}$ often matches the performance of $\texttt{L-BFGS}$, an efficient and practical quasi-Newton method, using less memory and cheaper iterations.","Learning rate selection is a critical hyperparameter that significantly influences the convergence speed of first-order optimization algorithms, yet adaptively choosing it can be challenging. We developed an efficient learning rate update strategy based on online learning, proved its convergence guarantees, and investigate its convergence behavior. Our analysis resolves a 25-year-old heuristic that was proposed for adaptive step size selection. The resulting algorithm significantly outperforms the state-of-the-art."
Poster,Provable Benefit of Random Permutations over Uniform Sampling in Stochastic Coordinate Descent,https://ICML.cc//virtual/2025/poster/45653,"Donghwa Kim, Jaewook Lee, Chulhee Yun","We analyze the convergence rates of two popular variants of coordinate descent (CD): random CD (RCD), in which the coordinates are sampled uniformly at random, and random-permutation CD (RPCD), in which random permutations are used to select the update indices. Despite abundant empirical evidence that RPCD outperforms RCD in various tasks, the theoretical gap between the two algorithms’ performance has remained elusive. Even for the benign case of positive-definite quadratic functions with permutation-invariant Hessians, previous efforts have failed to demonstrate a provable performance gap between RCD and RPCD. To this end, we present novel results showing that, for a class of quadratics with permutation-invariant structures, the contraction rate upper bound for RPCD is always strictly smaller than the contraction rate lower bound for RCD for every individual problem instance. Furthermore, we conjecture that this function class contains the worst-case examples of RPCD among all positive-definite quadratics. Combined with our RCD lower bound, this conjecture extends our results to the general class of positive-definite quadratic functions.","Many machine learning methods solve optimization problems by updating one variable at a time—this is called coordinate descent. One popular version picks a random coordinate at each step independently. Another uses a random order (a permutation) of all variables, updating them one by one. While this second method is often faster in practice, we didn’t have a solid mathematical explanation for why this is the case.Our work shows, for the first time, that this permutation-based method is provably faster in certain problem instances. We studied a particular type of well-behaved objective functions for which we have proved that the permutation approach always converges faster than the independently random version. We also suggest evidence that these cases are likely the worst-case examples, meaning that similar benefits might also be ensured for a more general class of problems.These results help us better understand how randomness can speed up optimization and may lead to faster algorithms in machine learning systems used in large-scale applications."
Poster,Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models,https://ICML.cc//virtual/2025/poster/44817,"Taj Jones-McCormick, Aukosh Jagannath, Subhabrata Sen","Unsupervised pre-training and transfer learning are commonly used techniques to initialize training algorithms for neural networks, particularly in settings with limited labeled data. In this paper, we study the effects of unsupervised pre-training and transfer learning on the sample complexity of high-dimensional supervised learning. Specifically, we consider the problem of training a single-layer neural network via online stochastic gradient descent. We establish that pre-training and transfer learning (under concept shift) reduce sample complexity by polynomial factors (in the dimension) under very general assumptions. We also uncover some surprising settings where pre-training grants exponential improvement over random initialization in terms of sample complexity.","Pre-training and transfer learning are important concepts in machine learning that are used widely in practice. Despite their use, these concepts are not well understood theoretically. In this paper we provide rigorous justification for the use of pre-training and transfer learning. In particular, we consider a common statistical model of interest in high dimensional probability (the single index model) under which parameters can be estimated with Stochastic Gradient Descent. We consider pre-training and transfer learning scenarios which one may use to provide initial parameter estimates. We prove for a class of single index models, parameter estimation can be achieved with significantly lower sample complexity then with random initialization."
Poster,Provable Efficiency of Guidance in Diffusion Models for General Data Distribution,https://ICML.cc//virtual/2025/poster/45240,"Gen Li, Yuchen Jiao","Diffusion models have emerged as a powerful framework for generative modeling, with guidance techniques playing a crucial role in enhancing sample quality. Despite their empirical success, a comprehensive theoretical understanding of the guidance effect remains limited. Existing studies only focus on case studies, where the distribution conditioned on each class is either isotropic Gaussian or supported on a one-dimensional interval with some extra conditions. How to analyze the guidance effect beyond these case studies remains an open question. Towards closing this gap, we make an attempt to analyze diffusion guidance under general data distributions. Rather than demonstrating uniform sample quality improvement, which does not hold in some distributions, we prove that guidance can improve the whole sample quality, in the sense that the ratio of bad samples (measured by the classifier probability) decreases in the presence of guidance. This aligns with the motivation of introducing guidance.","Diffusion models are a new type of artificial intelligence tool that can generate realistic images, text, or other data. To make the generated results more accurate or aligned with a goal—like drawing a specific kind of animal—researchers often add something called guidance to steer the model in the right direction. Despite their empirical success, a comprehensive theoretical understanding of the guidance effect remains limited.  Existing studies only focus on some simple situations, which don't reflect the wide variety of data we see in the real world. Towards closing this gap, we make an attempt to analyze diffusion guidance under general data distributions. Rather than demonstrating uniform sample quality improvement, which does not hold in some distributions, we prove that guidance can improve the whole sample quality, in the sense that the ratio of bad samples (measured by the classifier probability) decreases in the presence of guidance. This aligns with the motivation of introducing guidance."
Poster,Provable In-Context Vector Arithmetic via Retrieving Task Concepts,https://ICML.cc//virtual/2025/poster/45998,"Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Qingfu Zhang, Hau-San Wong, Taiji Suzuki","In-context learning (ICL) has garnered significant attention for its ability to grasp functions/tasks from demonstrations. Recent studies suggest the presence of a latent **task/function vector** in LLMs during ICL. Merullo et al. (2024) showed that LLMs leverage this vector alongside the residual stream for Word2Vec-like vector arithmetic, solving factual-recall ICL tasks. Additionally, recent work empirically highlighted the key role of Question-Answer data in enhancing factual-recall capabilities. Despite these insights, a theoretical explanation remains elusive. To move one step forward, we propose a theoretical framework building on empirically grounded *hierarchical* concept modeling. We develop an optimization theory, showing how nonlinear residual transformers trained via gradient descent on cross-entropy loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss convergence and show the strong generalization, including robustness to concept recombination and distribution shifts. These results elucidate the advantages of transformers over static embedding predecessors. Empirical simulations corroborate our theoretical insights.","Large language models (LLMs) can infer the underlying task or function from a few question-answer (QA) demostration pairs in a prompt and apply it to a new query — a capability known as in-context learning (ICL). But how do they actually do this? Recent work suggests that LLMs form internal “task vectors” representing the function to be performed, and for factual recall tasks, they solve ICL problems using simple vector arithmetic — reminiscent of how Word2Vec worked with static word embeddings.We build on these observations and offer the first mathematical framework that explains how this vector-based mechanism works. Based on recent insights into how transformers represent hierarchical concepts, we show that when trained via gradient descent on question–answer data using cross-entropy loss, transformers can retrieve task vectors and perform factual recall through vector addition within their residual pathways. Our analysis further reveals a key difference in training data: while training on ICL-style data often leads to harmful overfitting on low-level task-specific patterns, QA data helps the model better capture the high-level task itself. This provides a theoretical explanation for why QA data has been especially effective in enhancing factual recall abilities in LLMs.Our theory not only explains this emergent behavior but also demonstrates strong generalization across concept recombination and distribution shifts, outperforming static embedding models. These results serve as preliminary steps to theoretically justify emerging practices using task vector arithmetics such as concept erasure, alleviating forgetting , model editing and merging."
Poster,Provable Length Generalization in Sequence Prediction via Spectral Filtering,https://ICML.cc//virtual/2025/poster/46169,"Annie Marsden, Evan Dogariu, Naman Agarwal, Xinyi Chen, Daniel Suo, Elad Hazan",We consider the problem of length generalization in sequence prediction. We define a new metric of performance in this setting – the Asymmetric-Regret– which measures regret against a benchmark predictor with longer context length than available to the learner. We continue by studying this concept through the lens of the spectral filter-ing algorithm. We present a gradient-based learn-ing algorithm that provably achieves length generalization for linear dynamical systems. We conclude with proof-of-concept experiments which are consistent with our theory.,"Many modern machine learning problems involve making predictions using a history of past observations (text generation from an LLM, weather prediction, etc.). A desirable property for an algorithm is the ability to effectively use longer histories than those that are seen in the training data, which is referred to as “length generalization”. We investigate this question theoretically through the lens of a powerful framework for describing algorithmic learning, known as online learning.In our paper, we first introduce a notion – the Asymmetric-Regret – to concretely and mathematically define what it means for an algorithm to exhibit length generalization. We then look at the linear dynamical system (LDS): a simple but general problem which (1) allows provable learning algorithms and (2) teaches us about algorithmic properties that hold on more complicated problems. For our main result, we prove that a particular method for learning an LDS, which is known as spectral filtering and is currently being used as a layer in modern deep neural networks for sequential data, length generalizes without modification.This work provides a natural language to describe length generalization of learning algorithms. Furthermore, the length generalization of spectral filtering is surprising and indicative of its usefulness in neural networks."
Poster,Provable Maximum Entropy Manifold Exploration via Diffusion Models,https://ICML.cc//virtual/2025/poster/45982,"Riccardo De Santi, Marin Vlastelica, Ya-Ping Hsieh, Zebang Shen, Niao He, Andreas Krause","Exploration is critical for solving real-world decision-making problems such as scientific discovery, where the objective is to generate truly novel designs rather than mimic existing data distributions.  In this work, we address the challenge of leveraging the representational power of generative models for exploration without relying on explicit uncertainty quantification. We introduce a novel framework that casts exploration as entropy maximization over the approximate data manifold implicitly defined by a pre-trained diffusion model. Then, we present a novel principle for exploration based on density estimation, a problem well-known to be challenging in practice. To overcome this issue and render this method truly scalable, we leverage a fundamental connection between the entropy of the density induced by a diffusion model and its score function. Building on this, we develop an algorithm based on mirror descent that solves the exploration problem as sequential fine-tuning of a pre-trained diffusion model. We prove its convergence to the optimal exploratory diffusion model under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we empirically evaluate our approach on both synthetic and high-dimensional text-to-image diffusion, demonstrating promising results.","Modern generative models, like diffusion models, excel at reproducing patterns from their training data, but they struggle to venture into truly novel regions of valid designs. For tasks such as discovering new molecules or materials, it’s crucial not just to mimic known examples but to systematically explore under-sampled, potentially groundbreaking areas of the design space. We recast “exploration” as maximizing how uniformly samples cover the space (“manifold”) of valid designs learned by a pre-trained diffusion model. By iteratively adapting a pre-trained diffusion model in a specific manner, our method pushes the model toward one that spreads its samples across the space of valid structures. This work outlines a practical, theory-backed approach for encouraging diffusion models to explore more broadly across their learned design spaces. While additional empirical validation is needed to assess its impact on real-world discovery tasks, this method could, for example, be applied to molecule or material design—potentially uncovering under-explored but valid regions that standard models might miss"
Poster,Provable Policy Gradient for Robust Average-Reward MDPs  Beyond Rectangularity,https://ICML.cc//virtual/2025/poster/44551,"Qiuhao Wang, Yuqi Zha, Chin Pang Ho, Marek Petrik","Robust Markov Decision Processes (MDPs) offer a promising framework for computing reliable policies under model uncertainty. While policy gradient methods have gained increasing popularity in robust discounted MDPs, their application to the average-reward criterion remains largely unexplored. This paper proposes a Robust Projected Policy Gradient (RP2G), the first generic policy gradient method for robust average-reward MDPs (RAMDPs) that is applicable beyond the typical rectangularity assumption on transition ambiguity. In contrast to existing robust policy gradient algorithms, RP2G incorporates an adaptive decreasing tolerance mechanism for efficient policy updates at each iteration. We also present a comprehensive convergence analysis of RP2G for solving ergodic tabular RAMDPs. Furthermore, we establish the first study of the inner worst-case transition evaluation problem in RAMDPs, proposing two gradient-based algorithms tailored for rectangular and general ambiguity sets, each with provable convergence guarantees. Numerical experiments confirm the global convergence of our new algorithm and demonstrate its superior performance.","Many real-world applications — such as supply chains or autonomous vehicles — require making reliable decisions over time and achieving strong long-term performance. However, the dynamics of the environment are often uncertain or not fully known, which makes it difficult to find reliable strategies.To address this challenge, we propose a new learning algorithm, RP2G, which computes reliable long-term strategies under uncertainty. The algorithm includes two variants designed for different structural assumptions about the unknown environment. In addition, we introduce an adaptive update rule that improves learning efficiency over time.Our method is supported by theoretical guarantees and shows strong performance in experiments. It offers a general solution framework for making reliable long-term strategies when the environment is uncertain or only partially known, and can adapt to different structural assumptions."
Poster,Provable Zero-Shot Generalization in Offline Reinforcement Learning,https://ICML.cc//virtual/2025/poster/46618,"Zhiyong Wang, Chen Yang, John C. S. Lui, Dongruo Zhou","In this work, we study offline reinforcement learning (RL) with zero-shot generalization property (ZSG), where the agent has access to an offline dataset including experiences from different environments, and the goal of the agent is to train a policy over the training environments which performs well on test environments without further interaction. Existing work showed that classical offline RL fails to generalize to new, unseen environments. We propose pessimistic empirical risk minimization (PERM) and pessimistic proximal policy optimization (PPPO), which leverage pessimistic policy evaluation to guide policy learning and enhance generalization. We show that both PERM and PPPO are capable of finding a near-optimal policy with ZSG. Our result serves as a first step in understanding the foundation of the generalization phenomenon in offline reinforcement learning.","Imagine trying to teach a robot to cook using only videos of people working in many different kitchens—but once training ends, the robot must work flawlessly in a brand-new kitchen it has never seen. That, in a nutshell, is the problem we tackle. Today’s “offline” reinforcement-learning methods can study past experiences but often over-fit to the training kitchens and stumble in new ones.We propose pessimistic empirical risk minimization (PERM) and pessimistic proximal policy optimization (PPPO), which leverage pessimistic policy evaluation to guide policy learning and enhance generalization. We show that both PERM and PPPO are capable of finding a near-optimal policy in this generalization setting. Our result serves as a first step in understanding the foundation of the generalization phenomenon in offline reinforcement learning."
Poster,Provably Cost-Sensitive Adversarial Defense via Randomized Smoothing,https://ICML.cc//virtual/2025/poster/46025,"Yuan Xin, Dingfan Chen, Michael Backes, Xiao Zhang","As machine learning models are deployed in critical applications, robustness against adversarial perturbations is crucial. While numerous defensive algorithms have been proposed to counter such attacks, they typically assume that all adversarial transformations are equally important, an assumption that rarely aligns with real-world applications. To address this, we study the problem of robust learning against adversarial perturbations under cost-sensitive scenarios, where the potential harm of different types of misclassifications is encoded in a cost matrix. Our solution introduces a provably robust learning algorithm to certify and optimize for cost-sensitive robustness,  building on the scalable certification framework of randomized smoothing. Specifically, we formalize the definition of cost-sensitive certified radius and propose our novel adaptation of the standard certification algorithm to generate tight robustness certificates tailored to any cost matrix.  In addition, we design a robust training method that improves certified cost-sensitive robustness without compromising model accuracy. Extensive experiments on benchmark datasets, including challenging ones unsolvable by existing methods, demonstrate the effectiveness of our certification algorithm and training method across various cost-sensitive scenarios.","Adversarial robustness is essential as machine learning models are increasingly deployed in critical settings. However, existing robust learning methods typically assume all adversarial perturbations and misclassifications are equally costly, which rarely matches real-world needs. To address this, we study robust learning under cost-sensitive scenarios, where the varying potential harm of different misclassifications is described by a cost matrix. We propose a provably robust learning algorithm that certifies and optimizes cost-sensitive robustness, building on the scalable framework of randomized smoothing. Specifically, we formalize the concept of a cost-sensitive certified radius and adapt the standard certification algorithm to generate tight robustness certificates tailored for any given cost matrix. Furthermore, we design a robust training method that directly improves certified cost-sensitive robustness while maintaining model accuracy. Our experiments on benchmark datasets—including challenging cases unsolvable by prior approaches—demonstrate that our certification and training methods consistently achieve higher cost-sensitive robustness across a range of realistic scenarios."
