type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Bayesian Basis Function Approximation for Scalable Gaussian Process Priors in Deep Generative Models,https://ICML.cc//virtual/2025/poster/44629,"Mehmet Yiğit Balık, Maksim Sinelnikov, Priscilla Ong, Harri Lähdesmäki","High-dimensional time-series datasets are common in domains such as healthcare and economics. Variational autoencoder (VAE) models, where latent variables are modeled with a Gaussian process (GP) prior, have become a prominent model class to analyze such correlated datasets. However, their applications are challenged by the inherent cubic time complexity that requires specific GP approximation techniques, as well as the general challenge of modeling both shared and individual-specific correlations across time. Though inducing points enhance GP prior VAE scalability, optimizing them remains challenging, especially since discrete covariates resist gradient‑based methods. In this work, we propose a scalable basis function approximation technique for GP prior VAEs that mitigates these challenges and results in linear time complexity, with a global parametrization that eliminates the need for amortized variational inference and the associated amortization gap, making it well-suited for conditional generation tasks where accuracy and efficiency are crucial. Empirical evaluations on synthetic and real-world benchmark datasets demonstrate that our approach not only improves scalability and interpretability but also drastically enhances predictive performance.","In many areas, including healthcare, we collect vast amounts of time-stamped data, like patients’ vital signs over time. Traditional tools for analyzing this kind of data either compare every point to every other (which becomes very slow as datasets grow) or simplify the problem by picking a few representative examples (which can miss important differences, especially when tracking many different groups).In our work, we propose a more efficient way to capture how data evolves. We use a set of simple mathematical building blocks, basis functions, to represent smooth patterns in the data instead of comparing everything or picking key samples. This approach keeps the computation growing at the same rate as the dataset itself, even when it includes many different types of subjects. We also simplify the learning process by using a single shared model for the entire dataset which helps us to avoid errors introduced by extra estimation steps.Our method provided more accurate predictions than previous approaches when tested on both simulated and real-world datasets. This makes it especially useful for tasks that require generating or forecasting time-series data quickly and reliably."
Poster,Bayesian Inference for Correlated Human Experts and Classifiers,https://ICML.cc//virtual/2025/poster/43804,"Markelle Kelly, Alex Boyd, Samuel Showalter, Mark Steyvers, Padhraic Smyth","Applications of machine learning often involve making predictions based on both model outputs and the opinions of human experts. In this context, we investigate the problem of querying experts for class label predictions, using as few human queries as possible, and leveraging the class probability estimates of pre-trained classifiers. We develop a general Bayesian framework for this problem, modeling expert correlation via a joint latent representation, enabling simulation-based inference about the utility of additional expert queries, as well as inference of posterior distributions over unobserved expert labels. We apply our approach to two real-world medical classification problems, as well as to CIFAR-10H and ImageNet-16H, demonstrating substantial reductions relative to baselines in the cost of querying human experts while maintaining high prediction accuracy.","In many high-stakes domains (such as healthcare), human experts and machine learning (ML) models work together to make decisions. However, consulting multiple human experts for every case (e.g., radiologists reviewing an X-ray) is often impractical and expensive. Our research addresses the challenge of accurately predicting what a group of human experts would conclude—without always having to ask them.To this end, we developed a statistical approach that learns how each member of a group of human experts (and ML models) usually makes predictions, capturing relationships between the different agents. By leveraging these relationships, our method helps to minimize querying of human experts, choosing whom to query and then predicting the remaining, unobserved opinions.We tested our approach on several real-world image classification tasks, showing that it can accurately predict the final expert conclusion while making fewer expert queries on average. Altogether, this work makes collaborative human-AI decision-making more efficient and affordable—especially in high-stakes settings where expert input is valuable but limited."
Poster,Bayesian Neural Scaling Law Extrapolation with Prior-Data Fitted Networks,https://ICML.cc//virtual/2025/poster/44943,"Dongwoo Lee, Dong Bok Lee, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Frank Hutter, Seon Joo Kim, Hae Beom Lee","Scaling has been a major driver of recent advancements in deep learning. Numerous empirical studies have found that scaling laws often follow the power-law and proposed several variants of power-law functions to predict the scaling behavior at larger scales. However, existing methods mostly rely on point estimation and do not quantify uncertainty, which is crucial for real-world applications involving decision-making problems such as determining the expected performance improvements achievable by investing additional computational resources. In this work, we explore a Bayesian framework based on Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. Specifically, we design a prior distribution that enables the sampling of infinitely many synthetic functions resembling real-world neural scaling laws, allowing our PFN to meta-learn the extrapolation. We validate the effectiveness of our approach on real-world neural scaling laws, comparing it against both the existing point estimation methods and Bayesian approaches. Our method demonstrates superior performance, particularly in data-limited scenarios such as Bayesian active learning, underscoring its potential for reliable, uncertainty-aware extrapolation in practical applications.","As AI systems gain more resources, such as computing power or data, their performance typically improves. Predicting these improvements is vital for deciding whether to invest in additional resources. However, current tools often provide a single best guess without showing how certain it is, complicating decisions, especially with limited data.Our research presents the first method of its kind, using an uncertainty-aware Bayesian approach to predict AI scaling trends. We created a unique system, fine-tuned on numerous simulated examples of AI performance growth. This enables our method to forecast future performance, detect complex patterns—even if they shift unexpectedly—and indicate prediction confidence.Compared to traditional statistical methods, our approach offers more accurate predictions and better insight into their reliability, as demonstrated with real-world AI development data. This provides a trustworthy way to estimate future AI capabilities, supporting investment decisions and enhancing research efficiency by suggesting optimal experiments."
Poster,Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds,https://ICML.cc//virtual/2025/poster/43611,"Aya Kayal, Sattar Vakili, Laura Toni, Da-shan Shiu, Alberto Bernacchia","Bayesian optimization (BO) with preference-based feedback has recently garnered significant attention due to its emerging applications. We refer to this problem as Bayesian Optimization from Human Feedback (BOHF), which differs from conventional BO by learning the best actions from a reduced feedback model, where only the preference between two actions is revealed to the learner at each time step. The objective is to identify the best action using a limited number of preference queries, typically obtained through costly human feedback. Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model, provides regret bounds for the performance of several algorithms. In this work, within the same framework we develop tighter performance guarantees. Specifically, we derive regret bounds of $\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the maximum information gain—a kernel-specific complexity term—and $T$ is the number of queries. Our results significantly improve upon existing bounds. Notably, for common kernels, we show that the order-optimal sample complexities of conventional BO—achieved with richer feedback models—are recovered. In other words, the same number of preferential samples as scalar-valued samples is sufficient to find a nearly optimal solution.","In many decision-making tasks—like tuning a chatbot, designing a product, or choosing ad content—it’s unrealistic to ask people to assign precise scores to every option. But people are usually much better at comparing two options and saying which one they prefer. This kind of preference feedback is often more reliable, even if it’s less detailed. Still, collecting it can be costly and time-consuming. That’s why it’s important to design smart algorithms that can figure out the best choice using as few of these comparisons as possible.Our work focuses on building such efficient algorithms. The goal is to learn the best possible option by carefully choosing which pairs of options to compare, making the most of each piece of feedback.What we found is both surprising and exciting: even though preference feedback is less detailed than numerical ratings, it’s still powerful enough to learn almost as well. In fact, we show that the number of comparisons needed to find a near-optimal decision is about the same as if we had access to full numeric scores. This result brings us closer to making machine learning more user-friendly and practical in real-world settings."
Poster,Bayesian Weight Enhancement with Steady-State Adaptation for Test-time Adaptation in Dynamic Environments,https://ICML.cc//virtual/2025/poster/43893,Jae-Hong Lee,"Test-time adaptation (TTA) addresses the machine learning challenge of adapting models to unlabeled test data from shifting distributions in dynamic environments. A key issue in this online setting arises from using unsupervised learning techniques, which introduce explicit gradient noise that degrades model weights. To invest in weight degradation, we propose a Bayesian weight enhancement framework, which generalizes existing weight-based TTA methods that effectively mitigate the issue. Our framework enables robust adaptation to distribution shifts by accounting for diverse weights by modeling weight distributions.Building on our framework, we identify a key limitation in existing methods: their neglect of time-varying covariance reflects the influence of the gradient noise. To address this gap, we propose a novel steady-state adaptation (SSA) algorithm that balances covariance dynamics during adaptation. SSA is derived through the solution of a stochastic differential equation for the TTA process and online inference. The resulting algorithm incorporates a covariance-aware learning rate adjustment mechanism. Through extensive experiments, we demonstrate that SSA consistently improves state-of-the-art methods in various TTA scenarios, datasets, and model architectures, establishing its effectiveness in instability and adaptability.","In our daily lives, artificial intelligence (AI) systems—like those in autonomous vehicles or smart devices—often face new and changing environments. These changes can confuse the AI, leading to a drop in performance. Our research tackles the challenge of helping AI systems adapt to new data in real-time, even when correct answers aren’t available for learning.We found that traditional methods for real-time learning can unintentionally damage the system’s knowledge. To address this, we propose  Bayesian Weight Enhancement with Steady-State Adaptation. This technique uses probability and mathematical modeling to adjust the system smoothly, avoiding damage and improving adaptability.This framework and algorithm consistently enhance the performance of state-of-the-art methods, provide a theoretical explanation of real-time learning, and reveal key principles behind performance improvements. Our work strengthens the practical stability and theoretical foundation of AI systems operating in dynamic environments."
Poster,BCE vs. CE in Deep Feature Learning,https://ICML.cc//virtual/2025/poster/44778,"Qiufu Li, Huibin Xiao, Linlin Shen","When training classification models, it expects that the learned features are compact within classes, and can well separate different classes. As the dominant loss function for training classification models, minimizing cross-entropy (CE) loss maximizes the compactness and distinctiveness, i.e., reaching neural collapse (NC). The recent works show that binary CE (BCE) performs also well in multi-class tasks. In this paper, we compare BCE and CE in deep feature learning. For the first time, we prove that BCE can also maximize the intra-class compactness and inter-class distinctiveness when reaching its minimum, i.e., leading to NC. We point out that CE measures the relative values of decision scores in the model training, implicitly enhancing the feature properties by classifying samples one-by-one. In contrast, BCE measures the absolute values of decision scores and adjust the positive/negative decision scores across all samples to uniformly high/low levels. Meanwhile, the classifier biases in BCE present a substantial constraint on the decision scores to explicitly enhance the feature properties in the training. The experimental results are aligned with above analysis, and show that BCE could improve the classification and leads to better compactness and distinctiveness among sample features. The codes have be released.","This paper explores two loss functions for training classification models in machine learning: Binary Cross-Entropy (BCE) and Cross-Entropy (CE). It highlights how BCE can enhance the quality of learned features, leading to better classification.Key Findings:Feature Learning: BCE promotes compactness within classes and distinctness between classes, improving model performance.Bias Impact: Classifier biases in BCE significantly enhance feature properties compared to CE.Empirical Results: Experiments show that models trained with BCE outperform those trained with CE in classification accuracy and feature properties.Overall, this research suggests that BCE is often a more effective loss function for training classification models."
Poster,BDC-CLIP: Brownian Distance Covariance for Adapting CLIP to Action Recognition,https://ICML.cc//virtual/2025/poster/44508,"Fei Long, Xiaoou Li, jiaming Lv, Yang Haoyuan, Xianjun Cheng, Peihua Li","Bridging contrastive language-image pre-training (CLIP) to video action recognition has attracted growing interest. Human actions are inherently rich in spatial and temporal contexts, involving dynamic interactions among people, objects, and the environment. Accurately recognizing actions requires effectively capturing these fine-grained elements and modeling their relationships with language. However, most existing methods rely on cosine similarity--practically equivalent to the Pearson correlation coefficient--between global tokens for video-language alignment. As a result, they have limited capacity to model complex dependencies and tend to overlook local tokens that encode critical spatio-temporal cues. To overcome these limitations, we propose BDC-CLIP, a novel framework that leverages Brownian Distance Covariance (BDC) to align visual and textual representations. Our method can capture complex relationships--both linear and nonlinear--between all visual and textual tokens, enabling fine-grained modeling in space, time, and language. BDC-CLIP achieves state-of-the-art performance across zero-shot, few-shot, base-to-novel, and fully supervised action recognition settings, demonstrating its effectiveness and broad applicability.","Image-language models like CLIP can identify novel objects from few—or even zero—examples. In contrast, video understanding poses greater challenges: actions evolve over time, and the most informative cues are often found in localized image patches and specific words. Yet, most existing methods squash entire clips and whole sentences to two global vectors and compute their similarity using cosine similarity—capturing only basic linear patterns. In doing so they discard the fine-grained cues that are crucial for accurate video recognition. We introduce BDC-CLIP, a novel framework that retains all visual patches across frames and all words from captions, aligning them using Brownian Distance Covariance (BDC)—a statistical dependency measure that captures both linear and non-linear relationships. A lightweight temporal adapter further aggregates BDC signals across frames, enabling the model to track interactions among objects, people, and actions over time. This results in a richer, token-level alignment between visual content and language. BDC-CLIP outperforms prior work in zero-shot, few-shot, and fully supervised action recognition tasks. Its fine-grained alignment mechanism also benefits downstream tasks such as video retrieval, captioning, and safety filtering, providing a reliable bridge between dynamic visual content and natural language descriptions."
Poster,Be a Goldfish: Forgetting Bad Conditioning in Sparse Linear Regression via Variational Autoencoders,https://ICML.cc//virtual/2025/poster/44794,"Kuheli Pratihar, Debdeep Mukhopadhyay","Variational Autoencoders (VAEs), a class of latent-variable generative models, have seen extensive use in high-fidelity synthesis tasks, yet their loss landscape remains poorly understood. Prior theoretical works on VAE loss analysis have focused on their latent-space representational capabilities, both in the optimal and limiting cases. Although these insights have guided better VAE designs, they also often restrict VAEs to problem settings where classical algorithms, such as Principal Component Analysis (PCA), can trivially guarantee globally optimal solutions. In this work, we push the boundaries of our understanding of VAEs beyond these traditional regimes to tackle NP-hard sparse inverse problems, for which no classical algorithms exist. Specifically, we examine the nontrivial Sparse Linear Regression (SLR) problem of recovering optimal sparse inputs in the presence of an ill-conditioned design matrix having correlated features. We provably show that, under a linear encoder-decoder architecture incorporating the product of the SLR design matrix with a trainable, sparsity-promoting diagonal matrix, any minimum of VAE loss is guaranteed to be an optimal solution. This property is especially useful for identifying (a) a preconditioning factor that reduces the eigenvalue spread, and (b) the corresponding optimal sparse representation. Lastly, our empirical analysis with different types of design matrices validates these findings and even demonstrates a higher recovery rate at low sparsity where traditional algorithms fail. Overall, this work highlights the flexible nature of the VAE loss, which can be adapted to efficiently solve computationally hard problems under specific constraints.","Suppose you need to purchase a generous bouquet, but the price increases with the choice of more expensive flowers. The difficulty of deciding which flowers to choose to create a visually appealing bouquet within a given budget increases if you are limited to selecting only a few from many. This challenge, also known as sparse recovery, is a hard problem in computer science, for which efficient and reliable solutions do not yet exist.We look at this problem through the lens of generative modeling using machine learning. Their success when applied to text, speech, and image synthesis is due to their ability of learning the hidden properties of the object. However, we demonstrate that they can also be applied to hard, sparse recovery problems by making a sparsity-informed design change to the network architecture.Our proposed generative modeling solution achieves a better sparse recovery rate compared to state-of-the-art algorithms. This is possible because a generative model optimizes for both the number as well as the hidden properties of the features, thereby creating the optimal set of features. Our work opens up the use of generative models for the new problem of sparse recovery, which can even be applied to selecting the flowers for the best-looking bouquet."
Poster,BECAME: Bayesian Continual Learning with Adaptive Model Merging,https://ICML.cc//virtual/2025/poster/44474,"Mei Li, Yuxiang Lu, Qinyan Dai, Suizhi Huang, Yue Ding, Hongtao Lu","Continual Learning (CL) strives to learn incrementally across tasks while mitigating catastrophic forgetting. A key challenge in CL is balancing stability (retaining prior knowledge) and plasticity (learning new tasks). While representative gradient projection methods ensure stability, they often limit plasticity. Model merging techniques offer promising solutions, but prior methods typically rely on empirical assumptions and carefully selected hyperparameters. In this paper, we explore the potential of model merging to enhance the stability-plasticity trade-off, providing theoretical insights that underscore its benefits. Specifically, we reformulate the merging mechanism using Bayesian continual learning principles and derive a closed-form solution for the optimal merging coefficient that adapts to the diverse characteristics of tasks. To validate our approach, we introduce a two-stage framework named BECAME, which synergizes the expertise of gradient projection and adaptive merging. Extensive experiments show that our approach outperforms state-of-the-art CL methods and existing merging strategies https://github.com/limei0818/BECAME.","AI systems often struggle to learn new information without forgetting what they have already learned. This ""catastrophic forgetting"" makes it hard to balance remembering old tasks (stability) with adapting to new ones (plasticity). Existing solutions often make the AI too rigid to learn new things well, or require complex manual tuning to combine different approaches.We explored a smarter way to merge models: one that remembers old tasks and another specialized for a new task. Instead of guesswork, we developed a method based on Bayesian principles to automatically find the ideal way to combine them, adapting to each task's unique characteristics. Our framework, BECAME, first carefully learns the new task while protecting old knowledge, then learns it more freely, and finally adaptively merges these two resulting models.Our experiments show BECAME helps AI learn new skills more effectively with retaining old knowledge, outperforming current methods. This offers a more robust and principled way to build AI systems that can learn continuously, much like humans do."
Poster,Be Confident: Uncovering Overfitting in MLLM Multi-Task Tuning,https://ICML.cc//virtual/2025/poster/44726,"Wenke Huang, Jian Liang, Guancheng Wan, Didi Zhu, He Li, Jiawei Shao, Mang Ye, Bo Du, Dacheng Tao","Fine-tuning Multimodal Large Language Models (MLLMs) in multi-task learning scenarios has emerged as an effective strategy for achieving cross-domain specialization. However, multi-task fine-tuning frequently induces performance degradation on open-response datasets. We posit that free-form answer generation primarily depends on language priors, and strengthening the integration of visual behavioral cues is critical for enhancing prediction robustness. In this work, we propose Noise Resilient Confidence Alignment to address the challenge of open-response overfitting during multi-task fine-tuning. Our approach prioritizes maintaining consistent prediction patterns in MLLMs across varying visual input qualities. To achieve this, we employ Gaussian perturbations to synthesize distorted visual inputs and enforce token prediction confidence alignment towards the normal visual branch. By explicitly linking confidence calibration to visual robustness, this method reduces over-reliance on language priors. We conduct extensive empirical evaluations across diverse multi-task downstream settings via popular MLLM architectures. The comprehensive experiment demonstrates the effectiveness of our method, showcasing its ability to alleviate open-response overfitting while maintaining satisfying multi-task fine-tuning performance.","Fine-tuning Multimodal Large Language Models (MLLMs) in multi-task learning scenarios has emerged as an effective strategy for achieving cross-domain specialization. However, multi-task fine-tuning frequently induces performance degradation on open-response datasets. We posit that free-form answer generation primarily depends on language priors, and strengthening the integration of visual behavioral cues is critical for enhancing prediction robustness. In this work, we propose Noise Resilient Confidence Alignment to address the challenge of open-response overfitting during multi-task fine-tuning. Our approach prioritizes maintaining consistent prediction patterns in MLLMs across varying visual input qualities. To achieve this, we employ Gaussian perturbations to synthesize distorted visual inputs and enforce token prediction confidence alignment towards the normal visual branch. By explicitly linking confidence calibration to visual robustness, this method reduces over-reliance on language priors. We conduct extensive empirical evaluations across diverse multi-task downstream settings via popular MLLM architectures. The comprehensive experiment demonstrates the effectiveness of our method, showcasing its ability to alleviate open-response overfitting while maintaining satisfying multi-task fine-tuning performance."
