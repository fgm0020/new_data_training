type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Best of Both Worlds: Regret Minimization versus Minimax Play,https://ICML.cc//virtual/2025/poster/44207,"Adrian Müller, Jon Schneider, EFSTRATIOS PANTELEIMON SKOULAKIS, Luca Viano, Volkan Cevher","In this paper, we investigate the existence of online learning algorithms with bandit feedback that simultaneously guarantee $O(1)$ regret compared to a given comparator strategy, and $\tilde{O}(\sqrt{T})$ regret compared to any fixed strategy, where $T$ is the number of rounds. We provide the first affirmative answer to this question whenever the comparator strategy supports every action. In the context of zero-sum games with min-max value zero, both in normal- and extensive form, we show that our results allow us to guarantee to risk at most $O(1)$ loss while being able to gain $\Omega(T)$ from exploitable opponents, thereby combining the benefits of both no-regret algorithms and minimax play.","When repeatedly playing a game such as Rock-Paper-Scissors or Poker against an unknown opponent, the following dilemma arises: Should one rather a) compute a strong strategy and play it in every round, or b) run a learning algorithm that automatically adapts to the opponent's play over time? The first approach (a) would guarantee that one can expect to lose nothing against the opponent. Yet, this static approach comes at the cost of potentially missing out on systematically winning against the opponent if they are weak. Indeed, the second approach (b) would guarantee to systematically win against such weak opponents. However, in this case we also risk losing a significant amount due to the slow learning process. In this paper, we show that, perhaps surprisingly, it is possible to essentially guarantee the benefits of both of these approaches in many games of interest, even if one does not observe all information the learning algorithm may benefit from. This implies that in such games, one can indeed hope to systematically win against weak opponents while risking only a small expected loss, even if the opponent turns out to be strong."
Poster,BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute,https://ICML.cc//virtual/2025/poster/43788,"Dujian Ding, Ankur Mallick, Shaokun Zhang, Chi Wang, Daniel Madrigal, Mirian Hipolito Garcia, Menglin Xia, Laks Lakshmanan, Qingyun Wu, Victor Ruehle","Large language models (LLMs) are powerful tools but are often expensive to deploy at scale.  LLM query routing mitigates this by dynamically assigning queries to models of varying cost and quality to obtain a desired tradeoff. Prior query routing approaches generate only one response from the selected model and a single response from a small (inexpensive) model was often not good enough to beat a response from a large (expensive) model due to which they end up overusing the large model and missing out on potential cost savings. However, it is well known that for small models, generating multiple responses and selecting the best can enhance quality while remaining cheaper than a single large-model response. We leverage this idea to propose BEST-Route, a novel routing framework that chooses a model and the number of responses to sample from it based on query difficulty and the quality thresholds. Experiments on real-world datasets demonstrate that our method reduces costs by up to 60% with less than 1% performance drop.","Imagine you have a groups of AI tools for answering a question: ranging from brilliant-but-expensive expert AIs to quicker-and-cheaper assistant AIs. This research introduces a new system, called BEST-Route, that intelligently decides which one to use based on how difficult your question is. For simpler questions, instead of always turning to the costly expert AI, the system asks the cheaper assistant AI to provide several different answers and then selects the best one. This clever approach often yields a response just as good as the expert's but at a much lower cost. For truly tough questions, the system still relies on the top-tier expert AI to ensure a high-quality answer. By using this method, the researchers were able to reduce operational costs by up to 60% while maintaining nearly the same level of performance, making powerful AI much more affordable."
Poster,Best Subset Selection: Optimal Pursuit for Feature Selection and Elimination,https://ICML.cc//virtual/2025/poster/46078,"Zhihan Zhu, Yanhao Zhang, Yong Xia","This paper introduces two novel criteria: one for feature selection and another for feature elimination in the context of best subset selection, which is a benchmark problem in statistics and machine learning. From the perspective of optimization, we revisit the classical selection and elimination criteria in traditional best subset selection algorithms, revealing that these classical criteria capture only partial variations of the objective function after the entry or exit of features. By formulating and solving optimization subproblems for feature entry and exit exactly, new selection and elimination criteria are proposed, proved as the optimal decisions for the current entry-and-exit process compared to classical criteria. Replacing the classical selection and elimination criteria with the proposed ones generates a series of enhanced best subset selection algorithms. These generated algorithms not only preserve the theoretical properties of the original algorithms but also achieve significant meta-gains without increasing computational cost across various scenarios and evaluation metrics on multiple tasks such as compressed sensing and sparse regression.","Selecting the most important features in high-dimensional data is a fundamental challenge in statistics and machine learning. Best subset selection is considered the gold standard for feature selection, but the problem is NP-hard. Many polynomial-time approximation algorithms have been proposed to solve the best subset selection problem, which use classic criteria to select/eliminate features, but these criteria only partially capture how each feature affects model performance. We revisited this problem through an optimization lens and discovered that existing criteria overlook interactions between features. By mathematically modeling the exact impact of adding/removing features as a subset, rather than just individually, we developed two novel criteria: one for feature selection and one for elimination.When integrated into existing algorithms, our criteria boost performance almost without increasing computational cost. They preserve theoretical guarantees while consistently improving accuracy across diverse tasks, scenarios and evaluation metrics. Both theoretical analysis and experimental results demonstrate the significant advantage of the new criteria in scenarios with high feature correlation. This provides researchers fundamentally new perspectives for algorithm design in the field of best subset selection. The idea of optimal pursuit also has potential applications in a wider range of machine learning scenarios and tasks."
Poster,Better to Teach than to Give: Domain Generalized Semantic Segmentation via Agent Queries with Diffusion Model Guidance,https://ICML.cc//virtual/2025/poster/44281,"Fan Li, Xuan Wang, Min Qi, Zhaoxiang Zhang, yuelei xu","Domain Generalized Semantic Segmentation (DGSS) trains a model on a labeled source domain to generalize to unseen target domains with consistent contextual distribution and varying visual appearance.Most existing methods rely on domain randomization or data generation but struggle to capture the underlying scene distribution, resulting in the loss of useful semantic information. Inspired by the diffusion model's capability to generate diverse variations within a given scene context, we consider harnessing its rich prior knowledge of scene distribution to tackle the challenging DGSS task.In this paper, we propose a novel agent \textbf{Query}-driven learning framework based on \textbf{Diff}usion model guidance for DGSS, named QueryDiff. Our recipe comprises three key ingredients: (1) generating agent queries from segmentation features to aggregate semantic information about instances within the scene; (2) learning the inherent semantic distribution of the scene through agent queries guided by diffusion features; (3) refining segmentation features using optimized agent queries for robust mask predictions.Extensive experiments across various settings demonstrate that our method significantly outperforms previous state-of-the-art methods. Notably, it enhances the model's ability to generalize effectively to extreme domains, such as cubist art styles. Code is available at https://github.com/FanLiHub/QueryDiff.","Deep learning models often struggle to accurately recognize and segment objects in images that differ significantly from their training data, such as those captured under varying weather conditions or with unusual artistic styles. To address this challenge, we introduce QueryDiff, a novel method that leverages the distributional knowledge of diffusion models to guide generalizable visual understanding. QueryDiff constructs “agent queries” that extract and aggregate semantic information about objects within a scene. These queries learn the scene’s underlying semantic distribution through guidance from diffusion features, allowing the model to build a robust understanding of the scene’s structure. The refined queries are then used to enhance the segmentation predictions. Experiments demonstrate that QueryDiff achieves significant performance gains across diverse scenarios and generalizes well to visually extreme domains, such as cubist-style art. This advancement not only improves the robustness of deep learning models but also enhances their practicality in real-world tasks that involve diverse and unpredictable visual conditions."
Poster,Beyond Atoms: Enhancing Molecular Pretrained Representations with 3D Space Modeling,https://ICML.cc//virtual/2025/poster/45004,"Shuqi Lu, Xiaohong Ji, Bohang Zhang, Lin Yao, Siyuan Liu, Zhifeng Gao, Linfeng Zhang, Guolin Ke","Molecular pretrained representations (MPR) has emerged as a powerful approach for addressing the challenge of limited supervised data in applications such as drug discovery and material design. While early MPR methods relied on 1D sequences and 2D graphs, recent advancements have incorporated 3D conformational information to capture rich atomic interactions. However, these prior models treat molecules merely as discrete atom sets, overlooking the space surrounding them. We argue from a physical perspective that only modeling these discrete points is insufficient. We first present a simple yet insightful observation: naively adding randomly sampled virtual points beyond atoms can surprisingly enhance MPR performance. In light of this, we propose a principled framework that incorporates the entire 3D space spanned by molecules. We implement the framework via a novel Transformer-based architecture, dubbed SpaceFormer, with three key components:(1)grid-based space discretization; (2)grid sampling/merging; and (3)efficient 3D positional encoding.Extensive experiments show that SpaceFormer significantly outperforms previous 3D MPR models across various downstream tasks with limited data, validating the benefit of leveraging the additional 3D space beyond atoms in MPR models.","Molecular pretrained representations has emerged as a powerful approach for addressing the challenge of limited supervised data in applications such as drug discovery and material design. Early models for these tasks analyze molecules as simple strings of atoms or flat diagrams, while newer approaches study their 3D shapes (including atom types and coordinates). However, even these advanced methods focus only on the atoms themselves, ignoring the empty spaces around them. We argue from a physical perspective that only modeling these discrete atomic points is insufficient. We first present a simple yet insightful observation: naively adding randomly sampled points beyond atoms to represent the space can surprisingly improved performance. In light of this, we propose a principled framework, SpaceFormer, which incorporates the entire 3D space spanned by molecules rather than isolated atoms. Extensive experiments show that SpaceFormer significantly outperforms existing models across various downstream tasks with limited data, validating the benefit of leveraging the additional 3D space beyond atoms for molecular pretrained representations."
Poster,Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment,https://ICML.cc//virtual/2025/poster/45103,"Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, Quanquan Gu","Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. In this paper, we introduce \emph{preference embedding}, an approach that embeds responses into a latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback (RLHF). Experimental results show that our General Preference embedding Model (GPM) consistently outperforms the BT reward model on the RewardBench benchmark and effectively models cyclic preferences where any BT reward model behaves like a random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0, following the language model post-training with GPO and our general preference model, reveal performance improvements over BT models. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://github.com/general-preference/general-preference-model.","When we try to teach AI to understand human preferences, current methods often struggle, especially when our choices are complex or seem contradictory (like preferring rock over scissors, scissors over paper, but paper over rock). This makes it hard to build AI that truly aligns with our nuanced values.We developed a new approach called ""preference embedding,"" where instead of assigning a simple score to each option, our ""General Preference embedding Model"" (GPM) maps options into a richer, multi-dimensional space. This allows the GPM to efficiently grasp these complex, even contradictory, preferences. We also use this deeper understanding to fine-tune AI language models through a process we call ""General Preference Optimization"" (GPO).Our GPM consistently outperforms older models, particularly in understanding contradictory preferences where others might behave like a random guess. Using it with GPO improves AI performance on tasks requiring sensitive alignment with human values. This work paves the way for AI that better reflects our subtle judgments."
Poster,Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning,https://ICML.cc//virtual/2025/poster/43695,"Ze&#x27;ev Zukerman, Bassel Hamoud, Kfir Levy","Distributed learning methods have gained substantial momentum in recent years, with communication overhead often emerging as a critical bottleneck. Gradient compression techniques alleviate communication costs but involve an inherent trade-off between the empirical efficiency of biased compressors and the theoretical guarantees of unbiased compressors. In this work, we introduce a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased compressors to construct statistically unbiased estimates. This approach effectively bridges the gap between biased and unbiased methods, combining the strengths of both. To showcase the versatility of our method, we apply it to popular compressors, like Top-$k$ and bit-wise compressors, resulting in enhanced variants. Furthermore, we derive an adaptive version of our approach to further improve its performance. We validate our method empirically on distributed deep learning tasks.","In large-scale machine learning, especially when training very large models like ChatGPT, computers often work together by exchanging information, but this communication can become a major bottleneck. To save bandwidth, systems compress the data they send. However, this introduces a trade-off: the most efficient compressions reduce theoretical reliability, while the safest ones reduce the efficiency of the training process. Our work introduces a new technique that uses a concept from statistics called “Multilevel Monte Carlo” to get the best of both worlds: fast, efficient communication with reliable learning guarantees. We show how this approach turns even biased, aggressive compressions into accurate and trustworthy information. This helps machine learning systems train faster across many devices, without sacrificing robustness or accuracy."
Poster,Beyond Confidence: Exploiting Homogeneous Pattern for Semi-Supervised Semantic Segmentation,https://ICML.cc//virtual/2025/poster/45744,"Rui Sun, Huayu Mai, Wangkai Li, Yujia Chen, Naisong Luo, Yuan Wang, Tianzhu Zhang","The critical challenge of semi-supervised semantic segmentation lies in how to fully exploit a large volume of unlabeled data to improve the model's generalization performance for robust segmentation. Existing methods mainly rely on confidence-based scoring functions in the prediction space to filter pseudo labels, which suffer from the inherent trade-off between true and false positive rates. In this paper, we carefully design an agent construction strategy to build clean sets of correct (positive) and incorrect (negative) pseudo labels, and propose the Agent Score function (AgScore)  to measure the consensus between candidate pixels and these sets. In this way, AgScore takes a step further to capture homogeneous patterns in the embedding space, conditioned on clean positive/negative agents stemming from the prediction space, without sacrificing the merits of confidence score, yielding better trad-off. We provide theoretical analysis to understand the mechanism of AgScore, and demonstrate its effectiveness by integrating it into three semi-supervised segmentation frameworks on Pascal VOC, Cityscapes, and COCO datasets, showing consistent improvements across all data partitions.","Training AI to understand images often needs many labeled examples, which is costly. Semi-supervised learning reduces this need by using both labeled and unlabeled images. A key challenge is deciding which AI-generated “pseudo labels” are reliable. Most existing methods rely on the model’s confidence, which can be misleading. Our work introduces a new scoring method called AgScore. It looks at how similar each prediction is to trusted patterns in the model’s internal features, instead of just using confidence. This helps select better training signals from unlabeled data. AgScore improves learning across several datasets, making it easier to train accurate models with less labeled data."
Poster,Beyond Cropped Regions: New Benchmark and Corresponding Baseline for Chinese Scene Text Retrieval in Diverse Layouts,https://ICML.cc//virtual/2025/poster/45853,"Li gengluo, Huawen Shen, Yu ZHOU","Chinese scene text retrieval is a practical task that aims to search for images containing visual instances of a Chinese query text. This task is extremely challenging because Chinese text often features complex and diverse layouts in real-world scenes. Current efforts tend to inherit the solution for English scene text retrieval, failing to achieve satisfactory performance. In this paper, we establish a Diversified Layout benchmark for Chinese Street View Text Retrieval (DL-CSVTR), which is specifically designed to evaluate retrieval performance across various text layouts, including vertical, cross-line, and partial alignments. To address the limitations in existing methods, we propose Chinese Scene Text Retrieval CLIP (CSTR-CLIP), a novel model that integrates global visual information with multi-granularity alignment training. CSTR-CLIP applies a two-stage training process to overcome previous limitations, such as the exclusion of visual features outside the text region and reliance on single-granularity alignment, thereby enabling the model to effectively handle diverse text layouts. Experiments on existing benchmark show that CSTR-CLIP outperforms the previous state-of-the-art model by 18.82% accuracy and also provides faster inference speed. Further analysis on DL-CSVTR confirms the superior performance of CSTR-CLIP in handling various text layouts. The dataset and code will be publicly available to facilitate research in Chinese scene text retrieval.","This paper focuses on helping computers find images that contain a specific piece of Chinese text provided by the user. For example, if someone searches for a certain phrase, the system will look through a large image collection to find pictures where that phrase appears. This task is challenging because Chinese text in real-world images can appear in many complex ways—such as written vertically, spread across lines, or partially cut off. To address this, the authors created a new dataset that includes many different and difficult text layouts. They also developed a new system that helps computers better understand both the image and the text it contains. Their approach performs much better than previous systems and is faster, making it useful for real-world applications like document search, visual archiving, or navigation aids. The dataset and code will be released to support future research."
Poster,Beyond CVaR: Leveraging Static Spectral Risk Measures for Enhanced Decision-Making in Distributional Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45002,"Mehrdad Moghimi, Hyejin Ku","In domains such as finance, healthcare, and robotics, managing worst-case scenarios is critical, as failure to do so can lead to catastrophic outcomes. Distributional Reinforcement Learning (DRL) provides a natural framework to incorporate risk sensitivity into decision-making processes. However, existing approaches face two key limitations: (1) the use of fixed risk measures at each decision step often results in overly conservative policies, and (2) the interpretation and theoretical properties of the learned policies remain unclear. While optimizing a static risk measure addresses these issues, its use in the DRL framework has been limited to the simple static CVaR risk measure. In this paper, we present a novel DRL algorithm with convergence guarantees that optimizes for a broader class of static Spectral Risk Measures (SRM). Additionally, we provide a clear interpretation of the learned policy by leveraging the distribution of returns in DRL and the decomposition of static coherent risk measures. Extensive experiments demonstrate that our model learns policies aligned with the SRM objective, and outperforms existing risk-neutral and risk-sensitive DRL models in various settings.","In critical fields like finance and healthcare, AI systems must be able to handle worst-case scenarios to prevent disasters. Existing methods for managing risk often become overly cautious, reducing their effectiveness, or produce decisions that are hard to interpret. Meanwhile, alternative approaches that avoid these issues tend to be overly simplistic.To address these challenges, we developed a new learning algorithm that adopts a broader and more flexible definition of risk, moving beyond simpler strategies. Importantly, our method also offers a clear interpretation of its decision-making process and underlying risk preferences.Extensive testing shows that our approach not only aligns with the intended risk objectives but also outperforms existing risk-neutral and risk-sensitive methods across a range of scenarios. This work contributes to the development of more intelligent systems that manage risk more effectively and make safer, more reliable decisions in real-world applications."
