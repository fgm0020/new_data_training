type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Low-Rank Adapting Models for Sparse Autoencoders,https://ICML.cc//virtual/2025/poster/44210,"Matthew Chen, Josh Engels, Max Tegmark","Sparse autoencoders (SAEs) aim to decompose language model representations into a sparse set of linear latent vectors. Recent works have improved SAEs using language model gradients, but these techniques require many expensive backward passes during training and still cause a significant increase in cross entropy loss when SAE reconstructions are inserted into the model. In this work, we improve on these limitations by taking a fundamentally different approach: we use low-rank adaptation (LoRA) to finetune the *language model itself* around a previously trained SAE. We analyze our method across SAE sparsity, SAE width, language model size, LoRA rank, and model layer on the Gemma Scope family of SAEs. In these settings, our method reduces the cross entropy loss gap by 30% - 55% when SAEs are inserted during the forward pass. We also find that compared to end-to-end (e2e) SAEs, our approach achieves the same downstream cross entropy loss 3$\times$ to 20$\times$ faster on Gemma-2-2B and 2$\times$ to 10$\times$ faster on Llama-3.2-1B. We further show that our technique improves downstream metrics and can adapt multiple SAEs at once. Our results demonstrate that improving model interpretability is not limited to post-hoc SAE training; Pareto improvements can also be achieved by directly optimizing the model itself.","Large language models (LLMs) have demonstrated profound capabilities. In an effort to ensure these models are not doing something us humans disapprove of, researchers are interested in understanding the underlying mechanisms these models use to function. One tool researchers use has recently gained traction for being able to translate the models' internal representations into human-interpretable concepts.Unfortunately, the tool seems to fall short of fully interpreting the model's internal thoughts, as when we restrict our lens to only the interpretable concepts the tool finds, the model performs significantly worse. In other words, we cannot be confident the model is faithful to our interpretation of its internal mechanism.In this work, we explore how we can cheaply train the model to more faithfully use the interpretable concepts we do identify with the tool without sacrificing its profound capabilities. We can therefore be more confident than before in understanding what this modified model is doing."
Poster,Low-Rank Tensor Transitions (LoRT) for Transferable Tensor Regression,https://ICML.cc//virtual/2025/poster/44389,"Andong Wang, Yuning Qiu, Zhong Jin, Guoxu Zhou, Qibin Zhao","Tensor regression is a powerful tool for analyzing complex multi-dimensional data in fields such as neuroimaging and spatiotemporal analysis, but its effectiveness is often hindered by insufficient sample sizes. To overcome this limitation, we adopt a transfer learning strategy that leverages knowledge from related source tasks to improve performance in data-scarce target tasks. This approach, however, introduces additional challenges including model shifts, covariate shifts, and decentralized data management. We propose the Low-Rank Tensor Transitions (LoRT) framework, which incorporates a novel fusion regularizer and a two-step refinement to enable robust adaptation while preserving low-tubal-rank structure. To support decentralized scenarios, we extend LoRT to D-LoRT, a distributed variant that maintains statistical efficiency with minimal communication overhead. Theoretical analysis and experiments on tensor regression tasks, including compressed sensing and completion, validate the robustness and versatility of the proposed methods. These findings indicate the potential of LoRT as a robust method for tensor regression in settings with limited data and complex distributional structures.","When data are organized in complex formats like videos or medical scans, they naturally form a structure called a *tensor* — a multi-dimensional array. Making predictions using such data usually requires a lot of labeled examples, which are often expensive or hard to collect. In this work, we explore whether information from other related datasets (called *source tasks*) can help improve learning on a new dataset (the *target task*) that has very limited data.We propose a method called Low-Rank Tensor Transitions (LoRT), which tries to find shared structure across tasks using low-rank assumptions — a way of summarizing complex data with fewer key components. LoRT works in two stages: it first finds patterns shared between tasks, then it adjusts the result to better fit the new task. We also develop a version for decentralized settings, where raw data cannot be shared — only model parameters are communicated.This is an early step toward making tensor-based learning more practical in data-scarce environments. While there are still many challenges ahead — such as reducing computational costs or extending to more complex settings — we hope this work provides a foundation for future exploration."
Poster,Low-Rank Thinning,https://ICML.cc//virtual/2025/poster/44371,"Annabelle Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey","The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.","The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, recently-developed thinning algorithms can match the quality of sampling without replacement while substantially reducing the number of summary points. However, existing guarantees are overly restrictive and pessimistic. To address these deficiencies, we introduce a new analysis of thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical thinning approaches that improve upon the best known guarantees for approximating the quadratic-time computations in neural networks, speeding up model training through example reordering, and rapidly detecting salient differences between datasets."
Poster,LRA-QViT: Integrating Low-Rank Approximation and Quantization for Robust and Efficient Vision Transformers,https://ICML.cc//virtual/2025/poster/45855,"Beom Jin Kang, NamJoon Kim, Hyun Kim","Recently, transformer-based models have demonstrated state-of-the-art performance across various computer vision tasks, including image classification, detection, and segmentation. However, their substantial parameter count poses significant challenges for deployment in resource-constrained environments such as edge or mobile devices. Low-rank approximation (LRA) has emerged as a promising model compression technique, effectively reducing the number of parameters in transformer models by decomposing high-dimensional weight matrices into low-rank representations. Nevertheless, matrix decomposition inherently introduces information loss, often leading to a decline in model accuracy. Furthermore, existing studies on LRA largely overlook the quantization process, which is a critical step in deploying practical vision transformer (ViT) models. To address these challenges, we propose a robust LRA framework that preserves weight information after matrix decomposition and incorporates quantization tailored to LRA characteristics. First, we introduce a reparameterizable branch-based low-rank approximation (RB-LRA) method coupled with weight reconstruction to minimize information loss during matrix decomposition. Subsequently, we enhance model accuracy by integrating RB-LRA with knowledge distillation techniques. Lastly, we present an LRA-aware quantization method designed to mitigate the large outliers generated by LRA, thereby improving the robustness of the quantized model. To validate the effectiveness of our approach, we conducted extensive experiments on the ImageNet dataset using various ViT-based models. Notably, the Swin-B model with RB-LRA achieved a 31.8\% reduction in parameters and a 30.4\% reduction in GFLOPs, with only a 0.03\% drop in accuracy. Furthermore, incorporating the proposed LRA-aware quantization method reduced accuracy loss by an additional 0.83\% compared to naive quantization.","Recently, Vision Transformer (ViT) models have demonstrated state-of-the-art performance across a wide range of visual recognition tasks, including image classification and object detection. However, their substantial parameter counts and high computational complexity pose significant challenges for deployment in resource-constrained environments such as mobile and edge devices. To address these limitations, we propose a novel compression framework that synergistically combines low-rank approximation (LRA) and quantization. Specifically, we introduce a reparameterizable branch-based low-rank approximation (RB-LRA) method in conjunction with weight reconstruction (WR) initialization to mitigate the information loss incurred during matrix decomposition. Additionally, to reduce quantization errors caused by outliers emerging from the LRA process, we develop a weight-aware distribution scaling (WADS) method tailored to the structure of compressed models. The proposed framework significantly reduces model size and inference latency on real-world mobile and edge hardware while maintaining high predictive accuracy. Furthermore, it exhibits robust generalization performance across diverse modalities, including speech and language domains. These findings suggest that the proposed approach provides a practical and effective solution for compressing transformer-based models, enabling their efficient deployment in low-resource environments."
Poster,LSCD: Lomb--Scargle Conditioned Diffusion for Time series Imputation,https://ICML.cc//virtual/2025/poster/45821,"Elizabeth M Fons Etcheverry, Alejandro Sztrajman, Yousef El-Laham, Luciana Ferrer, Svitlana Vyetrenko, Manuela Veloso","Time series with missing or irregularly sampled data are a persistent challenge in machine learning. Many methods operate on the frequency-domain, relying on the Fast Fourier Transform (FFT) which assumes uniform sampling, therefore requiring prior interpolation that can distort the spectra. To address this limitation, we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data.We integrate this layer into a novel score-based diffusion model (LSCD) for time series imputation conditioned on the entire signal spectrum. Experiments on synthetic and real-world benchmarks demonstrate that our method recovers missing data more accurately than purely time-domain baselines, while simultaneously producing consistent frequency estimates. Crucially, our method can be easily integrated into learning frameworks, enabling broader adoption of spectral guidance in machine learning approaches involving incomplete or irregular data.","Many fields rely on data collected over time, such as patient health records, environmental sensors, or financial transactions. However, this data often has gaps or is recorded at irregular intervals, which makes it difficult for computers to analyze or fill in missing values accurately. Most existing methods require this type of data to be evenly spaced and use techniques that can distort the original information when gaps are present.Our research introduces a new method to handle this challenge by using a mathematical tool (called the Lomb–Scargle periodogram) that can analyze unevenly spaced data without first filling in the gaps. We combine this tool with a modern machine learning approach called a diffusion model, which learns to “guess” the missing values in a way that matches both the original data and its hidden patterns.We tested our approach on both simulated and real-world data and found that it not only predicts missing values more accurately, but also better preserves important patterns over time. This work can help researchers and professionals in healthcare, climate science, and finance make better decisions using incomplete or irregular data."
Poster,LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models,https://ICML.cc//virtual/2025/poster/44230,"Tzu-Tao (Tommy) Chang, Shivaram Venkataraman","Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs, the size of the query block is typically much smaller than that of the key-value blocks.  Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique to support longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with Llama 3-V, mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 10.62$\times$ end-to-end speedup compared to existing approaches.","AI models that understand videos (imagine movies!) are extremely large -- often too big to fit on a single computer. To handle them, researchers typically split the workload across multiple computers. However, this can be very slow because the computers need to exchange large amounts of data during processing. A major bottleneck comes from a part of these models called **cross-attention**, which helps the AI connect visual information (like video frames) to language. In our work, we introduce a new way of splitting the cross-attention workload, called **LV-XAttn**, that significantly reduces the amount of data computers need to exchange without changing the model’s output. The key idea is to keep the largest pieces of data local and only exchange the smaller ones, thereby reducing time spent on communication. We also design a memory-efficient technique that allows the model to handle even longer videos. Our approach works seamlessly with several popular AI models and can make them up to 10 times faster. This makes it more practical to train and deploy powerful AI models that can understand long videos."
Poster,M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation,https://ICML.cc//virtual/2025/poster/45360,"Tao Zhang, Zhenhai Liu, Feipeng Qi, Yongjun Jiao, Tailin Wu","Multiphysics simulation, which models the interactions between multiple physical processes, and multi-component simulation of complex structures are critical in fields like nuclear and aerospace engineering. Previous studies use numerical solvers or ML-based surrogate models for these simulations. However, multiphysics simulations typically require integrating multiple specialized solvers-each for a specific physical process-into a coupled program, which introduces significant development challenges. Furthermore, existing numerical algorithms struggle with highly complex large-scale structures in multi-component simulations. Here we propose compositional Multiphysics and Multi-component PDE Simulation with Diffusion models (M2PDE) to overcome these challenges. During diffusion-based training, M2PDE learns energy functions modeling the conditional probability of one physical process/component conditioned on other processes/components. In inference, M2PDE generates coupled multiphysics and multi-component solutions by sampling from the joint probability distribution. We evaluate M2PDE on two multiphysics tasks-reaction-diffusion and nuclear thermal coupling--where it achieves more accurate predictions than surrogate models in challenging scenarios. We then apply it to a multi-component prismatic fuel element problem, demonstrating that M2PDE scales from single-component training to a 64-component structure and outperforms existing domain-decomposition and graph-based approaches. The code is available at github.com/AI4Science-WestlakeU/M2PDE.","Complex simulations involving multiple interacting physical processes (like in nuclear or aerospace engineering) and large structures with many components are difficult. Existing methods either require integrating many different specialized simulation programs, which is hard to develop, or struggle with the complexity of large, multi-component structures. We develop a new method called M2PDE, which uses diffusion models to simulate these complex systems. M2PDE learns how different physical processes or components interact. During training, M2PDE learns how one physical process/component influenced on other processes/components. During the inference, it generates solutions by considering the combined probabilities of all the interactions. We evaluate M2PDE on two multiphysics tasks-reaction-diffusion and nuclear thermal coupling--where it achieves more accurate predictions than surrogate models in challenging scenarios. We then apply it to a multi-component prismatic fuel element problem, demonstrating that M2PDE scales from single-component training to a 64-component structure and outperforms existing domain-decomposition and graph-based approaches. M2PDE provides a novel and important approach for addressing complex multiphysics and multi-component PDE simulations, which is crucial across a wide range of scientific and engineering disciplines. The code is available at github.com/AI4Science-WestlakeU/M2PDE."
Poster,M³HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality,https://ICML.cc//virtual/2025/poster/46583,"Ziyan Wang, Zhicheng Zhang, Fei Fang, Yali Du","Designing effective reward functions in multi-agent reinforcement learning (MARL) is a significant challenge, often leading to suboptimal or misaligned behaviors in complex, coordinated environments. We introduce Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality ($\text{M}^3\text{HF}$), a novel framework that integrates multi-phase human feedback of mixed quality into the MARL training process. By involving humans with diverse expertise levels to provide iterative guidance, $\text{M}^3\text{HF}$ leverages both expert and non-expert feedback to continuously refine agents' policies. During training, we strategically pause agent learning for human evaluation, parse feedback using large language models to assign it appropriately and update reward functions through predefined templates and adaptive weights by using weight decay and performance-based adjustments. Our approach enables the integration of nuanced human insights across various levels of quality, enhancing the interpretability and robustness of multi-agent cooperation. Empirical results in challenging environments demonstrate that $\text{M}^3\text{HF}$ significantly outperforms state-of-the-art methods, effectively addressing the complexities of reward design in MARL and enabling broader human participation in the training process.","Coordinating multiple AI agents on complex tasks is hard without clear guidance, leading to slow learning and poor teamwork. We leverage human language feedback—using a large language model to convert simple comments into reward signals and relabel past experiences with these language-driven rewards. This accelerates training and boosts success rates in teamwork benchmarks by focusing agents on helpful behaviors. Our method makes multi-agent learning faster, more reliable, and easier to interpret, paving the way for transparent, efficient AI teams."
Poster,M3-JEPA: Multimodal Alignment via Multi-gate MoE based on the Joint-Embedding Predictive Architecture,https://ICML.cc//virtual/2025/poster/43776,"Hongyang Lei, Xiaolong Cheng, Qi Qin, Dan Wang, Huazhen Huang, Qingqing Gu, Yetao Wu, Luo Ji","Current multimodal learning strategies primarily optimize in the original token space. Such a framework is easy to incorporate with the backbone of pretrained language model, but might result in modality collapse. To alleviate such issues, we leverage the Joint-Embedding Predictive Architecture (JEPA) on the multimodal tasks, which converts the input embedding into the output embedding space by a predictor and then conducts the cross-modal alignment on the latent space. We implement this predictor by a Multi-Gate Mixture of Experts (MMoE) and name the framework as M3-JEPA, accordingly. The gating function disentangles the modality-specific and shared information and derives information-theoretic optimality. The framework is implemented with both contrastive and regularization loss, and solved by alternative gradient descent (AGD) between different multimodal tasks. By thoroughly designed experiments, we show that M3-JEPA can obtain state-of-the-art performance on different modalities and tasks, generalize to unseen datasets and domains, and is computationally efficient in both training and inference. Our observation suggests that M3-JEPA might become a new basis to self-supervised learning in the open world.","Modern AI systems need to connect different types of information, like image, text, and audio. Current methods generally predict one type of information directly, usually in the word or pixel level, given another type of information. However, they sometimes suffer from the sampling noise, information bias or content ambiguity, therefore produces inaccurate results. This work introduces M3-JEPA, a new AI method that better connects different information types, primarily understand their contents in the deeper, hidden space of modeling.  of data by understanding their deeper meaning.  We also employ a special system called a “mixture of experts”, which allows different networks to study different field-specific knowledge, and automatically select the most appropriate expert in the current situation. We show that M3-JEPA has reasonable experimental performance, good  adaptability to unseen conditions, and higher computational efficiency. Overall, M3-JEPA offers a new path to the artificial general intelligence, by a better modeling of the natural world."
Poster,Machine Learning meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics,https://ICML.cc//virtual/2025/poster/43763,"Herman Chau, Helen Jenne, Davis Brown, Jesse He, Mark Raugas, Sara Billey, Henry Kvinge","With recent dramatic increases in AI system capabilities, there has been growing interest in utilizing machine learning for reasoning-heavy, quantitative tasks, particularly mathematics. While there are many resources capturing mathematics at the high-school, undergraduate, and graduate level, there are far fewer resources available that align with the level of difficulty and open endedness encountered by professional mathematicians working on open problems. To address this, we introduce a new collection of datasets, the Algebraic Combinatorics Dataset Repository (ACD Repo), representing either foundational results or open problems in algebraic combinatorics, a subfield of mathematics that studies discrete structures arising from abstract algebra. Further differentiating our dataset collection is the fact that it aims at the conjecturing process. Each dataset includes an open-ended research level question and a large collection of examples (up to 10M in some cases) from which conjectures should be generated. We describe all nine datasets, the different ways machine learning models can be applied to them (e.g., training with narrow models followed by interpretability analysis or program synthesis with LLMs), and discuss some of the challenges involved in designing datasets like these.","With recent dramatic increases in AI system capabilities, there has been growing interest in utilizing machine learning for reasoning-heavy, quantitative tasks, particularly mathematics. While there are many resources capturing mathematics at the high-school, undergraduate, and graduate level, there are far fewer resources available that align with the level of difficulty and open endedness encountered by professional mathematicians working on open problems. To address this, we introduce a new collection of datasets, the Algebraic Combinatorics Dataset Repository (ACD Repo), representing either foundational results or open problems in algebraic combinatorics, a subfield of mathematics that studies discrete structures arising from abstract algebra. Further differentiating our dataset collection is the fact that it aims at the conjecturing process. Each dataset includes an open-ended research level question and a large collection of examples (up to 10M in some cases) from which conjectures should be generated. We describe all nine datasets, the different ways machine learning models can be applied to them, and discuss some of the challenges involved in designing datasets like these."
