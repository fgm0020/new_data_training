type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Adversarial Robustness via Deformable Convolution with Stochasticity,https://ICML.cc//virtual/2025/poster/43682,"Yanxiang Ma, Zixuan Huang, Minjing Dong, Shan You, Chang Xu","Random defense represents a promising strategy to protect neural networks from adversarial attacks. Most of these methods enhance robustness by injecting randomness into the data, increasing uncertainty for attackers.However, this randomness could reduce the generalization capacity of defense, as defense performance could be sensitive to the hyperparameters of noise added to the data, making it difficult to generalize across different datasets. Additionally, the involvement of randomness always comes with a reduction of natural accuracy, which leads to a delicate trade-off between them, which is seldom studied in random defense. In this work, we propose incorporating randomness into the network structure instead of data input by designing stochastic deformable convolution, where a random mask replaces the convolutional offset. This process promotes data independence, enhancing generalization across datasets. To study the trade-off, we conduct a theoretical analysis of both robust and clean accuracy, from a perspective of gradient cosine similarity and natural inference. Based on the analysis, we reformulate the adversarial training in our random defense framework. Extensive experiments show that our method achieves SOTA adversarial robustness and clean accuracy compared with other random defense methods.","This paper introduced an advanced method called DCS to avoid the risk of attacks on neural networks by adding some randomized structures. In addition, a detailed learning and reasoning framework is designed for this method to help users quickly apply it to their own application scenarios.The main advancements of this work are:1. The DCS method still has the current Most Advanced (state-of-the-art) stability under strong attacks.2. The DCS method can be lightly deployed in complex scenarios: it does not require customers to re-parameterize for the usage scenarios, which will save a lot of deployment costs.3. The DCS method is applicable to most of the current mainstream neural network classifiers, including most of the Vision Transformers and Convolution neural networks.4. The DCS method has a complete set of training and inference solutions: users can simply follow our method, or easily port DCS to their models and use the training and inference solution given by this paper."
Poster,Adversaries Can Misuse Combinations of Safe Models,https://ICML.cc//virtual/2025/poster/45845,"Erik Jones, Anca Dragan, Jacob Steinhardt","Developers try to evaluate whether an AI system can accomplish malicious tasks before releasing it; for example, they might test whether a model enables cyberoffense, user manipulation, or bioterrorism. In this work, we show that individually testing models for such misuse is inadequate; adversaries can misuse combinations of models even when each individual model is safe. The adversary accomplishes this by first decomposing tasks into subtasks, then solving each subtask with the best-suited model. For example, an adversary might solve challenging-but-benign subtasks with an aligned frontier model, and easy-but-malicious subtasks with a weaker misaligned model. We study two decomposition methods: manual decomposition where a human identifies a natural decomposition of a task, and automated decomposition where a weak model generates benign tasks for a frontier model to solve, then uses the solutions in-context to solve the original task. Using these decompositions, we empirically show that adversaries can create vulnerable code, explicit images, python scripts for hacking, and manipulative tweets at much higher rates with combinations of models than either individual model. Our work suggests that even perfectly-aligned frontier systems enable misuse without ever producing malicious outputs, and that red-teaming efforts should extend beyond single models in isolation.","Frontier AI systems undergo extensive testing to ensure they cannot accomplish malicious tasks before they are deployed. However, we demonstrate that adversaries can misuse combinations of models even when each individual model is safe, and do this without ever using the frontier model to produce malicious outputs.Our threat model captures how adversaries can use combinations of models by decomposing malicious tasks. They identify which subtasks require capability (but are benign) versus which require willingness to produce malicious content (but are simple). They then route requests accordingly: frontier models handle complex benign subtasks, while unrestricted weak models complete malicious portions.We evaluate this threat through two decomposition methods. In manual decomposition, humans identify natural task breakdowns (e.g., generating secure code, then editing it to add vulnerabilities). In automated decomposition, weak models generate related benign tasks, frontier models solve them, and weak models leverage these solutions in-context for the original malicious task. Across malicious code generation, manipulation, and explicit content generation, we find model combinations dramatically outperform individual models—sometimes by a factor of ten. This suggests red-teaming must consider the broader model ecosystem to reliably assess deployment risks."
Poster,AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion Models,https://ICML.cc//virtual/2025/poster/45719,"Yaopei Zeng, Yuanpu Cao, Bochuan Cao, Yurui Chang, Jinghui Chen, Lu Lin","Recent advances in diffusion models have significantly enhanced the quality of image synthesis, yet they have also introduced serious safety concerns, particularly the generation of Not Safe for Work (NSFW) content. Previous research has demonstrated that adversarial prompts can be used to generate NSFW content. However, such adversarial text prompts are often easily detectable by text-based filters, limiting their efficacy. In this paper, we expose a previously overlooked vulnerability: adversarial image attacks targeting Image-to-Image (I2I) diffusion models. We propose AdvI2I, a novel framework that manipulates input images to induce diffusion models to generate NSFW content. By optimizing a generator to craft adversarial images, AdvI2I circumvents existing defense mechanisms, such as Safe Latent Diffusion (SLD), without altering the text prompts. Furthermore, we introduce AdvI2I-Adaptive, an enhanced version that adapts to potential countermeasures and minimizes the resemblance between adversarial images and NSFW concept embeddings, making the attack more resilient against defenses. Through extensive experiments, we demonstrate that both AdvI2I and AdvI2I-Adaptive can effectively bypass current safeguards, highlighting the urgent need for stronger security measures to address the misuse of I2I diffusion models.","Diffusion models can edit images with given instructions, but they can also be misused to generate harmful content, such as violent or explicit imagery. While previous research focused on blocking harmful text prompts, we discovered a new vulnerability: these models can also be manipulated through wrought images. Our method, called AdvI2I, subtly edits an image so that the model creates inappropriate results—even when the text looks safe. These attacks bypass existing safety filters and can target widely used models. This research highlights both a critical security risk and a new direction for making AI image generation safer."
Poster,AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs,https://ICML.cc//virtual/2025/poster/44613,"Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, Yuandong Tian","Large Language Models (LLMs) are vulnerable to **jailbreaking attacks** that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well.In this paper, we present a novel method that uses another LLM, called **AdvPrompter**, to generate human-readable adversarial prompts in seconds.AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response.Experimental results on popular open source TargetLLM show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs.We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.","Modern chatbots are built with “guardrails” that should stop them from giving dangerous or hateful answers. Yet with the right wording, people can still slip past these safeguards—a practice known as “jailbreaking.” Finding such tricks by hand is slow, and existing automated searches often spit out nonsense that real users would never type.Our study introduces AdvPrompter, a second chatbot that acts like a mischievous sparring partner. Given any user question, it invents a short, understandable add-on phrase that hides the request from the guardrails while keeping the meaning intact. In seconds, AdvPrompter uncovers jailbreaks that work against popular open-source models and even some commercial black-box systems.We then turn the tables: by retraining the original chatbot on the failures exposed by AdvPrompter, we make it noticeably harder to fool. Because the tool is open-sourced, companies and researchers can use it to stress-test their own systems and, ultimately, build safer and more trustworthy AI."
Poster,A Dynamical Systems-Inspired Pruning Strategy for Addressing Oversmoothing in Graph Attention Networks,https://ICML.cc//virtual/2025/poster/46508,"Biswadeep Chakraborty, Harshit Kumar, Saibal Mukhopadhyay","Graph Neural Networks (GNNs) face a critical limitation known as oversmoothing, where increasing network depth leads to homogenized node representations, severely compromising their expressiveness. We present a novel dynamical systems perspective on this challenge, revealing oversmoothing as an emergent property of GNNs' convergence to low-dimensional attractor states. Based on this insight, we introduce **DYNAMO-GAT**, which combines noise-driven covariance analysis with Anti-Hebbian learning to dynamically prune attention weights, effectively preserving distinct attractor states. We provide theoretical guarantees for DYNAMO-GAT's effectiveness and demonstrate its superior performance on benchmark datasets, consistently outperforming existing methods while requiring fewer computational resources. This work establishes a fundamental connection between dynamical systems theory and GNN behavior, providing both theoretical insights and practical solutions for deep graph learning.","Graph Neural Networks (GNNs), a type of AI designed for network data like social or molecular networks, suffer from a problem called ""oversmoothing."" As these AIs get deeper to learn more complex patterns, they often start seeing all data points (nodes) as increasingly similar, losing crucial distinctions and harming performance. We introduce a fresh perspective, viewing this issue through the lens of dynamical systems – like a physical system settling into an overly simple, uniform state.Based on this, we developed DYNAMO-GAT. This method cleverly ""prunes"" or removes connections within the network that contribute most to this uniformity. It identifies these connections by observing how node similarities evolve when a little noise is introduced, a bit like tracking how ripples spread to understand water's flow. This dynamic pruning prevents the AI from collapsing into a single, uninformative state, allowing it to maintain diverse and meaningful insights. Our experiments show DYNAMO-GAT significantly outperforms current methods, achieving higher accuracy in deep networks while being more computationally efficient. This work provides a new way to understand and solve oversmoothing, paving the way for more powerful AI on complex graphs."
Poster,AEQA-NAT : Adaptive End-to-end Quantization Alignment Training Framework for Non-autoregressive Machine Translation,https://ICML.cc//virtual/2025/poster/44152,"Xiangyu Qu, Guojing Liu, Liang Li","Non-autoregressive Transformers (NATs) have garnered significant attention due to their efficient decoding compared to autoregressive methods. However, existing conditional dependency modeling schemes based on masked language modeling introduce a *training-inference gap* in NATs. For instance, while NATs sample target words during training to enhance input, this condition cannot be met during inference, and simply annealing the sampling rate to zero during training leads to model performance degradation. We demonstrate that this *training-inference gap* prevents NATs from fully realizing their potential. To address this, we propose an adaptive end-to-end quantization alignment training framework, which introduces a semantic consistency space to adaptively align NAT training, eliminating the need for target information and thereby bridging the *training-inference gap*.Experimental results demonstrate that our method outperforms most existing fully NAT models, delivering performance on par with Autoregressive Transformer (AT) while being 17.0 times more efficient in inference.","Conditional masked language modeling is one of the common methods for optimizing non-autoregressive machine translation models. We sought to explore the impact of retaining the target sequence information introduced during the training phase during the inference phase. Surprisingly, we found that maintaining the same paradigm during inference as during training significantly enhances the translation quality of non-autoregressive machine translation models. To achieve consistent input methods between the training and inference phases, we employed the concept of vector quantization alignment to ensure that the decoding process does not rely on target sequence information. Our findings are of significant importance for understanding and exploring the text generation capabilities of non-autoregressive models, providing a new avenue for further investigation."
Poster,Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks,https://ICML.cc//virtual/2025/poster/45151,"Nurbek Tastan, Samuel Horváth, Karthik Nandakumar","Collaborative learning enables multiple participants to learn a single global model by exchanging focused updates instead of sharing data. One of the core challenges in collaborative learning is ensuring that participants are rewarded fairly for their contributions, which entails two key sub-problems: contribution assessment and reward allocation. This work focuses on fair reward allocation, where the participants are incentivized through model rewards - differentiated final models whose performance is commensurate with the contribution. In this work, we leverage the concept of slimmable neural networks to collaboratively learn a shared global model whose performance degrades gracefully with a reduction in model width. We also propose a post-training fair allocation algorithm that determines the model width for each participant based on their contributions. We theoretically study the convergence of our proposed approach and empirically validate it using extensive experiments on different datasets and architectures. We also extend our approach to enable training-time model reward allocation.","Collaborative learning allows multiple participants, such as devices, organizations, or users, to work together to train a shared machine learning model without sharing their private data. One of the biggest challenges in this setting is making sure everyone is rewarded fairly based on how much they actually contributed to the final model. This paper focuses on solving this fairness problem. Instead of giving everyone the same model, we propose giving each participant a personalized version of the final model, where the model's size (or ""width"") reflects their contribution. To do this, we use a special kind of technique called a slimmable neural network, which can flexibly adjust its width without needing to be retrained from scratch. After training, we use a new algorithm to decide what width each participant deserves, so that those who contributed more get better-performing models. We also show that this method is theoretically sound and performs well in practice. Finally, we extend our approach so that fair model allocation can happen not only after training, but also during the training process itself. This helps ensure participants are fairly treated throughout the learning process."
Poster,"AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models",https://ICML.cc//virtual/2025/poster/43565,"Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao","The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level—from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption) and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results show AffectGPT's robust performance across various MER tasks. We have released both the code and the dataset to advance research and development in emotion understanding: https://github.com/zeroQiaoba/AffectGPT.","How can we teach machines to truly understand human emotions in videos? Current AI struggles with nuanced emotions because most datasets are too small and lack detailed annotations. To fix this, we introduce MER-Caption, the largest descriptive emotion dataset ever built—with over 2K fine-grained emotion categories and 115K video samples. Using a novel crowd-sourcing approach driven by AI, we gathered detailed emotional descriptions to improve machine learning models. We also developed AffectGPT, a new AI model designed to better integrate video and text information for emotion understanding. Unlike traditional methods that treat emotions as simple labels, AffectGPT generates natural language descriptions of emotions, making its predictions more interpretable and human-like. To evaluate it, we created MER-UniBench, a new benchmark with metrics tailored for both typical MER tasks and MLLM-based free-form outputs. Our experiments show AffectGPT performs strongly across various emotion recognition tasks. By releasing our dataset and model, we aim to advance research in multimodal emotion understanding and make AI systems more emotionally intelligent."
Poster,AffinityFlow: Guided Flows for Antibody Affinity Maturation,https://ICML.cc//virtual/2025/poster/45745,"Can Chen, Karla-Luise Herpoldt, Chenchao Zhao, Zichen Wang, Marcus Collins, Shang Shang, Ron Benson","Antibodies are widely used as therapeutics, but their development requires costly affinity maturation, involving iterative mutations to enhance binding affinity. This paper explores a sequence-only scenario for affinity maturation, using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold within flow matching to generate diverse protein structures, enabling a sequence-conditioned generative model of structure. Building on this, we propose an \textit{alternating optimization} framework that (1) fixes the sequence to guide structure generation toward high binding affinity using a structure-based predictor, then (2) applies inverse folding to create sequence mutations, refined by a sequence-based predictor. A key challenge is the lack of labeled data for training both predictors. To address this, we develop a \textit{co-teaching} module that incorporates valuable information from noisy biophysical energies into predictor refinement. The sequence-based predictor selects consensus samples to teach the structure-based predictor, and vice versa. Our method, \textit{AffinityFlow}, achieves state-of-the-art performance in proof-of-concept affinity maturation experiments.",Camera ready
Poster,A First-order Generative Bilevel Optimization Framework for Diffusion Models,https://ICML.cc//virtual/2025/poster/46520,"Quan Xiao, Hui Yuan, A F M Saif, Gaowen Liu, Ramana Kompella, Mengdi Wang, Tianyi Chen","Diffusion models, which iteratively denoise data samples to synthesize high-quality outputs, have achieved empirical success across domains. However, optimizing these models for downstream tasks often involves nested bilevel structures, such as tuning hyperparameters for fine-tuning tasks or noise schedules in training dynamics, where traditional bilevel methods fail due to the infinite-dimensional probability space and prohibitive sampling costs. We formalize this challenge as a generative bilevel optimization problem and address two key scenarios: (1) fine-tuning pre-trained models via an inference-only lower-level solver paired with a sample-efficient gradient estimator for the upper level, and (2) training diffusion model from scratch with noise schedule optimization by reparameterizing the lower-level problem and designing a computationally tractable gradient estimator. Our first-order bilevel framework overcomes the incompatibility of conventional bilevel methods with diffusion processes, offering theoretical grounding and computational practicality. Experiments demonstrate that our method outperforms existing fine-tuning and hyperparameter search baselines.","Diffusion models create images by gradually removing noise until a realistic sample appears. They already work well, but can still squeeze out extra performance to generate sharper images or have faster convergence by tweaking their hyperparameters. That tuning job is naturally a bilevel optimization problem: an inner loop solves the generative task, while an outer loop adjusts the hyperparameter to make the final output as good as possible. Classic bilevel methods stumble here because diffusion model lives in an infinite-dimensional probability space and every evaluation step needs lots of expensive sampling. This paper reformulates the hyperparameter search problem for diffusion model as a generative bilevel optimization task and introduces an efficient first order framework to solve it. We tackle two practical cases: 1.**Fine tuning a pre trained diffusion model with entropy regularization.** We treat inference (the denoising steps) as the inner problem and craft a sample efficient gradient estimator so the outer loop can update entropy strength efficiently. 2.**Training diffusion model from scratch while also learning the best noise schedule.** By re parameterizing the inner training dynamics and using the zeroth-order estimation, we can update the hyperparameter at the outer loop without Monte Carlo overload. The resulting bilevel method has theory to back it up, fits neatly into standard diffusion training and fine-tuning pipelines, and, in experiments, beats popular grid/random/Bayesian search baselines on both fine tuning and full training tasks."
