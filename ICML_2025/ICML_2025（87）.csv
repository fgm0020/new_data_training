type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity,https://ICML.cc//virtual/2025/poster/45656,"Erpai Luo, Xinran Wei, Lin Huang, Yunyang Li, Han Yang, Zaishuo Xia, Zun Wang, Chang Liu, Bin Shao, Jia Zhang","Hamiltonian matrix prediction is pivotal in computational chemistry, serving as the foundation for determining a wide range of molecular properties. While SE(3) equivariant graph neural networks have achieved remarkable success in this domain, their substantial computational cost—driven by high-order tensor product (TP) operations—restricts their scalability to large molecular systems with extensive basis sets. To address this challenge, we introduce **SPH**Net, an efficient and scalable equivariant network, that incorporates adaptive **SP**arsity into **H**amiltonian prediction. SPHNet employs two innovative sparse gates to selectively constrain non-critical interaction combinations, significantly reducing tensor product computations while maintaining accuracy. To optimize the sparse representation, we develop a Three-phase Sparsity Scheduler, ensuring stable convergence and achieving high performance at sparsity rates of up to 70\%. Extensive evaluations on QH9 and PubchemQH datasets demonstrate that SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup over existing models. Beyond Hamiltonian prediction, the proposed sparsification techniques also hold significant potential for improving the efficiency and scalability of other SE(3) equivariant networks, further broadening their applicability and impact.","Hamiltonian matrix prediction is pivotal in computational chemistry, serving as the foundation for determining a wide range of molecular properties. While existing methods have achieved remarkable success in this domain, their substantial computational cost—driven by high-order tensor product (TP) operations—restricts their scalability to large molecular systems with extensive basis sets. To address this challenge, we introduce **SPH**Net, an efficient and scalable equivariant network, that incorporates adaptive **SP**arsity into **H**amiltonian prediction. SPHNet employs two innovative sparse gates to selectively constrain non-critical interaction combinations in the tensor product operation, significantly reducing the computation cost while maintaining accuracy. To optimize the sparse representation, we develop a Three-phase Sparsity Scheduler, ensuring stable convergence and achieving high performance at sparsity rates of up to 70\%. Extensive evaluations on different scale datasets demonstrate that SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup over existing models. Beyond Hamiltonian prediction, the proposed sparsification techniques also hold significant potential for improving the efficiency and scalability of other SE(3) equivariant networks, further broadening their applicability and impact."
Poster,Efficient and Separate Authentication Image Steganography Network,https://ICML.cc//virtual/2025/poster/44692,"Junchao Zhou, Yao Lu, Jie Wen, Guangming Lu","Image steganography hides multiple images for multiple recipients into a single cover image. All secret images are usually revealed without authentication, which reduces security among multiple recipients. It is elegant to design an authentication mechanism for isolated reception. We explore such mechanism through sufficient experiments, and uncover that additional authentication information will affect the distribution of hidden information and occupy more hiding space of the cover image. This severely decreases effectiveness and efficiency in large-capacity hiding. To overcome such a challenge, we first prove the authentication feasibility within image steganography. Then, this paper proposes an image steganography network collaborating with separate authentication and efficient scheme. Specifically, multiple pairs of lock-key are generated during hiding and revealing. Unlike traditional methods, our method has two stages to make appropriate distribution adaptation between locks and secret images, simultaneously extracting more reasonable primary information from secret images, which can release hiding space of the cover image to some extent. Furthermore, due to separate authentication, fused information can be hidden in parallel with a single network rather than traditional serial hiding with multiple networks, which can largely decrease the model size. Extensive experiments demonstrate that the proposed method achieves more secure, effective, and efficient image steganography. Code is available at https://github.com/Revive624/Authentication-Image-Steganography.","Image steganography hides multiple images for multiple recipients into a single cover image. All secret images are usually revealed without authentication, which reduces security among multiple recipients. We discovered that additional authentication information will affect the distribution of hidden information and occupy more hiding space of the cover image. This severely decreases effectiveness and efficiency in large-capacity hiding. To overcome these challenges, we prove the authentication feasibility within image steganography and propose an image steganography network collaborating with separate authentication and efficient scheme. Specifically, multiple pairs of lock-key are generated during hiding and revealing. Unlike traditional methods, our method has two stages to make appropriate distribution adaptation between locks and secret images, simultaneously extracting more reasonable primary information from secret images, which can release hiding space of the cover image to some extent. Furthermore, due to separate authentication, fused information can be hidden in parallel with a single network rather than traditional serial hiding with multiple networks, which can largely decrease the model size. Extensive experiments demonstrate that the proposed method achieves more secure, effective, and efficient image steganography."
Poster,Efficient ANN-SNN Conversion with Error Compensation Learning,https://ICML.cc//virtual/2025/poster/46208,"chang liu, Jiangrong Shen, Xuming Ran, Mingkun Xu, Qi Xu, Yi Xu, Gang Pan","Artificial neural networks (ANNs) have demonstrated outstanding performance in numerous tasks, but deployment in resource-constrained environments remains a challenge due to their high computational and memory requirements. Spiking neural networks (SNNs) operate through discrete spike events and offer superior energy efficiency, providing a bio-inspired alternative. However, current ANN-to-SNN conversion often results in significant accuracy loss and increased inference time due to conversion errors such as clipping, quantization, and uneven activation. This paper proposes a novel ANN-to-SNN conversion framework based on error compensation learning. We introduce a learnable threshold clipping function, dual-threshold neurons, and an optimized membrane potential initialization strategy to mitigate the conversion error. Together, these techniques address the clipping error through adaptive thresholds, dynamically reduce the quantization error through dual-threshold neurons, and minimize the non-uniformity error by effectively managing the membrane potential. Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our method achieves high-precision and ultra-low latency among existing conversion methods. Using only two time steps, our method significantly reduces the inference time while maintains competitive accuracy of 94.75% on CIFAR-10 dataset under ResNet-18 structure. This research promotes the practical application of SNNs on low-power hardware, making efficient real-time processing possible.","Artificial neural networks (ANNs) are powerful tools for tasks like image recognition, but they consume a lot of energy, making them less ideal for use on small devices. Spiking neural networks (SNNs), inspired by how the brain works, use brief electrical pulses called ""spikes"" to transmit information and can operate much more efficiently. A common way to build an SNN is to convert an already-trained ANN into an SNN. However, this conversion often causes accuracy loss and slow processing.Our work introduces a new method to convert ANNs into SNNs more accurately and efficiently. We solve three major problems in the conversion process—errors from rounding, clipping, and timing—by using three techniques: a trainable threshold function, a special dual-threshold neuron model, and a smart way to initialize the network’s state. These changes allow us to build SNNs that are fast and accurate, even when running in just two processing steps.Experiments on standard image datasets like CIFAR-10, CIFAR-100, and ImageNet show that our method outperforms previous techniques, achieving high accuracy with much less delay and significantly lower energy use. This work helps bring low-power, brain-like computing closer to real-world applications."
Poster,Efficient Bisection Projection to Ensure Neural-Network Solution Feasibility for Optimization over General Set,https://ICML.cc//virtual/2025/poster/45779,"Enming Liang, Minghua Chen","Neural networks (NNs) have emerged as promising tools for solving constrained optimization problems in real-time. However, ensuring constraint satisfaction for NN-generated solutions remains challenging due to prediction errors. Existing methods to ensure NN feasibility either suffer from high computational complexity or are limited to specific constraint types.We present Bisection Projection, an efficient approach to ensure NN solution feasibility for optimization over general compact sets with non-empty interiors.Our method comprises two key components:(i) a dedicated NN (called IPNN) that predicts interior points (IPs) with low eccentricity, which naturally accounts for approximation errors;(ii) a bisection algorithm that leverages these IPs to recover solution feasibility when initial NN solutions violate constraints.We establish theoretical guarantees by providing sufficient conditions for IPNN feasibility and proving bounded optimality loss of the bisection operation under IP predictions. Extensive evaluations on real-world non-convex problems demonstrate that Bisection Projection achieves superior feasibility and computational efficiency compared to existing methods, while maintaining comparable optimality gaps.","Neural networks can solve complex constrained optimization problems incredibly fast, but they sometimes produce solutions that violate critical safety constraints, such as predicting power generation beyond safe operational limits. While these predictions are often nearly optimal, ensuring they satisfy all necessary constraints remains challenging due to inherent prediction errors.We developed Bisection Projection, a two-step method to correct constraint violations. First, we train a specialized neural network to predict ""interior points"" – safe positions well within constraints that naturally accommodate prediction errors. Second, when the main neural network produces an invalid solution, we use a bisection algorithm to find a valid solution along the line segment connecting the invalid prediction to our interior point.Our method guarantees 100% constraint satisfaction while maintaining solution quality and achieving up to 10,000 times faster performance than traditional projection methods. Experiments on real-world problems like power grid management and inventory optimization demonstrate that Bisection Projection works for both simple and complex constraints, making neural network optimization both reliable and practical for time-critical applications."
Poster,Efficient Core-set Selection for Deep Learning Through Squared Loss Minimization,https://ICML.cc//virtual/2025/poster/45105,Jianting Chen,"Core-set selection (CS) for deep learning has become crucial for enhancing training efficiency and understanding datasets by identifying the most informative subsets. However, most existing methods rely on heuristics or complex optimization, struggling to balance efficiency and effectiveness. To address this, we propose a novel CS objective that adaptively balances losses between core-set and non-core-set samples by minimizing the sum of squared losses across all samples. Building on this objective, we introduce theMaximum Reduction as Maximum Contribution criterion (MRMC), which identifies samples with the maximal reduction in loss as those making the maximal contribution to overall convergence. Additionally, a balance constraint is incorporated to ensure an even distribution of contributions from the core-set. Experimental results demonstrate that MRMC improves training efficiency significantly while preserving model performance with minimal cost.","In training deep learning models, selecting the most representative data subsets can significantly improve training efficiency and help us better understand the data. However, most existing methods either rely on heuristic rules or require cumbersome computations, making it difficult to achieve both efficiency and effectiveness. To address this issue, we propose a novel approach that automatically balances the importance of different data samples and selects the most valuable data subsets by calculating each sample's actual contribution to the model's training progress. Our experiments demonstrate that this method not only accelerates model training but also maintains performance comparable to using the full dataset, while saving substantial computational resources."
Poster,Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization,https://ICML.cc//virtual/2025/poster/44051,"Youran Dong, Junfeng Yang, Wei Yao, Jin Zhang","Bilevel optimization is a powerful tool for many machine learning problems, such as hyperparameter optimization and meta-learning. Estimating hypergradients (also known as implicit gradients) is crucial for developing gradient-based methods for bilevel optimization. In this work, we propose a computationally efficient technique for incorporating curvature information into the approximation of hypergradients and present a novel algorithmic framework based on the resulting enhanced hypergradient computation. We provide convergence rate guarantees for the proposed framework in both deterministic and stochastic scenarios, particularly showing improved computational complexity over popular gradient-based methods in the deterministic setting. This improvement in complexity arises from a careful exploitation of the hypergradient structure and the inexact Newton method. In addition to the theoretical speedup, numerical experiments demonstrate the significant practical performance benefits of incorporating curvature information.","Many machine learning tasks involve tuning settings while training the model—like learning how to teach while also teaching. This setup, known as bilevel optimization, is common in areas like choosing hyperparameters or learning how to learn.To make this process work well, it’s important to estimate how changing one part (like the settings) affects another (like the training). In this work, we developed a faster way to do that by capturing not just the direction of change, but also how sharply things change—like adjusting not just the steering wheel but also feeling the slope of the road. A traditional mathematical tool called the Newton method helped guide this improvement.We created a new method that’s both efficient and reliable, and we proved that it performs better than many existing approaches—especially when things are predictable. These improvements are not just on paper; experiments show it really works in practice. Our findings make bilevel optimization faster and more practical, helping machine learning systems learn and adapt more smoothly."
Poster,Efficient Diffusion Models for Symmetric Manifolds,https://ICML.cc//virtual/2025/poster/43856,"Oren Mangoubi, Neil He, Nisheeth K. Vishnoi","We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere,  special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) *arithmetic* operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an ""average-case"" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group.","Diffusion models have recently achieved remarkable success in generating synthetic data, such as realistic images, audio, and video. These models work well when data lives in flat, Euclidean space. However, in many scientific and engineering applications—such as molecular drug discovery, quantum physics, and robotics—data naturally lies on curved, non-Euclidean spaces known as manifolds. Training diffusion models on these spaces is often computationally expensive, requiring either many gradient computations or exponentially large runtimes in the data dimension.In this paper, we develop a new type of diffusion model that is efficient to train and sample from on a broad class of non-Euclidean spaces called symmetric manifolds, including spheres, tori, and the special orthogonal and unitary groups. Our key idea is to design a diffusion process that incorporates a curvature-aware covariance term. This allows us to simulate the diffusion by projecting simple Euclidean noise onto the manifold, significantly reducing computational cost.As a result, each step of our training algorithm requires only a constant number of gradient evaluations and a number of arithmetic operations nearly-linear in the data dimension, narrowing the performance gap between manifold-based and Euclidean diffusion models. We also prove that our model satisfies a probabilistic smoothness condition that guarantees accurate and stable sample generation.Experiments on synthetic datasets show that our method trains faster and produces higher-quality samples compared to previous approaches, across a variety of manifolds commonly used in scientific applications."
Poster,Efficient Distributed Optimization under Heavy-Tailed Noise,https://ICML.cc//virtual/2025/poster/46381,"Su Hyeong Lee, Manzil Zaheer, Tian Li","Distributed optimization has become the default training paradigm in modern machine learning due to the growing scale of models and datasets. To mitigate communication overhead, local updates are often applied before global aggregation, resulting in a nested optimization approach with inner and outer steps. However, heavy-tailed stochastic gradient noise remains a significant challenge, particularly in attention-based models, hindering effective training. In this work, we propose TailOPT, an efficient framework designed to address heavy-tailed noise by leveraging adaptive optimization and novel clipping techniques. We establish convergence guarantees for the TailOPT framework under heavy-tailed noise with local updates and potentially unbounded gradient variance. Among its variants, we propose a memory- and communication-efficient instantiation (named $Bi^2Clip$) that performs coordinate-wise clipping from both above and below at both the inner and outer optimizers. $Bi^2Clip$ brings about benefits of adaptive optimization (e.g., Adam) without the cost of maintaining or transmitting additional gradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates superior performance on various tasks and models compared with state-of-the-art methods, while being more efficient.","As deep neural networks grow larger and more powerful, making them efficient and scalable becomes increasingly important, especially for models like transformers used in language and AI applications. A major bottleneck is the optimizer, the tool that guides how these models learn. Popular choices like the Adam optimizer use a lot of memory, which limits how big models can get or how quickly they can be trained. In this work, we introduce a new optimizer called **$BiClip$**, which cuts memory significantly compared with state-of-the-art approaches while still improving performance. We also incorporate this into distributed settings by proposing general framework called **TailOPT**, and rigorously verify that our approach works, using novel theoretical arguments.  This makes training large models faster, cheaper, and more accessible without sacrificing quality."
Poster,Efficient Federated Incomplete Multi-View Clustering,https://ICML.cc//virtual/2025/poster/43800,"Suyuan Liu, Hao Yu, Hao Tan, KE LIANG, Siwei Wang, Shengju Yu, En Zhu, Xinwang Liu","Multi-view clustering (MVC) leverages complementary information from diverse data sources to enhance clustering performance. However, its practical deployment in distributed and privacy-sensitive scenarios remains challenging. Federated multi-view clustering (FMVC) has emerged as a potential solution, but existing approaches suffer from substantial limitations, including excessive communication overhead, insufficient privacy protection, and inadequate handling of missing views. To address these issues, we propose Efficient Federated Incomplete Multi-View Clustering (EFIMVC), a novel framework that introduces a localized optimization strategy to significantly reduce communication costs while ensuring theoretical convergence. EFIMVC employs both view-specific and shared anchor graphs as communication variables, thereby enhancing privacy by avoiding the transmission of sensitive embeddings. Moreover, EFIMVC seamlessly extends to scenarios with missing views, making it a practical and scalable solution for real-world applications. Extensive experiments on benchmark datasets demonstrate the superiority of EFIMVC in clustering accuracy, communication efficiency, and privacy preservation. Our code is publicly available at https://github.com/Tracesource/EFIMVC.","Many real-world applications collect data from different sources (or “views”), like text, images, or sensor signals. Grouping such data without labels—called multi-view clustering—can reveal meaningful patterns. But in privacy-sensitive settings like healthcare or finance, this becomes tricky: data is often stored separately (due to privacy), some views may be missing, and communication between locations is costly. We address these challenges by developing EFIMVC, a new federated multi-view clustering method. Instead of sharing sensitive data or complex models, EFIMVC only communicates simple graph-based summaries, preserving privacy and cutting down on communication. It also works well when some views are missing—something most existing methods can’t handle. Our method is theoretically sound, practical, and highly accurate. It’s a step toward more efficient, privacy-aware learning from complex, distributed data."
Poster,Efficient Fine-Grained Guidance for Diffusion Model Based Symbolic Music Generation,https://ICML.cc//virtual/2025/poster/44043,"Tingyu Zhu, Haoyu Liu, Ziyu Wang, Zhimin Jiang, Zeyu Zheng","Developing generative models to create or conditionally create symbolic music presents unique challenges due to the combination of limited data availability and the need for high precision in note pitch. To address these challenges, we introduce an efficient Fine-Grained Guidance (FGG) approach within diffusion models. FGG guides the diffusion models to generate music that aligns more closely with the control and intent of expert composers, which is critical to improve the accuracy, listenability, and quality of generated music. This approach empowers diffusion models to excel in advanced applications such as improvisation, and interactive music creation. We derive theoretical characterizations for both the challenges in symbolic music generation and the effects of the FGG approach. We provide numerical experiments and subjective evaluation to demonstrate the effectiveness of our approach. We have published a demo page to showcase performances, which enables real-time interactive generation.","Creating music with AI is challenging because it requires both creativity and precision. A single wrong note in a melody can ruin the entire piece — much like a typo can change the meaning of a sentence. Yet most existing AI music systems can’t reliably follow detailed instructions, especially when the music data is limited.Our research introduces a new technique called Fine-Grained Guidance based on Diffusion Models that helps AI models generate symbolic music (like sheet music or MIDI) more accurately and in real-time. This method ensures that each note fits correctly within the intended harmony and rhythm, just like a skilled musician would do.We tested our method with a dataset of pop music and showed that it produces better-sounding accompaniments than previous systems. Our approach also allows users — even those with limited technical skill — to guide the music generation interactively by specifying chords or melodies.This makes AI music tools more useful for composers, hobbyists, and educators, and opens the door to more intuitive collaboration between humans and machines in creative fields."
