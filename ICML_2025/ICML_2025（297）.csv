type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models,https://ICML.cc//virtual/2025/poster/46593,"Shishir G. Patil, Huanzhi Mao, Fanjia Yan, Charlie Ji, Vishnu Suresh, Ion Stoica, Joseph E Gonzalez","Function calling, also called tool use, refers to an LLM's ability to invoke external functions, APIs, or user-defined tools in response to user queries—an essential capability for agentic LLM applications. Despite its prominence, there did not exist a standard benchmark to evaluate function calling abilities, due to two reasons – the challenging nature of evaluating when a function call is valid, and the challenge of acquiring diverse, real-world functions. We present the Berkeley Function Calling Leaderboard (BFCL), a comprehensive benchmark designed to evaluate function calling capabilities in a wide range of real-world settings. The BFCL benchmark evaluates serial and parallel function calls, across various programming languages using a novel Abstract Syntax Tree (AST) evaluation method that can easily scale to thousands of functions. We construct the benchmark using a combination of expert curated, and user-contributed functions and associated prompts. Finally, BFCL benchmark evaluates the ability of models to abstain and reason in stateful multi-step agentic setting. Evaluating a wide range of models, we observe that while state-of-the-art LLMs excel at singleturn calls, memory, dynamic decision-making, and long-horizon reasoning remain open challenges. Since its preview, BFCL has become the defacto standard for evaluating function-calls, and can be accessed at gorilla.cs.berkeley.edu/leaderboard.html.","Imagine a helpful AI that can look up the weather, book a flight, or crunch numbers for you simply by “calling” the right online tool or app—much like a human who opens the correct website or piece of software on command. Today’s large language models (LLMs) are starting to do exactly that, but the community lacked a fair, comprehensive, and real-world evaluation to measure how well they perform these tool-using skills.Our work introduces the Berkeley Function Calling Leaderboard (BFCL), a public “obstacle course” that puts AIs through thousands of real-world tasks. Some tests are short and simple (one-off questions), others mimic back-and-forth conversations, and the most demanding ones ask the AI to juggle memory, reasoning and multiple steps—just as an autonomous assistant would in daily life. To grade the answers quickly and reliably, we created a novel checking method that looks at the structure of each tool call rather than running every tool for real, letting the benchmark scale to thousands of functions.Early results reveal a split personality: top AIs ace the one-shot questions but still stumble when they must remember context, manage long conversations, or decide when not to act. By spotlighting these gaps, the BFCL gives researchers and companies a clear target for the next generation of more capable, trustworthy AI assistants. Anyone can explore the live leaderboard online and watch the field improve in real time."
Poster,The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph,https://ICML.cc//virtual/2025/poster/44113,"Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Reza Haffari","The performance of large language models (LLMs) is strongly influenced by the quality and diversity of data used during supervised fine-tuning (SFT). However, current data selection methods often prioritize one aspect over the other, resulting in suboptimal training outcomes. To address this, we formulate data selection as a set cover problem and present GraphFilter, a novel approach that balances both quality and diversity in data selection. GraphFilter models the dataset as a bipartite graph connecting sentences to their constituent n-grams, then employs a priority function that combines quality and diversity metrics multiplicatively. GraphFilter iteratively selects sentences with the highest priority, removes covered n-grams from the bipartite graph, and recomputes priorities to reflect the changing data landscape. We validate GraphFilter using three model backbones across six widely-used benchmarks, demonstrating that it outperforms nine existing baselines in both model performance and computational efficiency. Further analysis shows that our design choices lead to more effective subset selection, underscores the value of instruction diversity, and provides insights into how quality and diversity interact with different subset sizes.","Training large language models (LLMs) requires selecting the right examples from massive datasets. If we only focus on quality, we might miss out on important diversity; if we only focus on diversity, we might include less useful data. Our work introduces a new method that helps pick examples that are both high-quality and diverse by modeling the data as a network connecting sentences to their key phrases. This approach ensures the selected examples cover a wide range of topics and are useful for training. We show that our method leads to better-performing models and is more efficient than previous techniques. By making data selection smarter, our work can help train language models that are both more accurate and less costly to develop."
Poster,The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning,https://ICML.cc//virtual/2025/poster/44019,"Dulhan Jayalath, Gilad Landau, Brendan Shillingford, Mark Woolrich, ʻŌiwi Parker Jones","The past few years have seen remarkable progress in the decoding of speech from brain activity, primarily driven by large single-subject datasets. However, due to individual variation, such as anatomy, and differences in task design and scanning hardware, leveraging data across subjects and datasets remains challenging. In turn, the field has not benefited from the growing number of open neural data repositories to exploit large-scale deep learning. To address this, we develop neuroscience-informed self-supervised objectives, together with an architecture, for learning from heterogeneous brain recordings. Scaling to nearly **400 hours** of MEG data and **900 subjects**, our approach shows generalisation across participants, datasets, tasks, and even to *novel* subjects. It achieves **improvements of 15-27%** over state-of-the-art models and **matches *surgical* decoding performance with *non-invasive* data**. These advances unlock the potential for scaling speech decoding models beyond the current frontier.","Although more and more data from participants recorded with safe brain imaging devices are appearing publicly on the internet, we do not have methods that can effectively train AI on all of this data together because each data source uses different methods to collect their data and provide different labels. We built a method that resolves the differences between these data sources and is able to work without labels, allowing all of the brain imaging data on the internet to be used to train AI. In turn, this unblocks the path to building better AI-driven brain-computer interfaces by collecting and combining even more data. We hope to eventually use these methods to help paralysed patients communicate through AI that deciphers their intended speech from their brain signals."
Poster,The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive to Initial Conditions,https://ICML.cc//virtual/2025/poster/45615,"Gül Sena Altıntaş, Devin Kwok, Colin Raffel, David Rolnick","Neural network training is inherently sensitive to initialization and the randomness induced by stochastic gradient descent. However, it is unclear to what extent such effects lead to meaningfully different networks, either in terms of the models' weights or the underlying functions that were learned. In this work, we show that during the initial ""chaotic"" phase of training, even extremely small perturbations reliably causes otherwise identical training trajectories to diverge-an effect that diminishes rapidly over training time. We quantify this divergence through (i) $L^2$ distance between parameters, (ii) the loss barrier when interpolating between networks, (iii) $L^2$ and barrier between parameters after permutation alignment, and (iv) representational similarity between intermediate activations; revealing how perturbations across different hyperparameter or fine-tuning settings drive training trajectories toward distinct loss minima. Our findings provide insights into neural network training stability, with practical implications for fine-tuning, model merging, and diversity of model ensembles.","Due to noise, two neural networks trained from the same random starting point can learn one of many different solutions to the same problem, whereas pre-trained networks tend to learn the same solution. What we don’t know is, when and how do networks switch from learning different solutions to the same solution? To answer this question, we train twin copies of neural networks in exactly the same way, but add a tiny change (perturbation) to one of the copies during training. We find that for networks at random starting points, even the tiniest change (far smaller than typical random effects) causes training to learn different solutions, whereas pre-trained networks only learn different solutions when changes much larger than random effects are applied. Our findings are significant because we often need to retrain and combine knowledge from several huge networks (such as large language models). As some methods work better with similar solutions versus different solutions, we can tailor our retraining or model combining methods to best target each case."
Poster,The Canary’s Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text,https://ICML.cc//virtual/2025/poster/44552,"Matthieu Meeus, Lukas Wutschitz, Santiago Zanella-Beguelin, Shruti Tople, Reza Shokri","How much information about training samples can be leaked through synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we assume an adversary has access to some synthetic data generated by a LLM. We design membership inference attacks (MIAs) that target the training data used to fine-tune the LLM that is then used to synthesize data. The significant performance of our MIA shows that synthetic data leak information about the training data. Further, we find that canaries crafted for model-based MIAs are sub-optimal for privacy auditing when only synthetic data is released. Such out-of-distribution canaries have limited influence on the model’s output when prompted to generate useful, in-distribution synthetic data, which drastically reduces their effectiveness. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries with an in-distribution prefix and a high-perplexity suffix that leave detectable traces in synthetic data. This enhances the power of data-based MIAs and provides a better assessment of the privacy risks of releasing synthetic data generated by LLMs.","Large Language Models (LLMs) can generate synthetic data that closely mimics human-written content, making them an attractive alternative to sharing real data. However, the fact that data is synthetic does not guarantee that it preserves privacy. To evaluate the true privacy risks, we use privacy auditing; specifically, by designing attacks that test whether specific data points influenced the model’s outputs. We show that even when only synthetic text is released, attackers can still infer whether particular records were part of the model’s training set. Existing auditing tools, such as inserting unique “canary” phrases into the training data, prove ineffective in this context, as such atypical phrases are unlikely to appear in useful synthetic outputs. To address this, we design a new class of canaries tailored to the behavior of LLMs: they begin with typical, in-distribution prefixes and end with rare, particular suffixes that leave detectable traces in the generated data. This makes them more effective for auditing privacy risks in synthetic data. Together, we hope that the attack and optimal canary design we propose enable a thorough assessment of the privacy risks of releasing synthetic data generated by LLMs."
Poster,The Case for Learned Provenance-based System Behavior Baseline,https://ICML.cc//virtual/2025/poster/45219,"Yao Zhu, Zhenyuan LI, yangyang wei, Shouling Ji","Provenance graphs describe data flows and causal dependencies of host activities, enabling to track the data propagation and manipulation throughout the systems, which provide a foundation for intrusion detection. However, these Provenance-based Intrusion Detection Systems (PIDSes) face significant challenges in storage, representation, and analysis, which impede the efficacy of machine learning models such as Graph Neural Networks (GNNs) in processing and learning from these graphs. This paper presents a novel learning-based anomaly detection method designed to efficiently embed and analyze large-scale provenance graphs. Our approach integrates dynamic graph processing with adaptive encoding, facilitating compact embeddings that effectively address out-of-vocabulary (OOV) elements and adapt to normality shifts in dynamic real-world environments. Subsequently, we incorporate this refined baseline into a tag-propagation framework for real-time detection. Our evaluation demonstrates the method's accuracy and adaptability in anomaly path mining, significantly advancing the state-of-the-art in handling and analyzing provenance graphs for anomaly detection.","Computer systems handle large amounts of information every day. A provenance graph acts like a timeline that records what happened, who did it, and when it happened. When a hacker attacks a system, this graph can help us trace their actions and find out which users and data were affected. However, since the graph keeps track of even the smallest events in the system,  it grows quickly and can become huge. This makes it hard to spot what the hacker did. We decompose system activities into individual events, such as one process reading a file. By checking how frequent each event happens, we can tell whether it is normal or unusual. We also take advantage of out-of-vocabulary words -- those not seen during training -- to help us mine for threats. Using a lightweight detection model, our system monitors the growing provenance graph in real time and instantly raises an alert when it detects an anomaly path. Our method can recognize new types of system behaviors and tell them apart from actual attacks. It runs efficiently, using only a small amount of computing resources, and can manage and analyze large-scale provenance graphs."
Poster,The Complexity of Learning Sparse Superposed Features with Feedback,https://ICML.cc//virtual/2025/poster/45975,Akash Kumar,"The success of deep networks is crucially attributed to their ability to capture latent features within a representation space. In this work, we investigate whether the underlying learned features of a model can be efficiently retrieved through feedback from an agent, such as a large language model (LLM), in the form of relative triplet comparisons. These features may represent various constructs, including dictionaries in LLMs or components of a covariance matrix of Mahalanobis distances. We analyze the feedback complexity associated with learning a feature matrix in sparse settings. Our results establish tight bounds when the agent is permitted to construct activations and demonstrate strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machine-trained models and dictionary extraction from sparse autoencoders trained on Large Language Models.","Deep networks are known to capture meaningful latent features within a representation space. Recent work in mechanistic interpretability investigates how to retrieve interpretable features from Large networks such as LLMs. We propose a framework with feedback (from an agent/teacher) to retrieve sparse features in the form of a dictionary, and provide tight theoretical guarantees (backed with experimental results) under both constructive and distributional settings. Our results provide insights into theoretical bottlenecks in feature retrieval and learning across various settings and show a trade-off in expressiveness-versus-recoverability of features."
Poster,The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45044,"Jiashun Liu, Johan Obando-Ceron, Pablo Samuel Castro, Aaron Courville, Ling Pan","Off-policy deep reinforcement learning (RL) agents typically leverage replay buffers for reusing past experiences during learning. This can help sample efficiency when the collected data is informative and aligned with the learning objectives; when that is not the case, it has the effect of ``polluting'' the replay buffer with data that can exacerbate optimization challenges in addition to wasting environment interactions due to redundant sampling. We argue that sampling these uninformative and wasteful transitions can be avoided by addressing the **sunk cost fallacy** which, in the context of deep RL, is the tendency towards continuing an episode until termination. To address this, we propose the *learn to stop* (**LEAST**) mechanism which uses statistics based on $Q$-values and gradient to guide early episode termination which helps agents recognize when to terminate unproductive episodes early. We demonstrate that our method improves learning efficiency on a variety of RL algorithms, evaluated on both the MuJoCo and DeepMind Control Suite benchmarks.","In this paper, we observe the sunk cost fallacy in mainstream deep RL algorithms. This fallacy manifests as agents executing each episode blindly, incur significant interaction costs, and may pollute the data distribution in the buffer, limiting their learning potential. This observation highlights an overlooked aspect within the community: the inability of existing algorithms to stop before entering into suboptimal trajectories may be a hidden factor that limits their performance. To address this issue, we propose a direct optimization approach called **LEAST** to quantify the current situation and control the termination of sampling. **LEAST** effectively mitigates the sunk cost fallacy without the need for additional networks and enhances learning efficiency across diverse scenarios. We hope that the problem presented in this paper could inspire future research aimed at optimizing sampling and learning efficiency."
Poster,The dark side of the forces: assessing non-conservative force models for atomistic machine learning,https://ICML.cc//virtual/2025/poster/45458,"Filippo Bigi, Marcel Langer, Michele Ceriotti","The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, have revolutionized the fields of computational chemistry and materials discovery.In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically constrained approach, suggesting that directly predicting the forces yields a better trade-off between accuracy and computational efficiency -- and that energy conservation can be learned during training.This work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics.Contrary to the case of rotational symmetry, energy conservation is hard to learn, monitor, and correct for.The best approach to exploit the acceleration afforded by direct force prediction might be to use it in tandem with a conservative model, reducing -- rather than eliminating -- the additional cost of backpropagation, but avoiding the pathological behavior associated with non-conservative forces.","Computer simulations are an important part of the search for high-performance materials and the development of pharmaceuticals. Unfortunately, these simulations are very expensive since they involve numerically solving physical equations to high precision. To lower this cost, machine learning models can be trained on example calculations to predict their outcome much more efficiently.There is a lot of debate about how to design and improve these surrogate models, particularly around the question to what extent the models should respect the underlying physics of the calculations they are supposed to model. Disregarding some of the physical constraints of the simulations, for instance that the forces acting on atoms should be the derivative of the energy, can make models faster to evaluate and train, as demonstrated in some recent work.In this paper, we argue that this particular constraint, also called energy conservation, is essential to perform physically meaningful simulations and that it should not be disregarded. We make this point both theoretically and through a series of experiments, which highlight different, sometimes subtle, ways in which simulations with such models can fail. We also discuss strategies to keep the speedup of disregarding energy conservation while avoiding unphysical simulation outcomes."
Poster,The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models,https://ICML.cc//virtual/2025/poster/44766,"Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun","Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling. Our source code is provided on https://github.com/alignrm/Generalizable-MM-RM.","Reward models, which teach AI systems what makes a good response to images and text, often take dangerous shortcuts. Our research found these models primarily judge responses based on text patterns alone, ignoring visual information. This causes them to fail in new situations where text-only shortcuts don't work.We developed a ""Shortcut-aware"" algorithm that identifies when reward models over-rely on text patterns. It emphasizes training examples where text-only understanding fails, forcing the model to develop genuine comprehension of both visual and textual information together, rather than taking the easier text-only route.Our approach significantly improves reward models' ability to handle unfamiliar data, making AI systems more reliable when responding to image-based questions. In real-world tests, our models produced better responses with fewer visual errors and hallucinations. This advancement is crucial for developing trustworthy AI assistants that accurately understand both what they see and read."
