type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,EGPlace: An Efficient Macro Placement Method via Evolutionary Search with Greedy Repositioning Guided Mutation,https://ICML.cc//virtual/2025/poster/46407,"ji deng, Zhao Li, Ji Zhang, Jun Gao","Macro placement, which involves optimizing the positions of modules, is a critical phase in modern integrated circuit design and significantly influences chip performance. The growing complexity of integrated circuits demands increasingly sophisticated placement solutions. Existing approaches have evolved along two primary paths (e.g., constructive and adjustment methods), but they face significant practical limitations that affect real-world chip design. Recent hybrid frameworks such as WireMask-EA have attempted to combine these strategies, but significant technical barriers still remain, including the computational overhead from separated layout adjustment and reconstruction that often require complete layout rebuilding, the inefficient exploration of design spaces due to random mutation operations, and the computational complexity of mask-based construction methods that limit scalability.  To overcome these limitations, we introduce EGPlace, a novel evolutionary optimization framework that combines guided mutation strategies with efficient layout reconstruction. EGPlace introduces two key innovations: a greedy repositioning-guided mutation operator that systematically identifies and optimizes critical layout regions, and an efficient mask computation algorithm that accelerates layout evaluation. Our extensive evaluation using ISPD2005 and Ariane RISC-V CPU benchmarks demonstrate that EGPlace reduces wirelength by \textbf{10.8\%} and \textbf{9.3\%} compared to WireMask-EA and the state-of-the-art reinforcement learning-based constructive method EfficientPlace, respectively, while achieving speedups of 7.8$\times$ and 2.8$\times$ over these methods.","The macro placement problem is an importance stage in designing modern computer chips. The task involves figuring out the best way to arrange large blocks on the chip. The way these blocks are placed can significantly affect the chip performance. The vast number of possible placement configurations makes macro placement a challenging task.Current methods for macro placement either construct layouts from scratch or iteratively refine existing ones. However, both approaches have drawbacks: they can involve high computational costs, lack sufficient contextual information to effectively guide placement decisions, or struggle to produce high-quality solutions. Some recent approaches combine these strategies by integrating a greedy placement technique within an evolutionary search framework, yet they still encounter challenges such as inefficient exploration caused by random mutations and slow computation due to the need to rebuild the entire layout even after minor adjustments.We introduce EGPlace, an efficient placement method that improves evolutionary search for chip layout by incorporating a novel guided mutation operator. This operator smartly selects a subset of blocks that have the greatest impact on layout quality and greedily repositions only those. It improves sample efficiency by increasing the likelihood of beneficial mutations, and reduce computational costs by avoiding the cost of rebuilding the entire layout. Experiments on modern circuit benchmarks show that EGPlace generates higher-quality layouts with up to 11% shorter wirelengths, while achieving up to 8× speedup over state-of-the-art methods."
Poster,Ehrenfeucht-Haussler Rank and Chain of Thought,https://ICML.cc//virtual/2025/poster/46252,"Pablo Barcelo, Alexander Kozachinskiy, Tomasz Steifer","The notion of _rank_ of a Boolean function has been a cornerstone in PAC learning, enabling quasipolynomial-time learning algorithms for polynomial-size decision trees. We present a novel characterization of rank, grounded in the well-known Transformer architecture. We show that the rank of a function $f$ corresponds to the minimum number of _Chain of Thought_ (CoT) steps required by a single-layer Transformer with hard attention to compute $f$. Based on this characterization we establish tight bounds on the number of CoT steps required for specific problems, showing that $\ell$-fold function composition necessitates exactly $\ell$ CoT steps. Furthermore, we analyze the problem of identifying the position of the $k$-th occurrence of 1 in a Boolean sequence, proving that it requires $k$ CoT steps.","The ability of Transformers to perform function composition has garnered increasing attention in recent years, as understanding this capability sheds light on the computational resources they require to infer implicit knowledge from a given set of facts. Peng et al. demonstrated that single-layer, soft-attention Transformers without Chain-of-Thought (CoT) reasoning are fundamentally incapable of function composition. However, when CoT is introduced, they can achieve iterated composition—albeit at the cost of requiring a growing number of steps, which depends on both vector dimensionality and feature precision. Our work precisely quantifies the number of steps needed for t-th iterated composition and establishes that, under the idealized assumption of hard-attention, the number of required CoT steps is exactly t. This finding underscores a key insight: while CoT enables function composition, it does so incrementally—one step at a time."
Poster,Eigen Analysis of Conjugate Kernel and Neural Tangent Kernel,https://ICML.cc//virtual/2025/poster/45907,"Xiangchao Li, Xiao Han, Qing Yang","In this paper, we investigate deep feedforward neural networks with random weights. The input data matrix $\boldsymbol{X}$ is drawn from a Gaussian mixture model. We demonstrate that certain eigenvalues of the conjugate kernel and neural tangent kernel may lie outside the support of their limiting spectral measures in the high-dimensional regime. The existence and asymptotic positions of such isolated eigenvalues are rigorously analyzed. Furthermore, we provide a precise characterization of the entrywise limit of the projection matrix onto the eigenspace associated with these isolated eigenvalues. Our findings reveal that the eigenspace captures inherent group features present in $\boldsymbol{X}$. This study offers a quantitative analysis of how group features from the input data evolve through hidden layers in randomly weighted neural networks.","Imagine we have a large set of high-dimensional data—such as images or customer profiles—and we feed them into a neural network that has not been trained yet, with randomly initialized weights. Even at this untrained stage, the network transforms the data in meaningful ways as it propagates through each layer.In our paper, we focus on two important matrices in neural networks: the Conjugate Kernel (CK) and the Neural Tangent Kernel (NTK). These matrices play a central role in understanding how neural networks process and learn from data. Specifically, we investigate the presence of isolated eigenvalues in these matrices—eigenvalues that are clearly separated from the bulk of the spectrum. In methods like PCA, such isolated eigenvalues often correspond to meaningful patterns or hidden group structures in the data. This leads us to ask: can the eigenspaces corresponding to isolated eigenvalues of the CK and NTK matrices similarly capture useful information about the data?Our work demonstrates that, under mild conditions, isolated eigenvalues do indeed appear in the CK and NTK matrices. More importantly, we show that these eigenvalues and their associated eigenspaces contain valuable information about the underlying group structures in the data. We further support our theoretical findings with numerical simulations."
Poster,Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias,https://ICML.cc//virtual/2025/poster/46300,"Yuanzhe Hu, Kinshuk Goel, Vlad Killiakov, Yaoqing Yang","Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight matrices has been an active area of research in recent years. At a high level, eigenspectrum analysis of DNNs involves measuring the heavytailness of the empirical spectral densities (ESD) of weight matrices. It provides insight into how well a model is trained and can guide decisions on assigning better layer-wise training hyperparameters. In this paper, we address a challenge associated with such eigenspectrum methods: the impact of the aspect ratio of weight matrices on estimated heavytailness metrics. We demonstrate that matrices of varying sizes (and aspect ratios) introduce a non-negligible bias in estimating heavytailness metrics, leading to inaccurate model diagnosis and layer-wise hyperparameter assignment. To overcome this challenge, we propose FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the weight matrices by subsampling submatrices with a fixed aspect ratio. Instead of measuring the heavytailness of the original ESD, we measure the average ESD of these subsampled submatrices. We show that measuring the heavytailness of these submatrices with the fixed aspect ratio can effectively mitigate the aspect ratio bias. We validate our approach across various optimization techniques and application domains that involve eigenspectrum analysis of weights, including image classification in computer vision (CV) models, scientific machine learning (SciML) model training, and large language model (LLM) pruning. Our results show that despite its simplicity, FARMS uniformly improves the accuracy of eigenspectrum analysis while enabling more effective layer-wise hyperparameter assignment in these application domains. In one of the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model by 17.3\% when compared with the state-of-the-art method.","We assess the training status of each layer by calculating the heavy-tailedness of the eigenspectrum of the model's weight matrices. However, earlier methods seemed to overlook the bias introduced by different aspect ratios of weight matrices, which led to severe misjudgment of the training status for some layers.In this work, we adopt a simple fixed-window analysis method named FARMS. We sample many submatrices of the same size from the original weight matrix, like dividing farmland into many plots for cultivation. We then concatenate the eigenspectrum of these decomposed submatrices. By analyzing the heavy-tailedness of the concatenated spectrum, we can obtain a more accurate estimate of the training status of the weight matrix, thus eliminating the bias caused by different aspect ratios.We validated the effectiveness of our new method on various layer-wise optimization approaches. By using our method, we can optimize each layer of the model more precisely, such as by providing more accurate learning rates. This leads to a more balanced training status across all layers of the trained model."
Poster,E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time,https://ICML.cc//virtual/2025/poster/46643,Adam Breuer,"In this paper, we provide the first practical algorithms with provable guarantees for the problem of inferring the topics assigned to each document in an LDA topic model. This is the primary inference problem for many applications of topic models in social science, data exploration, and causal inference settings. We obtain this result by showing a novel non-gradient-based, combinatorial approach to estimating topic models. This yields algorithms that converge to near-optimal posterior probability in logarithmic parallel computation time (adaptivity)---exponentially faster than any known LDA algorithm.  We also show that our approach can provide interpretability guarantees such that each learned topic is formally associated with a known keyword. Finally, we show that unlike alternatives, our approach can maintain the independence assumptions necessary to use the learned topic model for downstream causal inference methods that allow researchers to study topics as treatments. In terms of practical performance, our approach consistently returns solutions of higher semantic quality than solutions from state-of-the-art LDA algorithms, neural topic models, and LLM-based topic models across a diverse range of text datasets and evaluation parameters.","Topic models are among the most popular techniques in machine learning and social science, where they are widely used to help researchers summarize the key themes that characterize large datasets containing many text documents. However, existing topic modeling algorithms are known to produce unreliable results that can be difficult to interpret, and they are also too slow to use on very large datasets. This paper introduces a new algorithm that solves topic models with strong mathematical guarantees, ensuring that the topics it finds accurately represent the data. Unlike previous methods, this algorithm runs exponentially faster and provides clear, interpretable results, making it especially valuable in sensitive areas such as detecting harmful or abusive content online, where transparency is not just desirable, but ethically and legally essential. Finally, we show that in experiments on real-world datasets, our algorithm learns topics that exhibit better semantic quality than alternatives, including recent LLM and neural network based algorithms."
Poster,ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics,https://ICML.cc//virtual/2025/poster/44449,"Letian Chen, Nina Moorman, Matthew Gombolay","Reinforcement learning (RL) has demonstrated compelling performance in robotic tasks, but its success often hinges on the design of complex, ad hoc reward functions. Researchers have explored how Large Language Models (LLMs) could enable non-expert users to specify reward functions more easily. However, LLMs struggle to balance the importance of different features, generalize poorly to out-of-distribution robotic tasks, and cannot represent the problem properly with only text-based descriptions. To address these challenges, we propose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a novel framework that combines natural language guidance with visual user demonstrations to align robot behavior with user intentions better. By incorporating visual inputs, ELEMENTAL overcomes the limitations of text-only task specifications, while leveraging inverse reinforcement learning (IRL) to balance feature weights and match the demonstrated behaviors optimally. ELEMENTAL also introduces an iterative feedback-loop through self-reflection to improve feature, reward, and policy learning. Our experiment results demonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and achieves 41.3% better generalization in out-of-distribution tasks, highlighting its robustness in LfD.","Teaching robots to perform tasks can be hard, especially when people struggle to describe exactly what they want. ELEMENTAL is a new method that helps robots learn by watching humans demonstrate tasks and following their natural language instructions. It uses vision-language models that understand both images and language to figure out what matters for success, and then lets users refine the robot’s behavior by giving feedback. ELEMENTAL was tested in both simulation and a real-world salad-making task, where people taught a robot using video recordings and natural-language instructions—no coding or manual labeling needed. The robot learned more human-aligned behaviors than existing systems, showing promise for easier and more intuitive robot training in the future."
Poster,Eliciting Language Model Behaviors with Investigator Agents,https://ICML.cc//virtual/2025/poster/46145,"Xiang Li, Neil Chowdhury, Daniel Johnson, Tatsunori Hashimoto, Percy Liang, Sarah Schwettmann, Jacob Steinhardt","Language models exhibit complex, diverse behaviors when prompted with free-form text, making it hard to characterize the space of possible outputs. We study the problem of behavioral elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations, harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train amortized investigator models to emulate the posterior distribution over the prompts, conditioned on the target behavior. Specifically, we first fit a reverse model and then use reinforcement learning to optimize likelihood of generating the target behavior. To improve the diversity of the prompt distribution, we further propose a novel iterative training objective based on the Frank-Wolfe algorithm that encourages each iteration to discover different sets of prompts not captured by previous iterations. Our investigator models produce prompts that exhibit a variety of effective and human-interpretable strategies for behavior elicitation, obtaining a 100% attack success rate on AdvBench (Harmful Behaviors) and an 85% hallucination rate.","Language models can unpredictably produce harmful or incorrect responses. We created ""investigator agents,"" AI models trained to find prompts that induce unwanted behaviors in these models. Our agents successfully find jailbreak prompts leading to harmful outputs in a variety of language models, including Llama, GPT, and Claude. We also find prompts that elicit factual inaccuracies and aberrant personas in a target Llama model."
Poster,ELITE: Enhanced Language-Image Toxicity Evaluation for Safety,https://ICML.cc//virtual/2025/poster/46445,"Wonjun Lee, Doehyeon Lee, Eugene Choi, Sangyoon Yu, Ashkan Yousefpour, Haon Park, Bumsub Ham, Suhyun Kim","Current Vision Language Models (VLMs) remain vulnerable to malicious prompts that induce harmful outputs. Existing safety benchmarks for VLMs primarily rely on automated evaluation methods, but these methods struggle to detect implicit harmful content or produce inaccurate evaluations. Therefore, we found that existing benchmarks have low levels of harmfulness, ambiguous data, and limited diversity in image-text pair combinations. To address these issues, we propose the ELITE benchmark, a high-quality safety evaluation benchmark for VLMs, underpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE evaluator explicitly incorporates a toxicity score to accurately assess harmfulness in multimodal contexts, where VLMs often provide specific, convincing, but unharmful descriptions of images. We filter out ambiguous and low-quality image-text pairs from existing benchmarks using the ELITE evaluator and generate diverse combinations of safe and unsafe image-text pairs. Our experiments demonstrate that the ELITE evaluator achieves superior alignment with human evaluations compared to prior automated methods, and the ELITE benchmark offers enhanced benchmark quality and diversity. By introducing ELITE, we pave the way for safer, more robust VLMs, contributing essential tools for evaluating and mitigating safety risks in real-world applications.","Vision-Language Models (VLMs) are AI systems that process both images and text, but they remain vulnerable to harmful prompts that can cause unsafe outputs. Current safety benchmarks rely heavily on automated evaluation methods, but these methods often miss subtle risks or misjudge responses. As a result, many existing benchmarks contain vague or low-quality data and fail to capture the full range of harmful behaviors.To address these issues, we introduce the ELITE benchmark, a new safety benchmark designed to more accurately evaluate harmful responses in VLMs. It is built using the ELITE evaluator, which adds a toxicity score to better detect harmful responses. This allows us to remove ambiguous image-text pairs and include more diverse and meaningful image-text combinations.Our experiments show that the ELITE evaluator aligns more closely with human judgment than previous methods. By providing a stronger benchmark and evaluator, our work supports the development of safer, more trustworthy VLMs."
Poster,ELMO : Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces,https://ICML.cc//virtual/2025/poster/44648,"Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar","Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations---gradient fusion and chunking---enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7GiB required by the optimized SOTA method, Renee without compromising accuracy.","Modern recommendation systems, like those used to tag content or suggest related products,  often need to choose from millions of possible tags/products. This entails storing and crunching through billions of numbers in the last layer of multi-layered deep learning systems, leading to a major computational and memory bottleneck. Current systems represent each of the billion parameters as a mix of 32 and 16-bit numbers to speed up calculations, but these are still memory-intensive, sometimes unstable, and still remain slow. We introduce a new method that uses a much coarser representation lower precision (16 and 8-bits) for representing these parameters leading to substantial memory savings and speeding up computation as it requires moving less data around.To maintain both stability and accuracy at lower precision, we applied techniques that prevent small numbers from being lost when added to much larger ones (a method known as Kahan summation) and introduced randomness to reduce systematic bias in rounding errors (known as stochastic rounding). We also combine operations to reduce intermediate read/write steps and break large label computations into smaller, manageable parts (known as chunking). These optimizations help cut down memory use even further. These help cut down memory use even further. With our approach, we reduced memory use for a 3-million-label model from 39.7 GB to just 6.6 GB. This also made it possible to train models with up to 8.6 million labels on a regular consumer GPU  RTX 4060 Ti, something previously thought impractical."
Poster,ELoRA: Low-Rank Adaptation for Equivariant GNNs,https://ICML.cc//virtual/2025/poster/44404,"Chen Wang, Siyu Hu, Guangming Tan, Weile Jia","Pre-trained interatomic potentials have become a new paradigm for atomistic materials simulations, enabling accurate and efficient predictions across diverse chemical systems. Despite their promise, fine-tuning is often required for complex tasks to achieve high accuracy. Traditional parameter-efficient fine-tuning approaches are effective in NLP and CV. However, when applied to SO(3) equivariant pre-trained interatomic potentials, these methods will inevitably break equivariance—a critical property for preserving physical symmetries. In this paper, we introduce ELoRA (Equivariant Low-Rank Adaptation), a novel fine-tuning method designed specifically for SO(3) equivariant Graph Neural Networks (GNNs), the backbones in multiple pre-trained interatomic potentials. ELoRA adopts a path-dependent decomposition for weights updating which offers two key advantages: (1) it preserves SO(3) equivariance throughout the fine-tuning process, ensuring physically consistent predictions, and (2) it leverages low-rank adaptations to significantly improve data efficiency. We prove that ELoRA maintains equivariance and demonstrate its effectiveness through comprehensive experiments. On the rMD17 organic dataset, ELoRA achieves a 25.5\% improvement in energy prediction accuracy and a 23.7\% improvement in force prediction accuracy compared to full-parameter fine-tuning. Similarly, across 10 inorganic datasets, ELoRA achieves average improvements of 12.3\% and 14.4\% in energy and force predictions, respectively. Code will be made publicly available at https://github.com/hyjwpk/ELoRA.","Designing a better battery or medicine starts with understanding how atoms interact. AI tools are increasingly used to predict these atomic behaviors, helping speed up the discovery process. But they often struggle when the material is new or especially complex. To improve them, scientists usually make small adjustments using new data. The problem is, many of these adjustments accidentally break an important physical rule: if you rotate the molecule, the prediction should rotate too.We present a new method called ELoRA that avoids this issue. It carefully updates the AI while making sure the model still respects this rotational symmetry. Surprisingly, it also works well with only a small amount of new data.In experiments, ELoRA makes predictions significantly more accurate across a variety of chemical systems. This could make AI tools more dependable for studying new materials in scientific research and industry."
