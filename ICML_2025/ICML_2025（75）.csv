type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models,https://ICML.cc//virtual/2025/poster/44313,"Rei Higuchi, Taiji Suzuki","Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model.This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences.To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO).DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling.We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure.Experiments demonstrate that DDRO achieves superior performance compared to existing methods, showcasing its effectiveness and potential for significant improvement.DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.","Large Language Models (LLMs) are useful, but they can sometimes generate content that is misaligned with human intent or preferences and may be harmful. Previous approaches to aligning LLMs with human preferences often fit complex human tastes into predefined and possibly oversimplified structures. This presents a fundamental challenge: even with extensive training data, LLMs might not truly learn desired responses. To address this issue, our research proposes a new LLM alignment method called Direct Density Ratio Optimization (DDRO). DDRO provides a way to guide LLMs towards human preferences without needing to rely on these predefined and potentially restrictive assumptions about the nature of human preferences. It learns directly from examples of preferred and unpreferred responses, allowing for a more flexible and data-driven alignment with genuine human tastes.A key aspect of DDRO is its theoretical guarantee: as training data increases, the LLM will generate responses more accurately reflecting true human preferences. This property of our method helps develop LLMs better aligned with human values, fostering safer, more reliable, and truly beneficial AI systems."
Poster,Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator,https://ICML.cc//virtual/2025/poster/45454,"Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang","While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1\% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512$\times$512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256$\times$256.","Modern AI image generators can produce stunning visuals, but the way they are trained still has a key weakness: they tend to play it safe by trying to cover all possible outputs, which leads to blurry or less realistic images when the model size is limited. This happens because the most common training method (called maximum likelihood estimation) encourages covering every possibility rather than focusing on the most likely or high-quality results.To overcome this, we introduce a new approach called Direct Discriminative Optimization (DDO). It improves training by helping the model learn from its own mistakes — identifying and correcting low-quality outputs — without needing a separate ""judge"" model like in GANs. Inspired by recent techniques in AI alignment, DDO works by comparing a model to a fixed reference and using that difference as a learning signal.DDO can upgrade existing models efficiently and drastically improve image quality. In our experiments, it helped leading AI models produce sharper, more realistic images across several standard benchmarks — outperforming previous best results without needing extra tricks or more training data."
Poster,Directed Graph Grammars for Sequence-based Learning,https://ICML.cc//virtual/2025/poster/44192,"Michael Sun, Orion Foo, Gang Liu, Wojciech Matusik, Jie Chen","Directed acyclic graphs (DAGs) are a class of graphs commonly used in practice, with examples that include electronic circuits, Bayesian networks, and neural architectures. While many effective encoders exist for DAGs, it remains challenging to decode them in a principled manner, because the nodes of a DAG can have many different topological orders. In this work, we propose a grammar-based approach to constructing a principled, compact and equivalent sequential representation of a DAG. Specifically, we view a graph as derivations over an unambiguous grammar, where the DAG corresponds to a unique sequence of production rules. Equivalently, the procedure to construct such a description can be viewed as a lossless compression of the data. Such a representation has many uses, including building a generative model for graph generation, learning a latent space for property prediction, and leveraging the sequence representational continuity for Bayesian Optimization over structured data.","Directed acyclic graphs (DAGs) are used to represent everything from electronic circuits to neural network architectures, but they are hard to generate because there’s no single “correct” order in which to build their nodes and edges, leading to order-dependent sequential descriptions that lead to brittle decoders. We introduce Directed Graph Grammar Embedded Derivations (DIGGED), which rewrites a DAG as a sequence of rule-based construction steps. We learn these rules by repeatedly mining common patterns, solving compatibility puzzles to derive maximally shared rewrite rules via a description length minimization objective, and iteratively compressing the provided data without losing information. Crucially, the DIGGED representation establishes a one-to-one problem mapping between DAG generation and sequence modeling, and we take full advantage by training Transformer-based decoders for generation and downstream optimization. On benchmarks spanning neural architectures, Bayesian networks, and analog circuits, DIGGED achieves perfect validity and uniqueness, substantial gains in predictive accuracy and Bayesian optimization performance over prior DAG decoders, and delivers an interpretable, compact representation that bridges graph-structured data with powerful sequence-based methods. By turning any DAG collection into a compact, principled “design language,” DIGGED opens the door to more reliable, scalable graph design and optimization across scientific and engineering domains."
Poster,Directly Forecasting Belief for Reinforcement Learning with Delays,https://ICML.cc//virtual/2025/poster/45241,"Qingyuan Wu, Yuhui Wang, Simon Zhan, Yixuan Wang, Chung-Wei Lin, Chen Lv, Qi Zhu, Jürgen Schmidhuber, Chao Huang","Reinforcement learning (RL) with delays is challenging as sensory perceptions lag behind the actual events: the RL agent needs to estimate the real state of its environment based on past observations. State-of-the-art (SOTA) methods typically employ recursive, step-by-step forecasting of states. This can cause the accumulation of compounding errors. To tackle this problem, our novel belief estimation method, named Directly Forecasting Belief Transformer (DFBT), directly forecasts states from observations without incrementally estimating intermediate states step-by-step. We theoretically demonstrate that DFBT greatly reduces compounding errors of existing recursively forecasting methods, yielding stronger performance guarantees. In experiments with D4RL offline datasets, DFBT reduces compounding errors with remarkable prediction accuracy. DFBT's capability to forecast state sequences also facilitates multi-step bootstrapping, thus greatly improving learning efficiency. On the MuJoCo benchmark, our DFBT-based method substantially outperforms SOTA baselines. Code is available at \href{https://github.com/QingyuanWuNothing/DFBT}{https://github.com/QingyuanWuNothing/DFBT}.","Robots and AI systems often make decisions based on delayed information, which can cause mistakes to build up over time. Most current techniques try to guess the missing information step-by-step, but small errors add up quickly. Our research introduces a new method called the Directly Forecasting Belief Transformer (DFBT). Unlike existing approaches, DFBT makes direct predictions all at once instead of in a chain. This helps it stay accurate and learn faster. We tested it on popular benchmarks and found it remarkably outperforms existing methods."
Poster,Direct Motion Models for Assessing Generated Videos,https://ICML.cc//virtual/2025/poster/43920,"Kelsey Allen, Carl Doersch, Guangyao Zhou, Mohammed Suhail, Danny Driess, Ignacio Rocco, Yulia Rubanova, Thomas Kipf, Mehdi S. M. Sajjadi, Kevin Murphy, Joao Carreira, Sjoerd van Steenkiste","A current limitation of video generative video models is that they generate plausible looking frames, but poor motion --- an issue that is not well captured by FVD and other popular methods for evaluating generated videos. Here we go beyond FVD by developing a metric which better measures plausible object interactions and motion. Our novel approach is based on auto-encoding point tracks and yields motion features that can be used to not only compare distributions of videos (as few as one generated and one ground truth, or as many as two datasets), but also for evaluating motion of single videos. We show that using point tracks instead of pixel reconstruction or action recognition features results in a metric which is markedly more sensitive to temporal distortions in synthetic data, and can predict human evaluations of temporal consistency and realism in generated videos obtained from open-source models better than a wide range of alternatives. We also show that by using a point track representation, we can spatiotemporally localize generative video inconsistencies, providing extra interpretability of generated video errors relative to prior work. An overview of the results and link to the code can be found on the project page: trajan-paper.github.io.","Current artificial intelligence models that create videos often make believable-looking individual frames, but the way things move in the videos isn't very realistic. Existing ways of checking video quality don't do a good job of spotting these poor movements, and usually require access to a whole set of videos rather than being applicable to just one.We created a new way to measure video quality that focuses specifically on how well objects move and interact. Our method works by tracking points on objects throughout the video and using this information to understand the motion. This allows us to see how realistic the movement is, even for individual videos.We found that our new approach, which uses these tracked points, is much better at detecting weird or unnatural movements in computer-generated videos compared to other methods. It also does a better job of matching what people think looks realistic and consistent in videos made by AI. Additionally, our method can help pinpoint exactly where in a video the movement looks wrong, which makes it easier to understand the errors being made."
Poster,Direct Prediction Set Minimization via Bilevel Conformal Classifier Training,https://ICML.cc//virtual/2025/poster/45700,"Yuanjie Shi, Hooman Shahrokhi, Xuesong Jia, Xiongzhi Chen, Jana Doppa, Yan Yan","Conformal prediction (CP) is a promising uncertainty quantification framework which works as a wrapper around a black-box classifier to construct prediction sets (i.e., subset of candidate classes) with provable guarantees. However, standard calibration methods for CP tend to produce large prediction sets which makes them less useful in practice. This paper considers the problem of integrating conformal principles into the training process of deep classifiers to directly minimize the size of prediction sets. We formulate conformal training as a bilevel optimization problem and propose the {\em Direct Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight behind DPSM is to minimize a measure of the prediction set size (upper level) that is conditioned on the learned quantile of conformity scores (lower level). We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training samples),while prior conformal training methods based on stochastic approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size $s$ and typically $s \ll \sqrt{n}$).Experiments on various benchmark datasets and deep models show that DPSM significantly outperforms the best prior conformal training baseline with $20.46\\%\downarrow$ in the prediction set size and validates our theory.","When machine learning models make predictions, it is important to know how confident they are—especially in high-stakes situations like medical diagnosis. Conformal prediction is a method that can wrap around any model to provide a set of likely answers, with a guarantee that the true answer is included. However, these sets are often too large to be useful.In this work, we propose a new method called Direct Prediction Set Minimization (DPSM) that helps models produce much smaller prediction sets—while still maintaining the same reliability. The main idea is to directly train the model to minimize the size of these prediction sets by combining two levels of learning: one that focuses on selecting the right threshold, and another that uses this threshold to guide learning. Our theoretical analysis shows that DPSM learns more efficiently than existing methods. Experiments on real-world datasets show that DPSM reduces the prediction set size by over $20\\%$ compared to previous best methods, making machine learning models both reliable and more practical."
Poster,DIS-CO: Discovering Copyrighted Content in VLMs Training Data,https://ICML.cc//virtual/2025/poster/43970,"André Duarte, Xuandong Zhao, Arlindo Oliveira, Lei Li","*How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data?* Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model’s training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. We provide the code in the supplementary materials.","Large Vision-Language Models learn by training on *huge collections of images and text*. However, there is *growing concern* that these collections might include **copyrighted material**, raising important legal and ethical questions.Motivated by the idea that models can *recognize images they have seen before*, we developed **DIS-CO**: a *new approach* for detecting whether a model has **memorized specific visual content**, with a special focus on *copyrighted material*.**DIS-CO works by repeatedly prompting the model to identify the source of carefully selected images**: for example, asking, *“Which movie is this frame from?”*, and requiring the model to generate its answer *freely*, instead of picking from multiple-choice options. This *free-form format is crucial*: it makes it *extremely unlikely* for the model to guess the correct answer by chance. If the model can *reliably provide the correct titles for challenging frames*, it offers **strong evidence** that it encountered this content during training.To *rigorously evaluate DIS-CO*, we created **MovieTection**, a *benchmark featuring 14,000 frames from 100 movies*, and our results are clear: *all tested models showed signs of exposure to copyrighted content*. With **DIS-CO**, even when companies do *not disclose their training data*, it becomes possible to reveal whether specific copyrighted material was used. As such, we believe this offers a *new path toward greater transparency and accountability in AI development*."
Poster,DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction,https://ICML.cc//virtual/2025/poster/46390,"Rudy Morel, Jiequn Han, Edouard Oyallon","We address the problem of predicting the next states of a dynamical system governed by *unknown* temporal partial differential equations (PDEs) using only a short trajectory. While standard transformers provide a natural black-box solution to this task, the presence of a well-structured evolution operator in the data  suggests a more tailored and efficient approach. Specifically, when the PDE is fully known, classical numerical solvers can evolve the state accurately with only a few parameters. Building on this observation, we introduce DISCO, a model that uses a large hypernetwork to process a short trajectory and generate the parameters of a much smaller operator network, which then predicts the next states through time integration. Our framework decouples dynamics estimation -- i.e., DISCovering an evolution Operator from a short trajectory -- from state prediction -- i.e., evolving this operator. Experiments show that pretraining our model on diverse physics datasets achieves state-of-the-art performance while requiring significantly fewer epochs. Moreover, it generalizes well to unseen initial conditions and remains competitive when fine-tuned on downstream tasks.","Imagine dropping ink into a still glass of water -- the way it spreads and twists is governed by complex physical laws. But what if we don’t know these laws exactly, or they change from one situation to another? Can a computer still predict how the system will move?Our model, called DISCO, tackles this challenge. It looks at a short sequence showing how the system evolves -- like the first few moments of ink dispersing -- and learns to uncover the hidden rules behind it. Then it uses these rules to forecast what happens next.This two-step process is both more efficient and more accurate than existing black-box models. Trained on a wide variety of physical systems, DISCO generalizes well and could become a useful tool to model real-world phenomena when equations are unknown or measurements are limited."
Poster,Discovering a Zero  (Zero-Vector Class of Machine Learning),https://ICML.cc//virtual/2025/poster/43749,"Harikrishna Metta, Venkatesh Babu Radhakrishnan","In Machine learning, separating data into classes is a very fundamental problem. A mathematical framework around the classes is presented in this work to deepen the understanding of classes. The classes are defined as vectors in a Vector Space, where addition corresponds to the union of classes, and scalar multiplication resembles set complement of classes. The Zero-Vector in the vector space corresponds to a class referred to as the Metta-Class. This discovery enables numerous applications. One such application, termed 'clear learning' in this work, focuses on learning the true nature (manifold) of the data instead of merely learning a boundary sufficient for classification. Another application, called 'unary class learning', involves learning a single class in isolation rather than learning by comparing two or more classes. Additionally, 'set operations on classes' is another application highlighted in this work. Furthermore, Continual Learning of classes is facilitated by smaller networks. The Metta-Class enables neural networks to learn only the data manifold; therefore, it can also be used for generation of new data. Results for the key applications are shown using the MNIST dataset. To further strengthen the claims, some results are also produced using the CIFAR-10 and ImageNet-1k embeddings. The code supporting these applications is publicly available at: github.com/hm-4/Metta-Class.","Machine learning models are often designed to tell different categories apart — like separating cats from dogs — by simply drawing a boundary between them. This idea comes from traditional statistical thinking, where the goal is to find a separating point or surface that distinguishes classes. For example, logistic regression and linear discriminant analysis (LDA) are well-known techniques that focus on learning such decision boundaries. While effective for classification, this approach doesn’t help the model truly understand what each group looks like on its own. As a result, the model may struggle to generalize to new situations, adapt to changing data, or generate meaningful new examples.Classical statistical models are often limited to simple two-class problems and can’t handle the rich complexity of the real world. In contrast, neural networks are much more powerful — they can model multiple categories at once and learn more complex structures within the data. This makes them better suited for real-world tasks. A very important property of classes is that set operations can be performed on them. We leveraged this idea in our work — we incorporated set operations on the classes learned by the neural network. We did this by modeling classes as vectors, where set operations like union and complement correspond to vector addition and scalar multiplication.This opens up new capabilities for machine learning, such as learning one class in isolation, generating new examples, or learning continuously over time. Finally, we help neural networks break free from the constraints of a statistical mindset."
Poster,Discovering Global False Negatives On the Fly for Self-supervised Contrastive Learning,https://ICML.cc//virtual/2025/poster/45576,"Vicente Balmaseda, Bokun Wang, Lin, Tianbao Yang","In self-supervised contrastive learning, negative pairs are typically constructed using an anchor image and a sample drawn from the entire dataset, excluding the anchor. However, this approach can result in the creation of negative pairs with similar semantics, referred to as ""false negatives"", leading to their embeddings being falsely pushed apart. To address this issue, we introduce *GloFND*, an optimization-based approach that automatically learns on the fly the threshold for each anchor data to *identify* its false negatives during training. In contrast to previous methods for false negative discovery, our approach *globally* detects false negatives across the entire dataset rather than locally within the mini-batch. Moreover, its per-iteration computation cost remains independent of the dataset size. Experimental results on image and image-text data demonstrate the effectiveness of the proposed method. Our implementation is available at https://github.com/vibalcam/GloFND.","Modern AI systems can learn to understand images even without being told what each image shows—a technique known as self-supervised learning. A common method is contrastive learning, where the AI compares images, bringing similar ones closer and pushing different ones apart. This creates a vector representation for each image, useful for tasks like object classification and image–text search. But without labels, the model assumes all images are different, leading to false negatives—related images, like two dog breeds, mistakenly treated as unrelated. These errors weaken learning, and checking all possible image pairs is impractical for large datasets with millions of images.We present GLOFND, a method that automatically detects false negatives during training. Unlike prior approaches that depend on limited samples or intensive computation, GLOFND searches across the full dataset by learning a custom similarity threshold for each image. It does so while keeping computation costs low and independent of dataset size.GLOFND improves the quality of vector representations and yields more accurate results on tasks like classification and image–text retrieval. It is easy to apply, handles large datasets effectively, and integrates seamlessly into existing contrastive learning frameworks, boosting their performance."
