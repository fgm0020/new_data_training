type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Online Clustering of Dueling Bandits,https://ICML.cc//virtual/2025/poster/45922,"Zhiyong Wang, Jiahang Sun, Mingze Kong, Jize Xie, Qinghua Hu, John C. S. Lui, Zhongxiang Dai","The contextual multi-armed bandit (MAB) is a widely used framework for problems requiring sequential decision-making under uncertainty, such as recommendation systems. In applications involving a large number of users, the performance of contextual MAB can be significantly improved by facilitating collaboration among multiple users. This has been achieved by the clustering of bandits (CB) methods, which adaptively group the users into different clusters and achieve collaboration by allowing the users in the same cluster to share data. However, classical CB algorithms typically rely on numerical reward feedback, which may not be practical in certain real-world applications.  For instance, in recommendation systems, it is more realistic and reliable to solicit preference feedback between pairs of recommended items rather than absolute rewards. To address this limitation, we introduce the first ""clustering of dueling bandit algorithms"" to enable collaborative decision-making based on preference feedback. We propose two novel algorithms: (1) Clustering of Linear Dueling Bandits (COLDB) which models the user reward functions as linear functions of the context vectors, and (2) Clustering of Neural Dueling Bandits (CONDB) which uses a neural network to model complex, non-linear user reward functions. Both algorithms are supported by rigorous theoretical analyses, demonstrating that user collaboration leads to improved regret bounds. Extensive empirical evaluations on synthetic and real-world datasets further validate the effectiveness of our methods, establishing their potential in real-world applications involving multiple users with preference-based feedback.","Imagine you’re using a recommendation system, like one that suggests movies or products. Instead of giving a rating (like 5 stars), you might prefer to compare two options and say which one you like better. This kind of feedback is more natural and easier for people to provide. But how can a recommendation system learn from these comparisons and improve its suggestions over time, especially when there are many users with similar tastes?Our work tackles this problem by introducing a new way for recommendation systems to collaborate and learn from users’ preferences. Traditionally, systems group users together to share data and improve recommendations, but they rely on numerical ratings, which aren’t always practical. We propose two new methods that work with preference feedback (like “I prefer A over B”):- COLDB: This method assumes that user preferences can be modeled as a linear function based on the features of the items (like genre or price).- CONDB: This method uses neural network to handle complex non-linear preferences.Both methods come with strong theoretical guarantees, showing that collaboration among users leads to better recommendations. We tested them on simulated and real-world datasets, and they outperformed existing techniques. This work opens the door to more practical and effective recommendation systems that rely on natural human feedback."
Poster,Online Conformal Prediction via Online Optimization,https://ICML.cc//virtual/2025/poster/45619,"Felipe Areces, Christopher Mohri, Tatsunori Hashimoto, John Duchi","We introduce a family of algorithms for online conformal prediction with coverage guarantees for both adversarial and stochastic data. In the adversarial setting, we establish the standard guarantee: over time, a pre-specified target fraction of confidence sets cover the ground truth. For stochastic data, we provide a guarantee at every time instead of just on average over time: the probability that a confidence set covers the ground truth—conditioned on past observations—converges to a pre-specified target when the conditional quantiles of the errors are a linear function of past data. Complementary to our theory, our experiments spanning over $15$ datasets suggest that the performance improvement of our methods over baselines grows with the magnitude of the data’s dependence, even when baselines are tuned on the test set. We put these findings to the test by pre-registering an experiment for electricity demand forecasting in Texas, where our algorithms achieve over a $10$\% reduction in confidence set sizes, a more than a $30$\% improvement in quantile and absolute losses with respect to the observed errors, and significant outcomes on all $78$ out of $78$ pre-registered hypotheses. We provide documentation for the pypi package implementing our algorithms here: \url{https://conformalopt.readthedocs.io/}.","Accurate predictions with reliable uncertainty estimates are crucial, but existing approaches to quantify this uncertainty rely on assumptions that are either too strong to be widely applicable, or too weak to provide robust guarantees. We developed new prediction algorithms that produce uncertainty estimates (""confidence intervals"") guaranteed to capture the true outcome at a specified rate, which are adaptive to both unpredictable and highly correlated datasets. In real-world tests—such as forecasting electricity demand in Texas—our methods provided significantly improved confidence intervals compared to existing techniques. This improvement grows larger when data points are more correlated over time. To encourage practical adoption, we offer open-source tools that let anyone easily apply our methods."
Poster,Online Curvature-Aware Replay: Leveraging $\mathbf{2^{nd}}$ Order Information for Online Continual Learning,https://ICML.cc//virtual/2025/poster/44567,"Edoardo Urettini, Antonio Carta","Online Continual Learning (OCL) models continuously adapt to nonstationary data streams, usually without task information. These settings are complex and many traditional CL methods fail, while online methods (mainly replay-based) suffer from instabilities after the task shift. To address this issue, we formalize replay-based OCL as a second-order online joint optimization with explicit KL-divergence constraints on replay data. We propose Online Curvature-Aware Replay (OCAR) to solve the problem: a method that leverages second-order information of the loss using a K-FAC approximation of the Fisher Information Matrix (FIM) to precondition the gradient. The FIM acts as a stabilizer to prevent forgetting while also accelerating the optimization in non-interfering directions. We show how to adapt the estimation of the FIM to a continual setting, stabilizing second-order optimization for non-iid data, uncovering the role of the Tikhonov damping in the stability-plasticity tradeoff.  Empirical results show that OCAR outperforms state-of-the-art methods in continual metrics, achieving higher average accuracy throughout the training process in three different benchmarks.","As neural networks are used in real-world settings, it's important that they can learn new information in real-time without forgetting what they already know. This is especially challenging when new data comes in continuously and without clear breaks: a situation known as online continual learning.Many traditional learning methods struggle in these conditions, especially when the type of task or data changes suddenly. One common approach, called replay, helps by mixing in examples from past data, but it often becomes unstable when the data shifts.To tackle this, we developed a new method called Online Curvature-Aware Replay (OCAR). It helps neural networks remember old knowledge while learning new things more effectively. From a mathematical point of view, as the objective is to find the minimum of a function, OCAR, instead of following only the direction with the highest slope, also considers the curvature of the function. Doing this with replay allows us to avoid changing the parameters that are important for past knowledge. Our experiments showed that OCAR consistently performs better than existing methods, helping AI models stay accurate and stable over time across different types of learning tasks. This brings us closer to creating AI that learns more like humans do — continuously and reliably."
Poster,Online Detection of LLM-Generated Texts via Sequential Hypothesis Testing by Betting,https://ICML.cc//virtual/2025/poster/44240,"Can Chen, Jun-Kun Wang","Developing algorithms to differentiate between machine-generated texts and human-written texts has garnered substantial attention in recent years. Existing methods in this direction typically concern an offline setting where a dataset containing a mix of real and machine-generated texts is given upfront, and the task is to determine whether each sample in the dataset is from a large language model (LLM) or a human. However, in many practical scenarios, sources such as news websites, social media accounts, and online forums publish content in a streaming fashion. Therefore, in this online scenario, how to quickly and accurately determine whether the source is an LLM with strong statistical guarantees is crucial for these media or platforms to function effectively and prevent the spread of misinformation and other potential misuse of LLMs. To tackle the problem of online detection, we develop an algorithm based on the techniques of sequential hypothesis testing by betting that not only builds upon and complements existing offline detection techniques but also enjoys statistical guarantees, which include a controlled false positive rate and the expected time to correctly identify a source as an LLM. Experiments were conducted to demonstrate the effectiveness of our method.","Large language models (LLMs) can produce content with qualities on par with human-level writing. While powerful, they may also be misused to spread fake news or manipulate public opinion—especially on online platforms like news websites or social media, where texts arrive sequentially. How can we detect whether a stream of texts is from an LLM or a human, and do so quickly and reliably?Many existing detectors classify individual texts effectively but are not designed for online settings, and some require threshold tuning. A naive adaptation—flagging the source as an LLM once any text is detected as such—can lead to high false positive rates as the number of texts becomes sufficiently large. We frame online LLM detection as a sequential hypothesis testing problem and address it using a method that builds on existing detectors with a game-theoretic approach called testing by betting. Our method offers rigorous statistical guarantees and does not assume any specific form for the underlying data distribution. It can be combined with existing detectors to help platforms achieve fast and robust detection of LLM sources in online settings."
Poster,Online Differentially Private Conformal Prediction for Uncertainty Quantification,https://ICML.cc//virtual/2025/poster/44617,"Qiangqiang Zhang, Ting Li, Xinwei Feng, Xiaodong Yan, Jinhan Xie","Traditional conformal prediction faces significant challenges with the rise of streaming data and increasing concerns over privacy. In this paper, we introduce a novel online differentially private conformal prediction framework, designed to construct dynamic, model-free private prediction sets. Unlike existing approaches that either disregard privacy or require full access to the entire dataset, our proposed method ensures individual privacy with a one-pass algorithm, ideal for real-time, privacy-preserving decision-making. Theoretically, we establish guarantees for long-run coverage at the nominal confidence level. Moreover, we extend our method to conformal quantile regression, which is fully adaptive to heteroscedasticity. We validate the effectiveness and applicability of the proposed method through comprehensive simulations and real-world studies on the ELEC2 and PAMAP2 datasets.","As more devices generate continuous streams of data—like smart meters tracking electricity or wearables monitoring heart rate—it becomes crucial to make accurate predictions while preserving personal privacy. Our research introduces a new method that enables real-time prediction without compromising individual data.Unlike traditional techniques that require storing and processing all past data, our approach uses each data point only once, making it highly efficient. To protect privacy, we add carefully designed randomness to each update, ensuring strong privacy guarantees while maintaining reliable predictive performance.This work offers a practical solution for real-time, privacy-preserving decision-making in sensitive domains such as healthcare, finance, and smart environments. It brings us one step closer to deploying safe and adaptive machine learning systems in everyday life."
Poster,Online Episodic Convex Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44644,"Bianca Marin Moreno, Khaled Eldowa, Pierre Gaillard, Margaux Brégère, Nadia Oudjane","We study online learning in episodic finite-horizon Markov decision processes (MDPs) with convex objective functions, known as the concave utility reinforcement learning (CURL) problem. This setting generalizes RL from linear to convex losses on the state-action distribution induced by the agent’s policy. The non-linearity of CURL invalidates classical Bellman equations and requires new algorithmic approaches. We introduce the first algorithm achieving near-optimal regret bounds for online CURL without any prior knowledge on the transition function. To achieve this, we use a novel online mirror descent algorithm with variable constraint sets and a carefully designed exploration bonus. We then address for the first time a bandit version of CURL, where the only feedback is the value of the objective function on the state-action distribution induced by the agent's policy. We achieve a sub-linear regret bound for this more challenging problem by adapting techniques from bandit convex optimization to the MDP setting.","This paper explores a more general form of reinforcement learning (RL) called Convex Reinforcement Learning (CURL). Unlike traditional RL, which focuses on maximizing a reward signal over the agent trajectory, CURL allows to optimize more complex, convex objective functions based on the distribution of the agents over time. This flexibility makes CURL applicable to a wider range of real-world problems such as in energy grid optimization, in mean-field games or in multi-objective goals. However, the non-linear nature of these objectives breaks standard RL tools like the Bellman equation, requiring new algorithmic approaches. The paper introduces the first algorithm that achieves near-optimal learning performance in this setting when the objective changes over time, while requiring no prior knowledge of the environment's dynamics. The paper also addresses a more challenging version of the problem called the bandit setting, where the agent only observes the outcome of the strategy it actually used, without receiving any information about how other possible strategies would have performed."
Poster,Online Laplacian-Based Representation Learning in Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45493,"Maheed Ahmed, Jayanth Bhargav, Mahsa Ghasemi","Representation learning plays a crucial role in reinforcement learning, especially in complex environments with high-dimensional and unstructured states. Effective representations can enhance the efficiency of learning algorithms by improving sample efficiency and generalization across tasks. This paper considers the Laplacian-based framework for representation learning, where the eigenvectors of the Laplacian matrix of the underlying transition graph are leveraged to encode meaningful features from raw sensory observations of the states. Despite the promising algorithmic advances in this framework, it remains an open question whether the Laplacian-based representations can be learned online and with theoretical guarantees along with policy learning. We address this by formulating an online optimization approach using the Asymmetric Graph Drawing Objective (AGDO) and analyzing its convergence via online projected gradient descent under mild assumptions. Our extensive simulation studies empirically validate the convergence guarantees to the true Laplacian representation. Furthermore, we provide insights into the compatibility of different reinforcement learning algorithms with online representation learning.","Reinforcement learning (RL) teaches AI agents to make decisions through interacting with an environment, but it could struggle in complex situations with a large amount of data. One way to improve RL is to teach the agents how to better understand the data by compressing what they see into simpler, more useful pieces. This method is called representation learning.Our work explores a method inspired by how networks or maps are drawn using something called the graph Laplacian to help the agents create better representations of their environments that capture the environments' dynamics. While prior methods could only build this representation ahead of time before training of the agents starts, we explore how to build these representations on the fly in an online manner. We show theoretically that learning these representations in real-time is possible and that it works reliably in practical conditions."
Poster,Online Learning in Risk Sensitive constrained MDP,https://ICML.cc//virtual/2025/poster/46405,"Arnob Ghosh, Mehrdad Moharrami","We consider a setting in which the agent aims to maximize the expected cumulative reward, subject to a constraint that the entropic risk of the total utility exceeds a given threshold. Unlike the risk-neutral case, standard primal-dual approaches fail to directly yield regret and violation bounds, as value iteration with respect to a combined state-action value function is not applicable in the risk-sensitive setting. To address this, we adopt the Optimized Certainty Equivalent (OCE) representation of the entropic risk measure and reformulate the problem by augmenting the state space with a continuous budget variable. We then propose a primal-dual algorithm tailored to this augmented formulation. In contrast to the standard approach for risk-neutral CMDPs, our method incorporates a truncated dual update to account for the possible absence of strong duality. We show that the proposed algorithm achieves regret of $\tilde{\mathcal{O}}\big(V_{g,\max}K^{3/4} + \sqrt{H^4 S^2 A \log(1/\delta)}K^{3/4}\big)$ and constraint violation of $\tilde{\mathcal{O}}\big(V_{g,\max} \sqrt{ {H^3 S^2 A \log(1/\delta)}}K^{3/4} \big)$ with probability at least $1-\delta$, where $S$ and $A$ denote the cardinalities of the state and action spaces, respectively, $H$ is the episode length, $K$ is the number of episodes, $\alpha < 0$ is the risk-aversion parameter, and $V_{g,\max} = \frac{1}{|\alpha|}(\exp(|\alpha|H) - 1)$.  *To the best of our knowledge, this is the first result establishing sublinear regret and violation bounds for the risk-sensitive CMDP problem.*","In many practical sequential decision-making applications (e,g, finance, safe navigation, etc..), it is important to consider that the risk-based measure of the cost is below a certain threshold. Currently, the CMDP setting can only address the scenario where the expected cost is below a certain threshold.  This is the first paper that obtains the regret and the violation bound for an MDP with entropic risk constraints. We show that, unlike the unconstrained setup, here, the Markovian policy may not be optimal. Hence, we augment the state-space and consider a constrained optimized certainty equivalence. In order to obtain the regret and the violation bounds, we overcome specific challenges of infinite augmented state-space and the lack of strong duality because of the non-linearity, unlike the traditional CMDP setting. Some key important questions remain open, like whether we can improve the bound."
Poster,Online Learning in the Random-Order Model,https://ICML.cc//virtual/2025/poster/45025,"Martino Bernasconi, Andrea Celli, Riccardo Colini Baldeschi, Federico Fusco, Stefano Leonardi, Matteo Russo","In the random-order model for online learning, the sequence of losses is chosen upfront by an adversary and presented to the learner after a random permutation. Any random-order input is *asymptotically* equivalent to a stochastic i.i.d.~one, but, for finite times, it may exhibit significant *non-stationarity*, which can hinder the performance of stochastic learning algorithms.While algorithms for adversarial inputs naturally maintain their regret guarantees in random order, simple no-regret algorithms exist for the stochastic model that fail against random-order instances.  In this paper, we propose a general procedure to adapt stochastic learning algorithms to the random-order model without substantially affecting their regret guarantees. This allows us to recover improved regret bounds for prediction with delays, bandits with switching costs, and online learning with constraints. Finally, we investigate online classification and prove that, in random order, learnability is characterized by the VC dimension rather than by the Littlestone dimension, thus providing a further separation from the general adversarial model.","We study how to address online learning problems under random order inputs, which is an intermediate setting between the fully stochastic and fully adversarial input models."
Poster,Online Learning with Unknown Constraints,https://ICML.cc//virtual/2025/poster/44095,"Karthik Sridharan, Seung Won Wilson Yoo","We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that characterizes the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned complexity measure is necessary. When the constraints are linear, we instantiate our result to provide a concrete algorithm with $\sqrt{T}$ regret using a scaling transformation that balances optimistic exploration with pessimistic constraint satisfaction.","How can we design machine learning algorithms that are always safe, even when we don't fully know what 'safe' means at the start? Compared to previous works that study being safe on average, our work tackles the more challenging setting of always being safe. This is useful in settings where you have to be safe, such as preventing a robot from crashing or following laws and regulations. We built algorithms that perform well while also discovering what safe means despite noisy feedback. Our algorithm takes optimistic actions that are guaranteed to perform well and converts them into pessimistic actions that are guaranteed to be safe. We also came up with a rule - technically called a complexity measure - that captures the trade-off between performance and being safe. We also give some examples of our algorithm in action for useful settings."
