type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Stay-Positive: A Case for Ignoring Real Image Features in Fake Image Detection,https://ICML.cc//virtual/2025/poster/45069,"Anirudh Sundara Rajan, Yong Jae Lee","Detecting AI-generated images is a challenging yet essential task. A primary difficulty arises from the detector’s tendency to rely on spurious patterns, such as compression artifacts, which can influence its decisions. These issues often stem from specific patterns that the detector associates with the real data distribution, making it difficult to isolate the actual generative traces. We argue that an image should be classified as fake if and only if it contains artifacts introduced by the generative model. Based on this premise, we propose Stay-Positive, an algorithm designed to constrain the detector’s focus to generative artifacts while disregarding those associated with real data. Experimental results demonstrate that detectors trained with Stay-Positive exhibit reduced susceptibility to spurious correlations, leading to improved generalization and robustness to post-processing. Additionally, unlike detectors that associate artifacts with real images, those that focus purely on fake artifacts are better at detecting inpainted real images.","Detecting AI-generated images can be tricky because systems often rely on patterns that look like they belong to real images, but these patterns can be misleading. This causes the system to make mistakes, especially when dealing with edited or new images. We believe an image should only be considered real only if it doesn't show signs of being made by an AI generator. Our method, Stay-Positive, helps these systems focus only on the fake signs and ignore the misleading ones, making them better at spotting fake images, even if they've been changed or edited."
Poster,STD-FD: Spatio-Temporal Distribution Fitting Deviation for AIGC Forgery Identification,https://ICML.cc//virtual/2025/poster/45231,"Hengrui Lou, Zunlei Feng, Jinsong Geng, Erteng Liu, Jie Lei, Lechao Cheng, Jie Song, Mingli Song, Yijun Bei","With the rise of AIGC technologies, particularly diffusion models, highly realistic fake images that can deceive human visual perception has become feasible. Consequently, various forgery detection methods have emerged. However, existing methods treat the generation process of fake images as either a black-box or an auxiliary tool, offering limited insights into its underlying mechanisms. In this paper, we propose Spatio-Temporal Distribution Fitting Deviation (STD-FD) for AIGC forgery detection, which explores the generative process in detail. By decomposing and reconstructing data within generative diffusion models, initial experiments reveal temporal distribution fitting deviations during the image reconstruction process. These deviations are captured through reconstruction noise maps for each spatial semantic unit, derived via a super-resolution algorithm. Critical discriminative patterns, termed DFactors, are identified through statistical modeling of these deviations. Extensive experiments show that STD-FD effectively captures distribution patterns in AIGC-generated data, demonstrating strong robustness and generalizability while outperforming state-of-the-art (SOTA) methods on major datasets. The source code is available at [this link](https://github.com/HengruiLou/STDFD).","Fake photos and artwork made by today’s AI image generators can look so real that even experts struggle to spot them. We asked a simple question: When an AI “paints” a picture step-by-step, does it leave behind hidden fingerprints that a human eye can’t see?To find out, we zoomed in on the tiny changes the generator makes while refining an image. By replaying this process in slow motion, we discovered subtle, time-based “wobbles” in how colours and textures settle across different parts of the picture. We captured these wobbles as noise maps, then turned the strongest patterns—called DFactors—into tell-tale signs of fakery.Our detector, named STD-FD, reliably flags AI-generated images across several public test sets and beats today’s best forensic tools. This technique could help journalists, social-media platforms, and law-enforcement agencies spot deepfakes quickly, protecting the public from misinformation and visual fraud."
Poster,Stealing That Free Lunch: Exposing the Limits of Dyna-Style Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44829,"Brett Barkley, David Fridovich-Keil","Dyna-style off-policy model-based reinforcement learning (DMBRL) algorithms are a family of techniques for generating synthetic state transition data and thereby enhancing the sample efficiency of off-policy RL algorithms. This paper identifies and investigates a surprising performance gap observed when applying DMBRL algorithms across different benchmark environments with proprioceptive observations. We show that, while DMBRL algorithms perform well in control tasks in OpenAI Gym, their performance can drop significantly in DeepMind Control Suite (DMC), even though these settings offer similar tasks and identical physics backends. Modern techniques designed to address several key issues that arise in these settings do not provide a consistent improvement across all environments, and overall our results show that adding synthetic rollouts to the training process --- the backbone of Dyna-style algorithms --- significantly degrades performance across most DMC environments. Our findings contribute to a deeper understanding of several fundamental challenges in model-based RL and show that, like many optimization fields, there is no free lunch when evaluating performance across diverse benchmarks in RL.","Many AI systems learn by trial and error, using simulations to practice before making real-world decisions. A popular technique to speed up this learning process is to let algorithms imagine ""what-if"" scenarios using a model of the world. This idea, called model-based reinforcement learning, is supposed to make learning more efficient by generating synthetic training data.However, our research found that this approach doesn’t always work as expected. We compared its performance on two popular testing platforms for robotic control tasks, OpenAI Gym and DeepMind Control Suite, which have similar physics and task types. Surprisingly, model-based methods performed well in Gym but often failed in the DeepMind environments.We investigated why this gap exists and found that adding these ""what-if"" experiences - the core idea of this technique - can sometimes hurt performance. Our findings challenge the assumption that model-based learning is always a method to improve efficiency and highlight the need for more robust techniques that work consistently across different environments."
Poster,Stealix: Model Stealing via Prompt Evolution,https://ICML.cc//virtual/2025/poster/44026,"Zhixiong Zhuang, Hui-Po Wang, Irina Nicolae, Mario Fritz","Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information.Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise.To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names.In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model’s data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images.Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized.","Machine learning models are valuable intellectual property, but they can be stolen by simply querying them and using the outputs to train a similar model. Prior attacks use image generators with hand-crafted prompts, assuming attackers have prompt expertise. This simplifies the threat and overlooks how easily models can be copied. Our work shows that even attackers with no prompt knowledge can effectively copy a model, raising the alarm about how easy model theft has become.We introduce Stealix, the first fully automated model stealing method that begins with just one real image per class. Stealix evolves text prompts that generate synthetic images, which the target model labels as belonging to the correct class. Without any human-crafted prompts or knowledge of class names, Stealix creates training data that mimics the target model’s data and leads to stronger stolen models than prior methods.Our findings show that open-source image generators greatly reduce the effort needed for model theft. This raises concerns about their misuse and calls for more responsible deployment and stronger defenses against emerging model stealing threats."
Poster,StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models,https://ICML.cc//virtual/2025/poster/44621,"Ya Jiang, Chuxiong Wu, Massieh Kordi Boroujeny, Brian Mark, Kai Zeng","Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts.  We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications.","As powerful AI models like ChatGPT become more common, so does the risk of misuse—such as generating fake news, scams, or academic plagiarism. To help address these concerns, researchers have been exploring ways to add hidden “watermarks” to AI-generated text. These invisible tags can help detect whether a text was written by a machine and even trace where it came from.Our work introduces StealthInk, a new technique that embeds more information into AI-generated text while keeping the text natural and hard to distinguish from human writing. Unlike many past methods that either distort the language or offer only simple yes/no answers (like “is this text AI-generated?”), StealthInk can encode rich details such as user ID and timestamps. It also holds up well even when attackers try to hide or fake the watermark, ensuring both privacy and accountability. This makes StealthInk a strong step toward safer and more trustworthy AI-generated content."
Poster,Steerable Transformers for Volumetric Data,https://ICML.cc//virtual/2025/poster/46143,"Soumyabrata Kundu, Risi Kondor","We introduce Steerable Transformers, an extension of the Vision Transformer mechanism that maintains equivariance to the special Euclidean group $\mathrm{SE}(d)$. We propose an equivariant attention mechanism that operates on features extracted by steerable convolutions. Operating in Fourier space, our network utilizes Fourier space non-linearities. Our experiments in both two and three dimensions show that adding steerable transformer layers to steerable convolutional networks enhances performance.","When you rotate a 2-D image or inspect a 3-D scan from another angle, you still recognise what you’re looking at—but most artificial-intelligence systems do not. They either make mistakes or must be shown thousands of extra, rotated examples to learn the same trick. Our research introduces steerable transformers, a new kind of neural-network layer that automatically understands when an object has simply been moved or rotated. We blend two ideas: (1) the “zoom-out and compare” power of transformers—the technology behind tools like ChatGPT—and (2) earlier convolution methods that focus on local image details while respecting how objects can move in space. The resulting model keeps its accuracy even when pictures or volumes are spun around, yet it needs no extra memory or training time. In tests on hand-written digits, 3-D shape models, skin-lesion photos, and brain-tumour MRI scans, steerable transformers outperform the previous rotation-aware networks. Because the method naturally handles full 3-D data, it could help doctors detect tumours, aid self-driving cars in spotting objects from odd angles, and improve any vision task where orientation should not matter."
Poster,Steering Protein Language Models,https://ICML.cc//virtual/2025/poster/43979,"Long-Kai Huang, Rongyi Zhu, Bing He, Jianhua Yao","Protein Language Models (PLMs), pre-trained on extensive evolutionary data from natural proteins, have emerged as indispensable tools for protein design. While powerful, PLMs often struggle to produce proteins with precisely specified functionalities or properties due to inherent challenges in controlling their outputs. In this work, we investigate the potential of Activation Steering, a technique originally developed for controlling text generation in Large Language Models (LLMs), to direct PLMs toward generating protein sequences with targeted properties. We propose a simple yet effective method that employs activation editing to steer PLM outputs, and extend this approach to protein optimization through a novel editing site identification module. Through comprehensive experiments on lysozyme-like sequence generation and optimization, we demonstrate that our methods can be seamlessly integrated into both auto-encoding and autoregressive PLMs without requiring additional training. These results highlight a promising direction for precise protein engineering using foundation models.","Designing new proteins with specific functions, such as improved stability or brightness, is a major goal in biotechnology and medicine. Today, scientists use powerful AI models trained on millions of natural proteins to help with this task. However, these models often struggle to create proteins with exactly the properties we want, usually requiring lots of trial and error.In our research, we adapted a technique from text-generating AI models, called ""activation steering,"" to guide protein-generating AI models more precisely. Instead of retraining the whole model or relying on special keywords, our method tweaks the model’s internal workings while it is generating new protein sequences. This allows us to nudge the model toward producing proteins with desired features, like higher stability or better solubility, without extra training or large datasets.We also developed a way to identify which parts of a protein should be changed to achieve these goals. Our experiments show that this approach works across different types of protein AI models and for various important protein properties. This could make it much easier and faster to design proteins for medicine, industry, and research."
Poster,Steer LLM Latents for Hallucination Detection,https://ICML.cc//virtual/2025/poster/45122,"Seongheon Park, Xuefeng Du, Min-Hsuan Yeh, Haobo Wang, Sharon Li","Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content.To this end, we propose the **T**ruthfulness **S**eparator **V**ector (**TSV**), a lightweight and flexible steering vector that reshapes the LLM’s representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters.Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters.It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process.Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.","Large language models (LLMs) like ChatGPT have shown impressive capabilities, but they sometimes generate ""hallucinations""—statements that appear plausible but are factually inaccurate or unsupported. These hallucinations pose serious risks when LLMs are used in sensitive areas such as healthcare, law, and education. Existing methods to detect hallucinations often rely on the model’s internal information, which is learned for linguistic fluency rather than factual correctness, often resulting in unreliable detection performance.To address this, we introduce the Truthfulness Separator Vector (TSV)—a lightweight, plug-and-play method that helps distinguish truthful from hallucinated responses by modifying the model's internal representation during inference to better separate ""factualness,"" without the need to retrain the model from scratch. TSV learns from a small set of labeled examples and then improves itself by smartly labeling additional unlabeled data using a technique based on optimal transport.Our method achieves state-of-the-art hallucination detection with few labeled examples and works across different tasks and datasets. TSV is computationally efficient and easy to integrate, making it a practical step toward safer, more trustworthy AI systems in real-world applications."
Poster,Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design,https://ICML.cc//virtual/2025/poster/45693,"Marcel Hedman, Desi Ivanova, Cong Guan, Tom Rainforth","We develop a semi-amortized, policy-based, approach to Bayesian experimental design (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing, fully amortized, policy-based BED approaches, Step-DAD trains a design policy upfront before the experiment. However, rather than keeping this policy fixed, Step-DAD periodically updates it as data is gathered, refining it to the particular experimental instance. This test-time adaptation improves both the flexibility and the robustness of the design strategy compared with existing approaches. Empirically, Step-DAD consistently demonstrates superior decision-making and robustness compared with current state-of-the-art BED methods.","The objective of Bayesian experimental design is to collect data that is informative as possible under a given generative model. In this setting, it’s often critical to adapt your future collection strategy as new data becomes available. Traditional Bayesian methods handle this by continually updating beliefs and selecting the next best experiment—but they are computationally expensive and myopic, focusing only one step ahead. More scalable approaches pre-train a neural network to map histories to future design choices, but these fixed policy networks can fail when real data deviates from training conditions.We introduce Step-DAD, a hybrid method that starts by pre-training a policy but then iteratively updates it during deployment. This refinement enables decisions tailored to the observed data, improving robustness and effectiveness without incurring the full cost of traditional methods.  We find that Step-DAD empirically achieves state-of-the-art performance across multiple problems."
Poster,"Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence",https://ICML.cc//virtual/2025/poster/45986,"Yinbin Han, Meisam Razaviyayn, Renyuan Xu","Diffusion models have emerged as powerful tools for generative modeling, demonstrating exceptional capability in capturing target data distributions from large datasets. However, fine-tuning these massive models for specific downstream tasks, constraints, and human preferences remains a critical challenge. While recent advances have leveraged reinforcement learning algorithms to tackle this problem, much of the progress has been empirical, with limited theoretical understanding. To bridge this gap, we propose a stochastic control framework for fine-tuning diffusion models. Building on denoising diffusion probabilistic models as the pre-trained reference dynamics, our approach integrates linear dynamics control with Kullback–Leibler regularization. We establish the well-posedness and regularity of the stochastic control problem and develop a {policy iteration algorithm (PI-FT)} for numerical solution. We show that PI-FT achieves global convergence at a linear rate. Unlike existing work that assumes regularities throughout training, we prove that the control and value sequences generated by the algorithm preserve the desired regularity. Finally, we extend our framework to parametric settings for efficient implementation and demonstrate the practical effectiveness of the proposed PI-FT algorithm through numerical experiments.","Modern AI systems can now generate realistic images, videos, and sounds using a family of models called diffusion models. These models are trained on huge datasets and can create impressive results. However, adapting them to perform well on specific tasks, follow human preferences, or respect certain constraints is still very difficult.Currently, many researchers use trial-and-error approaches or complex reinforcement learning techniques to fine-tune these models, but we still lack a clear theoretical understanding of how and why these methods work.In this paper, we introduce a new mathematical framework that treats fine-tuning as a stochastic control problem: we view the diffusion model as a system that can be guided toward better performance using carefully designed adjustments. We develop an efficient algorithm with proven guarantees that it will find the best way to adjust the model, with excellent computational efficiency.Our approach not only sheds light on how fine-tuning works but also provides practical tools to make diffusion models more flexible and useful in real-world applications."
