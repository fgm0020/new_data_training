type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization,https://ICML.cc//virtual/2025/poster/44709,"Dasol Hong, Wooju Lee, Hyun Myung","Prompt tuning, which adapts vision-language models by freezing model parameters and opti- mizing only the prompt, has proven effective for task-specific adaptations. The core challenge in prompt tuning is improving specialization for a specific task and generalization for unseen domains. However, frozen encoders often produce misaligned features, leading to confusion between classes and limiting specialization. To overcome this issue, we propose a confusion-aware loss (CoA-loss) that improves specialization by refining the decision boundaries between confusing classes. Additionally, we mathematically demonstrate that a mixture model can enhance generalization without compromising specialization. This is achieved using confidence-aware weights (CoA- weights), which adjust the weights of each prediction in the mixture model based on its confidence within the class domains. Extensive experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights, outperforms state-of-the-art methods by enhancing specialization and generalization. Our code is publicly available at https://github.com/url-kaist/CoCoA-Mix","Vision-language models are powerful, but adapting them to new tasks without retraining the entire model remains a challenge. Prompt tuning, which changes only the prompts the model receives, is efficient but often causes confusion between similar categories. We propose CoCoA-Mix, a method based on a mixture model that combines results from multiple prompts. It includes a confusion-aware loss (CoA-loss) to help the model avoid confusing similar categories, and confidence-aware weights (CoA-weights) that adjust each prediction based on how confident a prompt is for its class. CoCoA-Mix improves specialization and generalization over existing methods. Our code is publicly available for others to use and build upon."
Poster,Code-Generated Graph Representations Using Multiple LLM Agents for Material Properties Prediction,https://ICML.cc//virtual/2025/poster/44181,"Jiao Huang, Qianli Xing, Jinglong Ji, Bo Yang","Graph neural networks have recently demonstrated remarkable performance in predicting material properties. Crystalline material data is manually encoded into graph representations.Existing methods incorporate different attributes into constructing representations to satisfy the constraints arising from symmetries of material structure.However, existing methods for obtaining graph representations are specific to certain constraints, which are ineffective when facing new constraints.In this work, we propose a code generation framework with multiple large language model agents to obtain representations named Rep-CodeGen with three iterative stages simulating an evolutionary algorithm. To the best of our knowledge, Rep-CodeGen is the first framework for automatically generating code to obtain representations that can be used when facing new constraints. Furthermore, a type of representation from generated codes by our framework satisfies six constraints, with codes satisfying three constraints as bases. Extensive experiments on two real-world material datasets show that a property prediction method based on such a graph representation achieves state-of-the-art performance in material property prediction tasks.","Predicting material properties with AI holds great promise, but current methods face a key limitation: scientists must manually design complex rules to convert material data into computable graph structures. These rule-based approaches often fail when encountering new scientific scenarios beyond their original design.We present Rep-CodeGen, an innovative AI system that automatically writes and improves its own code through an evolutionary process. Like natural selection, our framework uses multiple AI agents that collaboratively generate, test, and refine code representations through iterative cycles. This breakthrough allows automatic adaptation to completely new material constraints - a capability traditional methods lack.This technology could revolutionize materials discovery by eliminating the bottleneck of manual representation design, potentially accelerating development in critical areas like battery technology and semiconductor materials. Our open framework also provides a foundation for addressing representation challenges in other scientific domains."
Poster,CodeIO: Condensing Reasoning Patterns via Code Input-Output Prediction,https://ICML.cc//virtual/2025/poster/44514,"Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He","Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives—like logic flow planning, state-space searching, decision tree traversal, and modular decomposition—while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models will be publicly available.","Improving how AI models reason generally—rather than just in specific areas like math or coding—is crucial for creating truly intelligent systems. Until now, most efforts to enhance reasoning have been narrowly focused. Our solution is remarkably simple: we teach models to predict inputs and outputs for a wide variety of existing code functions, but entirely in the form of natural language. This approach effectively captures fundamental reasoning patterns buried in codes. The result is a significant boost in performance across diverse reasoning tasks, bringing us closer to AI systems that can think robustly in all domains."
Poster,CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance,https://ICML.cc//virtual/2025/poster/44557,"Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, Chuchu Fan","Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-turn guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-turn supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0 and https://huggingface.co/yongchao98.","When reading through the papers on LLM Based Agents, I always feel curious why many testing tasks are apparently more suitable to be solved with code search/reasoning but we just ask LLMs to solve them via pure text generation. For example, the question '9.11 and 9.9 which is bigger' can be easily solved if we just prompt LLMs to use coding to answer. LLMs like ChatGPT can write essays or solve math problems, but they often struggle to decide when to use plain language versus actual computer code. Our project, CodeSteer, helps these models make better decisions between code and text when solving complex problems.We created a set of 37 tasks called SymBench, which test symbolic reasoning—such as solving puzzles or manipulating equations. We also generated a large amount of training data to help models learn when to switch between code and text. We then fine-tuned a powerful model using this data and added extra tools to help it double-check its answers.When we combined CodeSteer with OpenAI’s GPT-4o, the model’s performance significantly improved—beating the best existing systems. Even when used with other models like Claude and GPT-3.5, CodeSteer made them much better at solving symbolic problems. Our work shows that with the right guidance, AI models can fully use symbolic computing to solve difficult tasks more reliably and efficiently."
Poster,CodeSync: Synchronizing Large Language Models with Dynamic Code Evolution at Scale,https://ICML.cc//virtual/2025/poster/46283,"Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Hai Jin, Dongping Chen","Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly the frequent updates of third-party library APIs. This limitation, rooted in the static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, we introduce CodeSync, a data engine to identify outdated code patterns and collect real-time code knowledge updates from Python third-party libraries. Building upon CodeSync, we develop CodeSyncBench, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases spanning three evaluation tasks and an update-aware instruction tuning dataset of 2,200 training samples. Extensive experiments on 14 LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). Our CodeSync lays a strong foundation for developing more effective and robust methods for real-time code knowledge updating in the future. The experimental code is available at:  https://github.com/CGCL-codes/naturalcc/tree/main/examples/codesync.","AI tools that write software code, known as Large Language Models (LLMs), are incredibly powerful. However, they are often trained on outdated information. This means they don't know when programming languages and their toolkits (called libraries) change, causing them to generate code that is broken, inefficient, or insecure. To tackle this, we developed CodeSync, an automated system that detects recent updates in popular Python libraries and enable to create dataset and benchmark based on this. Using CodeSync, we created CodeSyncBench, a comprehensive test to measure how well LLMs adapt to this evolving code knowledge. We tested 14 major LLMs, including the latest models from OpenAI, Google, and Anthropic. Our results show a significant weakness: all current LLMs struggle to keep up with these updates, even when using advanced learning techniques. Our work provides a crucial tool for developers to measure this ""knowledge decay"" and lays the groundwork for building next-generation AI coding assistants that can stay synchronized with the fast-paced world of software development, making them far more reliable."
Poster,CoDy: Counterfactual Explainers for Dynamic Graphs,https://ICML.cc//virtual/2025/poster/45900,"Zhan Qu, Daniel Gomm, Michael Färber","Temporal Graph Neural Networks (TGNNs) are widely used to model dynamic systems where relationships and features evolve over time. Although TGNNs demonstrate strong predictive capabilities in these domains, their complex architectures pose significant challenges for explainability. Counterfactual explanation methods provide a promising solution by illustrating how modifications to input graphs can influence model predictions. To address this challenge, we present CoDy—Counterfactual Explainer for Dynamic Graphs—a model-agnostic, instance-level explanation approach that identifies counterfactual subgraphs to interpret TGNN predictions. CoDy employs a search algorithm that combines Monte Carlo Tree Search with heuristic selection policies, efficiently exploring a vast search space of potential explanatory subgraphs by leveraging spatial, temporal, and local event impact information. Extensive experiments against state-of-the-art factual and counterfactual baselines demonstrate CoDy's effectiveness, with improvements of 16% in AUFSC+ over the strongest baseline. Our code is available at: https://github.com/daniel-gomm/CoDy","Many real-world systems, like social media posts, traffic flows, or patient health conditions, change constantly over time. To understand and predict these changes, researchers use powerful models such as Temporal Graph Neural Networks (TGNNs). While TGNNs make highly accurate predictions, they are often complex and difficult to interpret, making it hard to understand why a particular decision was made. To address this, we introduce CoDy, a tool designed to explain the reasoning behind a model’s prediction by identifying the most important events that influenced the outcome. CoDy does this by exploring different “what-if” scenarios, such as removing key events, and observing how the model’s prediction changes in response. CoDy uses a smart search strategy that combines space, time, and sensitivity to changes, allowing it to efficiently pinpoint the events that matter most. Our experiments show that CoDy provides clearer and more accurate explanations than other leading methods. This makes it a powerful tool for anyone seeking to understand the behavior of dynamic, evolving systems."
Poster,COExpander: Adaptive Solution Expansion for Combinatorial Optimization,https://ICML.cc//virtual/2025/poster/45646,"Jiale Ma, Wenzheng Pan, Yang Li, Junchi Yan","Despite rapid progress in neural combinatorial optimization (NCO) for solving CO problems (COPs), as the problem scale grows, several bottlenecks persist: 1) solvers in the Global Prediction (GP) paradigm struggle in long-range decisions where the overly smooth intermediate heatmaps impede effective decoding, and 2) solvers in the Local Construction (LC) paradigm are time-consuming and incapable of tackling large instances due to the onerous auto-regressive process. Observing these challenges, we propose a new paradigm named Adaptive Expansion AE with its instantiation COExpander, positioned to leverage both advantages of GP and LC. COExpander utilizes informative heatmaps generated by a global predictor, which is learned under the guidance of locally determined partial solutions, to in turn direct the expansion of determined decision variables with adaptive step-sizes. To ensure transparent evaluation, we further take the lead to canonicalize 29 benchmarks spanning 6 popular COPs (MIS, MCl, MVC, MCut, TSP, ATSP) and various scales (50-10K nodes), upon which experiments demonstrate concrete SOTA performance of COExpander over these tasks. Source code and our standardized datasets will be made public.","We define the process of using machine learning to solve combinatorial optimization problems (COPs) as progressively determining decision variables. Previous approaches tend to be extreme: they either attempt to predict all decision variables in one shot (global prediction, GP) or determine them one by one in a sequential manner (local construction, LC). Through experiments, we find that GP solvers often lead to conflicts between predicted variables, while LC solvers is too inefficient. Therefore, we propose a new paradigm, namely adaptive expansion (AE), along with a supervised learning-based instance called COExpander. Compared to the LC solvers, COExpander requires fewer iterations. In each iteration, it adaptively selects a subset of variables using a global predictor, while the remaining variables are deferred for future inference. We evaluate COExpander on six classic COPs across 29 benchmarks and achieve SOTA results."
Poster,CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective,https://ICML.cc//virtual/2025/poster/44139,"Jiayu Liu, Zhenya Huang, Wei Dai, Cheng Cheng, Jinze Wu, Jing Sha, Song Li, Qi Liu, Shijin Wang, Enhong Chen","Although large language models (LLMs) show promise in solving complex mathematical tasks, existing evaluation paradigms rely solely on a coarse measure of overall answer accuracy, which are insufficient for assessing their authentic capabilities. In this paper, we propose \textbf{CogMath}, which comprehensively assesses LLMs' mathematical abilities through the lens of human cognition. Specifically, inspired by psychological theories, CogMath formalizes human reasoning process into 3 stages: \emph{problem comprehension}, \emph{problem solving}, and \emph{solution summarization}. Within these stages, we investigate perspectives such as numerical calculation, knowledge, and counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In each dimension, we develop an ``\emph{Inquiry}-\emph{Judge}-\emph{Reference}'' multi-agent system to generate inquiries that assess LLMs' mastery from this dimension. An LLM is considered to truly master a problem only when excelling in all inquiries from the 9 dimensions. By applying CogMath on three benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs are overestimated by 30\%-40\%. Moreover, we locate their strengths and weaknesses across specific stages/dimensions, offering in-depth insights to further enhance their reasoning abilities.","Large language models (LLMs) have shown impressive performance on a wide range of mathematical reasoning tasks. However, current evaluations only check if the final answer is right or wrong — a rough metric that fails to reflect what the model truly masters.We introduce CogMath, a new evaluation framework that assesses LLMs' math abilities through the lens of human cognition. Inspired by psychology, CogMath breaks down reasoning into three stages: understanding the problem, solving the problem, and summarizing the solution. Across these stages, we design 9 fine-grained evaluation dimensions, covering aspects like calculation, factual knowledge, and counterfactual reasoning.When applied to seven representative LLMs, CogMath reveals their mathematical abilities may be overestimated by 30%–40%. Our results also pinpoint strengths and weaknesses of each model, offering insights to guide the development of more trustworthy reasoning systems."
Poster,COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning,https://ICML.cc//virtual/2025/poster/45943,"Chamika Sudusinghe, Gerasimos Gerogiannis, Damitha Lenadora, Charles Block, Josep Torrellas, Charith Mendis","Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47× (up to 5.46×) for SpMM and 1.39× (up to 4.22×) for SDDMM.","Many advanced technologies in AI rely on a special type of program called sparse tensor programs. To run these programs faster, researchers are building special computer chips known as hardware accelerators. But there's a big challenge: these programs behave very differently depending on the input data, and testing them during the early design stages of new chips is slow and expensive. Existing tools that help optimize these programs require large amounts of data to work well, which isn't practical during the chip design phase. In this work, we introduce COGNATE, a smarter way to optimize these programs for new chips. Instead of collecting large amounts of expensive data from simulators of new hardware, COGNATE starts by learning from data collected on inexpensive, widely available devices such as regular computer CPUs. It then fine-tunes its knowledge using just a small amount of data from the new hardware. This approach works because COGNATE can recognize what’s similar and what’s different between the data samples from existing (CPUs) and new hardware. COGNATE can optimize these programs using only 5% of the data normally required, saving both time and resources. This makes it easier and more cost-effective to design AI chips that run efficiently, helping accelerate future technological development."
Poster,CogReact: A Reinforced Framework to Model Human Cognitive Reaction Modulated by Dynamic Intervention,https://ICML.cc//virtual/2025/poster/45494,"Songlin Xu, Xinyu Zhang","Using deep neural networks as computational models to simulate cognitive processes can provide key insights into human behavioral dynamics. Challenges arise when environments are highly dynamic, obscuring stimulus-behavior relationships. However, the majority of current research focuses on simulating human cognitive behaviors under ideal conditions, neglecting the influence of environmental disturbances. We propose CogReact, which integrates drift-diffusion with deep reinforcement learning to simulate granular effects of dynamic environmental stimuli on the human cognitive process. Quantitatively, it improves cognition modeling by considering the temporal effect of environmental stimuli on the cognitive process and captures both subject-specific and stimuli-specific behavioral differences. Qualitatively, it captures general trends in the human cognitive process under stimuli. We examine our approach under diverse environmental influences across various cognitive tasks. Overall, it demonstrates a powerful, data-driven methodology to simulate, align with, and understand the vagaries of human cognitive response in dynamic contexts.","Understanding how people make decisions in ever-changing environments is a major challenge in psychology and artificial intelligence. Most existing computer models that simulate human thinking mainly work well under ideal, controlled conditions — but real life is messy, with unpredictable events and shifting contexts.To tackle this, we created CogReact, a new system that combines deep reinforcement learning (a kind of data-driven AI model) with classical models from cognitive science. Our approach captures how people react over time to changing surroundings and how those changes affect their cognitive processing.CogReact doesn’t just model general trends; it also picks up on how different people and different types of situations lead to different behaviors. It outperforms existing models in both precision and realism.We tested CogReact across a variety of thinking tasks in dynamic settings, showing it can mimic human behavior more accurately than previous methods. This means we’re one step closer to building AI systems that understand how humans think — not just in theory, but in the unpredictable environments of the real world."
