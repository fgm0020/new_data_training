type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Behavior-agnostic Task Inference for Robust Offline In-context Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44307,"Long Ma, Fangwei Zhong, Yizhou Wang","The ability to adapt to new environments with noisy dynamics and unseen objectives is crucial for AI agents. In-context reinforcement learning (ICRL) has emerged as a paradigm to build adaptive policies, employing a **context** trajectory of the test-time interactions to infer the true task and the corresponding optimal policy efficiently without gradient updates. However, ICRL policies heavily rely on context trajectories, making them vulnerable to distribution shifts from training to testing and degrading performance, particularly in offline settings where the training data is static. In this paper, we highlight that most existing offline ICRL methods are trained for approximate Bayesian inference based on the training distribution, rendering them vulnerable to distribution shifts at test time and resulting in poor generalization. To address this, we introduce Behavior-agnostic Task Inference (BATI) for ICRL, a model-based maximum-likelihood solution to infer the task representation robustly. In contrast to previous methods that rely on a learned encoder as the approximate posterior, BATI focuses purely on dynamics, thus insulating itself against the behavior of the context collection policy. Experiments on MuJoCo environments demonstrate that BATI effectively interprets out-of-distribution contexts and outperforms other methods, even in the presence of significant environmental noise.","We want agents who can adapt to different environments and tasks and adjust their behaviors given the currently observed circumstances. However, the observed circumstances during training could be different from those of test time, misleading the agent to try to solve the wrong task. We propose a method that focuses the agent's attention on task-related characteristics only, filtering out irrelevant distractions and improving robustness."
Poster,Behavioral Exploration: Learning to Explore via In-Context Adaptation,https://ICML.cc//virtual/2025/poster/43764,"Andrew Wagenmaker, Zhiyuan Zhou, Sergey Levine","Developing autonomous agents that quickly explore an environment and adapt their behavior online is a canonical challenge in robotics and machine learning. While humans are able to achieve such fast online exploration and adaptation, often acquiring new information and skills in only a handful of interactions, existing algorithmic approaches tend to rely on random exploration and slow, gradient-based behavior updates. How can we endow autonomous agents with such capabilities on par with humans? Taking inspiration from recent progress on both in-context learning and large-scale behavioral cloning, in this work we propose behavioral exploration: training agents to internalize what it means to explore and adapt in-context over the space of ''expert'' behaviors. To achieve this, given access to a dataset of expert demonstrations, we train a long-context generative model to predict expert actions conditioned on a context of past observations and a measure of how ''exploratory'' the expert's behaviors are relative to this context. This enables the model to not only mimic the behavior of an expert, but also, by feeding its past history of interactions into its context, to select different expert behaviors than what have been previously selected, thereby allowing for fast online adaptation and targeted, ''expert-like'' exploration. We demonstrate the effectiveness of our method in both simulated locomotion and manipulation settings, as well as on real-world robotic manipulation tasks, illustrating its ability to learn adaptive, exploratory behavior.","In this work we propose a method to train a policy that learns to explore from demonstration data. In particular, our method learns to infer what actions taken by the demonstrator are likely to lead to observing novel states in the future, relative to what has been observed in the past."
Poster,Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44003,"Chen-Xiao Gao, Chenyang Wu, Mingjun Cao, Chenjun Xiao, Yang Yu, Zongzhang Zhang","Behavior regularization, which constrains the policy to stay close to some behavior policy, is widely used in offline reinforcement learning (RL) to manage the risk of hazardous exploitation of unseen actions. Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance.","This paper presents Behavior-Regularized Diffusion Policy Optimization (BDPO), a principled and efficient offline RL framework that integrates diffusion-based policy into offline RL. By introducing the pathwise KL regularization across intermediate diffusion steps and employing a two-time-scale actor-critic optimization, BDPO achieves theoretical grounding, efficient training, and superior empirical performance across standard benchmarks."
Poster,Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation,https://ICML.cc//virtual/2025/poster/46074,"Taehyun Cho, Seungyub Han, Seokhun Ju, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee","Distributional reinforcement learning improves performance by capturing environmental stochasticity, but a comprehensive theoretical understanding of its effectiveness remains elusive.In addition, the intractable element of the infinite dimensionality of distributions has been overlooked.In this paper, we present a regret analysis of distributional reinforcement learning with general value function approximation in a finite episodic Markov decision process setting. We first introduce a key notion of Bellman unbiasedness which is essential for exactly learnable and provably efficient distributional updates in an online manner.Among all types of statistical functionals for representing infinite-dimensional return distributions, our theoretical results demonstrate that only moment functionals can exactly capture the statistical information.Secondly, we propose a provably efficient algorithm, SF-LSVI, that achieves a tight regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$ where $H$ is the horizon, $K$ is the number of episodes, and $d_E$ is the eluder dimension of a function class.","Reinforcement learning(RL) helps computers learn to make decisions, like choosing the best move in a game or guiding robots through tasks. Traditional RL methods focus only on the average outcome of actions, which might not be enough for safe and reliable decisions in the real world. To solve this, researchers have developed distributional RL, a method that considers all possible outcomes and their probabilities, not just the average.However, handling these full distributions is tricky because they contain infinite information. This paper introduces a new concept called Bellman Unbiasedness, which allows us to estimate the key information from these distributions using moments efficiently—like the mean and variance—without errors, even when working with just a few samples. The authors also propose a new algorithm, SF-LSVI, that learns decision-making strategies effectively and unbiasedly, even when using general function approximations (such as neural networks).This work could make RL more trustworthy and applicable to real-world problems, such as safer robotic control, smarter navigation systems, and better AI decision-making."
Poster,Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective,https://ICML.cc//virtual/2025/poster/44727,"Qingchuan Ma, Yuhang Wu, Xiawu Zheng, Rongrong Ji","In this paper, we aim to establish a simple, effective, and theoretically grounded benchmark for rigorously probing abstract reasoning in Large Language Models (LLMs). To achieve this, we first develop a mathematic framework that defines abstract reasoning as the ability to: (i) extract essential patterns independent of surface representations, and (ii) apply consistent rules to these abstract patterns. Based on this framework, we introduce two novel complementary metrics: Γ measures basic reasoning accuracy, while ∆ quantifies a model's reliance on specific symbols rather than underlying patterns - a key indicator of true abstraction versus mere memorization. To implement this measurement, we design a benchmark: systematic symbol remapping in rule-based tasks, which forces models to demonstrate genuine pattern recognition beyond superficial token matching. Extensive LLM evaluations using this benchmark (commercial API models, 7B-70B, multi-agent) reveal:1) critical limitations in non-decimal arithmetic and symbolic reasoning; 2) persistent abstraction gaps despite chain-of-thought prompting; and 3) ∆'s effectiveness in robustly measuring memory dependence by quantifying performance degradation under symbol remapping, particularly highlighting operand-specific memorization. These findings underscore that current LLMs, despite domain-specific strengths, still lack robust abstract reasoning, highlighting key areas for future improvement.","Can advanced AI truly think abstractly, like humans? We developed a new benchmark to rigorously test this. Our method challenges AIs by systematically changing symbols in rule-based tasks (e.g., using letters for numbers in math), forcing them to understand underlying patterns rather than just memorizing specific examples. Our evaluations reveal critical limitations: current AIs struggle significantly with tasks like non-decimal arithmetic or reasoning with novel symbols, indicating a heavy reliance on memory over genuine abstraction. This work underscores that robust, flexible abstract reasoning remains a key challenge for AI, highlighting crucial areas for future improvement."
Poster,Benchmarking Quantum Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45915,"Nico Meyer, Christian Ufrecht, George Yammine, Georgios Kontes, Christopher Mutschler, Daniel Scherer","Benchmarking and establishing proper statistical validation metrics for reinforcement learning (RL) remain ongoing challenges, where no consensus has been established yet. The emergence of quantum computing and its potential applications in quantum reinforcement learning (QRL) further complicate benchmarking efforts. To enable valid performance comparisons and to streamline current research in this area, we propose a novel benchmarking methodology, which is based on a statistical estimator for sample complexity and a definition of statistical outperformance. Furthermore, considering QRL, our methodology casts doubt on some previous claims regarding its superiority. We conducted experiments on a novel benchmarking environment with flexible levels of complexity. While we still identify possible advantages, our findings are more nuanced overall. We discuss the potential limitations of these results and explore their implications for empirical research on quantum advantage in QRL.","Quantum computing is an exciting new technology that could revolutionize reinforcement learning (RL) and machine learning in general. At least, this statement is frequently made in the literature. Upon closer inspection, however, claims of quantum reinforcement learning (QRL) superiority are often based on insufficient statistical evaluation. This reflects the ongoing challenges of establishing proper statistical validation metrics for RL.In our work, we introduce a statistical estimator for benchmarking and establish a statistically robust comparison between RL and corresponding quantum versions, along with a suitable environment. In line with previous work, we find indications of QRL outperforming RL, but our findings are more nuanced. Finally, we contextualize these results in terms of their implications for empirical quantum advantage in QRL.With our results, we emphasize the importance of robust benchmarking and statistical backing as part of performance evaluation. We hope our suggestions will guide researchers to identify trends in small toy problems amenable to current quantum computers. As quantum hardware evolves and problem sizes grow, these trends might or might not persist, potentially guiding us toward quantum advantage in machine learning."
Poster,Benefits of Early Stopping in Gradient Descent for Overparameterized Logistic Regression,https://ICML.cc//virtual/2025/poster/44193,"Jingfeng Wu, Peter Bartlett, Matus Telgarsky, Bin Yu","In overparameterized logistic regression, gradient descent (GD) iterates diverge in norm while converging in direction to the maximum $\ell_2$-margin solution---a phenomenon known as the implicit bias of GD. This work investigates additional regularization effects induced by early stopping in well-specified high-dimensional logistic regression. We first demonstrate that the excess logistic risk vanishes for early stopped GD but diverges to infinity for GD iterates at convergence. This suggests that early stopped GD is well-calibrated, whereas asymptotic GD is statistically inconsistent. Second, we show that to attain a small excess zero-one risk, polynomially many samples are sufficient for early stopped GD, while exponentially many samples are necessary for any interpolating estimator, including asymptotic GD. This separation underscores the statistical benefits of early stopping in the overparameterized regime. Finally, we establish nonasymptotic bounds on the norm and angular differences between early stopped GD and $\ell_2$-regularized empirical risk minimizer, thereby connecting the implicit regularization of GD with explicit $\ell_2$-regularization.","In machine learning problems like overparameterized logistic regression, running the standard training method (gradient descent, or GD) for too long can lead to unreliable predictions and increasing errors. This paper demonstrates that simply stopping GD training early offers significant advantages. Early-stopped GD achieves vanishing errors and good calibration, unlike models trained to completion which can be inconsistent. Furthermore, early stopping allows models to reach high accuracy with a manageable (polynomial) amount of data, whereas fully trained or interpolating models often require impractical (exponential) amounts. This research also reveals that early stopping acts as an effective ""implicit regularization,"" with the path taken by early-stopped GD closely mirroring models trained with the well-known $\ell_2$-regularization technique."
Poster,Benign Overfitting in Token Selection of Attention Mechanism,https://ICML.cc//virtual/2025/poster/45799,"Keitaro Sakamoto, Issei Sato","Attention mechanism is a fundamental component of the transformer model and plays a significant role in its success.However, the theoretical understanding of how attention learns to select tokens is still an emerging area of research.In this work, we study the training dynamics and generalization ability of the attention mechanism, under classification problems with label noise.We show that, with the characterization of signal-to-noise ratio (SNR), the token selection of attention mechanism achieves ``benign overfitting'', i.e., maintaining high generalization performance despite fitting label noise.Our work also demonstrates an interesting delayed acquisition of generalization after an initial phase of overfitting.Finally, we provide experiments to support our theoretical analysis using both synthetic and real-world datasets.","Transformer models are the backbone of modern AI systems, including LLMs, and their attention mechanisms play a crucial role by selecting important tokens from input sequences.But how attention learns this selection during training with gradient descent, especially when the data contains incorrect labels (label noise), is poorly understood. We theoretically show that attention mechanisms can exhibit *benign* overfitting: even when the model memorizes incorrect labels, it can still generalize well to unseen data. We also find that generalization can emerge much later than memorization.Practically, our results indicate the possibility that overfitting may not be something to fear, even when training on low-quality data, as is often the case with LLMs.Theoretically, our work deepens understanding of attention dynamics and provides an analytical framework for studying token selection dynamics, not only in label noise settings, but in any scenario where token selection behavior varies across the training examples."
Poster,Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety,https://ICML.cc//virtual/2025/poster/45842,"Zihan Guan, Mengxuan Hu, Ronghang Zhu, Sheng Li, Anil Vullikanti","Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter.","We discovered a surprising and dangerous weakness in how the safety alignment of large language models (LLMs), like Llama, is broken in the fine-tuning stage: Even if you train these models on completely harmless, “benign” text, the model’s safety can still break down.We took this vulnerability further by designing a targeted attack. Instead of using random examples from a benign dataset, we carefully selected just 100 specific examples — the most “unusual” or “outlier” ones — using a new method we developed, called Self-Inf-N. When fine-tuned only on these samples, models started generating much more harmful content. This attack works across many popular LLMs, showing it's not just a fluke with one model. Worse, common defense strategies fail to stop it. Our work highlights an urgent problem: even seemingly safe training data can quietly undermine a model’s safety. Stronger safeguards are needed to ensure LLMs remain trustworthy and aligned, especially as they are increasingly used in sensitive areas."
Poster,Best of Both Worlds: Advantages of Hybrid Graph Sequence Models,https://ICML.cc//virtual/2025/poster/45837,"Ali Behrouz, Ali Parviz, Mahdi Karami, Clayton Sanford, Bryan Perozzi, Vahab Mirrokni","Modern sequence models (e.g., Transformers and linear RNNs) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Recently, adopting these sequence models for graph-structured data has gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, a lack of a common foundation about what constitutes a good graph sequence model, and a mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we introduce the Graph Sequence Model (GSM), a unifying framework for applying sequence models to graph data.  The GSM framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Building on this insight, we propose GSM++, a fast hybrid model that hierarchically tokenizes the graph using Hierarchical Affinity Clustering (HAC) and then encodes these sequences via a hybrid architecture. The  theoretical and experimental findings confirm that GSM++ outperforms baseline models on most benchmarks.","In recent years designing more efficient machine learning models that can learn to transfer a sequence to another sequence is gaining much attention. Although adopting these models for graph-structured data has gained popularity, there is a lack of a common foundation about what constitutes a good learning algorithm effective for graphs. In this work, we present a mathematical unifying framework that explains how we can translate a graph into a sequence and then use sequence models to learn the dependencies of entities.  Building on our theoretical insights, we propose GSM++, a fast model that hierarchically translates the interactions in graphs into sequences and then encodes these sequences via a novel machine learning model. We provide theoretical and experimental results that confirm the effectiveness of GSM++."
