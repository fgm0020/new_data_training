type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A Theoretical Framework For Overfitting In Energy-based Modeling,https://ICML.cc//virtual/2025/poster/45237,"Giovanni Catania, Aurélien Decelle, Cyril Furtlehner, Beatriz Seoane","We investigate the impact of limited data on training pairwise energy-based models for inverse problems aimed at identifying interaction networks. Utilizing the Gaussian model as testbed, we dissect training trajectories across the eigenbasis of the coupling matrix, exploiting the independent evolution of eigenmodes and revealing that the learning timescales are tied to the spectral decomposition of the empirical covariance matrix. We see that optimal points for early stopping arise from the interplay between these timescales and the initial conditions of training. Moreover, we show that finite data corrections can be accurately modeled through asymptotic random matrix theory calculations and provide the counterpart of generalized cross-validation in the energy based model context. Our analytical framework extends to binary-variable maximum-entropy pairwise models with minimal variations.These findings offer strategies to control overfitting in discrete-variable models through empirical shrinkage corrections, improving the management of overfitting in energy-based generative models.Finally, we propose a generalization to arbitrary energy-based models by deriving the neural tangent kernel dynamics of the score function under the score-matching algorithm.","This study investigates how machine learning models can be trained effectively when the amount of data is limited, focusing on energy-based generative models. These models are often used to uncover hidden structures in complex datasets, such as gene interactions or brain connectivity. However, with limited data, they tend to overfit — capturing noise rather than meaningful patterns.To understand this, the authors analyze a simplified model that allows for an exact mathematical treatment. They show that different features in the data are learned at different rates during training and that less relevant features —which are often dominated by noise — take longer to learn. This mismatch of time scales leads to overfitting and degradation of the model if training takes too long.Based on this insight, the study identifies optimal stopping points to prevent overfitting and introduces mathematical corrections that improve the reliability of the model without the need for additional data. The framework can also be generalized to more complex, non-solvable models.Overall, this work contributes to the theoretical foundations of machine learning by explaining how data scarcity affects training dynamics and overfitting, with the goal of making models more robust in real-world scenarios where data is often limited."
Poster,A Theoretical Justification for Asymmetric Actor-Critic Algorithms,https://ICML.cc//virtual/2025/poster/45909,"Gaspard Lambrechts, Damien Ernst, Aditya Mahajan","In reinforcement learning for partially observable environments, many successful algorithms have been developed within the asymmetric learning paradigm. This paradigm leverages additional state information available at training time for faster learning. Although the proposed learning objectives are usually theoretically sound, these methods still lack a precise theoretical justification for their potential benefits. We propose such a justification for asymmetric actor-critic algorithms with linear function approximators by adapting a finite-time convergence analysis to this setting. The resulting finite-time bound reveals that the asymmetric critic eliminates error terms arising from aliasing in the agent state.","Some intelligent agents learn faster by using extra information during training — like full knowledge of the environment’s state — even if that information is not available later. This is called asymmetric learning, and it works well in practice. But why does it work so well? In this paper, we offer a theoretical answer for a learning algorithm called the asymmetric actor-critic algorithm. We show that giving this extra information to part of the learning algorithm — the critic — reduces specific errors caused by limited observations. This makes learning more efficient, and our analysis explains when and why this advantage appears."
Poster,"A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization",https://ICML.cc//virtual/2025/poster/43622,"Muhammed Ustaomeroglu, Guannan Qu","Self-attention has emerged as a core component of modern neural architectures, yet its theoretical underpinnings remain elusive. In this paper, we study self-attention through the lens of *interacting entities*, ranging from agents in multi-agent reinforcement learning to alleles in genetic sequences, and show that a single layer linear self-attention can *efficiently* represent, learn, and generalize functions capturing pairwise interactions, including out-of-distribution scenarios. Our analysis reveals that self-attention acts as a *mutual interaction learner* under minimal assumptions on the diversity of interaction patterns observed during training, thereby encompassing a wide variety of real-world domains. In addition, we validate our theoretical insights through experiments demonstrating that self-attention learns interaction functions and generalizes across both population distributions and out-of-distribution scenarios. Building on our theories, we introduce *HyperFeatureAttention*, a novel neural network module designed to learn couplings of different feature-level interactions between entities. Furthermore, we propose *HyperAttention*, a new module that extends beyond pairwise interactions to capture multi-entity dependencies, such as three-way, four-way, or general $n$-way interactions.","Modern AI models such as ChatGPT rely on a mechanism called attention, which lets every word (or image patch, protein residue, or robot agent) decide how strongly it should “listen” to all the others.  Despite its success, we still lack a clear, mathematical picture of why this mechanism works so well. Our study views each word or agent as an interacting entity and proves that even a single simplified attention layer can efficiently capture pairwise relationships in the data, under some assumptions. We further show that ordinary training methods will reliably reach these ideal parameters and that the resulting model naturally handles entirely new data and even much longer sequences than it saw during training. Put simply, a self-attention block can serve as a near-perfect mutual interaction learner. Building on these insights, we introduce two new attention blocks -HyperFeatureAttention, which is coupled feature interaction learner, and HyperAttention which is high-order interaction learner (three-way or four-way n-way). Toy language-model experiments confirm the advantages of these richer blocks. By revealing how attention learns interactions and how to extend it, our work lays a foundation for more efficient, trustworthy AI systems in areas ranging from multi-agent control to genomics."
Poster,A Theory for Conditional Generative Modeling on Multiple Data Sources,https://ICML.cc//virtual/2025/poster/45415,"Rongzhen Wang, Yan Zhang, Chenyu Zheng, Chongxuan Li, Guoqiang Wu","The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper provides a first attempt to fill this gap by rigorously analyzing multi-source training in conditional generative modeling, where each condition represents a distinct data source. Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number.Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training.We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. Simulations and real-world experiments validate our theory.","Modern AI models often learn from data collected across many different sources—for example, text from websites, books, and social media. While this is observed to make models more powerful, we still don't fully understand how and when using multiple sources actually helps.Our work takes a first step in answering this question for conditional generative models, which learn to generate new data based on given conditions (like generating a photo based on a label). We provide a mathematical explanation showing that when the data sources are similar enough and the model is expressive, training on multiple sources can lead to better performance than training on just one. We also apply our theory to specific models, including deep learning methods, and support it with both simulations and real-world experiments. This helps us better understand how to use data from diverse environments to build stronger AI systems."
Poster,AtlasD: Automatic Local Symmetry Discovery,https://ICML.cc//virtual/2025/poster/44805,"Manu Bhat, Jonghyun Park, Jianke Yang, Nima Dehmamy, Robin Walters, Rose Yu","Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (AtlasD), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate AtlasD is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks. Our code is publicly available at https://github.com/Rose-STL-Lab/AtlasD.","Symmetry discovery is a problem in machine learning where one seeks to find important properties about a dataset in order to build more accurate prediction models. Existing methods for finding symmetries in data look for patterns that apply everywhere uniformly (like how a circle looks the same when rotated about any point). However, many real-world systems have more complex symmetries that vary from place to place, like how ocean currents might have rotational patterns in some regions but not others.We introduce a new framework called AtlasD (Automatic Local Symmetry Discovery) that can detect these ""local symmetries"". These are simply transformations that preserve structure in specific neighborhoods rather than globally. Our method works by training small neural networks to make predictions in local regions, then automatically discovering what transformations (called Lie groups) these networks are invariant to.We tested AtlasD on challenging problems in particle physics and solving differential equations, successfully finding complex symmetry groups that previous methods missed. When we use these discovered local symmetries as inductive biases in downstream tasks, we see significant performance improvements. This work opens up possibilities for understanding the hidden structure in complex datasets and leveraging these insights to build more effective machine learning models."
Poster,A Trichotomy for List Transductive Online Learning,https://ICML.cc//virtual/2025/poster/46471,"Steve Hanneke, Amirreza Shaeiri","List learning is an important topic in both theoretical and empirical machine learning research, playing a key role in the recent breakthrough result of (Brukhim et al., 2022) on the characterization of multiclass PAC learnability, as well as addressing label ambiguity in computer vision classification tasks, among others. In this paper, we study the problem of list transductive online learning. In this framework, the learner outputs a list of multiple labels for each instance rather than just one, as in traditional multiclass classification. In the realizable setting, we demonstrate a trichotomy of possible rates of the minimax number of mistakes. In particular, if the learner plays for $\text{T} \in \mathbb{N}$ rounds, its minimax number of mistakes can only be of the orders $\Theta(\text{T})$, $\Theta(\log \text{T})$, or $\Theta(1)$. This resolves an open question raised by (Hanneke et al., 2024). On the other hand, in the agnostic setting, we characterize the learnability by constructively proving the $\widetilde{\mathcal{O}}(\sqrt{\text{T}})$ upper bound on the minimax expected regret. Along the way, we also answer another open question asked by (Moran et al., 2023). To establish these results, we introduce two new combinatorial complexity dimensions, called the Level-constrained $(\mathrm{L+1})$-Littlestone dimension and Level-constrained $(\mathrm{L+1})$-Branching dimension, if the list size is $\mathrm{L} \in \mathbb{N}$. Eventually, we conclude our work by raising an open question regarding eliminating the factor list size, which seems to be a crucial step, as it has consistently appeared in previous works on this subject.","List learning is an important topic in both theoretical and empirical machine learning research, playing a key role in the recent breakthrough result of (Brukhim et al., 2022) on the characterization of multiclass PAC learnability, as well as the ambiguity of labels in computer vision classification tasks, among others. In this paper, we theoretically study the problem of list transductive online learning. In this framework, the sequence of instances is known to the learner before the start of the game, and the learner outputs a list of multiple labels for each instance rather than just one, as in traditional multiclass classification. We prove theoretical results by introducing two new combinatorial complexity dimensions. This resolves open questions raised by prior works. Eventually, we conclude our work by raising an open question regarding eliminating the factor of list size, which seems to be a crucial step, as it has consistently appeared in previous works on this subject."
Poster,Attention-Level Speculation,https://ICML.cc//virtual/2025/poster/46486,"Jack Cai, Ammar Vora, Randolph Zhang, Mark O&#x27;Connor, Mark Jeffrey","As Large Language Models (LLMs) grow in size and context length, efficient inference strategies are essential to maintain low-latency token generation. Unfortunately, conventional tensor and data parallelism face diminishing returns when scaling across multiple devices. We propose a novel form—attention-level speculative parallelism (ALSpec)—that predicts self-attention outputs to execute subsequent operations early on separate devices. Our approach overlaps attention and non-attention computations, reducing the attention latency overhead at 128K context length by up to 5x and improving end-to-end decode latency by up to 1.65x, all without sacrificing quality. We establish the fundamental pillars for speculative execution and provide an execution paradigm that simplifies implementation. We show that existing attention-approximation methods perform well on simple information retrieval tasks, but they fail in advanced reasoning and math. Combined with speculative execution, we can approximate up to 90% of self-attention without harming model correctness. Demonstrated on Tenstorrent's NPU devices, we scale up LLM inference beyond current techniques, paving the way for faster inference in transformer models.","When a large-language model reacts to your prompt to generate text, one of the slowest but important stages of the computation is called attention. One way to speed up attention is to use more computer chips to work on it, but once you use 8 or more computer chips, the communication between chips overwhelms the benefit of more computational ability. Another way to speed up attention is to use an approximation of its underlying math. Particular types of attention approximation from prior work can work very well in some cases, but in others, the quality of the generated text is degraded. Our paper proposes to sometimes use the approximation, but sometimes not, verifying on the fly whether the approximation was of good quality. Our experiments suggest that using our approach (attention-level speculation) with 8 computer chips can be 1.65x times faster than the conventional approach to use 8 chips to acceleration large-language models."
Poster,Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data,https://ICML.cc//virtual/2025/poster/46653,"Guan Zhong, Likang Wu, Hongke Zhao, Ming He, Jianping Fan","Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. Through a series of experiments, we uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention (as in LLMs) nor fixed connectivity (as in GNNs) is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \href{https://anonymous.4open.science/r/LLM_exploration-B21F}{anonymous.4open.science/LLM\_exploration-B21F}","We find that although LLMs can gradually become aware of graph data during training, they do not properly utilize the connectivity information within the graph. Subsequently, we analyze and explain this from two main aspects: attention distribution and attention window. Our findings indicate that graph data also exhibits phenomena similar to the ""attention sink"" observed in the NLP domain, as well as a unique phenomenon in graph data called ""Skewed Line Sink,"" both of which interfere with how LLMs allocate attention within the graph. The fully connected attention window and fixed connection window used in GNNs are not suitable for LLMs."
Poster,Attention-Only Transformers via Unrolled Subspace Denoising,https://ICML.cc//virtual/2025/poster/45735,"Peng Wang, Yifu Lu, Yaodong Yu, Druv Pai, Qing Qu, Yi Ma","Despite the popularity of transformers in practice, their architectures are empirically designed and neither mathematically justified nor interpretable. Moreover, as indicated by many empirical studies, some components of transformer architectures may be redundant. To derive a fully interpretable transformer architecture with only necessary components, we contend that the goal of representation learning is to compress a set of noisy initial token representations towards a mixture of low-dimensional subspaces. To compress these noisy token representations, an associated denoising operation naturally takes the form of a multi-head (subspace) self-attention. By unrolling such iterative denoising operations into a deep network, we arrive at a highly compact architecture that consists of \textit{only} self-attention operators with skip connections at each layer. Moreover, we show that each layer performs highly efficient denoising: it improves the signal-to-noise ratio of token representations \textit{at a linear rate} with respect to the number of layers. Despite its simplicity, extensive experiments on vision and language tasks demonstrate that such a transformer achieves performance close to that of standard transformer architectures such as GPT-2 and CRATE.","Transformers power many AI systems today, but their internal structure is complex and often built without clear explanations for why each part is needed. In this work, we take a step toward making transformers simpler and more understandable. We show that a core reason transformers work well is that they denoise noisy token representations into the corresponding subspaces. Based on this idea, we design a streamlined version of the transformer that uses only attention and skip connections—removing other common components like feedforward layers. Despite being much simpler, our model performs nearly as well as standard transformers on tasks in both language and vision, offering new insights into how these powerful models actually work."
Poster,Attributes Shape the Embedding Space of Face Recognition Models,https://ICML.cc//virtual/2025/poster/45064,"Pierrick Leroy, Antonio Mastropietro, Marco Nurisso, Francesco Vaccarino","Face Recognition (FR) tasks have made significant progress with the advent of Deep Neural Networks, particularly through margin-based triplet losses that embed facial images into high-dimensional feature spaces. During training, these contrastive losses focus exclusively on identity information as labels. However, we observe a multiscale geometric structure emerging in the embedding space, influenced by interpretable facial (e.g., hair color) and image attributes (e.g., contrast).We propose a geometric approach to describe the dependence or invariance of FR models to these attributes and introduce a physics-inspired alignment metric. We evaluate the proposed metric on controlled, simplified models and widely used FR models fine-tuned with synthetic data for targeted attribute augmentation. Our findings reveal that the models exhibit varying degrees of invariance across different attributes, providing insight into their strengths and weaknesses and enabling deeper interpretability.Code available here: https://github.com/mantonios107/attrs-fr-embs.","Face Recognition (FR) models map every image to a point in a high-dimensional, abstract space, called the embedding space. Here, faces of the same identity are represented closer than faces of different identities. To this end, models should filter out unidentifying image properties to perform correct recognition. Yet, we still lack a clear view of which properties, and particularly interpretable attributes, organise that space. We frame the question at two levels: the macroscale, across identities, and the microscale, inside identities. Without insight at either scale, hidden geometry can entangle demographic traits or lighting quirks, undermining fairness and robustness.We introduce a distance-based analysis for the macroscale and an invariance energy measure for the microscale to quantify how strongly each attribute shapes the embedding space. Our investigation uncovers a consistent behaviour of recent FR models. Further fine-tuning with targeted augmentations on an attribute increases the corresponding invariance energy, confirming that our measures accurately track FR invariance to interpretable cues.Taken together, the two scales provide a concise geometric fingerprint that lets practitioners audit and compare face-recognition systems. By exposing hidden biases and guiding attribute-specific training, our method advances face biometrics and possibly other metric-learning tasks toward greater transparency, fairness, and resilience."
