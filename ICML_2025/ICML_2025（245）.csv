type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,RAGGED: Towards Informed Design of Scalable and Stable RAG Systems,https://ICML.cc//virtual/2025/poster/46460,"Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig","Retrieval-augmented generation (RAG) enhances language models by integrating external knowledge, but its effectiveness is highly dependent on system configuration. Improper retrieval settings can degrade performance, making RAG less reliable than closed-book generation. In this work, we introduce RAGGED, a framework for systematically evaluating RAG systems across diverse retriever-reader configurations, retrieval depths, and datasets. Our analysis reveals that reader robustness to noise is the key determinant of RAG stability and scalability. Some readers benefit from increased retrieval depth, while others degrade due to their sensitivity to distracting content. Through large-scale experiments on open-domain, multi-hop, and specialized-domain datasets, we show that retrievers, rerankers, and prompts influence performance but do not fundamentally alter these reader-driven trends. By providing a principled framework and new metrics to assess RAG stability and scalability, RAGGED enables systematic evaluation of retrieval-augmented generation systems, guiding future research on optimizing retrieval depth and model robustness.","Some language models answer questions more accurately when they can look up relevant information, a setup known as retrieval-augmented generation (RAG). But adding more documents doesn’t always help. In fact, too much or irrelevant information can make answers worse.We developed RAGGED, a framework to study when and how retrieval helps. It introduces two new scores that measure how stable a model’s performance is as you change how much it retrieves, and how well it scales when given more context. Using this framework, we tested several widely used models and retrieval methods across different question-answering tasks.Our results show that a model’s ability to handle noisy or unnecessary information is more important than simply improving the retrieval quality. This challenges common assumptions about how to build better RAG systems. By using RAGGED, developers and researchers can better understand model behavior and build more reliable, adaptive, and efficient systems."
Poster,Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing,https://ICML.cc//virtual/2025/poster/46684,"Han Jiang, Xiaoyuan Yi, Zhihua Wei, Ziang Xiao, Shu Wang, Xing Xie","*Warning: Contains harmful model outputs.*Despite significant advancements, the propensity of Large Language Models (LLMs) to generate harmful and unethical content poses critical challenges.Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from *evaluation chronoeffect*, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, *overestimating* ever-developing LLMs. To tackle this problem, we propose GETA, a novel *generative evolving testing* approach based on adaptive testing methods in measurement theory. Unlike traditional adaptive testing methods that rely on a static test item pool, GETA probes the underlying moral boundaries of LLMs by dynamically generating test items tailored to model capability. GETA co-evolves with LLMs by learning a joint distribution of item difficulty and model value conformity, thus effectively addressing evaluation chronoeffect. We evaluated various popular LLMs with GETA and demonstrated that 1) GETA can dynamically create difficulty-tailored test items and 2) GETA's evaluation results are more consistent with models' performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms.","Large language models (LLMs), like ChatGPT, have made big strides in recent years—but they still sometimes produce harmful or unethical content. To use these tools responsibly, we need reliable ways to measure how well they align with human values. Unfortunately, most current evaluation methods rely on fixed test sets that can become outdated or even show up in the models’ training data, making them less useful over time.Our research introduces a new approach called GETA (Generative Evolving Testing Approach), which solves this problem by generating fresh, tailored test questions on the fly. Inspired by testing methods used in education, GETA adjusts the difficulty of each question based on the model’s abilities and adapts as models evolve.We tested GETA on several popular LLMs and found it could better track how well these models behave in unfamiliar situations—something static tests struggle with. This work matters because it offers a smarter, more future-proof way to evaluate the ethical behavior of LLMs, helping ensure they’re safe and trustworthy as they continue to grow more powerful."
Poster,Random Feature Representation Boosting,https://ICML.cc//virtual/2025/poster/44355,"Nikita Zozoulenko, Thomas Cass, Lukas Gonon","We introduce Random Feature Representation Boosting (RFRBoost), a novel method for constructing deep residual random feature neural networks (RFNNs) using boosting theory. RFRBoost uses random features at each layer to learn the functional gradient of the network representation, enhancing performance while preserving the convex optimization benefits of RFNNs. In the case of MSE loss, we obtain closed-form solutions to greedy layer-wise boosting with random features. For general loss functions, we show that fitting random feature residual blocks reduces to solving a quadratically constrained least squares problem. Through extensive numerical experiments on tabular datasets for both regression and classification, we show that RFRBoost significantly outperforms RFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime where RFNNs are typically applied. Moreover, RFRBoost offers substantial computational benefits, and theoretical guarantees stemming from boosting theory.","A surprisingly effective family of machine learning models is the class of single hidden layer neural networks whose weights are randomly initialized and never trained, where only the final linear layer is learned. These are known as random feature neural networks (RFNNs). We asked ourselves: can we significantly improve the performance of such untrained networks by increasing their depth, without relying on end-to-end training via stochastic gradient descent?One of the key innovations behind the success of deep neural networks has been the use of residual connections: shortcuts that help information flow more easily through very deep models. However, bringing this idea into RFNNs is challenging. Naively stacking randomly initialized, untrained layers can actually degrade performance rather than improve it.We found that the answer to our question is yes. By studying neural networks from the perspective of functional gradient boosting, we developed a method that builds deep models by adding random feature residual blocks one at a time, each designed to correct the errors of the previous layers. This approach preserves the computational efficiency of RFNNs, while improving accuracy via increased depth. Through extensive experiments on tabular regression and classification tasks, we show that this approach leads to significant performance gains over existing models."
Poster,Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures,https://ICML.cc//virtual/2025/poster/45270,"Jie Gao, Rajesh Jayaram, Benedikt Kolbe, Shay Sapir, Chris Schwiegelshohn, Sandeep Silwal, Erik Waingarten","Randomized dimensionality reduction is a widely-used algorithmic technique for speeding up large-scale Euclidean optimization problems. In this paper, we study dimension reduction for a variety of maximization problems, including max-matching, max-spanning tree, as well as various measures for dataset diversity. For these problems, we show that the effect of dimension reduction is intimately tied to the *doubling dimension* $\lambda_X$ of the underlying dataset $X$---a quantity measuring intrinsic dimensionality of point sets. Specifically, the dimension required is $O(\lambda_X)$, which we also show is necessary for some of these problems. This is in contrast to classical dimension reduction results, whose dependence grow with the dataset size $|X|$. We also provide empirical results validating the quality of solutions found in the projected space, as well as speedups due to dimensionality reduction.","The large sizes of vectors, measured in terms of the number of coordinates they have, creates significant computational bottlenecks in many post processing of these vectors. We study how to significantly reduce the number of coordinates (compression) of vectors without affecting downstream performance for a wide class of problems. We show that the amount of compression possible is mathematically tied to the intrinsic complexity of the input dataset. After reducing the number of coordinates, many post processing steps are much faster without sacrificing quality."
Poster,Random Policy Evaluation Uncovers Policies of Generative Flow Networks,https://ICML.cc//virtual/2025/poster/43990,"Haoran He, Emmanuel Bengio, Qingpeng Cai, Ling Pan","The Generative Flow Network (GFlowNet) is a probabilistic framework in which an agent learns a stochastic policy and flow functions to sample objects with probability proportional to an unnormalized reward function. GFlowNets share a strong connection with reinforcement learning (RL) that typically aims to maximize reward. A number of recent works explored connections between GFlowNets and maximum entropy (MaxEnt) RL, which incorporates entropy regularization into the standard RL objective. However, the relationship between GFlowNets and standard RL remains largely unexplored, despite the inherent similarities in their sequential decision-making nature.While GFlowNets can discover diverse solutions through specialized flow-matching objectives, connecting them to standard RL can simplify their implementation through well-established RL principles and also improve RL's capabilities in diverse solution discovery (a critical requirement in many real-world applications), and bridging this gap can further unlock the potential of both fields. In this paper, we bridge this gap by revealing a fundamental connection between GFlowNets and one of the most basic components of RL -- policy evaluation. Surprisingly, we find that the value function obtained from evaluating a uniform policy is closely associated with the flow functions in GFlowNets. Building upon these insights, we introduce a rectified random policy evaluation (RPE) algorithm, which achieves the same reward-matching effect as GFlowNets based on simply evaluating a fixed random policy, offering a new perspective. Empirical results across extensive benchmarks demonstrate that RPE achieves competitive results compared to previous approaches, shedding light on the previously overlooked connection between (non-MaxEnt) RL and GFlowNets.","This paper establishes a novel connection between Generative Flow Networks (GFlowNets) and standard (non-MaxEnt) reinforcement learning (RL), specifically through the lens of policy evaluation. While prior work linked GFlowNets to MaxEnt RL, our work reveals a direct relationship with standard RL, which shows that the flow functions in GFlowNets are fundamentally connected to the value function obtained by evaluating a uniform policy. Building on this insight, we introduce Rectified Policy Evaluation (RPE), which achieves the core reward-matching objective of GFlowNets (sampling proportionally to reward) simply by evaluating a fixed, uniform random policy within the standard RL framework. Our work reveals a previously overlooked fundamental link between standard RL (policy evaluation) and GFlowNets, enabling bidirectional benefits: simplifying GFlowNet implementation via RL tools and enhancing RL's capability for diverse solution discovery. Experiments across extensive benchmarks demonstrate that RPE achieves competitive performance compared to previous GFlowNet methods and RL methods."
Poster,Random Registers for Cross-Domain Few-Shot Learning,https://ICML.cc//virtual/2025/poster/46472,"Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li","Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a data-sufficient source domain to data-scarce target domains. Although Vision Transformer (ViT) has shown superior capability in many vision tasks, its transferability against huge domain gaps in CDFSL is still under-explored. In this paper, we find an intriguing phenomenon: during the source-domain training, prompt tuning, as a common way to train ViT, could be harmful for the generalization of ViT in target domains, but setting them to random noises (i.e., random registers) could consistently improve target-domain performance. We then delve into this phenomenon for an interpretation. We find that learnable prompts capture domain information during the training on the source dataset, which views irrelevant visual patterns as vital cues for recognition. This can be viewed as a kind of overfitting and increases the sharpness of the loss landscapes. In contrast, random registers are essentially a novel way of perturbing attention for the sharpness-aware minimization, which helps the model find a flattened minimum in loss landscapes, increasing the transferability. Based on this phenomenon and interpretation, we further propose a simple but effective approach for CDFSL to enhance the perturbation on attention maps by adding random registers on the semantic regions of image tokens, improving the effectiveness and efficiency of random registers. Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance. Codes and models are available at https://github.com/shuaiyi308/REAP.","We find prompt tuning, as a common way to train ViT, could consistently decrease the performance on target domains, while random noises can lead to an increase; we delve into this phenomenon for an interpretation, and propose a method based on it."
Poster,Ranked Entropy Minimization for Continual Test-Time Adaptation,https://ICML.cc//virtual/2025/poster/44208,"Jisu Han, Jaemin Na, Wonjun Hwang","Test-time adaptation aims to adapt to realistic environments in an online manner by learning during test time.  Entropy minimization has emerged as a principal strategy for test-time adaptation due to its efficiency and adaptability. Nevertheless, it remains underexplored in continual test-time adaptation, where stability is more important. We observe that the entropy minimization method often suffers from model collapse, where the model converges to predicting a single class for all images due to a trivial solution. We propose ranked entropy minimization to mitigate the stability problem of the entropy minimization method and extend its applicability to continuous scenarios. Our approach explicitly structures the prediction difficulty through a progressive masking strategy. Specifically, it gradually aligns the model's probability distributions across different levels of prediction difficulty while preserving the rank order of entropy. The proposed method is extensively evaluated across various benchmarks, demonstrating its effectiveness through empirical results.","Artificial intelligence is getting better at classifying images, but it can still make mistakes in unfamiliar conditions, like bad weather or poor image quality. Letting it learn during prediction can help, but this often leads to unstable behavior, where AI gives the same answer for every image.We developed a new approach where AI learns gradually by comparing easy and hard examples. For instance, it first sees a complete image and then learns to handle increasingly difficult versions by hiding important parts. The key is to maintain the correct order of prediction difficulty while learning in a stable and structured way.This idea is inspired by the ancient Greek story of Achilles and the tortoise. Rather than rushing, AI takes deliberate steps to reduce errors. Our method helps AI adapt more reliably in changing environments, making it useful for real-world applications such as self-driving cars and robotics."
Poster,Ranked from Within: Ranking Large Multimodal Models Without Labels,https://ICML.cc//virtual/2025/poster/45911,"Weijie Tu, Weijian Deng, Dylan Campbell, Yu Yao, Jiyang Zheng, Tom Gedeon, Tongliang Liu","Can the relative performance of a pre-trained large multimodal model (LMM) be predicted without access to labels? As LMMs proliferate, it becomes increasingly important to develop efficient ways to choose between them when faced with new data or tasks. The usual approach does the equivalent of giving the models an exam and marking them. We opt to avoid marking and the associated labor of determining the ground-truth answers. Instead, we explore other signals elicited and ascertain how well the models know their own limits, evaluating the effectiveness of these signals at unsupervised model ranking. We evaluate 47 state-of-the-art LMMs (e.g., LLaVA) across 9 visual question answering benchmarks, analyzing how well uncertainty-based metrics can predict relative model performance. Our findings show that uncertainty scores derived from softmax distributions provide a robust and consistent basis for ranking models across various tasks. This facilitates the ranking of LMMs on unlabeled data, providing a practical approach for selecting models for diverse target domains without requiring manual annotation.","Large AI models that understand both images and text are being used more and more, but it can be hard to know which one works best for a new task — especially when we don’t have labeled data to test them. Usually, people compare models by checking their answers against correct ones. But finding or creating those correct answers takes time and effort. And if we only have the questions — with no answers — there’s no easy way to tell which model to trust. Should we just pick one at random?In this work, we look at a different approach. Instead of checking answers, we see how confident each model is in its responses and use that to guess how well it might perform. We tested this idea using 47 of the latest models on nine different tasks. We found that a model’s confidence — measured in a specific way — can often predict how good it is, even without knowing the correct answers. This could help people quickly choose the right model for their needs without spending time labeling data."
Poster,Ranking with Multiple Oracles: From Weak to Strong Stochastic Transitivity,https://ICML.cc//virtual/2025/poster/44650,"Tao Jin, Yue Wu, Quanquan Gu, Farzad Farnoud","We study the problem of efficiently aggregating the preferences of items from multiple information sources (oracles) and infer the ranking under both the weak stochastic transitivity (WST) and the strong stochastic transitivity (SST) conditions. When the underlying preference model satisfies the WST condition, we propose an algorithm named RMO-WST, which has a bi-level design: at the higher level, it actively allocates comparison budgets to all undetermined pairs until the full ranking is recovered; at the lower level, it attempts to compare the pair of items and selects the more accurate oracles simultaneously. We prove that the sample complexity of RMO-WST is $ \tilde O( N\sum_{i=2}^{N}H_{\sigma^{-1}(i),{\sigma^{-1}(i-1)}} )$, where $N$ is the number of items to rank, $H$ is a problem-dependent hardness factor, and $\sigma^{-1}(i)$ represents the $i$-th best item. We also provide a tight lower bound that matches the upper bound of approximate ranking under the WST condition, answering a previously open problem. In addition, when the SST condition is satisfied, we propose an algorithm named RMO-SST, which can achieve an $\tilde{O}(\sum_{i=1}^{N} H_i \log(N))$ sample complexity. This outperforms the best-known sample complexity by a factor of $\log(N)$. The theoretical advantages of our algorithms are verified by empirical experiments in a simulated environment.","We looked at how to accurately figure out the ranking of items (like products or choices) based on opinions from several different sources (which could be people, systems, or tools). These sources might not always be perfect, and their preferences might conflict or vary in quality. By looking at the consistency of the response from various data sources we can estimate the quality of a particular one. Thus in return we can save effort by only consulting the higher quality ones to get the ranking."
Poster,Rank-One Modified Value Iteration,https://ICML.cc//virtual/2025/poster/46510,"Arman Sharifi Kolarijani, Tolga Ok, Peyman Mohajerin Esfahani, Mohamad Amin Sharifi Kolarijani","In this paper, we provide a novel algorithm for solving planning and learning problems of Markov decision processes. The proposed algorithm follows a policy iteration-type update by using a rank-one approximation of the transition probability matrix in the policy evaluation step. This rank-one approximation is closely related to the stationary distribution of the corresponding transition probability matrix, which is approximated using the power method. We provide theoretical guarantees for the convergence of the proposed algorithm to optimal (action-)value function with the same rate and computational complexity as the value iteration algorithm in the planning problem and as the Q-learning algorithm in the learning problem. Through our extensive numerical simulations, however, we show that the proposed algorithm consistently outperforms first-order algorithms and their accelerated versions for both planning and learning problems.","This paper introduces a new algorithm for solving Markov Decision Processes (MDPs), which are widely used models for decision-making in uncertain environments, such as robotics, operations research, and reinforcement learning. The algorithm is based on a policy iteration framework but departs from classical methods in how it performs policy evaluation.Specifically, it uses a rank-one approximation of the transition probability matrix during the policy evaluation step. This approximation captures the dominant behavior of the system and is closely tied to the stationary distribution of the policy-induced Markov chain. To compute this efficiently, the algorithm leverages the power method, a classical iterative technique for estimating dominant eigenvectors.Theoretically, we show that this algorithm converges to the optimal value or action-value function, with rates and computational complexity matching standard algorithms such as value iteration (for planning) and Q-Learning (for model-free learning).Empirically, however, the proposed method shows clear advantages in a range of simulations; it consistently outperforms both first-order methods and their accelerated variants. This suggests that the rank-one approximation not only simplifies the policy evaluation step but also enhances performance in practice, making it a promising alternative for both planning and learning tasks in MDPs."
