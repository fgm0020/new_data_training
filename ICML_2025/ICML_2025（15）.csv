type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Aligning Multimodal Representations through an Information Bottleneck,https://ICML.cc//virtual/2025/poster/43456,"Antonio Almudévar, Jose Miguel Hernandez-Lobato, Sameer Khurana, Ricard Marxer, Alfonso Ortega","Contrastive losses have been extensively used as a tool for multimodal representation learning. However, it has been empirically observed that their use is not effective to learn an aligned representation space.In this paper, we argue that this phenomenon is caused by the presence of modality-specific information in the representation space. Although some of the most widely used contrastive losses maximize the mutual information between representations of both modalities, they are not designed to remove the modality-specific information.We give a theoretical description of this problem through the lens of the Information Bottleneck Principle. We also empirically analyze how different hyperparameters affect the emergence of this phenomenon in a controlled experimental setup.Finally, we propose a regularization term in the loss function that is derived by means of a variational approximation and aims to increase the representational alignment.We analyze in a set of controlled experiments and real-world applications the advantages of including this regularization term.","Multimodal models learn from different types of data—such as images and text—by bringing their internal representations closer together using contrastive losses. While these losses are popular, they often fail to ensure that the representations from each modality (e.g., image and caption) are truly aligned.This paper investigates why this misalignment happens. The key insight is that contrastive losses, while encouraging shared information across modalities, do not remove information that is unique to each modality (like the color of a dog in an image but not mentioned in the caption). As a result, the learned representations can remain misaligned.We explain this issue using the Information Bottleneck Principle, a theoretical framework that helps understand how much relevant information a representation keeps. We also show, through controlled experiments, how certain hyperparameters influence this problem.To address it, we propose a new regularization term—based on a variational approximation—that helps remove irrelevant, modality-specific details from the representations. Our approach improves alignment both in synthetic setups and in real-world multimodal tasks."
Poster,Aligning Protein Conformation Ensemble Generation with Physical Feedback,https://ICML.cc//virtual/2025/poster/46147,"Jiarui Lu, Xiaoyin Chen, Stephen Lu, Aurelie Lozano, Vijil Chenthamarakshan, Payel Das, Jian Tang","Protein dynamics play a crucial role in protein biological functions and properties, and their traditional study typically relies on time-consuming molecular dynamics (MD) simulations conducted in silico. Recent advances in generative modeling, particularly denoising diffusion models, have enabled efficient accurate protein structure prediction and conformation sampling by learning distributions over crystallographic structures. However, effectively integrating physical supervision into these data-driven approaches remains challenging, as standard energy-based objectives often lead to intractable optimization. In this paper, we introduce Energy-based Alignment (EBA), a method that aligns generative models with feedback from physical models, efficiently calibrating them to appropriately balance conformational states based on their energy differences. Experimental results on the MD ensemble benchmark demonstrate that EBA achieves state-of-the-art performance in generating high-quality protein ensembles. By improving the physical plausibility of generated structures, our approach enhances model predictions and holds promise for applications in structural biology and drug discovery.","Proteins are dynamic molecules that constantly change their shape—a process fundamental to their biological functions. Understanding these shape-shifting behaviours is essential for biology and drug discovery. While scientists traditionally rely on physics-based simulations to study them, such simulations are extremely time-consuming, particularly for large proteins. AI models can predict protein structures more quickly, but they often lack a crucial component: the physical principles that govern which shapes are naturally possible. This limitation reduces these models' real-world scientific value.Our research introduces a new approach called Energy-based Alignment (EBA). This method enables AI models to generate protein structures that are both realistic and physically sound by incorporating feedback from physics-based energy functions during the training process, leading to more accurate predictions.When tested on benchmark dataset, our approach generates more accurate protein structure ensembles. By bridging the gap between AI and physics-based methods, we provide a faster and more reliable tool for studying protein behavior and supporting drug development."
Poster,Aligning Spoken Dialogue Models from User Interactions,https://ICML.cc//virtual/2025/poster/44228,"Anne Wu, Laurent Mazaré, Neil Zeghidour, Alexandre Défossez","We propose a novel preference alignment framework for improving spoken dialogue models on real-time conversations from user interactions. Current preference learning methods primarily focus on text-based language models, and are not directly suited to the complexities of real-time speech interactions, with richer dynamics (e.g. interruption, interjection) and no explicit segmentation between speaker turns.We create a large-scale dataset of more than 150,000 preference pairs from raw multi-turn speech conversations, annotated with AI feedback, to cover preferences over both linguistic content and temporal context variations. We leverage offline alignment methods to finetune a full-duplex autoregressive speech-to-speech model. Extensive experiments demonstrate that feedback on generic conversations can be consistently effective in improving spoken dialogue models to produce more factual, safer and more contextually aligned interactions. We deploy the finetuned model and conduct holistic human evaluations to assess the impact beyond single-turn conversations. Our findings shed light on the importance of a well-calibrated balance among various dynamics, crucial for natural real-time speech dialogue systems.","It's only very recently that we have started to see chatbots that are not pure ""walkie-talkies"" - ones that can have fully real-time conversations with humans. This leads to additional challenges: we want the models to behave well, providing factual information and responding appropriately to sensitive topics, but real-world real-time spoken conversations are very different and much messier than written ones, as people interrupt each other, overlap, and don't always take turns cleanly.We collected a large number of spontaneous conversations with Moshi, a voice chatbot that can listen and respond to users in real time, and used a second AI ""judge"" to automatically identify undesirable replies and propose better alternatives. We used both replies to create ""bad-versus-good example pairs"" that teach the chatbot to improve.  Using feedback on generic conversations consistently helps the model behave better, and human evaluations show improvements extend beyond individual responses to overall conversation quality. Since this pipeline runs offline on everyday interactions, we provide a practical way to make voice chatbots safer and better at natural conversations, paving the way for more reliable real-time voice systems."
Poster,"Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",https://ICML.cc//virtual/2025/poster/45083,"Yinhong Liu, Zhijiang Guo, Tianya Liang, Ehsan Shareghi, Ivan Vulić, Nigel Collier","Large Language Models (LLMs) are expected to be predictable and trustworthy to support reliable decision-making systems. Yet current LLMs often show inconsistencies in their judgments. In this work, we examine \textit{logical preference consistency} as a foundational requirement for building more dependable LLM systems, ensuring stable and coherent decision-making while minimizing erratic or contradictory outputs.To quantify the logical preference consistency, we propose a universal evaluation framework based on three fundamental properties: *transitivity*, *commutativity* and *negation invariance*.Through extensive experimentation across diverse LLMs, we demonstrate that these properties serve as strong indicators of judgment robustness.Furthermore, we introduce a data refinement and augmentation technique, REPAIR, that enhances logical consistency while maintaining alignment with human preferences. Finally, we show that improving consistency leads to better performance in LLM-driven logic-based algorithms, reinforcing stability and coherence in decision-making systems.","Large language models (like ChatGPT) are used in many systems that help people make decisions, so it’s important that these models give consistent and reliable answers. However, they often show contradictions in how they make choices.This study looks at a basic idea called ""logical preference consistency""—making sure the model's decisions follow simple and sensible rules, like not contradicting itself. The researchers created a way to test how consistent these models are using three simple rules of logic.They tested this on different models and found that the more consistently a model follows these rules, the more reliable its decisions are. They also developed a method called REPAIR to clean and improve the data the models learn from, which made the models even more consistent without losing their ability to agree with human values.In short, making language models more logically consistent helps them make better, more stable decisions."
Poster,A Likelihood Based Approach to Distribution Regression Using Conditional  Deep Generative Models,https://ICML.cc//virtual/2025/poster/46645,"Shivam Kumar, Yun Yang, Lizhen Lin","In this work, we explore the theoretical properties of conditional deep generative models under the statistical framework of distribution regression where the response variable lies in a high-dimensional ambient space but concentrates around a potentially lower-dimensional manifold. More specifically, we study the large-sample properties of a likelihood-based approach for estimating these models. Our results lead to the convergence rate of a sieve maximum likelihood estimator (MLE) for estimating the conditional distribution (and its devolved counterpart) of the response given predictors in the Hellinger (Wasserstein) metric. Our rates depend solely on the intrinsic dimension and smoothness of the true conditional distribution. These findings provide an explanation of why conditional deep generative models can circumvent the curse of dimensionality from the perspective of statistical foundations and demonstrate that they can learn a broader class of nearly singular conditional distributions. Our analysis also emphasizes the importance of introducing a small noise perturbation to the data when they are supported sufficiently close to a manifold. Finally, in our numerical studies, we demonstrate the effective implementation of the proposed approach using both synthetic and real-world datasets, which also provide complementary validation to our theoretical findings.","**Why does this matter?**Modern data often live in huge, messy spaces (think 4-K images or single-cell genomes) but actually hide on thin “surfaces” inside those spaces.  Learning *how the entire distribution* of outcomes changes with predictors in such settings is critical for tasks like probabilistic forecasting or conditional image generation, yet classical statistics choke on the sheer dimensionality.**What did we do?**We show—on paper, with proofs—that a familiar deep-learning workhorse, the conditional likelihood based deep neural network (for example: VAE, Normalizing Flows, and kindred generative models), can estimate those high-dimensional conditional distributions *as if* the data were low-dimensional.  The trick is a likelihood-based fit called a **sieve maximum-likelihood estimator** that grows in complexity with sample size.  We derive crisp formulas for its error: they depend only on the *intrinsic* dimension and smoothness of the true data manifold, not the giant ambient space.**Why is it cool?**1. It mathematically explains why deep generative models sidestep the curse of dimensionality.2. It covers noisy, near-manifold data—realistic for cameras and sensors.3. It even says how much “dither” noise to add when data are *too* clean.   Simulations (two-moons, ellipses, MNIST) line up with the theory, hinting at reliable uncertainty quantification for next-gen generative AI."
Poster,All-atom Diffusion Transformers: Unified generative modelling of molecules and materials,https://ICML.cc//virtual/2025/poster/46288,"Chaitanya Joshi, Xiang Fu, Yi-Lun Liao, Vahe Gharakhanyan, Benjamin Kurt Miller, Anuroop Sriram, Zachary Ulissi","Diffusion models are the standard toolkit for generative modelling of 3D atomic systems. However, for different types of atomic systems -- such as molecules and materials -- the generative processes are usually highly specific to the target system despite the underlying physics being the same. We introduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusion framework for jointly generating both periodic materials and non-periodic molecular systems using the same model: (1) An autoencoder maps a unified, all-atom representations of molecules and materials to a shared latent embedding space; and (2) A diffusion model is trained to generate new latent embeddings that the autoencoder can decode to sample new molecules or materials. Experiments on MP20, QM9 and GEOM-DRUGS datasets demonstrate that jointly trained ADiT generates realistic and valid molecules as well as materials, obtaining state-of-the-art results on par with molecule and crystal-specific models. ADiT uses standard Transformers with minimal inductive biases for both the autoencoder and diffusion model, resulting in significant speedups during training and inference compared to equivariant diffusion models. Scaling ADiT up to half a billion parameters predictably improves performance, representing a step towards broadly generalizable foundation models for generative chemistry. Open source code: https://github.com/facebookresearch/all-atom-diffusion-transformer","Molecules form the foundation of biological life and physical materials in our world. Designing new molecules has potential applications in clean energy, drug discovery, and other areas critical to human health and sustainability. However, current AI systems of generating molecular structures are customised to specific types of molecules. Our work, the All-atom Diffusion Transformer (ADiT), is the first general-purpose architecture for molecular modelling that can learn to generate a wide range molecular systems, from small organic molecules to inorganic periodic crystals."
Poster,All-atom inverse protein folding through discrete flow matching,https://ICML.cc//virtual/2025/poster/46259,"Kai Yi, Kiarash Jamali, Sjors Scheres","The recent breakthrough of AlphaFold3 in modeling complex biomolecular interactions, including those between proteins and ligands, nucleotides, or metal ions, creates new opportunities for protein design. In so-called inverse protein folding, the objective is to find a sequence of amino acids that adopts a target protein structure. Many inverse folding methods struggle to predict sequences for complexes that contain non-protein components, and perform poorly with complexes that adopt multiple structural states. To address these challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein folding), a generative model based on discrete flow-matching for designing protein sequences conditioned on all-atom structural contexts. ADFLIP progressively incorporates predicted amino acid side chains as structural context during sequence generation and enables the design of dynamic protein complexes through ensemble sampling across multiple structural states. Furthermore, ADFLIP implements training-free classifier guidance sampling, which allows the incorporation of arbitrary pre-trained models to optimise the designed sequence for desired protein properties. We evaluated the performance of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or metal ions, including dynamic complexes for which structure ensembles were determined by nuclear magnetic resonance (NMR). Our model achieves state-of-the-art performance in single-structure and multi-structure inverse folding tasks, demonstrating excellent potential for all-atom protein design. The code is available at https://github.com/ykiiiiii/ADFLIP .","Designing proteins with specific functions is a major challenge in biology, especially when they must work together with other molecules like DNA, small drugs, or metal ions. These interactions are often complex and flexible, and many existing tools struggle when protein structures can change shape or involve non-protein components.We developed ADFLIP, a new method for designing protein sequences based on detailed 3D atomic structures. Unlike earlier approaches, ADFLIP takes into account all atoms in the environment, such as side chains and interacting molecules, and builds the sequence step by step using a technique called discrete flow matching. It can also be guided by other models to optimize for specific goals, like improving stability or binding strength.We evaluated ADFLIP on challenging protein design tasks, including flexible structures captured by NMR experiments. It outperformed existing methods and offers a powerful new tool for designing realistic and functional proteins."
Poster,All-Purpose Mean Estimation over R: Optimal Sub-Gaussianity with Outlier Robustness and Low Moments Performance,https://ICML.cc//virtual/2025/poster/43938,"Jasper Lee, Walter McKelvie, Maoyuan Song, Paul Valiant","We consider the basic statistical challenge of designing an ""all-purpose"" mean estimation algorithm that is recommendable across a variety of settings and models.Recent work by [Lee and Valiant 2022] introduced the first 1-d mean estimator whose error in the standard finite-variance+i.i.d. setting is optimal even in its constant factors; experimental demonstration of its good performance was shown by [Gobet et al. 2022].Yet, unlike for classic (but not necessarily practical) estimators such as median-of-means and trimmed mean, this new algorithm lacked proven robustness guarantees in other settings, including the settings of adversarial data corruption and heavy-tailed distributions with infinite variance.Such robustness is important for practical use cases.This raises a research question: is it possible to have a mean estimator that is robust, *without* sacrificing provably optimal performance in the standard i.i.d. setting?In this work, we show that Lee and Valiant's estimator is in fact an ""all-purpose"" mean estimator by proving:(A) It is robust to an $\eta$-fraction of data corruption, even in the strong contamination model; it has optimal estimation error $O(\sigma\sqrt{\eta})$ for distributions with variance $\sigma^2$.(B) For distributions with finite $z^\text{th}$ moment, for $z \in (1,2)$, it has optimal estimation error, matching the lower bounds of [Devroye et al. 2016] up to constants.We further show (C) that outlier robustness for 1-d mean estimators in fact implies neighborhood optimality, a notion of beyond worst-case and distribution-dependent optimality recently introduced by [Dang et al. 2023].Previously, such an optimality guarantee was only known for median-of-means, but now it holds also for all estimators that are simultaneously *robust* and *sub-Gaussian*, including Lee and Valiant's, resolving a question raised by Dang et al.Lastly, we show (D) the asymptotic normality and efficiency of Lee and Valiant's estimator, as further evidence for its performance across many settings.","Suppose we have a large population of numbers (say, the individual income of people in a country), and we're trying to estimate the population mean via sampling. The conventional method is to take a bunch of samples, and just compute the average of the samples in the hopes that it is  a reasonable extrapolation. However, the sample average is very sensitive to extreme values, which might occur in our data set if we get unlucky in our sampling. Moreover, real-world data sampling can introduce errors, for example through mistakes in data entry or even through malicious meddling from bad actors. This paper mathematically proves that the recent Lee and Valiant mean estimator achieves essentially the smallest possible error in a wide variety of settings, including in badly-behaved populations where extreme values are relatively common, and also in settings where there is data corruption."
Poster,Almost Optimal Fully Dynamic $k$-Center Clustering with Recourse,https://ICML.cc//virtual/2025/poster/45262,"Sayan Bhattacharya, Martín Costa, Ermiya Farokhnejad, Silvio Lattanzi, Nikos Parotsidis","In this paper, we consider the *metric $k$-center* problem in the fully dynamic setting, where we are given a metric space $(V,d)$ evolving via a sequence of point insertions and deletions and our task is to maintain a subset $S \subseteq V$ of at most $k$ points that minimizes the objective $\max_{x \in V} \min_{y \in S}d(x, y)$. We want to design our algorithm so that we minimize its *approximation ratio*, *recourse* (the number of changes it makes to the solution $S$) and *update time* (the time it takes to handle an update). We give a simple algorithm for dynamic $k$-center that maintains a $O(1)$-approximate solution with $O(1)$ amortized recourse and $\tilde O(k)$ amortized update time, *obtaining near-optimal approximation, recourse and update time simultaneously*. We obtain our result by combining a variant of the dynamic $k$-center algorithm of Bateni et al. [SODA'23] with the dynamic sparsifier of Bhattacharya et al. [NeurIPS'23].","Clustering data is a fundamental task in unsupervised learning. In this task, we need to partition the elements of a dataset into different groups (called clusters) so that elements in the same group are similar to each other, and elements in different groups are not. In recent years, massive and rapidly changing datasets have become increasingly common. In our paper, we design an algorithm for maintaining a clustering of such a dataset, showing how to efficiently maintain a high-quality and stable clustering as the dataset changes over time."
Poster,ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling,https://ICML.cc//virtual/2025/poster/45316,"Dongchao Yang, Songxiang Liu, Haohan Guo, Jiankun Zhao, Yuanyuan Wang, Helin Wang, Zeqian Ju, Xubo Liu, Xueyuan Chen, Xu Tan, Xixin Wu, Helen M Meng","Recent advancements in audio language models have underscored the pivotal role of audio tokenization, which converts audio signals into discrete tokens, thereby facilitating the application of language model architectures to the audio domain. In this study, we introduce ALMTokenizer, a novel low-bitrate and semantically rich audio codec tokenizer for audio language models. Prior methods, such as Encodec, typically encode individual audio frames into discrete tokens without considering the use of context information across frames. Unlike these methods, we introduce a novel query-based compression strategy to capture holistic information with a set of learnable query tokens by explicitly modeling the context information across frames. This design not only enables the codec model to capture more semantic information but also encodes the audio signal with fewer token sequences. Additionally, to enhance the semantic information in audio codec models, we introduce the following: (1) A masked autoencoder (MAE) loss, (2) Vector quantization based on semantic priors, and (3) An autoregressive (AR) prediction loss. As a result, ALMTokenizer achieves competitive reconstruction performance relative to state-of-the-art approaches while operating at a lower bitrate. Within the same audio language model framework, ALMTokenizer outperforms previous tokenizers in audio understanding and generation tasks.\footnote{http://dongchaoyang.top/ALMTokenizer/}","In recent years, we've seen remarkable advances in artificial intelligence models that understand and generate text. But what if these same models could also understand and generate audio—such as music, speech, and everyday sounds? To make this possible, a poential solution is that audio first be transformed into a format that language model can work with: small, discrete units called tokens, similar to words in a sentence. This process is called audio tokenization.Our work introduces ALMTokenizer, a new tool that turns audio into compact, information-rich tokens. It is designed to help audio language models (similar to ChatGPT but for audio) perform better across tasks like speech recognition, text-to-speech synthesis, sound effect generation, and music captioning.Unlike previous methods that compress audio frame-by-frame, ALMTokenizer takes a smarter approach: it looks at larger chunks of audio and learns to summarize the most important information using a small number of trainable queries. This not only saves storage space (low bitrate) but also preserves meaning and context (semantic richness).We also design new ways to teach our model to focus on meaningful information in audio. These include training it to predict missing parts of audio and optimizing it to better represent the sound structure that language models rely on. As a result, ALMTokenizer delivers better audio quality at lower bitrate and helps AI models understand and generate sound more accurately.In short, ALMTokenizer moves us closer to general-purpose AI that can handle audio as well as it handles text—efficiently, meaningfully, and with high quality."
