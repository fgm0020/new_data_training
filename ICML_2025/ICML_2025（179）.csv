type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,MARS: Unleashing the Power of Variance Reduction for Training Large Models,https://ICML.cc//virtual/2025/poster/45479,"Huizhuo Yuan, Yifeng Liu, Shuang Wu, zhou Xun, Quanquan Gu","Training deep neural networks--and more recently, large models--demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained a less favored approach in modern AI. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose a unified optimization framework, MARS (**M**ake v**A**riance **R**eduction **S**hine), which reconciles preconditioned gradient methods with variance reduction via a scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of MARS that leverage preconditioned gradient updates based on AdamW, Lion, and Shampoo, respectively. We also draw a connection between our algorithms and existing optimizers. Experimental results on training GPT-2 models indicate that MARS consistently outperforms AdamW by a large margin.","Training large models requires a lot of computing power and time. Scientists use specialized algorithms called optimizers to make this learning process more efficient. While popular adaptive learning optimizers like AdamW are helpful in adaptively adjusting step size of learning, they aren't perfect. Another set of techniques, known as variance reduction, aims to make the AI's learning steps more consistent and less erratic, but these haven't worked well for today's giant AI models.We propose a new framework called MARS (**M**ake v**A**riance **R**eduction **S**hine) that offers a breakthrough. MARS cleverly brings together the benefits of established optimizers (like AdamW, Lion, and Shampoo) with a new, effective way to reduce variance. It uses scaled stochastic recursive momentum to balance the variance reduction with adaptive learning. This helps the AI learn more efficiently with less variance. In tests training GPT-2, a well-known large language model, MARS performed significantly better than the widely used AdamW optimizer. This new approach could lead to faster and more efficient training for the next generation of large AI systems."
Poster,MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems,https://ICML.cc//virtual/2025/poster/46543,"Rui Ye, shuo tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, Jing Shao","LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks.However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs.In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS.To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs.Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high effectiveness, efficiency and strong generalization ability.The codes are released at \url{https://github.com/rui-ye/MAS-GPT}.","Building powerful AI systems with multiple collaborating agents (like a team of experts) is exciting, but it’s currently too difficult and expensive for most people to use. Each new task requires manually setting up the team or making many costly AI calls, making these systems inflexible and slow.To fix this, we created MAS-GPT, an AI that learns to automatically design these multi-agent teams. We teach MAS-GPT by showing it many examples of problems and the best team setups to solve them. By representing these teams as executable code, MAS-GPT can now generate a custom team for any new problem in a single step, just like asking ChatGPT.This means building advanced AI systems becomes fast, cheap, and adaptable. Our experiments show MAS-GPT consistently outperforms existing methods across various tasks and different AIs, even boosting complex problem-solving abilities. MAS-GPT paves the way for wider use of powerful multi-agent AI, making sophisticated AI accessible to everyone."
Poster,Masked Autoencoders Are Effective Tokenizers for Diffusion Models,https://ICML.cc//virtual/2025/poster/44606,"Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj","Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76× faster training and 31× higher inference throughput for 512×512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models will be released.","Diffusion models are a leading approach in AI for generating realistic images. To make them efficient, these models often rely on compressing images into a simplified form—called a latent space—before generation begins. This compression is handled by a component known as a tokenizer. But what makes a tokenizer good for generating high-quality images?This paper introduces MAETok, a new tokenizer based on masked autoencoders (MAE), which learn by hiding parts of the input and trying to reconstruct them. The authors show that tokenizers trained in this way produce more discriminative latent spaces—that is, they better capture meaningful differences between image features. This leads to more effective training and better results when used in diffusion models.Through both theoretical analysis and practical experiments, the authors find that having a well-structured latent space is more important than using complex designs like variational autoencoders (VAE). MAETok achieves state-of-the-art image generation on standard benchmarks while being significantly faster and more efficient, using just 128 tokens—far fewer than most previous methods.This work provides new insights into how latent representations impact generative performance and offers a practical, scalable solution for high-quality image synthesis."
Poster,Masked Generative Nested Transformers with Decode Time Scaling,https://ICML.cc//virtual/2025/poster/44435,"Sahil Goyal, Debapriya Tula, Gagan Jain, Pradeep Shenoy, Prateek Jain, Sujoy Paul","Recent advances in visual generation have made significant strides in producing content of exceptional quality. However, most methods suffer from a fundamental problem - a bottleneck of inference computational efficiency. Most of these algorithms involve multiple passes over a transformer model to generate tokens or denoise inputs. However, the model size is kept consistent throughout all iterations, which makes it computationally expensive. In this work, we aim to address this issue primarily through two key ideas - (a) not all parts of the generation process need equal compute, and we design a decode time model scaling schedule to utilize compute effectively, and (b) we can cache and reuse some of the intermediate computation. Combining these two ideas leads to using smaller models to process more tokens while large models process fewer tokens. These different-sized models do not increase the parameter size, as they share parameters. We rigorously experiment with ImageNet256$\times$256 , UCF101, and Kinetics600 to showcase the efficacy of the proposed method for image/video generation and frame prediction. Our experiments show that with almost $3\times$ less compute than baseline, our model obtains competitive performance.","Recently, creating realistic images or videos using computers has seen amazing progress. But there's a big hurdle, these systems are often slow and require a lot of computational power, especially when they're actually creating the content (what we call ""inference""). Typically, they follow a multi-step approach, starting with rough outlines and gradually adding fine details. However, they use the same amount of computing power at every step, even when it's not needed. In our work, we show that by adjusting the computational power based on how complex each step is, we can speed up the process by up to $3\times$ without sacrificing quality. We also reuse the relevant computations from the past steps to decrease the computational power required in the current generation step. To show efficacy of our approach, we show performance of our method on publicly available datasets like ImageNet for image generation and UCF/Kinetics for video generation."
Poster,Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More,https://ICML.cc//virtual/2025/poster/46344,"Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu","Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77% percentage points. Our analysis indicates that MEAP’s effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model’s focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models. Code has been submitted.","Modern large language models (LLMs), like GPT, LLaMa, and DeepSeek, are trained by learning to predict the next word in a sentence. This method, called Next-Token Prediction (NTP), helps models generate coherent text but struggles with finding important information in long documents.This paper introduces a new training method called Mask-Enhanced Autoregressive Prediction (MEAP). MEAP adds a twist to the standard training process: it randomly hides (or masks) some words in the input, forcing the model to focus more on the remaining visible words when learning. Surprisingly, this small change leads to much better understanding and recall of important facts, especially when dealing with long or complex text.Unlike older methods that use masking (like BERT), MEAP does not require more complicated architectures or extra computing power. It simply combines the strengths of two training methods—masking and next-token prediction—into one seamless process.Experiments show that models trained with MEAP:- Perform better at retrieving key information (e.g., finding a “needle in a haystack” within long documents).- Handle longer contexts more effectively.- Make fewer factual errors when summarizing.- Maintain or improve general reasoning skills compared to standard training.Because MEAP is simple to implement, works with existing model architectures, and improves both accuracy and efficiency, it offers a promising new direction for training the next generation of LLMs."
Poster,MaskTwins: Dual-form Complementary Masking for Domain-Adaptive Image Segmentation,https://ICML.cc//virtual/2025/poster/46243,"Jiawen Wang, Yinda Chen, Xiaoyu Liu, che liu, Dong Liu, Jianqing Gao, Zhiwei Xiong","Recent works have correlated Masked Image Modeling (MIM) with consistency regularization in Unsupervised Domain Adaptation (UDA). However, they merely treat masking as a special form of deformation on the input images and neglect the theoretical analysis, which leads to a superficial understanding of masked reconstruction and insufficient exploitation of its potential in enhancing feature extraction and representation learning. In this paper, we reframe masked reconstruction as a sparse signal reconstruction problem and theoretically prove that the dual form of complementary masks possesses superior capabilities in extracting domain-agnostic image features. Based on this compelling insight, we propose MaskTwins, a simple yet effective UDA framework that integrates masked reconstruction directly into the main training pipeline. MaskTwins uncovers intrinsic structural patterns that persist across disparate domains by enforcing consistency between predictions of images masked in complementary ways, enabling domain generalization in an end-to-end manner. Extensive experiments verify the superiority of MaskTwins over baseline methods in natural and biological image segmentation.These results demonstrate the significant advantages of MaskTwins in extracting domain-invariant features without the need for separate pre-training, offering a new paradigm for domain-adaptive segmentation. The source code is available at https://github.com/jwwang0421/masktwins.","Masking is to occlude parts of an image. By increasing the learning difficulty, this technique enables models to better understand and analyze images. However, many machine learning scientists traditionally treat masking merely as a special form of data augmentation and overlook its theoretical analysis.We aim to further explore the potential of masked reconstruction in enhancing feature extraction and representation learning and we focus on complementary masks, defined as a pair of non-overlapping masks that together cover the entire image without redundancy. Our study theoretically demonstrates that the complementary masking strategy enhances model robustness. Based on this strategy, we propose MaskTwins, a simple yet effective training framework for domain adaptation tasks, enabling models trained in one scenario to learn essential features that generalize to another different scenario.Our findings provide a new paradigm for domain-adaptive segmentation and offer theoretical insights for masked image modeling."
Poster,Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding,https://ICML.cc//virtual/2025/poster/46637,"Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang","Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show for the first time that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs. Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (i.e., knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model’s parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE) and it appears since very first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The code is available at https://github.com/MingyuJ666/Rope_with_LLM.","Problem:Large Language Models (LLMs) like ChatGPT understand context through attention mechanisms, but it remains unclear why certain attention values become extremely large and are highly concentrated in specific dimensions.Solution:We discovered, for the first time, that these concentrated massive values exclusively occur in the query (Q) and key (K) vectors of the attention module, not in the value (V) vectors. Through extensive experiments and quantization analysis, we uncovered their origins and mechanisms.Impact:We demonstrate that these massive values are critical for models to interpret contextual knowledge (information from the current input) rather than retrieving parametric memory (information stored within the model). This provides new insights into optimizing attention mechanisms in LLMs."
Poster,MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models,https://ICML.cc//virtual/2025/poster/45257,"Jiazheng Li, Lu Yu, Qing Cui, Zhiqiang Zhang, JUN ZHOU, Yanfang Ye, Chuxu Zhang","High-quality data plays a critical role in the pretraining and fine-tuning of large language models (LLMs), even determining their performance ceiling to some degree. Consequently, numerous data selection methods have been proposed to identify subsets of data that can effectively and efficiently enhance model performance. However, most of these methods focus on general data selection and tend to overlook the specific nuances of domain-related data. In this paper, we introduce MASS, a Mathematical data Selection framework using the Skill graph for pretraining LLMs in the mathematical reasoning domain. By taking into account the unique characteristics of mathematics and reasoning, we construct a skill graph that captures the mathematical skills and their interrelations from a reference dataset. This skill graph guides us in assigning quality scores to the target dataset, enabling us to select the top-ranked subset which is further used to pretrain LLMs. Experimental results demonstrate the efficiency and effectiveness of MASS across different model sizes (1B and 7B) and pretraining datasets (web data and synthetic data). Specifically, in terms of efficiency, models trained on subsets selected by MASS can achieve similar performance to models trained on the original datasets, with a significant reduction in the number of trained tokens - ranging from 50\% to 70\% fewer tokens. In terms of effectiveness, when trained on the same amount of tokens, models trained on the data selected by MASS outperform those trained on the original datasets by 3.3\% to 5.9\%. These results underscore the potential of MASS to improve both the efficiency and effectiveness of pretraining LLMs.","General-purpose language models (LMs) are designed as generalists rather than specialists, meaning they lack deep expertise in specific domains such as mathematics. To adapt these models for domain-specific tasks (e.g., solving mathematical problems), conventional approaches require continued pretraining on vast amounts of domain-related text—a process that is both computationally expensive and time-consuming. A key inefficiency lies in the sheer scale of the training corpus, which often contains redundant or low-value samples. To address this inefficiency, we propose a novel method that significantly reduces the required training data and time (by over 50%) while achieving comparable or superior model performance. Our core insight is that existing pretraining datasets contain substantial redundancy, including repetitive or irrelevant samples (e.g., trivial math operations or unrelated skills). To systematically identify high-quality data, we introduce a skill graph that quantifies the importance of mathematical skills and their interdependencies. This graph serves as a guide for selecting the most informative subset of training data, enabling efficient domain adaptation with minimal computational overhead."
Poster,Mastering Board Games by External and Internal Planning with Language Models,https://ICML.cc//virtual/2025/poster/45648,"John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, Tom Zahavy, Petar Veličković, Laurel Prince, Satinder Singh, Eric Malmi, Nenad Tomasev","Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In *external search*, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in *internal search*, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.","1. Large Language Models (LLMs) demonstrate impressive performance across various tasks that require complex reasoning. Yet, they still struggle to play board games as simple as tic-tac-toe. 2. We developed an LLM that can play different board games, reaching Grandmaster-level chess performance. We investigated different planning strategies that enable the LLM to improve its performance, the more “thinking time” we provide to the model. 3. In the future, similar planning strategies can unlock strong performance improvements in LLMs applied to other reasoning problems."
Poster,Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer,https://ICML.cc//virtual/2025/poster/43936,"Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao","Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper,  we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model’s parameter scalability.  Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.","Sequence modeling–based offline reinforcement learning holds great promise, but current approaches struggle to scale to large-scale multi-task reinforcement learning (MTRL). In this work, we first revisit the limitations of such methods in terms of scalability with respect to both the number of tasks and model parameters. To address these challenges, we propose M3DT, a novel framework that enables task scalability by unlocking parameter scalability. M3DT leverages a mixture-of-experts (MoE) architecture to achieve efficient parameter decoupling and expansion, employs task grouping to reduce the number of tasks handled by each expert and ease the learning burden, and adopts a three-stage training paradigm for targeted optimization. Experimental results demonstrate that our method achieves strong performance, paving the way for future research in sequence modeling–based offline RL and MTRL."
