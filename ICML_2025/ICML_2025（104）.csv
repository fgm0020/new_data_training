type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms,https://ICML.cc//virtual/2025/poster/44259,"Rickard K.A. Karlsson, Jesse H. Krijthe","A major challenge in estimating treatment effects in observational studies is the reliance on untestable conditions such as the assumption of no unmeasured confounding. In this work, we propose an algorithm that can falsify the assumption of no unmeasured confounding in a setting with observational data from multiple heterogeneous sources, which we refer to as environments. Our proposed falsification strategy leverages a key observation that unmeasured confounding can cause observed causal mechanisms to appear dependent. Building on this observation, we develop a novel two-stage procedure that detects these dependencies with high statistical power while controlling false positives. The algorithm does not require access to randomized data and, in contrast to other falsification approaches, functions even under transportability violations when the environment has a direct effect on the outcome of interest. To showcase the practical relevance of our approach, we show that our method is able to efficiently detect confounding on both simulated and semi-synthetic data.","Estimating how effective an intervention is—such as giving a drug to a patient or implementing a new public policy—using real-world data is incredibly important, but also challenging. To make these estimates, researchers often assume that all relevant factors influencing both who receives the treatment and what happens afterward have been measured. If these factors, known as confounders, are missing, the estimated effects can be misleading; potentially leading to unsafe or ineffective recommendations. We introduce a new way to assess whether all confounders have been accounted for, in settings where data is collected from multiple groups, such as different hospitals, schools, or regions. Our approach builds on a principle from causal reasoning: in a well-functioning system, different parts of the cause-and-effect process should operate independently. If they appear unexpectedly linked, it may indicate the presence of unmeasured confounding. We propose a method that uses this idea to detect such unexpected links to assess whether all relevant confounders have been measured. This allows researchers to evaluate the reliability of their treatment effect estimates and avoid drawing incorrect conclusions from real-world data."
Poster,"Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization",https://ICML.cc//virtual/2025/poster/44296,"Shiyu Wang, Mariam Avagyan, Yihan Shen, Arnaud Lamy, Tingran Wang, Szabolcs Marka, Zsuzsanna Marka, John Wright","Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown manifold $\mathcal M \in \mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of  ""learning-to-denoise"" as  *""learning-to-optimize""*. We have two technical innovations: (i) *online learning* methods which learn to optimize over the manifold of clean signals using only noisy data, effectively ""growing"" an optimizer one sample at a time. (ii) *mixed-order* methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search.","Much progress in machine learning has come from making models bigger and feeding them more data using powerful computers. While this approach works, it uses a lot of energy and isn't sustainable in the long run. We believe there’s a better way: instead of just scaling up, we should focus on using the structure hidden in the data and the problem itself to build more efficient models.Our work focuses on denoising -- the process of removing noise from signals like images, videos, or scientific measurements. Denoising is a core building block in many signal generation and reconstruction models. However, most existing models typically use generic learners to approximate the denoising function without incorporating the inherent structure of the data or the problem into the architecture design.In reality, most data -- whether it's from medical imaging, astronomy, or neuroscience -- looks complex but actually follows simpler, low-dimensional patterns. In this work, we argue that we can use this structure to develop more computationally efficient denoisers, by reinterpreting denoising as an optimization problem. This leads to a provable method that is more efficient than standard approaches like nearest neighbor search and generic models such as autoencoders, while achieving similar performance. This suggests that this approach could be used as a foundational building block in broader learning architectures -- making them more efficient and transparent."
Poster,Fast and Low-Cost Genomic Foundation Models via Outlier Removal,https://ICML.cc//virtual/2025/poster/44032,"Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry Yao-Chieh Hu, Han Liu","To address the challenge of scarce computational resources in genomic modeling, we introduce GERM, a genomic foundation model optimized for accessibility and adaptability. GERM improves upon models like DNABERT-2 by eliminating outliers that hinder low-rank adaptation and post-training quantization, enhancing both efficiency and robustness. We replace the vanilla attention layer with an outlier-free mechanism inspired by associative memory models. By removing outliers during both pre-training and fine-tuning, this approach accelerates adaptation, reduces computational costs, and enhances quantization robustness within acceptable loss margins. Additionally, we propose GERM-T, a strategy that employs small-step continual learning within the outlier-free framework, leveraging original checkpoints to avoid retraining from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline model. It also reduces average kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading methods, GERM consistently delivers superior performance, offering a practical solution for genomic modeling in resource-constrained settings.","Large AI models for genomic analysis often require significant computing resources, making them difficult to use in resource-constrained environments. To address this, we introduce GERM, a genomic foundation model optimized for accessibility and adaptability. GERM improves upon models like DNABERT-2 by removing outliers that impede low-rank adaptation and post-training quantization, thereby improving efficiency and robustness. It replaces the standard attention layer with an outlier-free mechanism inspired by associative memory models, enabling faster adaptation, lower computational cost, and improved quantization robustness with minimal performance loss. We also propose GERM-T, a continual learning strategy that supports small-step updates using existing checkpoints, avoiding the need for full retraining.Our paper reveals that GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline. Moreover, it effectively suppresses outlier indicators, achieving a 92.14% reduction in average kurtosis and an 82.77% decrease in the maximum infinity norm. These results make GERM and GERM-T practical tools for genomic modeling in resource-constrained settings."
Poster,Fast and Provable Algorithms for Sparse PCA with Improved Sample Complexity,https://ICML.cc//virtual/2025/poster/44023,"Jian-Feng Cai, Zhuozhi XIAN, Jiaxi Ying","We explore the single-spiked covariance model within the context of sparse principal component analysis (PCA), which aims to recover a sparse unit vector from noisy samples. From an information-theoretic perspective, $O(k \log p)$ observations are sufficient to recover a $k$-sparse $p$-dimensional vector $\mathbf{v}$. However, existing polynomial-time methods require at least $O(k^2)$ samples for successful recovery, highlighting a significant gap in sample efficiency. To bridge this gap, we introduce a novel thresholding-based algorithm that requires only $\Omega(k \log p)$ samples, provided the signal strength $\lambda = \Omega(||\mathbf{v}||_\infty^{-1})$. We also propose a two-stage nonconvex algorithm that further enhances estimation performance. This approach integrates our thresholding algorithm with truncated power iteration, achieving the minimax optimal rate of statistical error under the desired sample complexity. Numerical experiments validate the superior performance of our algorithms in terms of estimation accuracy and computational efficiency.","We explore how many samples are required to recover an unknown sparse signal from noisy data in a fundamental high-dimensional statistics problem called sparse principal component analysis (PCA). The number of samples required by existing polynomial-time algorithms is greater than the theoretical minimum.We design two algorithms to bridge this gap. The first, a thresholding-based method, achieves the theoretical sample limit when the signal is strong enough relative to its largest component. The second combines the first with iterative refinement, enhancing estimation performance.Our work partly resolves a basic challenge in sparse PCA. Experiments confirm that our methods outperform existing approaches, achieving both speed and precision."
Poster,Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments,https://ICML.cc//virtual/2025/poster/46425,"Yun Qu, Cheems Wang, Yixiu Mao, Yiqin Lv, Xiangyang Ji","Task robust adaptation is a long-standing pursuit in sequential decision-making.Some risk-averse strategies, e.g., the conditional value-at-risk principle, are incorporated in domain randomization or meta reinforcement learning to prioritize difficult tasks in optimization, which demand costly intensive evaluations.The efficiency issue prompts the development of robust active task sampling to train adaptive policies, where risk-predictive models can surrogate policy evaluation. This work characterizes robust active task sampling as a secret Markov decision process, posits theoretical and practical insights, and constitutes robustness concepts in risk-averse scenarios.Importantly, we propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making.Extensive experiments show that PDTS unlocks the potential of robust active task sampling, significantly improves the zero-shot and few-shot adaptation robustness in challenging tasks, and even accelerates the learning process under certain scenarios.","Task robust adaptation is a long-standing pursuit in sequential decision-making.Some risk-averse strategies, e.g., the conditional value-at-risk principle, are incorporated in domain randomization or meta reinforcement learning to prioritize difficult tasks in optimization, which demand costly intensive evaluations.The efficiency issue prompts the development of robust active task sampling to train adaptive policies, where risk-predictive models can surrogate policy evaluation. This work characterizes robust active task sampling as a secret Markov decision process, posits theoretical and practical insights, and constitutes robustness concepts in risk-averse scenarios.Importantly, we propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making.Extensive experiments show that PDTS unlocks the potential of robust active task sampling, significantly improves the zero-shot and few-shot adaptation robustness in challenging tasks, and even accelerates the learning process under certain scenarios."
Poster,FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks,https://ICML.cc//virtual/2025/poster/44251,"Laines Schmalwasser, Niklas Penzel, Joachim Denzler, Julia Niebling","Concepts such as objects, patterns, and shapes are how humans understand the world. Building on this intuition, concept-based explainability methods aim to study representations learned by deep neural networks in relation to human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an important tool and can identify whether a model learned a concept or not. However, the computational cost and time requirements of existing CAV computation pose a significant challenge, particularly in large-scale, high-dimensional architectures. To address this limitation, we introduce FastCAV, a novel approach that accelerates the extraction of CAVs by up to 63.6× (on average 46.4×). We provide a theoretical foundation for our approach and give concrete assumptions under which it is equivalent to established SVM-based methods. Our empirical results demonstrate that CAVs calculated with FastCAV maintain similar performance while being more efficient and stable. In downstream applications, i.e., concept-based explanation methods, we show that FastCAV can act as a replacement leading to equivalent insights. Hence, our approach enables previously infeasible investigations of deep models, which we demonstrate by tracking the evolution of concepts during model training.","To understand how complex AI systems learn, researchers try to see if they recognize human-understandable concepts (like ""stripes"" or ""zigzagged""). A method called Concept Activation Vectors (CAVs) is used to identify if a model has learned such concepts. However, calculating CAVs for modern, large AI models is often too slow and computationally expensive, limiting their practical use.We introduce FastCAV, a new approach to compute these CAVs much more quickly, on average 46 times faster. We provide theoretical support and demonstrate through experiments that FastCAV produces results of similar quality to established approaches, but with significantly improved efficiency and stability.This significant speed-up makes concept-based explanations more practical for researchers. FastCAV enables investigations that were previously too costly or time-consuming, such as tracking how an AI develops an understanding of different concepts throughout its training process. This allows for a deeper understanding of how complex AI models function."
Poster,Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking Calculation,https://ICML.cc//virtual/2025/poster/44986,"Zecheng Hao, Qichao Ma, Kang Chen, Yi Zhang, Zhaofei Yu, Tiejun Huang","Spiking Neural Network (SNN), as a brain-inspired and energy-efficient network, is currently facing the pivotal challenge of exploring a suitable and efficient learning framework. The predominant training methodologies, namely Spatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered by substantial training overhead or pronounced inference latency, which impedes the advancement of SNNs in scaling to larger networks and navigating intricate application domains. In this work, we propose a novel parallel conversion learning framework, which establishes a mathematical mapping relationship between each time-step of the parallel spiking neurons and the cumulative spike firing rate. We theoretically validate the lossless and sorting properties of the conversion process, as well as pointing out the optimal shifting distance for each step. Furthermore, by integrating the above framework with the distribution-aware error calibration technique, we can achieve efficient conversion towards more general activation functions or training-free circumstance. Extensive experiments have confirmed the significant performance advantages of our method for various conversion cases under ultra-low time latency. To our best knowledge, this is the first work which jointly utilizes parallel spiking calculation and ANN-SNN Conversion, providing a highly promising approach for SNN supervised training. Code is available at https://github.com/hzc1208/Parallel_Conversion.","This work tackles a key challenge in a type of energy-efficient and brain-inspired artificial neural network called Spiking Neural Network (SNN). SNNs are promising for future AI due to their low power usage, but they're hard to train efficiently. The current methods either consume too much computational overhead to train or are slow during actual inference. To solve this, we create a new conversion method that connects how SNNs behave over time with how traditional artificial neural networks work. Our approach allows for faster and more accurate results, even when using limited computing resources. To our best knowledge, this is the first work that successfully combines fast parallel computation with efficient training techniques, paving the way for more practical and powerful SNN-based AI systems."
Poster,Faster Approximation Algorithms for k-Center via Data Reduction,https://ICML.cc//virtual/2025/poster/44656,"Arnold Filtser, Shaofeng Jiang, Yi Li, Anurag Murty Naredla, Ioannis Psarros, Qiaoyuan Yang, Qin Zhang","We study efficient algorithms for the Euclidean $k$-Center problem, focusing on the regime of large $k$. We take the approach of data reduction by considering $\alpha$-coreset, which is a small subset $S$ of the dataset $P$ such that any $\beta$-approximation on $S$ is an $(\alpha + \beta)$-approximation on $P$. We give efficient algorithms to construct coresets whose size is $k \cdot o(n)$, which immediately speeds up existing approximation algorithms. Notably, we obtain a near-linear time $O(1)$-approximation when $k = n^c$ for any $0 < c < 1$. We validate the performance of our coresets on real-world datasets with large $k$, and we observe that the coreset speeds up the well-known Gonzalez algorithm by up to $4$ times, while still achieving similar clustering cost. Technically, one of our coreset results is based on a new efficient construction of consistent hashing with competitive parameters. This general tool may be of independent interest for algorithm design in high dimensional Euclidean spaces.","$k$-Center clustering is a fundamental data clustering problem. In general, this problem aims to find a ""best"" partition of a dataset into $k$ parts, such that each part has a smallest size (in distance).We aim to devise efficient algorithms for $k$-Center, with a focus on high dimensional data and general parameter $k$, which is a parameter regime relevant to many popular large scale applications. We give the first near-linear time algorithm whose solution is constant factor away from the optimal, for any $k = o(n)$. Our algorithm also demonstrates promising performance on real datasets. Our result is obtained via a novel data reduction method for $k$-Center.This work not only advances the theoretical front of $k$-Center, but also makes impact on the practical side to enable more efficient clustering analysis on high dimensional big data."
Poster,Faster Global Minimum Cut with Predictions,https://ICML.cc//virtual/2025/poster/45877,"Helia Niaparast, Benjamin Moseley, Karan Singh","Global minimum cut is a fundamental combinatorial optimization problem with wide-ranging applications. Often in practice, these problems are solved repeatedly on families of similar or related instances. However, the de facto algorithmic approach is to solve each instance of the problem from scratch discarding information from prior instances.  In this paper, we consider how predictions informed by prior instances can be used to warm-start practical minimum cut algorithms. The paper considers the widely used Karger's algorithm and its counterpart, the Karger-Stein algorithm. Given good predictions, we show these algorithms become near-linear time and have robust performance to erroneous predictions.  Both of these algorithms are randomized edge-contraction algorithms. Our natural idea is to probabilistically prioritize the contraction of edges that are unlikely to be in the minimum cut.","Solving Network Problems Faster by Learning from the PastMany real-world problems involve breaking a network, like a transportation map or a social network, into two parts while cutting as few connections as possible. This kind of task arises in areas like data analysis, communication systems, and machine learning.Often, similar versions of the problem are solved over and over. But most methods start from scratch each time, without using what was learned from earlier problems.In this work, we explore how to speed things up by using predictions based on past experience. We focus on two well-known methods that solve the problem by repeatedly merging parts of the network at random until only two groups remain. Normally, this merging process is completely random, which helps the algorithms to find the best solution most of the time after a sufficient number of attempts, but it can also be slow.We show that by guiding the process using predictions and favoring merges that are likely to be safe based on earlier problems, we can solve new ones much faster. Even when the predictions are not perfect, the performance stays strong.This makes it more practical to handle repeated or related network problems quickly, which is common in real-world applications."
Poster,Faster Rates for Private Adversarial Bandits,https://ICML.cc//virtual/2025/poster/44182,"Hilal Asi, Vinod Raman, Kunal Talwar","We design new differentially private algorithms for the problems of adversarial bandits and bandits with expert advice. For adversarial bandits, we give a simple and efficient conversion of any non-private bandit algorithm to a private bandit algorithm. Instantiating our conversion with existing non-private bandit algorithms gives a regret upper bound of $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all $\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first known separation between central and local differential privacy for this problem. For bandits with expert advice, we give the first differentially private algorithms, with expected regret $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right), O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of actions and experts respectively. These rates allow us to get sublinear regret for different combinations of small and large $K, N$ and $\epsilon.$","In many applications—like online advertising, medical trials, or recommendation systems—machine learning algorithms must sequentially make decisions over time while protecting sensitive user data. This paper tackles this challenge in a difficult setting called adversarial bandits, where the environment can behave unpredictably and only limited feedback is available at each time step. We design new algorithms that are both differentially private (meaning they rigorously protect individual data) and competitive (meaning their performance remains strong over time). We show how to transform any existing bandit algorithm into a private one with improved performance guarantees, particularly in regimes with high privacy requirements. Importantly, our methods achieve better outcomes than all previous approaches and uncover a fundamental gap between two models of privacy (central and local). Using these techniques, we also provide the first private algorithms for a related problem called adversarial bandits with expert advice, enabling private decision-making for more personalized applications."
