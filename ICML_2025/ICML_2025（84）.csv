type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,DS-VLM: Diffusion Supervision Vision Language Model,https://ICML.cc//virtual/2025/poster/45511,"Zhen Sun, Yunhang Shen, Jie Li, Xing Sun, Pingyang Dai, Liujuan Cao, Rongrong Ji","Vision-Language Models (VLMs) face two critical limitations in visual representation learning: degraded supervision due to information loss during gradient propagation, and the inherent semantic sparsity of textual supervision compared to visual data. We propose the Diffusion Supervision Vision-Language Model (DS-VLM), a plug-and-play framework that introduces diffusion-based direct supervision for vision-language alignment. By reconstructing input images through a diffusion model conditioned on outputs of the visual encoder and the connector, our method establishes a short-path gradient propagation channel from pixel space to visual features. This approach simultaneously preserves high-level semantic alignment through conventional text supervision while enhancing visual feature quality via pixel-level reconstruction constraints. Extensive experiments conducted across various visual encoders and LLMs of different scales demonstrate the effectiveness of our approach.","Today’s AI systems that connect pictures and words often miss fine image details because they learn mainly from text feedback that must travel through a very large language model. We created a simple training add-on that asks the computer to “redraw” each picture, pixel by pixel, with a generative “diffusion” engine. This direct exercise gives the vision part of the model much clearer guidance—like letting an art student repaint a scene instead of only hearing comments—while the usual text guidance still keeps words and images in sync. After this training, the same models answer image-based questions more accurately on ten public tests, yet they run just as fast as before because the extra diffusion step is needed only during training. The result is a practical path toward sharper, more reliable visual understanding in everyday AI tools."
Poster,DTZO: Distributed Trilevel Zeroth Order Learning with Provable Non-Asymptotic Convergence,https://ICML.cc//virtual/2025/poster/48830,"Yang Jiao, Kai Yang, Chengtao Jian","Trilevel learning (TLL) with zeroth order constraints is a fundamental problem in machine learning, arising in scenarios where gradient information is inaccessible due to data privacy or model opacity, such as in federated learning, healthcare, and financial systems. These problems are notoriously difficult to solve due to their inherent complexity and the lack of first order information. Moreover, in many practical scenarios, data may be distributed across various nodes, necessitating strategies to address trilevel learning problems without centralizing data on servers to uphold data privacy. To this end, an effective distributed trilevel zeroth order learning framework DTZO is proposed in this work to address the trilevel learning problems with level-wise zeroth order constraints in a distributed manner. The proposed DTZO is versatile and can be adapted to a wide range of (grey-box) trilevel learning problems with partial zeroth order constraints. In DTZO, the cascaded polynomial approximation can be constructed without relying on gradients or sub-gradients, leveraging a novel cut, i.e., zeroth order cut. Furthermore, we theoretically carry out the non-asymptotic convergence rate analysis for the proposed DTZO in achieving the $\epsilon$-stationary point. Extensive experiments have been conducted to demonstrate and validate the superior performance of the proposed DTZO.","(1) Nested optimization has attracted significant attention in Machine Learning, with applications in areas such as meta-learning, adversarial learning, hyperparameter optimization, and continual learning. Solving nested optimization problems without relying on gradient information has become increasingly important, especially with the rise of LLMs, where commercial LLM APIs often do not expose gradients. (2) Tackling nested optimization problems without gradient information is highly challenging. We propose the first framework, DTZO, to address three-level nested optimization problems in a zeroth-order manner, and we provide theoretical guarantees for the proposed trilevel zeroth-order algorithm. (3) This helps bridge the gap between nested optimization and zeroth-order methods, making trilevel learning more widely applicable and filling an important theoretical gap."
Poster,Dual Feature Reduction for the Sparse-group Lasso and its Adaptive Variant,https://ICML.cc//virtual/2025/poster/45252,"Fabio Feser, Marina Evangelou","The sparse-group lasso performs both variable and group selection, simultaneously using the strengths of the lasso and group lasso. It has found widespread use in genetics, a field that regularly involves the analysis of high-dimensional data, due to its sparse-group penalty, which allows it to utilize grouping information. However, the sparse-group lasso can be computationally expensive, due to the added shrinkage complexity, and its additional hyperparameter that needs tuning. This paper presents a novel feature reduction method, Dual Feature Reduction (DFR), that uses strong screening rules for the sparse-group lasso and the adaptive sparse-group lasso to reduce their input space before optimization, without affecting solution optimality. DFR applies two layers of screening through the application of dual norms and subdifferentials. Through synthetic and real data studies, it is shown that DFR drastically reduces the computational cost under many different scenarios.","Discovering which features are important in large datasets, like in genetics, can lead to better predictions and stronger scientific understanding. These discoveries reveal the features that truly drive outcomes, enabling more accurate models and insights. However, identifying these features is challenging, especially when analyzing thousands of them. One popular method, the sparse-group lasso, selects both individual features and groups of them by utilizing grouping information. While powerful, it can be computationally demanding. We introduce Dual Feature Reduction (DFR), a method that speeds up this process without losing any accuracy. DFR applies two layers of mathematical checks to eliminate irrelevant features and groups before the full analysis. This allows the computations to be performed on a small fraction of the total data, leading to very large computational savings. Across synthetic and real datasets, DFR was shown to significantly reduce computation time, making feature selection more efficient and scalable."
Poster,Dueling Convex Optimization with General Preferences,https://ICML.cc//virtual/2025/poster/45022,"Aadirupa Saha, Tomer Koren, Yishay Mansour","We address the problem of convex optimization with dueling feedback, where the goal is to minimize a convex function given a weaker form of \emph{dueling} feedback. Each query consists of two points and the dueling feedback returns a (noisy) single-bit binary comparison of the function values of the two queried points.The translation of the function values to the single comparison bit is through a \emph{transfer function}.This problem has been addressed previously for some restricted classes of transfer functions, but here we consider a very general transfer function class which includes all functions that admit a series expansion about the origin.Our main contribution is an efficient algorithm with convergence rate of $O(\epsilon^{-4p})$ for smooth convex functions, and an optimal rate of $\widetilde O(\epsilon^{-2p})$ when the objective is both smooth and strongly convex, where $p$ is the minimal degree (with a non-zero coefficient) in the transfer's series expansion about the origin.","We study how to solve optimization problems in situations where we can't directly measure how good each option is, but can only make noisy comparisons between pairs of options—like asking someone which of two choices they prefer, without knowing why. This kind of feedback is called ""dueling feedback."" While earlier research has only worked for limited types of such comparisons, our work allows for a much broader and more realistic range of comparison styles. We design a new method that can efficiently find the best choice even when these comparisons are noisy and indirect. Our approach works especially well when the problem has a smooth and well-behaved structure, in fact, optimally when the underlying function is smooth and strongly convex. This work pushes the boundaries of optimization with pairwise comparisons, offering a powerful and general solution for real-world decision-making with limited, noisy relative feedback."
Poster,DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications,https://ICML.cc//virtual/2025/poster/45688,"Ibrahim Fayad, Max Zimmer, Martin Schwartz, Fabian Gieseke, Philippe CIAIS, Gabriel Belouze, Sarah Brood, Aurélien de Truchis, Alexandre d&#x27;Aspremont","Significant efforts have been directed towards adapting self-supervised multimodal learning for Earth observation applications. However, most current methods produce coarse patch-sized embeddings, limiting their effectiveness and integration with other modalities like LiDAR. To close this gap, we present DUNIA, an approach to learn pixel-sized embeddings through cross-modal alignment between images and full-waveform LiDAR data. As the model is trained in a contrastive manner, the embeddings can be directly leveraged in the context of a variety of environmental monitoring tasks in a zero-shot setting. In our experiments, we demonstrate the effectiveness of the embeddings for seven such tasks: canopy height mapping, fractional canopy cover, land cover mapping, tree species identification, plant area index, crop type classification, and per-pixel waveform-based vertical structure mapping. The results show that the embeddings, along with zero-shot classifiers, often outperform specialized supervised models, even in low-data regimes. In the fine-tuning setting, we show strong performances near or better than the state-of-the-art on five out of six tasks.","Satellite images are invaluable for many essential tasks that support environmental sustainability, such as managing forests, protecting biodiversity, addressing climate change, managing natural resources, and monitoring agriculture.Recently, supervised deep learning models have been proposed for these tasks and have produced impressive results. However, most of these models depend on large amounts of labeled data and are designed for specific applications, limiting their adaptability to new tasks.Self-supervised learning approaches reduce the need for labeled data and offer greater flexibility. Nonetheless, many current methods understand satellite images at a coarse scale. Akin to condensing an entire photograph into a single sentence. This limits their usefulness for fine-grained, per-pixel analysis.We introduce DUNIA, a method that learns detailed, per-pixel representations by combining optical and radar satellite imagery with spaceborne LiDAR data. Instead of relying on labeled examples, DUNIA uses contrastive learning to align different views of the same landscape, capturing both horizontal and vertical patterns.These learned representations can be used directly for a wide range of environmental monitoring tasks. Our results show that DUNIA often matches or exceeds the performance of specialized models, even when labeled data are limited."
Poster,DVI:A Derivative-based Vision Network for INR,https://ICML.cc//virtual/2025/poster/46476,"RUNZHAO YANG, Xiaolong Wu, Zhihong Zhang, Fabian Zhang, Tingxiong Xiao, Zongren Li, Kunlun He, Jinli Suo","Recent advancements in computer vision have seen Implicit Neural Representations (INR) becoming a dominant representation form for data due to their compactness and expressive power. To solve various vision tasks with INR data, vision networks can either be purely INR-based, but are thereby limited by simplistic operations and performance constraints, or include raster-based methods, which then tend to lose crucial structural information of the INR during the conversion process. To address these issues, we propose DVI, a novel Derivative-based Vision network for INR, capable of handling a variety of vision tasks across various data modalities, while achieving the best performance among the existing methods by incorporating state of the art raster-based methods into a INR based architecture. DVI excels by extracting semantic information from the high order derivative map of the INR, then seamlessly fusing it into a pre-existing raster-based vision network, enhancing its performance with deeper, task-relevant semantic insights. Extensive experiments on five vision tasks across three data modalities demonstrate DVI's superiority over existing methods. Additionally, our study encompasses comprehensive ablation studies to affirm the efficacy of each element of DVI, the influence of different derivative computation techniques and the impact of derivative orders. Reproducible codes are provided in the supplementary materials.","Computer vision systems struggle to effectively process Implicit Neural Representations (INR) data, either using limited INR-based methods or losing crucial structural information when converting to traditional formats.We developed DVI, a Derivative-based Vision network that extracts structural information from high-order derivatives of INR data and seamlessly integrates it with existing vision networks.DVI handles multiple vision tasks across various data types with superior performance. This advancement helps computers better ""understand"" complex visual data, potentially improving applications from medical imaging to autonomous driving."
Poster,DyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination,https://ICML.cc//virtual/2025/poster/46547,"Simin Chen, Pranav Pusarla, Baishakhi Ray","The rapid advancement of code large language models (Code LLMs) underscores the critical need for effective and transparent benchmarking methods. However, current benchmarking predominantly relies on publicly available, human-created datasets. The widespread use of these static benchmark datasets makes the evaluation process particularly susceptible to data contamination—an unavoidable consequence of the extensive data collection processes employed during LLM training. Existing methods for addressing data contamination typically face significant limitations, including reliance on substantial human effort and difficulty in managing class imbalances. To overcome these challenges, we propose DyCodeEval, a novel benchmarking suite specifically designed to evaluate Code LLMs under realistic contamination scenarios. Given an initial seed programming problem, DyCodeEval utilizes multiple agents to systematically extract and modify contextual information without changing the core logic, generating semantically equivalent variations. We introduce a dynamic data generation method and conduct extensive empirical studies on two seed datasets involving 18 Code LLMs. The results demonstrate that DyCodeEval effectively assesses the reasoning capabilities of Code LLMs under contamination conditions while producing diverse problem variants, thereby ensuring robust and consistent benchmarking outcomes.","Large language models (LLMs) are increasingly used to write code and solve programming tasks. But evaluating whether these models truly understand code is challenging. Most benchmarks rely on public test problems, which may already appear in the models’ training data—like giving students an exam they've already seen.To address this, we developed a new ""dynamic"" way to evaluate code LLMs more fairly. Starting from a few original problems, we automatically generate new but logically equivalent versions using LLM agents. This helps test whether the model can reason about the problem rather than just recall memorized answers.Our approach provides a more robust and diverse benchmark for assessing code LLMs, helping researchers and developers better understand what these models can and cannot do in realistic settings."
Poster,Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data,https://ICML.cc//virtual/2025/poster/44275,"Mohammad Hosseini, Maryam Shanechi","High-dimensional imaging of neural activity, such as widefield calcium and functional ultrasound imaging, provide a rich source of information for understanding the relationship between brain activity and behavior. Accurately modeling neural dynamics in these modalities is crucial for understanding this relationship but is hindered by the high-dimensionality, complex spatiotemporal dependencies, and prevalent behaviorally irrelevant dynamics in these modalities. Existing dynamical models often employ preprocessing steps to obtain low-dimensional representations from neural image modalities. However, this process can discard behaviorally relevant information and miss spatiotemporal structure. We propose SBIND, a novel data-driven deep learning framework to model spatiotemporal dependencies in neural images and disentangle their behaviorally relevant dynamics from other neural dynamics. We validate SBIND on widefield imaging datasets, and show its extension to functional ultrasound imaging, a recent modality whose dynamical modeling has largely remained unexplored. We find that our model effectively identifies both local and long-range spatial dependencies across the brain while also dissociating behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing models in neural-behavioral prediction. Overall, SBIND provides a versatile tool for investigating the neural mechanisms underlying behavior using imaging modalities.","Modern brain imaging provides detailed ""videos"" of brain-wide activity, thus providing an unprecedented opportunity for understanding how the brain controls behavior. However, these image data are quite complex as they capture numerous behavioral and mental processes that are simultaneously generated by the brain at once. Also, these images present patterns that vary over time and space, both locally in a given brain area and globally across brain areas. As a result, it is challenging to isolate the patterns in these images linked to a specific behavior, like a mouse licking a spout, from all the other background brain activity. Existing methods often simplify and reduce dimensionality of these video recordings before any analysis, which can lead to the loss of important information.We present a novel framework, SBIND, that learns directly from the raw, high-resolution video recordings of brain activity, rather than from simplified data. SBIND automatically identifies important patterns as they evolve over time across both local brain regions and larger, brain-wide networks. SBIND does this by first disentangling the brain patterns that are most closely tied to the measured behavior, and then separately modeling other ongoing brain patterns. Further, SBIND is designed to capture both the local and global brain-wide patterns. This approach helps SBIND accurately distinguish behavior-related patterns.On two diverse high-resolution neural imaging modalities, one optical and one acoustic, SBIND outperformed existing approaches at predicting behavior and future neural activity. By capturing both local and global brain patterns that are relevant to behavior, SBIND offers a more holistic view of brain-behavior relations. It also opens the door to developing brain-computer interfaces that are more advanced and less invasive, for example to restore lost function in individuals with movement disabilities or mental disorders."
Poster,Dynamical phases of short-term memory mechanisms in RNNs,https://ICML.cc//virtual/2025/poster/43521,"Bariscan Kurtkaya, Fatih Dinc, Mert Yuksekgonul, Marta Blanco-Pozo, Ege Cirakman, Mark Schnitzer, Yucel Yemez, Hidenori Tanaka, Yuan, Nina Miolane","Short-term memory is essential for cognitive processing, yet our understanding of its neural mechanisms remains unclear. Neuroscience has long focused on how sequential activity patterns, where neurons fire one after another within large networks, can explain how information is maintained. While recurrent connections were shown to drive sequential dynamics, a mechanistic understanding of this process still remains unknown. In this work, we introduce two unique mechanisms that can support this form of short-term memory: slow-point manifolds generating direct sequences or limit cycles providing temporally localized approximations. Using analytical models, we identify fundamental properties that govern the selection of each mechanism. Precisely, on short-term memory tasks (delayed cue-discrimination tasks), we derive theoretical scaling laws for critical learning rates as a function of the delay period length, beyond which no learning is possible. We empirically verify these results by training and evaluating approximately 80,000 recurrent neural networks (RNNs), which are publicly available for further analysis. Overall, our work provides new insights into short-term memory mechanisms and proposes experimentally testable predictions for systems neuroscience.","How does the brain hold on to information for a few seconds, like remembering a phone number just long enough to dial it? This kind of short-term memory is critical for decision-making and everyday thinking, but how it works in the brain is still not fully understood. Earlier works have discovered that patterns of activity in the brain, where different neurons fire in a particular sequence, help maintain information over short timescales. However, the exact mechanisms behind how these sequences are formed and sustained remain unclear. In our study, we use simple mathematical models to show that short-term memory can be supported in two main ways: either through slow-moving activity patterns (like stepping through a series of states) or through repeating cycles (like a loop). We find clear mathematical rules that explain when each type of memory emerges and show that there's a limit to how long a system can hold information using these mechanisms. To test our theory, we trained nearly 80,000 artificial neural networks, which are made freely available to other researchers. Our work helps explain how short-term memory might work in both brains and machines, and we offer new predictions that can be tested in neuroscience experiments."
Poster,Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning,https://ICML.cc//virtual/2025/poster/43454,"Chendi Ge, Xin Wang, Zeyang Zhang, Hong Chen, Jiapei Fan, Longtao Huang, Hui Xue&#x27;, Wenwu Zhu","Continual multimodal instruction tuning is crucial for adapting Multimodal Large Language Models (MLLMs) to evolving tasks. However, most existing methods adopt a fixed architecture, struggling with adapting to new tasks due to static model capacity. We propose to evolve the architecture under parameter budgets for dynamic task adaptation, which remains unexplored and imposes two challenges: 1) task architecture conflict, where different tasks require varying layer-wise adaptations, and 2) modality imbalance, where different tasks rely unevenly on modalities, leading to unbalanced updates. To address these challenges, we propose a novel Dynamic Mixture of Curriculum LoRA Experts (D-MoLE) method, which automatically evolves MLLM's architecture with controlled parameter budgets to continually adapt to new tasks while retaining previously learned knowledge. Specifically, we propose a dynamic layer-wise expert allocator, which automatically allocates LoRA experts across layers to resolve architecture conflicts, and routes instructions layer-wisely to facilitate knowledge sharing among experts. Then, we propose a gradient-based inter-modal continual curriculum, which adjusts the update ratio of each module in MLLM based on the difficulty of each modality within the task to alleviate the modality imbalance problem. Extensive experiments show that D-MoLE significantly outperforms state-of-the-art baselines, achieving a 15 percent average improvement over the best baseline. To the best of our knowledge, this is the first study of continual learning for MLLMs from an architectural perspective.","Modern AI models that process both language and images, such as those used in assistants that understand pictures, need to keep learning as new tasks appear. However, most existing methods use a fixed model structure and cannot easily adapt to new tasks without losing what they have already learned.Our research addresses this problem by proposing a way for the model to adjust its internal structure over time, while keeping the total number of parameters within a fixed budget. We found two main challenges. First, different tasks prefer to update different layers of the model. Second, some tasks rely more on language while others depend more on images, which creates an imbalance during training.To tackle this, we introduce a method called Dynamic Mixture of Curriculum LoRA Experts (D-MoLE). It selects which parts of the model to update based on the current task and balances updates across text and image components. Our experiments show that this approach outperforms existing baselines, helping the model continue learning effectively without forgetting earlier knowledge."
