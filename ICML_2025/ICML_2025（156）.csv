type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation,https://ICML.cc//virtual/2025/poster/45607,"Piyush Lalitkumar Tiwary, Kinjawl Bhattacharyya, Prathosh AP","Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DA). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DA methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel **Lang**evin **D**ata **Aug**mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.","When hospitals adopt AI tools for medical image analysis, they often discover a frustrating problem: systems that work perfectly at one facility fail at another. This happens because medical scanners, like cameras with different settings, produce images with subtle but important variations.We tackled this challenge by developing LangDAug, a technique inspired by how physicists model particle movement. Instead of trying to eliminate differences between imaging styles, we embrace them. LangDAug creates a spectrum of synthetic training images that gradually morph from one hospital's imaging style to another's, teaching AI to handle the full range of natural variations.Our method proved highly effective in experiments with retinal scans for eye disease and MRI scans for prostate analysis. AI systems trained with LangDAug maintained their accuracy even when tested on images from completely new hospitals they had never encountered. This breakthrough could help deploy medical AI more broadly, ensuring that advanced diagnostic tools benefit patients everywhere, not just those at well-resourced medical centers."
Poster,LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization,https://ICML.cc//virtual/2025/poster/45059,"Wenzhe Niu, Zongxia Xie, Yanru Sun, Wei He, Man Xu, Chao Hao","Recent research has shown an increasing interest in utilizing pre-trained large language models (LLMs) for a variety of time series applications. However, there are three main challenges when using LLMs as foundational models for time series forecasting: (1) Cross-domain generalization. (2) Cross-modality alignment. (3) Error accumulation in autoregressive frameworks. To address these challenges, we proposed **LangTime**, a **lan**guage-**g**uided unified model for **time** series forecasting that incorporates cross-domain pre-training with reinforcement learning-based fine-tuning. Specifically, LangTime constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise and channel-wise instructions, to facilitate domain adaptation and condense time series into a single token, enabling LLMs to understand better and align temporal data. To improve autoregressive forecasting, we introduce TimePPO, a reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error accumulation by leveraging a multidimensional rewards function tailored for time series and a repeat-based value estimation strategy. Extensive experiments demonstrate that LangTime achieves state-of-the-art cross-domain forecasting performance, while TimePPO fine-tuning effectively enhances the stability and accuracy of autoregressive forecasting.","From traffic flows to stock markets, our world runs on time-based data ,but forecasting these patterns remains challenging, especially across different domains like energy and retail.While powerful language models (like GPT) show promise, they struggle to understand numerical time series. Our solution, LangTime, bridges this gap by giving these models:（1) Data instruction manuals that explain unique characteristics of each dataset. (2) Domain guides highlighting differences between fields (e.g., healthcare vs. transportation).For long-term predictions where small errors can snowball, we adapted the AI training method behind ChatGPT (called PPO) specifically for time series. This acts as a coach providing continuous feedback to correct mistakes and improve prediction.Tested across energy, traffic and finance domains, LangTime outperforms existing methods. Our framework unlocks language AI's potential for real-world numerical forecasting challenges."
Poster,Language Models as Implicit Tree Search,https://ICML.cc//virtual/2025/poster/44753,"Ziliang Chen, Zhao-Rong Lai, Yufeng Yang, Liangda Fang, ZHANFU YANG, Liang Lin","Despite advancing language model (LM) alignment, direct preference optimization (DPO) falls short in LM reasoning with the free lunch from reinforcement learning (RL). As the breakthrough, this work proposes a new RL-free preference optimization method aiming to achieve DPO along with learning another LM, whose response generation policy holds the asymptotic equivalence with AlphaZero-like search, the apex of algorithms for complex reasoning missions like chess Go. While circumventing explicit value and reward modeling, the neural implicit tree search executed by the extra LM remains seeking to equip DPO with reasoning procedure technically akin to AlphaZero. Our experiments demonstrate that our methodology outperforms both regular DPO variants in human preference alignment, and MCTS-based LMs in mathematical reasoning and planning tasks.","Imagine teaching an AI to be helpful. It can learn to follow our instructions and understand our preference well (like a good assistant), but it often struggles with complex thinking needed for tough puzzles or mathematics. Making it good at both usually requires complicated training methods.We created a ""team"" of two AIs. The first AI learns to understand and follow human preferences. The second AI acts like a clever ""thinking coach,"" guiding the first one to explore ideas and find smart solutions, similar to how a chess AI master plans moves, but without the usual complex training steps.This teamwork means AI can become both a better listener (understanding our preferences) and a sharper thinker (solving difficult problems). This leads to more capable, reliable, and helpful AI for everyone, as well as keeping the talent for math, planning, or other complex tasks."
Poster,Language Models May Verbatim Complete Text They Were Not Explicitly Trained On,https://ICML.cc//virtual/2025/poster/44746,"Ken Ziyu Liu, Christopher A. Choquette Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot","An important question today is whether a given text was used to train a large language model (LLM). A completion test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the n-gram overlap between the target text and any text in the dataset. In this work, we demonstrate that this n-gram based membership definition can be effectively gamed. We study scenarios where sequences are non-members for a given n and we find that completion tests still succeed. We find many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of n for membership definitions. Using these insights, we design adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of n. Our findings highlight the inadequacy of n-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.","What exactly do we mean by ""training set inclusion"" under language models? A vast body work---across research, policy, and even lawsuits---has implicitly converged on definitions based on $n$-gram (substring) overlap. That is, a piece of text is considered a ""member"" of the training set, if some span of that text (n-gram) can be found in the training set.This paper is a tale of two experiments that demonstrates the fundamental limitations of all $n$-gram based membership definitions. We ask two questions from the lens of (verbatim) text completion with a language model:1. **Deletion:** can we *prevent* the verbatim generation of a text by deleting all of its n-grams and retraining the model from scratch? The answer is no! Many deleted texts can still be generated verbatim by the retrained LLM.2. **Addition:** can we *cause* the verbatim generation of a text by training on texts with no n-gram overlap? The answer is yes! And it only takes a few gradient steps of fine-tuning.The key message of this work is that data membership in LLMs extends beyond set membership of text in the raw dataset; it also encompasses data neighborhoods (“soft membership”) due to LLM generalization, data provenance, preprocessing, and other auxiliary information that the training algorithm gets access to throughout the ML pipeline.Many subfields, such as copyright, unlearning, membership inference, and data transparency, require a membership definition, and our work shows overly simplistic notions of membership hinder progress in these areas."
Poster,Language Models over Canonical Byte-Pair Encodings,https://ICML.cc//virtual/2025/poster/44596,"Tim Vieira, Tianyu Liu, Clemente Pasti, Yahya Emara, Brian DuSell, Benjamin LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Timothy O&#x27;Donnell, Ryan Cotterell","Modern language models represent probability distributions over character strings as distributions over (shorter) token strings derived via a deterministic tokenizer, such as byte-pair encoding. While this approach is highly effective at scaling up language models to large corpora, its current incarnations have a concerning property: the model assigns nonzero probability mass to an exponential number of *noncanonical* token encodings of each character string—these are token strings that decode to valid character strings but are impossible under the deterministic tokenizer (i.e., they will never be seen in any training corpus, no matter how large). This misallocation is both erroneous, as noncanonical strings never appear in training data, and wasteful, diverting probability mass away from plausible outputs. These are avoidable mistakes!  In this work, we propose methods to enforce canonicality in token-level language models, ensuring that only canonical token strings are assigned positive probability. We present two approaches: (1) canonicality by conditioning, leveraging test-time inference strategies without additional training, and (2) canonicality by construction, a model parameterization that guarantees canonical outputs but requires training. We demonstrate that fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.","Large language models represent text using a process called byte-pair encoding; under this scheme, text is broken up into chunks called tokens, which allows the model to generate text more efficiently.However, this process introduces a subtle but important problem: although each string of text has one *correct* or *canonical* tokenization under BPE, current models can mistakenly assign probability to many invalid, noncanonical versions, i.e., strings of tokens that can never occur in real BPE-encoded text.  This misallocation of probability wastes modeling power and may even distort the model's understanding of language through noncanonical hallucinations.Our paper identifies and addresses this issue. We introduce two methods to ensure that language models only assign probability to valid, canonical token strings.  Our first method adjusts the model at inference time—without retraining—to only consider canonical strings.  Our second method builds the canonical constraint directly into the model's architecture and can be fine-tuned for better performance.We prove that both approaches improve model accuracy in theory, and we validate these improvements empirically across multiple popular models and benchmark datasets.  We also introduce a new, efficient method for checking whether a token string is canonical, making our solutions practical and easy to implement.Ultimately, this work helps language models align more closely with the data on which they were trained, thereby improving reliability and reducing errors in generated text."
Poster,Laplace Transform Based Low-Complexity Learning of Continuous Markov Semigroups,https://ICML.cc//virtual/2025/poster/45784,"Vladimir Kostic, Karim Lounici, Hélène Halconruy, Timothée Devergne, Pietro Novelli, Massimiliano Pontil","Markov processes serve as universal models for many real-world random processes. This paper presents a data-driven approach to learning these models through the spectral decomposition of the infinitesimal generator (IG) of the Markov semigroup. Its unbounded nature  complicates traditional methods such as vector-valued regression and Hilbert-Schmidt operator analysis. Existing techniques, including physics-informed kernel regression, are computationally expensive and limited in scope, with no recovery guarantees for transfer operator methods when the time-lag is small. We propose a novel method leveraging the IG's resolvent, characterized by the Laplace transform of transfer operators. This approach is robust to time-lag variations, ensuring accurate eigenvalue learning even for small time-lags. Our statistical analysis applies to a broader class of Markov processes than current methods while reducing computational complexity from quadratic to linear in the state dimension. Finally, we demonstrate our theoretical findings in several experiments.","Many natural and engineered systems—like weather, markets, or brain activity—can be modeled as Markov processes, where the future depends only on the present. But learning accurate models of these systems from data is difficult, especially when observations are noisy, irregular, or limited in time. Existing methods struggle when the time between observations is small or when the system is high-dimensional, often requiring expensive computations and offering limited guarantees.We tackled this by developing a new approach that learns key features of the system—its “modes of behavior”—through the infinitesimal generator, a mathematical object that describes how the system evolves continuously in time. Instead of trying to work with this complex object directly, we use its resolvent, which is more stable and computable from data using a trick involving Laplace transforms.Our method is fast (scales linearly with system size), works well even with short time intervals, and applies to a wider class of systems than existing techniques. It gives researchers a more reliable and efficient way to understand and simulate complex dynamic processes from data."
Poster,"LapSum - One Method to Differentiate Them All: Ranking, Sorting and Top-k Selection",https://ICML.cc//virtual/2025/poster/46060,"Łukasz Struski, Michal Bednarczyk, Igor Podolak, Jacek Tabor","We present a novel technique for constructing differentiable order-type operations, including soft ranking, soft top-k selection, and soft permutations. Our approach leverages an efficient closed-form formula for the inverse of the function LapSum, defined as the sum of Laplace distributions. This formulation ensures low computational and memory complexity in selecting the highest activations, enabling losses and gradients to be computed in $O(n \log n)$ time. Through extensive experiments, we demonstrate that our method outperforms state-of-the-art techniques for high-dimensional vectors and large $k$ values. Furthermore, we provide efficient implementations for both CPU and CUDA environments, underscoring the practicality and scalability of our method for large-scale ranking and differentiable ordering problems.","We present a novel technique for constructing differentiable order-type operations, including soft ranking, soft top-k selection, and soft permutations. Our approach leverages an efficient closed-form formula for the inverse of the function LapSum, defined as the sum of Laplace distributions. This formulation ensures low computational and memory complexity in selecting the highest activations, enabling losses and gradients to be computed in $O(n \log n)$ time. Through extensive experiments, we demonstrate that our method outperforms state-of-the-art techniques for high-dimensional vectors and large $k$ values. Furthermore, we provide efficient implementations for both CPU and CUDA environments, underscoring the practicality and scalability of our method for large-scale ranking and differentiable ordering problems."
Poster,LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs – No Silver Bullet for LC or RAG Routing,https://ICML.cc//virtual/2025/poster/46069,"Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, Minhao Cheng","As Large Language Model (LLM) context windows expand, the necessity of Retrieval-Augmented Generation (RAG) for integrating external knowledge is debated. Existing RAG vs. long-context (LC) LLM comparisons are often inconclusive due to benchmark limitations. We introduce LaRA, a novel benchmark with 2326 test cases across four QA tasks and three long context types, for rigorous evaluation. Our analysis of eleven LLMs reveals the optimal choice between RAG and LC depends on a complex interplay of model capabilities, context length, task type, and retrieval characteristics, offering actionable guidelines for practitioners. Our code and dataset is provided at:https://github.com/Alibaba-NLP/LaRA","(1) When a user asks a question based on a long text, should we have the LLM answer directly using the full context, or should we retrieve only the chunks relevant to the query using certain rules or similarity detection? (2) To answer this question, we propose LaRA, a comprehensive benchmark specifically designed to compare long-context LLMs and Retrieval-Augmented Generation (RAG). (3) We find that neither RAG nor long-context LLMs are a silver bullet; their relative strengths and weaknesses depend on a wide range of factors."
Poster,Large Continual Instruction Assistant,https://ICML.cc//virtual/2025/poster/44960,"Jingyang Qiao, zhizhong zhang, Xin Tan, Yanyun Qu, Shouhong Ding, Yuan Xie","Continual Instruction Tuning (CIT) is adopted to continually instruct Large Models to follow human intent data by data. It is observed that existing gradient update would heavily destroy the performance on previous datasets during CIT process. Instead, Exponential Moving Average (EMA), owns the ability to trace previous parameters, which can aid in decreasing forgetting. Nonetheless, its stable balance weight fails to deal with the ever-changing datasets, leading to the out-of-balance between plasticity and stability. In this paper, we propose a general continual instruction tuning framework to address the challenge. Starting from the trade-off prerequisite and EMA update, we propose the plasticity and stability ideal condition. Based on Taylor expansion in the loss function, we find the optimal balance weight can be automatically determined by the gradients and learned parameters. Therefore, we propose a stable-plasticity balanced coefficient to avoid knowledge interference. Based on the semantic similarity of the instructions, we can determine whether to retrain or expand the training parameters and allocate the most suitable parameters for the testing instances. Extensive experiments across multiple continual instruction tuning benchmarks demonstrate that our approach not only enhances anti-forgetting capabilities but also significantly improves overall continual tuning performance. Our code is available at https://github.com/JingyangQiao/CoIN.","We want Large Foundation Models to continually learn from new human instructions without forgetting what they have learned before, which is called Continual Instruction Tuning. However, current training paradigms often make the model forget old knowledge when learning new tasks.We improve this by proposing a novel technique called Large Continual Instruction Assistant, which helps the model remember past knowledge. By analyzing the trade-off between learning new information and avoiding forgetting, we derive a principled way to compute an optimal balance coefficient based on gradients and learned parameters. This allows the model to automatically adjust its updates based on what it is currently learning and what it has already learned.We also measure how similar new instructions are to past ones, so the model can decide whether to reuse what it already knows or learn something new. In conclusion, our method successfully helps Large Foundation Models mitigate forgetting during learning new knowledge."
Poster,Large Displacement Motion Transfer with Unsupervised Anytime Interpolation,https://ICML.cc//virtual/2025/poster/43896,"Guixiang Wang, Jianjun Li","Motion transfer is to transfer pose in driving video to object of source image, so that object of source image moves. Although great progress has been made recently in unsupervised motion transfer, many unsupervised methods still struggle to accurately model large displacement motions when large motion differences occur between source and driving images. To solve the problem, we propose an unsupervised anytime interpolation based large displacement motion transfer method, which can generate a series of anytime interpolated images between source and driving images. By decomposing large displacement motion into many small displacement motions, difficulty of large displacement motion estimation is reduced. In the process, we design a selector that can select optimal interpolated image from generated interpolated images for downstream tasks. Since there are no real images as labels in the interpolation process, we propose a bidirectional training strategy. Some constraints are added to optimal interpolated image to generate a reasonable interpolated image. To encourage network to generate high-quality images, a pre-trained Vision Transformer model is used to design constraint losses. Finally, experiments show that compared with the large displacement motion between source and driving images, small displacement motion between interpolated and driving images is easier to realize motion transfer. Compared with existing state-of-art methods, our method has significant improvements in motion-related metrics.","We perform unsupervised interpolation between the source image and the driving image, as a way to explore whether the interpolated image can solve the problem that the pose difference between the source image and the driving image is large and difficult to model.We use the keypoint information to interpolate, so as to generate multiple interpolated images, and then select the optimal interpolated image. The optimal interpolated image decomposes the complex large displacement motion between the source image and the driving image into two simple small displacement motions. To enhance realism,  we integrate constraints derived from a pre-trained Vision Transformer (ViT) to guide texture and motion coherence.By reducing large displacements to smaller steps, our method achieves very good results on motion metrics, in some cases even better than existing methods. Our study has important implications for how to learn the pose of the driving image and preserve the appearance of the source image."
