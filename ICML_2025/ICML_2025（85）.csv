type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Dynamic Similarity Graph Construction with Kernel Density Estimation,https://ICML.cc//virtual/2025/poster/43809,"Steinar Laenen, Peter Macgregor, He Sun","In the kernel density estimation (KDE) problem, we are given a set  $X$ of data points in $\mathbb{R}^d$, a kernel function $k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$, and a query point $\mathbf{q} \in \mathbb{R}^d$, and the objective is to quickly output an estimate of $\sum_{\mathbf{x} \in X} k(\mathbf{q}, \mathbf{x})$.In this paper, we consider $\textsf{KDE}$ in the dynamic setting, and introduce a data structure that efficiently maintains the _estimates_ for a set of query points as data points are added to $X$ over time.Based on this, we design a dynamic data structure that maintains a sparse approximation of the fully connected similarity graph on $X$, and develop a fast dynamic spectral clustering algorithm.We further evaluate the effectiveness of our algorithms on both synthetic and real-world datasets.","Imagine you have a constantly growing collection of items, like photos or social media posts, and you want to automatically group similar items together. A common way to achieve this is to first figure out how similar each pair of items is, which can be a lot of work if you have many items. This similarity information can be thought of as a network or ""graph"". The problem is, when new items arrive, recalculating all these similarities and updating the groups from scratch is very slow. A key part of this similarity calculation is quickly estimating how many items are ""around"" any given item, a process called Kernel Density Estimation (KDE).This research provides a new, much faster way to do this KDE even as new items are added, without redoing all the calculations each time. Building on this, the paper introduces a method to efficiently maintain a simplified version of the full similarity network that still captures the main groups or  ""clusters"". This means we can update our understanding of the groups quickly when new data arrives.This work makes it practical to find and track evolving groups in large, constantly changing datasets. This is useful for scenarios like understanding how communities form in social networks or how topics trend online, by making the process faster and more scalable than previous methods while still finding accurate groupings."
Poster,Dynamic Sparse Training of Diagonally Sparse Networks,https://ICML.cc//virtual/2025/poster/44756,"Abhishek Tyagi, Arjun Iyer, William Renninger, Christopher Kanan, Yuhao Zhu","Recent advances in Dynamic Sparse Training (DST) have pushed the frontier of sparse neural network training in structured and unstructured contexts, matching dense-model performance while drastically reducing parameter counts to facilitate model scaling. However, unstructured sparsity often fails to translate into practical speedups on modern hardware. To address this shortcoming, we propose DynaDiag, a novel structured sparse-to-sparse DST method that performs at par with unstructured sparsity. DynaDiag enforces a diagonal sparsity pattern throughout training and preserves sparse computation in forward and backward passes. We further leverage the diagonal structure to accelerate computation via a custom CUDA kernel, rendering the method hardware-friendly. Empirical evaluations on diverse neural architectures demonstrate that our method maintains accuracy on par with unstructured counterparts while benefiting from tangible computational gains. Notably, with 90\% sparse linear layers in ViTs, we observe up to a 3.13x speedup in online inference without sacrificing model performance and a 1.59x speedup in training on a GPU compared to equivalent unstructured layers.","Modern AI models keep getting bigger, which makes them expensive to train and slow to run. Much of that cost comes from doing math on millions of weights that the model doesn’t really need. Earlier research tried “pruning” away unnecessary weights, but the usual random‑looking patterns are not conducive to today’s computer chips-based acceleration, so the promised speed‑ups rarely appear.Our study introduces DynaDiag, a new way to keep only carefully chosen diagonal stripes of weights inside each layer. During training the algorithm constantly re‑evaluates which diagonals matter most and updates them, all while keeping the tidy diagonal layout that chips can process quickly. We also wrote custom GPU code to make sure the hardware takes full advantage. In tests on vision transformers and language models, DynaDiag kept virtually the same accuracy as the best unstructured method methods but ran up to three times faster for inference and about 1.6 × faster during training.Why does this matter? Faster, lighter models cut energy use, lower cloud costs. In short, DynaDiag helps make powerful AI more efficient, affordable, and widely accessible."
Poster,DynaMind: Reasoning over Abstract Video Dynamics for Embodied Decision-Making,https://ICML.cc//virtual/2025/poster/43462,"Ziru Wang, Mengmeng Wang, Jade Dai, Teli Ma, Guo-Jun Qi, Yong Liu, Guang Dai, Jingdong Wang","Integrating natural language instructions and visual perception with decision-making is a critical challenge for embodied agents. Existing methods often struggle to balance the conciseness of language commands with the richness of video content. To bridge the gap between modalities, we propose extracting key spatiotemporal patterns from video that capture visual saliency and temporal evolution, referred to as dynamic representation. Building on this, we introduce DynaMind, a framework that enhances decision-making through dynamic reasoning. Specifically, we design an adaptive FrameScorer to evaluate video frames based on semantic consistency and visual saliency, assigning each frame an importance score. These scores are used to filter redundant video content and synthesize compact dynamic representations. Leveraging these representations, we predict critical future dynamics and apply a dynamic-guided policy to generate coherent and context-aware actions. Extensive results demonstrate that DynaMind significantly outperforms the baselines across several simulation benchmarks and real-world scenarios.","Robots often need to interpret visual scenes and follow language instructions at the same time. However, videos are full of information, while language commands are brief, making it hard for robots to decide what matters most.To address this, we introduce DynaMind, a new method that helps robots “pick out the important parts” of a video. Much like how people skip repetitive parts in a tutorial and focus on key steps, DynaMind automatically assigns importance scores to video frames based on their visual relevance and alignment with the instruction. It then creates a compact summary of the video’s essential moments. Using this summary, the robot predicts what might happen next and decides its next move accordingly.Our experiments show that DynaMind not only performs well in simulated tasks but also works reliably in real-world settings. This work takes a step toward more intelligent robots by turning raw video into dynamic summaries that inform effective and context-aware actions."
Poster,DyPolySeg: Taylor Series-Inspired Dynamic Polynomial Fitting Network for Few-shot Point Cloud Semantic Segmentation,https://ICML.cc//virtual/2025/poster/46124,"Changshuo Wang, Xiang Fang, Prayag Tiwari","Few-shot point cloud semantic segmentation effectively addresses data scarcity by identifying unlabeled query samples through semantic prototypes generated from a small set of labeled support samples. However, pre-training-based methods suffer from domain shifts and increased training time. Additionally, existing methods using DGCNN as the backbone have limited geometric structure modeling capabilities and struggle to bridge the categorical information gap between query and support sets. To address these challenges, we propose DyPolySeg, a pre-training-free Dynamic Polynomial fitting network for few-shot point cloud semantic segmentation. Specifically, we design a unified Dynamic Polynomial Convolution (DyPolyConv) that extracts flat and detailed features of local geometry through Low-order Convolution (LoConv) and Dynamic High-order Convolution (DyHoConv), complemented by Mamba Block for capturing global context information. Furthermore, we propose a lightweight Prototype Completion Module (PCM) that reduces structural differences through self-enhancement and interactive enhancement between query and support sets. Experiments demonstrate that DyPolySeg achieves state-of-the-art performance on S3DIS and ScanNet datasets.","Imagine teaching a computer to recognize different objects in 3D scenes - like distinguishing chairs from tables - using only a few examples of each. This challenge, called few-shot learning, is crucial for robots and autonomous systems that encounter new environments with limited training data.Current methods require extensive pre-training on large datasets, which is time-consuming and often fails when new data differs from training examples. It's like trying to recognize modern office furniture after only seeing traditional home furniture.We developed DyPolySeg, a system that learns to identify 3D objects without extensive pre-training. Our approach has two key innovations: a flexible shape-analysis tool that automatically adapts to capture both simple flat surfaces and complex curved structures, and an intelligent comparison system that bridges differences between training examples and new objects to be identified.Our experiments show DyPolySeg significantly outperforms existing methods, achieving better accuracy while being more efficient. This advancement could improve robotics, autonomous vehicles, and augmented reality applications where machines must quickly understand 3D environments with minimal training data."
Poster,"EAGLES: Towards Effective, Efficient, and Economical Federated Graph Learning via Unified Sparsification",https://ICML.cc//virtual/2025/poster/46105,"Zitong Shi, Guancheng Wan, Wenke Huang, Guibin Zhang, He Li, Carl Yang, Mang Ye","Federated Graph Learning (FGL) has gained significant attention as a privacy-preserving approach to collaborative learning, but the computational demands increase substantially as datasets grow and Graph Neural Network (GNN) layers deepen. To address these challenges, we propose $\textbf{EAGLES}$, a unified sparsification framework. EAGLES applies client-consensus parameter sparsification to generate multiple unbiased subnetworks at varying sparsity levels, reducing the need for iterative adjustments and mitigating performance degradation. In the graph structure domain, we introduced a dual-expert approach: a $\textit{graph sparsification expert}$ uses multi-criteria node-level sparsification, and a $\textit{graph synergy expert}$ integrates contextual node information to produce optimal sparse subgraphs. Furthermore, the framework introduces a novel distance metric that leverages node contextual information to measure structural similarity among clients, fostering effective knowledge sharing. We also introduce the $\textbf{Harmony Sparsification Principle}$, EAGLES balances model performance with lightweight graph and model structures. Extensive experiments demonstrate its superiority, achieving competitive performance on various datasets, such as reducing training FLOPS by 82\% $\downarrow$ and communication costs by 80\% $\downarrow$ on the ogbn-proteins dataset, while maintaining high performance.","Federated graph learning faces serious scalability issues as model sizes and graph complexities continue to grow. In this work, we propose EAGLES to address the growing computational and communication challenges in federated graph learning. We design a unified framework that not only reduces redundant information in both graph structures and model parameters but also respects the privacy constraints of real-world applications. Through carefully crafted sparsification strategies and cross-client structural alignment, EAGLES enables efficient and scalable training while preserving performance. The method demonstrates substantial improvements, achieving up to 80% reduction in training and communication costs while maintaining competitive accuracy."
Poster,"EARL-BO: Reinforcement Learning for Multi-Step Lookahead, High-Dimensional Bayesian Optimization",https://ICML.cc//virtual/2025/poster/44958,"Mujin Cheon, Jay Lee, Dong-Yeun Koh, Calvin Tsay","To avoid myopic behavior, multi-step lookahead Bayesian optimization (BO) algorithms consider the sequential nature of BO and have demonstrated promising results in recent years. However, owing to the curse of dimensionality, most of these methods make significant approximations or suffer scalability issues. This paper presents a novel reinforcement learning (RL)-based framework for multi-step lookahead BO in high-dimensional black-box optimization problems. The proposed method enhances the scalability and decision-making quality of multi-step lookahead BO by efficiently solving the sequential dynamic program of the BO process in a near-optimal manner using RL. We first introduce an Attention-DeepSets encoder to represent the state of knowledge to the RL agent and subsequently propose a multi-task, fine-tuning procedure based on end-to-end (encoder-RL) on-policy learning. We evaluate the proposed method, EARL-BO (Encoder Augmented RL for BO), on synthetic benchmark functions and hyperparameter tuning problems, finding significantly improved performance compared to existing multi-step lookahead and high-dimensional BO methods.","Optimizing complex systems, such as tuning training settings for a machine learning model, often involves trial and error, which can be expensive and time-consuming. A smart approach called Bayesian optimization (BO) helps choose the best experiments to run, but most methods only think one step ahead. Looking multiple steps ahead can lead to better decisions, but this quickly becomes computationally overwhelming, especially with many variables involved. We introduce a method called EARL-BO, which uses reinforcement learning (RL) to make smarter, multi-step decisions efficiently, even in high-dimensional problems. Key contributions include a framework for encoding the state of information and for training the RL algorithm. Our experiments show that EARL-BO outperforms existing methods in both synthetic tasks and real-world scenarios like tuning machine learning models."
Poster,Earley-Driven Dynamic Pruning for Efficient Structured Decoding,https://ICML.cc//virtual/2025/poster/46365,"Xintong Sun, Chi Wei, Minghao Tian, Shiwen Ni","Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\textbf{ZapFormat}$, a novel $\textbf{dynamic pruning}$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\textbf{consistently maintains}$ high-precision compliant outputs but also achieves $\textbf{significant improvements}$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at https://github.com/Dan-wanna-M/formatron.","Large language models (LLMs) can draft essays, write code, and answer questions, but they sometimes stray from a required format, which, however, is necessary when integrating LLMs into a larger system. Today’s “constrained decoding” techniques keep LLMs inside the lines by checking every possible next word against a set of grammar rules, yet that safeguard slows things down because the model must re-check its entire vocabulary at every step.We introduce ZapFormat, a program that spots and discards impossible word sequences on the fly, using a classic parsing method called the Earley algorithm. By pruning away dead-end paths early, ZapFormat slashes the bookkeeping the model must do and lets us reuse its previous work through a cache.Built into our new program Formatron, this approach keeps answers perfectly in-format while making constrained decoding for structured output—like valid JSON, code snippets, or database queries—up to twice as fast. The technique works across many different LLM architectures, paving the way for quicker and more reliable AI agents in data-critical fields such as finance, healthcare, and software development."
Poster,EARTH: Epidemiology-Aware Neural ODE with Continuous Disease Transmission Graph,https://ICML.cc//virtual/2025/poster/46041,"Guancheng Wan, Zewen Liu, Xiaojun Shan, Max Lau, B. Aditya Prakash, Wei Jin","Effective epidemic forecasting is critical for public health strategies and efficient medical resource allocation, especially in the face of rapidly spreading infectious diseases. However, existing deep-learning methods often overlook the dynamic nature of epidemics and fail to account for the specific mechanisms of disease transmission. In response to these challenges, we introduce an innovative end-to-end framework called Epidemiology-Aware Neural ODE with Continuous Disease Transmission Graph (EARTH) in this paper. To learn continuous and regional disease transmission patterns, we first propose EANO, which seamlessly integrates the neural ODE approach with the epidemic mechanism, considering the complex spatial spread process during epidemic evolution. Additionally, we introduce GLTG to model global infection trends and leverage these signals to guide local transmission dynamically. To accommodate both the global coherence of epidemic trends and the local nuances of epidemic transmission patterns, we build a cross-attention approach to fuse the most meaningful information for forecasting. Through the smooth synergy of both components, EARTH offers a more robust and flexible approach to understanding and predicting the spread of infectious diseases. Extensive experiments show EARTH superior performance in forecasting real-world epidemics compared to state-of-the-art methods. The code is available at https://github.com/GuanchengWan/EARTH.","Predicting how diseases like COVID-19 or the flu will spread is crucial for protecting public health. Current computer models often struggle because they don't fully capture the constantly changing nature of epidemics or the specific ways diseases transmit between areas, especially when data is patchy or arrives irregularly. They also often overlook how big-picture trends, like national health policies, influence local outbreaks.We've developed a new AI framework called EARTH to tackle these challenges. EARTH learns how diseases spread over time by combining established knowledge of epidemic progression (like how people become susceptible, infected, and then recover) with a flexible, data-driven approach. Uniquely, it also analyzes overall infection trends across wider regions and uses this global view to understand and dynamically adjust for local transmission patterns. By intelligently blending these global insights with specific local details, EARTH makes more accurate predictions.Tested on real-world COVID-19 and flu data, EARTH has shown significantly better forecasting performance than existing leading methods. This provides a more robust tool to help public health experts understand disease dynamics and make timely decisions."
Poster,EasyInv: Toward Fast and Better DDIM Inversion,https://ICML.cc//virtual/2025/poster/44122,"Ziyue Zhang, Mingbao Lin, Shuicheng YAN, Rongrong Ji","This paper introduces EasyInv, an easy yet novel approach that significantly advances the field of DDIM Inversion by addressing the inherent inefficiencies and performance limitations of traditional iterative optimization methods. At the core of our EasyInv is a refined strategy for approximating inversion noise, which is pivotal for enhancing the accuracy and reliability of the inversion process. By prioritizing the initial latent state, which encapsulates rich information about the original images, EasyInv steers clear of the iterative refinement of noise items. Instead, we introduce a methodical aggregation of the latent state from the preceding time step with the current state, effectively increasing the influence of the initial latent state and mitigating the impact of noise. We illustrate that EasyInv is capable of delivering results that are either on par with or exceed those of the conventional DDIM Inversion approach, especially under conditions where the model's precision is limited or computational resources are scarce. Concurrently, our EasyInv offers an approximate threefold enhancement regarding inference efficiency over off-the-shelf iterative optimization techniques. It can be easily combined with most existing inversion methods by only four lines of code. See code at https://github.com/potato-kitty/EasyInv.","**What is diffusion inversion?**Diffusion inversion asks: “Given a pretrained diffusion model and a real image, can we recover the exact noise that produced that image?” If we succeed, feeding that noise back into the model reproduces the original photo. In other words, inversion is simply the reverse of the usual denoising process.**Why is this hard?**A good inversion method must satisfy a “fixed‑point” requirement: at each diffusion timestep, the latent representation (the model’s compact encoding of the image) should be unchanged whether you invert then denoise or vice versa. Most existing algorithms enforce this by running multiple iterative refinements at every timestep—often three or more passes—which makes inversion several times slower than a single forward pass.**Our solution: EasyInv**Inspired by the Kalman filter’s elegant way of blending new measurements with past estimates, we reuse the noise prediction from the previous timestep to inform the current inversion step. Crucially, we do this without any additional network calls or gradient computations or any other kinds of calculation. By slightly “dampening” each step’s noise update, we keep cumulative errors in check. A small correction early on compounds into a big boost in final accuracy—yet costs essentially zero extra time.**Why read the full paper?**This simple explanation aims to provide a brief view of our paper and omits the mathematical derivation, like how Kalman Filter degenerate to our approach. For the complete information, please consult the paper itself."
Poster,EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM,https://ICML.cc//virtual/2025/poster/45830,"Zhuofan Zong, Dongzhi Jiang, Bingqi Ma, Guanglu Song, Hao Shao, Dazhong Shen, Yu Liu, Hongsheng Li","Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging or concatenating their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although tuning-based approaches can effectively extract consistent elements within multiple images through the training process, it necessitates test-time finetuning for each distinct image group. This paper introduces EasyRef, a plug-and-play adaption method that empowers diffusion models to condition consistent visual elements (e.g., style and human facial identity, etc.) across multiple reference images under instruction controls. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM's representations into the diffusion process through adapters can easily generalize to unseen domains. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free and tuning-based methods, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.","When creating images with diffusion models, users often want to personalize the output based on multiple reference images, like combining features from several faces to make a new portrait. Existing methods either average the information from these images without considering their consistent elements or require costly retraining every time a user wants to personalize the model for a new set of references. To solve this, we developed EasyRef, an easy-to-use technique that lets diffusion models consistently incorporate visual elements, such as artistic style or facial identity, from multiple reference images, guided by simple instructions. We achieve this by leveraging a powerful visual-language model that understands multiple images together and identifies consistent features based on given prompts. By smoothly integrating this model’s understanding into the diffusion process, EasyRef adapts effortlessly even to types of images it has never encountered before. Additionally, our method uses an approach to combine reference images efficiently, saving computation and preserving fine details. We also introduce MRBench, a new benchmark for evaluating multi-reference image generation. Experiments show that EasyRef outperforms existing methods, producing more visually appealing and consistent images while working effectively across various visual styles without extra customization."
