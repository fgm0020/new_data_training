type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings,https://ICML.cc//virtual/2025/poster/46602,"Yilin Ye, Junchao Huang, Xingchen ZENG, Jiazhi Xia, Wei Zeng","Cross-modal embeddings form the foundation for multi-modal models. However, visualization methods for interpreting cross-modal embeddings have been primarily confined to traditional dimensionality reduction (DR) techniques like PCA and t-SNE. These DR methods primarily focus on feature distributions within a single modality, whilst failing to incorporate metrics (e.g., CLIPScore) across multiple modalities. This paper introduces AKRMap, a new DR technique designed to visualize cross-modal embeddings metric with enhanced accuracy by learning kernel regression of the metric landscape in the projection space. Specifically, AKRMap constructs a supervised projection network guided by a post-projection kernel regression loss, and employs adaptive generalized kernels that can be jointly optimized with the projection. This approach enables AKRMap to efficiently generate visualizations that capture complex metric distributions, while also supporting interactive features such as zoom and overlay for deeper exploration. Quantitative experiments demonstrate that AKRMap outperforms existing DR methods in generating more accurate and trustworthy visualizations. We further showcase the effectiveness of AKRMap in visualizing and comparing cross-modal embeddings for text-to-image models. Code and demo are available at https://github.com/yilinye/AKRMap.","An exponentially growing volume of images have been generated by AI with simple text prompt inputs, yet users of generative AI platforms like Midjourney or WebUI can only see a few images and prompts at a time on the interface, without knowing how well the model generates on a larger prompt corpus. We would like to provide a way for users to directly see and explore a large number of generated images together with their prompts through visualization.In our paper, we develop a new visualization method called AKRMap, which shows each pair of prompt and generated image as a point in a dense 2D scatterplot, and renders another contour/heat map to reveal the global distribution of a metric that reflects how well the generated image matches the prompt. Our visualization can be directly displayed in computational notebook and it also supports interactive features such as zooming.We conduct experiments to show that our visualization method can help explore large generated dataset of up to half a million images and compare the generated images of different AI models. We release our visualization tool to support transparent evaluation of generated data in a large scale."
Poster,A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks,https://ICML.cc//virtual/2025/poster/45715,"Thomas Schmied, Thomas Adler, Vihang Patil, Maximilian Beck, Korbinian Pöppel, Johannes Brandstetter, Günter Klambauer, Razvan Pascanu, Sepp Hochreiter","In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which results in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.","Existing large action models based on the Transformer architecture can be impractical for real-time applications, such as robotics, because they are computationally costly when being deployed. Recently, modern recurrent architectures have been introduced, which are more efficient. In this work, we study the aptitude of these modern recurrent architectures for large action models. To show this, we conduct experiments on 432 tasks from 6 domains, including simulated robotics environments and video games (such as Atari). We find that our approach compares favorably to Transformers in terms of performance and speed. Therefore, modern recurrent architectures may be a practical alternative for real world applications such as robotics."
Poster,Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery,https://ICML.cc//virtual/2025/poster/43481,"Pratinav Seth, Michelle Lin, BREFO YAW, Jade Boutot, Mary Kang, David Rolnick","Millions of abandoned oil and gas wells are scattered across the world, leaching methane into the atmosphere and toxic compounds into the groundwater. Many of these locations are unknown, preventing the wells from being plugged and their polluting effects averted. Remote sensing is a relatively unexplored tool for pinpointing abandoned wells at scale. We introduce the first large-scale Benchmark dataset for this problem, leveraging high-resolution multi-spectral satellite imagery from Planet Labs. Our curated Dataset comprises over 213,000 wells (abandoned, suspended, and active) from Alberta, a region with especially high well density, sourced from the Alberta Energy Regulator and verified by domain experts. We evaluate baseline algorithms for well detection and segmentation, showing the promise of computer vision approaches but also significant room for improvement.","Millions of abandoned oil and gas wells worldwide leak potent methane and other toxins, yet many are unmapped, hampering cleanup. To help solve this, we present the Alberta Wells Dataset, the first large-scale machine learning benchmark specifically designed for pinpointing these onshore wells – whether abandoned, suspended, or active. Our dataset offers over 200,000 well instances from Alberta, Canada, paired with high-resolution satellite imagery, aiming to drive breakthroughs in identifying these sites. We've framed the challenge for AI as detecting well locations and outlining their exact shapes, and our initial tests show promise but also the need for more advanced methods. By providing this benchmark, we enable the research community to develop more scalable and accurate AI techniques. The ultimate impact is to significantly reduce environmental harm: improved AI, built using this dataset, can help locate even unrecorded wells and identify high-emission sites that urgently need remediation. This directly contributes to mitigating climate change by helping stop potent methane leaks from these often hidden sources, paving the way for cleaner air and protected groundwater."
Poster,A Lens into Interpretable Transformer Mistakes via Semantic Dependency,https://ICML.cc//virtual/2025/poster/46303,"Ruo-Jing Dong, Yu Yao, Bo Han, Tongliang Liu","Semantic Dependency refers to the relationship between words in a sentence where the meaning of one word depends on another, which is important for natural language understanding.In this paper, we investigate the role of semantic dependencies in answering questions for transformer models, which is achieved by analyzing how token values shift in response to changes in semantics.Through extensive experiments on models including the BERT series, GPT, and LLaMA, we uncover the following key findings:1). Most tokens primarily retain their original semantic information even as they propagate through multiple layers.2). Models can encode truthful semantic dependencies in tokens in the final layer.3). Mistakes in model answers often stem from specific tokens encoded with incorrect semantic dependencies. Furthermore, we found that addressing the incorrectness by directly adjusting parameters is challenging because the same parameters can encode both correct and incorrect semantic dependencies depending on the context.Our findings provide insights into the causes of incorrect information generation in transformers and help the future development of robust and reliable models.","Semantic dependency is vital for computers to understand language. It refers to the relationship between words in a sentence, where the meaning of one word depends on another. In this study, we explore how popular language models (like BERT, GPT, and LLaMA) understand these dependencies and why they sometimes make mistakes.We discovered that even after processing through many layers, most final-layer tokens still retain their original semantic meanings. Moreover, in the final step, the models do a good job of capturing real and truthful word relationships. However, when these models make mistakes, it is often because some words are encoded with incorrect dependencies. Fixing these mistakes is not as simple as pruning some parameters, because the same parameters can encode both correct and incorrect dependencies depending on the context.Our findings provide insights into the causes of incorrect information generation in models and can help future researchers develop more accurate and reliable AI systems."
Poster,Algorithm Development in Neural Networks: Insights from the Streaming Parity Task,https://ICML.cc//virtual/2025/poster/46526,"Loek van Rossem, Andrew Saxe","Even when massively overparameterized, deep neural networks show a remarkable ability to generalize. Research on this phenomenon has focused on generalization within distribution, via smooth interpolation. Yet in some settings neural networks also learn to extrapolate to data far beyond the bounds of the original training set, sometimes even allowing for infinite generalization, implying that an algorithm capable of solving the task has been learned. Here we undertake a case study of the learning dynamics of recurrent neural networks trained on the streaming parity task in order to develop an effective theory of algorithm development. The streaming parity task is a simple but nonlinear task defined on sequences up to arbitrary length. We show that, with sufficient finite training experience, RNNs exhibit a phase transition to perfect infinite generalization. Using an effective theory for the representational dynamics, we find an implicit representational merger effect which can be interpreted as the construction of a finite automaton that reproduces the task. Overall, our results disclose one mechanism by which neural networks can generalize infinitely from finite training experience.","A child in primary school math class might be told that 3+7=10. The next day when asked what 3+7 equals, they can provide the correct answer, but cannot for instance say what 4+5 equals, since they have not seen this before. One day, after seeing enough examples, something clicks, and now they are now able to give the correct sum for any pair of numbers. When we train neural networks, sometimes we see that a similar ""click"" happens.In this paper, we explain this phenomenon for a simple computational task. We argue that the quickest way for a neural network to learn to reproduce examples is by learning an internal representation of these examples, which ignores details irrelevant to solving the task. This simpler representation will start to resemble the original task, and as a result the network will be able to compute previously unseen examples.Understanding why such a ""click"" can occur in a neural network might help understand why something similar happens in the brain. Additionally, knowing when neural networks really learn to compute is important, as a neural network that learned to replicate examples without understanding the underlying problem might behave unpredictably when it encounters new settings."
Poster,Algorithmic Recourse for Long-Term Improvement,https://ICML.cc//virtual/2025/poster/44455,"Kentaro Kanamori, Ken Kobayashi, Satoshi Hara, Takuya Takagi","Algorithmic recourse aims to provide a recourse action for altering an unfavorable prediction given by a model into a favorable one (e.g., loan approval). In practice, it is also desirable to ensure that an action makes the real-world outcome better (e.g., loan repayment). We call this requirement *improvement*. Unfortunately, existing methods cannot ensure improvement unless we know the true oracle.  To address this issue, we propose a framework for suggesting improvement-oriented actions from a long-term perspective. Specifically, we introduce a new online learning task of assigning actions to a given sequence of instances. We assume that we can observe delayed feedback on whether the past suggested action achieved improvement. Using the feedback, we estimate an action that can achieve improvement for each instance. To solve this task, we propose two approaches based on contextual linear bandit and contextual Bayesian optimization. Experimental results demonstrated that our approaches could assign improvement-oriented actions to more instances than the existing methods.","Machine learning models are increasingly used to make important decisions, like loan approvals. When a person is denied a loan, a bank is required to inform the person of what ""actions"" they can take to get approved in the future. This is where ""algorithmic recourse"" comes in, suggesting changes a person can make to achieve a desired outcome. However, current methods mainly focus on changing the model's prediction, without considering whether these changes actually improve the person's real-world situation (e.g., their ability to repay the loan). This research introduces a new approach that focuses on long-term improvement. The core idea is to learn from past experiences: observing whether suggested actions truly led to better results, and this feedback refines future suggestions. To achieve this, we developed new algorithms to learn which actions are most likely to lead to improvement. We utilize a well-established learning framework called bandit algorithms. By focusing on real-world outcomes and learning from them, our system adapts to provide more effective and reliable guidance. This leads to more helpful decision-making, empowering individuals with actions that genuinely benefit them in the long run."
Poster,Algorithms and Hardness for Active Learning on Graphs,https://ICML.cc//virtual/2025/poster/45966,"Vincent Cohen-Addad, Silvio Lattanzi, Simon Meierhans","We study the offline active learning problem on graphs. In this problem, one seeks to select k vertices whose labels are best suited for predicting the labels of all the other vertices in the graph.Guillory and Bilmes (Guillory & Bilmes, 2009) introduced a natural theoretical model motivated by a label smoothness assumption. Prior to our work, algorithms with theoretical guarantees were only known for restricted graph types such as trees (Cesa-Bianchi et al., 2010) despite the models simplicity. We present the first O(log n)-resource augmented algorithm for general weighted graphs. To complement our algorithm, we show constant hardness of approximation.","In active learning, you get to choose which data points are labeled. A real world example are surveys, where respondents are usually carefully picked rather than selected arbitrarily. We give a new algorithm for selecting which data points to label based on similarity information between data points. For a natural objective, our algorithm always performs at least as well as the optimal algorithm with a more limited budget, and we observe that it outperforms previous algorithms in experiments."
Poster,Algorithms with Calibrated Machine Learning Predictions,https://ICML.cc//virtual/2025/poster/45433,"Judy Hanwen Shen, Ellen Vitercik, Anders Wikum","The field of *algorithms with predictions* incorporates machine learning advice in the design of online algorithms to improve real-world performance. A central consideration is the extent to which predictions can be trusted—while existing approaches often require users to specify an aggregate trust level, modern machine learning models can provide estimates of prediction-level uncertainty. In this paper, we propose *calibration* as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the *ski rental* and *online job scheduling* problems. For ski rental, we design an algorithm that achieves near-optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions.","A line of recent research aims to make smarter decisions in uncertain situations by using machine learning (ML) to forecast the future. Naturally, the reliability of these forecasts plays a key role. Earlier methods typically require the decision maker to quantify their trust in the model's predictions as a whole. However, modern ML models can actually provide much more detailed information, like confidence levels on each individual prediction. One way this is possible is by ensuring that a model's forecasts are *calibrated*, that is, they match the real-world likelihood of outcomes. For example, on days that the model predicts there is a 70% chance of rain, it will actually rain 70% of the time. For two important decision-making scenarios—(1) deciding whether to rent or buy an item and (2) scheduling tasks that need to be completed—we show that access to calibrated forecasts of the future allow the decision-maker to make more reliable choices on average compared to forecasts that are not calibrated."
Poster,Aligned Multi Objective Optimization,https://ICML.cc//virtual/2025/poster/45445,"Yonathan Efroni, Ben Kretzu, Daniel Jiang, Jalaj Bhandari, Zheqing Zhu, Karen Ullrich","To date, the multi-objective optimization literature has mainly focused on conflicting objectives, studying the Pareto front, or requiring users to balance tradeoffs. Yet, in machine learning practice, there are many scenarios where such conflict does not take place. Recent findings from multi-task learning, reinforcement learning, and LLMs training show that diverse related tasks can enhance performance across objectives simultaneously. Despite this evidence, such phenomenon has not been examined from an optimization perspective. This leads to a lack of generic gradient-based methods that can scale to scenarios with a large number of related objectives.  To address this gap, we introduce the Aligned Multi-Objective Optimization framework, propose new algorithms for this setting, and provide theoretical guarantees of its superior performance compared to naive approaches.","It has become common wisdom that adding tasks or using multiple different datasets can improve the performance of a learning algorithm. We introduce the Aligned Multi-Objective Optimization (AMOO) framework that enables us to investigate this phenomenon from an optimization perspective. Specifically, we design a simple adaptation of gradient descent that is guaranteed to give better performance in the presence of multi-objective feedback when the objective functions are aligned."
Poster,Aligning LLMs by Predicting Preferences from User Writing Samples,https://ICML.cc//virtual/2025/poster/44582,"Stéphane Aroca-Ouellette, Natalie Mackraz, Barry-John Theobald, Katherine Metcalf","Accommodating human preferences is essential for creating aligned LLM agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs acting as writing agents to infer a description of user preferences. Agent alignment then comes from conditioning on the inferred preference description. However, existing methods often produce generic preference descriptions that fail to capture the unique and individualized nature of human preferences. This paper introduces PROSE, a method designed to enhance the precision of preference descriptions inferred from user writing samples. PROSE incorporates two key elements: (1) iterative refinement of inferred preferences, and (2) verification of inferred preferences across multiple user writing samples. We evaluate PROSE with several LLMs (i.e., Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an email writing task. We find that PROSE more accurately infers nuanced human preferences, improving the quality of the writing agent's generations over CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly, we demonstrate that ICL and PROSE are complementary methods, and combining them provides up to a 9\% improvement over ICL alone. Code: https://github.com/apple/ml-predict","Accommodating human preferences is essential for creating aligned LLM agents that deliver personalized and effective interactions. While recent methods have shown they can infer user preferences from writing samples, they often generate generic descriptions that miss individual nuances. We introduce PROSE, a method that enhances the precision of these inferred preferences through iterative refinement and cross-sample consistency verification. Tested on summarization and email writing tasks, PROSE improves generation quality by 33% over prior methods. It also complements in-context learning, yielding up to a 9% additional improvement. Code: https://github.com/apple/ml-predict"
