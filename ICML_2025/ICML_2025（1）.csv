type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,$\infty$-Video: A Training-Free Approach to Long Video Understanding via Continuous-Time Memory Consolidation,https://ICML.cc//virtual/2025/poster/44785,"Saúl Santos, António Farinhas, Daniel McNamee, Andre Martins","Current video-language models struggle with long-video understanding due to limited context lengths and reliance on sparse frame subsampling, which often leads to information loss. In this paper, we introduce $\infty$-Video, which is able to process arbitrarily long videos through a continuous-time long-term memory (LTM) consolidation mechanism. Our framework augments video Q-formers by making them able to process unbounded video contexts efficiently and without requiring additional training. Through continuous attention, our approach dynamically allocates higher granularity to the most relevant video segments, forming ""sticky"" memories which evolve over time. Experiments with Video-LLaMA and VideoChat2 demonstrate improved performance in video question-answering tasks, showcasing the potential of continuous-time LTM mechanisms to enable scalable and training-free comprehension of long videos.","The paper titled $\infty$-Video: A Training-Free Approach to Long Video Understanding via Continuous-Time Memory Consolidation introduces a novel method for comprehending long videos without the need for additional training. Traditional video-language models often struggle with lengthy content due to limited context windows and reliance on sparse frame sampling, which can lead to information loss. $\infty$-Video addresses this challenge by implementing a continuous-time long-term memory (LTM) consolidation mechanism. This approach enhances existing video Q-formers, enabling them to process unbounded video contexts efficiently. By employing continuous attention, the model dynamically allocates higher granularity to the most relevant video segments, forming ""sticky"" memories that evolve over time. This mechanism allows the model to retain and focus on critical information throughout the video, akin to human memory consolidation processes. The effectiveness of $\infty$-Video was demonstrated through experiments with models like Video-LLaMA and VideoChat2, showing improved performance in video question-answering tasks. This indicates the potential of continuous-time LTM mechanisms to facilitate scalable and training-free comprehension of long videos."
Poster,$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting,https://ICML.cc//virtual/2025/poster/46346,"Xingjian Wu, Xiangfei Qiu, Hongfan Gao, Jilin Hu, Bin Yang, Chenjuan Guo","Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.","Most generative models excell at short-tem probabilistic time series forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). We focus on developing an efficient one-step generative model to effectively handle the LPTSF.Our paper proposes $K^2$VAE,  an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution."
Poster,$\mathcal{V}ista\mathcal{DPO}$: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models,https://ICML.cc//virtual/2025/poster/45463,"Haojian Huang, Haodong Chen, Shengqiong Wu, Meng Luo, Jinlan Fu, Xinya Du, Hanwang Zhang, Hao Fei","Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce **VistaDPO**, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) **Instance Level**, aligning overall video content with responses; ii) **Temporal Level**, aligning video temporal semantics with event descriptions; and iii) **Perceptive Level**, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct **VistaDPO-7k**, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination.","## What is the problem being addressed?Today’s AI models can watch and “understand” videos, but they often make mistakes that humans would not—such as describing things that never happened in the video, or misunderstanding what’s important. These mistakes are called **hallucinations** and **misalignments**. Fixing this is important for making AI more trustworthy and useful in real-world video applications, like video search, education, or security.## What is the key idea or solution?The authors introduce **VistaDPO**, a new method that teaches AI to better match human preferences when interpreting videos. Unlike previous approaches that only look at the overall video or text, VistaDPO breaks down the problem into three levels:- **Instance Level**: Does the AI’s answer match the whole video’s main idea?- **Temporal Level**: Does the AI understand the order of events and what happens when?- **Perceptive Level**: Does the AI correctly recognize objects and actions at each moment?To train and test their method, the authors also built a new dataset called **VistaDPO-7k**, which contains thousands of video questions with detailed human feedback, including which answers are correct, which are wrong, and where/when important things happen in the video.## Why is this important?By teaching AI to align its understanding with human preferences at multiple levels, VistaDPO helps reduce hallucinations and improves the accuracy of video-based AI systems. This makes AI more reliable for tasks like answering questions about videos, generating captions, or summarizing video content.## What are the main results and evidence?VistaDPO was tested on several standard video AI tasks, such as detecting hallucinations, answering questions, and generating captions. Compared to previous leading methods, VistaDPO significantly reduced errors and hallucinations. For example, it improved performance by over 26% on some benchmarks compared to the best prior models. The authors also showed that VistaDPO is more robust to tricky or adversarial scenarios, where other models might be fooled.## Who might benefit from this research?- **General public**: More reliable video-based AI assistants, better video search and summarization.- **Researchers and developers**: New methods and data for building better video-language models.- **Industries**: Education, entertainment, surveillance, accessibility, and anywhere video understanding is important.## Are there any broader impacts or ethical considerations?The authors highlight that while their work can make AI more robust and trustworthy, it should be used responsibly—especially in sensitive areas like surveillance or automated decision-making. They took care to reduce bias and hallucinations in their dataset and encourage responsible use and further evaluation for fairness."
Poster,$S^2$FGL: Spatial Spectral Federated Graph Learning,https://ICML.cc//virtual/2025/poster/44020,"Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye","Federated Graph Learning (FGL) combines the privacy-preserving capabilities of federated learning (FL) with the strong graph modeling capability of Graph Neural Networks (GNNs). Current research addresses subgraph-FL only from the structural perspective, neglecting the propagation of graph signals on spatial and spectral domains of the structure. From a spatial perspective, subgraph-FL introduces edge disconnections between clients, leading to disruptions in label signals and a degradation in the class knowledge of the global GNN. From a spectral perspective, spectral heterogeneity causes inconsistencies in signal frequencies across subgraphs, which makes local GNNs overfit the local signal propagation schemes. As a result, spectral client drifts occur, undermining global generalizability. To tackle the challenges, we propose a global knowledge repository to mitigate label signal disruption and a frequency alignment to address spectral client drifts. The combination of $\textbf{S}$patial and $\textbf{S}$pectral strategies forms our framework $S^2$FGL. Extensive experiments on multiple datasets demonstrate the superiority of $S^2$FGL. The code is available at https://github.com/Wonder7racer/S2FGL.git.","In many real-world applications like fraud detection or social networks, data is distributed across different sources and cannot be shared due to privacy concerns. Federated Graph Learning (FGL) is a promising solution that trains models across multiple data holders without sharing their data. However, current FGL methods often struggle when the data is fragmented, leading to broken connections and inconsistent signals.Our paper introduces S2FGL, a new method that improves how graph neural networks learn from distributed data. It does this by addressing two key problems: the loss of useful label signals and differences in data frequency patterns across clients. S2FGL uses a shared knowledge base and frequency alignment to help each client learn more general and robust representations.This work provides a more reliable and effective way to train graph models in privacy-sensitive environments, with benefits for fields such as healthcare, finance, and social media analysis."
Poster,$\texttt{I$^2$MoE}$: Interpretable Multimodal Interaction-aware Mixture-of-Experts,https://ICML.cc//virtual/2025/poster/45919,"Jiayi Xin, Sukwon Yun, Jie Peng, Inyoung Choi, Jenna Ballard, Tianlong Chen, Qi Long","Modality fusion is a cornerstone of multimodal learning, enabling information integration from diverse data sources. However, existing approaches are limited by $\textbf{(a)}$ their focus on modality correspondences, which neglects heterogeneous interactions between modalities, and $\textbf{(b)}$ the fact that they output a single multimodal prediction without offering interpretable insights into the multimodal interactions present in the data. In this work, we propose $\texttt{I$^2$MoE}$ ($\underline{I}$nterpretable Multimodal $\underline{I}$nteraction-aware $\underline{M}$ixture-$\underline{o}$f-$\underline{E}$xperts), an end-to-end MoE framework designed to enhance modality fusion by explicitly modeling diverse multimodal interactions, as well as providing interpretation on a local and global level. First, $\texttt{I$^2$MoE}$ utilizes different interaction experts with weakly supervised interaction losses to learn multimodal interactions in a data-driven way. Second, $\texttt{I$^2$MoE}$ deploys a reweighting model that assigns importance scores for the output of each interaction expert, which offers sample-level and dataset-level interpretation. Extensive evaluation of medical and general multimodal datasets shows that $\texttt{I$^2$MoE}$ is flexible enough to be combined with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios. Code is available at https://github.com/Raina-Xin/I2MoE.","Modern artificial intelligence often works with data from multiple sources, like combining medical images, lab results, and patient records to help doctors make better decisions. But today’s AI models usually integrate this information in a “black box” way: they spit out a final answer, but they do not tell us how different pieces of information interact or which ones matter most.We developed a new system called $\texttt{I$^2$MoE}$ (Interpretable Multimodal Interaction-aware Mixture of Experts) that not only improves how AI combines information from different sources, but also explains what’s going on under the hood. Our model uses specialized “experts” that focus on different types of interactions between data sources, such as how lab results and imaging together affect the diagnosis. It then assigns scores to show which expert matters most for each patient diagnosis.We tested $\texttt{I$^2$MoE}$ on both medical and general datasets and found that it improves performance across tasks. More importantly, it helps researchers and practitioners understand the decision-making process involving multiple data sources, making AI systems more transparent and trustworthy."
Poster,3D-LMVIC: Learning-based Multi-View Image Compression with 3D Gaussian Geometric Priors,https://ICML.cc//virtual/2025/poster/46162,"Yujun Huang, Bin Chen, Niu Lian, Xin Wang, Baoyi An, Tao Dai, Shutao Xia","Existing multi-view image compression methods often rely on 2D projection-based similarities between views to estimate disparities. While effective for small disparities, such as those in stereo images, these methods struggle with the more complex disparities encountered in wide-baseline multi-camera systems, commonly found in virtual reality and autonomous driving applications. To address this limitation, we propose 3D-LMVIC, a novel learning-based multi-view image compression framework that leverages 3D Gaussian Splatting to derive geometric priors for accurate disparity estimation. Furthermore, we introduce a depth map compression model to minimize geometric redundancy across views, along with a multi-view sequence ordering strategy based on a defined distance measure between views to enhance correlations between adjacent views. Experimental results demonstrate that 3D-LMVIC achieves superior performance compared to both traditional and learning-based methods. Additionally, it significantly improves disparity estimation accuracy over existing two-view approaches.","Modern applications like virtual reality use many cameras to capture scenes from different angles, producing large amounts of image data. Compressing this data is challenging, especially when camera views are far apart. Our method, 3D-LMVIC, uses 3D scene understanding to better align and compress images from different views. It achieves higher compression efficiency and quality by estimating 3D geometry and reordering the views smartly. This helps reduce storage and transmission costs in real-world 3D applications."
Poster,3D Question Answering via only 2D Vision-Language Models,https://ICML.cc//virtual/2025/poster/45722,"FENGYUN WANG, Sicheng Yu, Jiawei Wu, Jinhui Tang, Hanwang Zhang, Qianru Sun","Large vision-language models (LVLMs) have significantly advanced numerous fields. In this work, we explore how to harness their potential to address 3D scene understanding tasks, using 3D question answering (3D-QA) as a representative example. Due to the limited training data in 3D, we do not train LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a 3D point cloud and feed them into 2D models to answer a given question. When the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters the most. We propose cdViews, a novel approach to automatically selecting critical and diverse Views for 3D-QA. cdViews consists of two key components: viewSelector prioritizing critical views based on their potential to provide answer-specific information, and viewNMS enhancing diversity by removing redundant views based on spatial overlap. We evaluate cdViews on the widely-used ScanQA and SQA benchmarks, demonstrating that it achieves state-of-the-art performance in 3D-QA while relying solely on 2D models without fine-tuning. These findings support our belief that 2D LVLMs are currently the most effective alternative (of the resource-intensive 3D LVLMs) for addressing 3D tasks.","Understanding 3D scenes is crucial in AI, but training 3D models is costly and data-limited. We take a different approach by using only 2D images and a well-pretrained 2D vision-language model in a zero-shot inference manner. We introduce cdViews, a lightweight view selection strategy that identifies the most critical and diverse views for each question. This enables the model to process views that are most likely to contain the visual information necessary to answer the question, without relying on any 3D-specific training or feature alignment. Our method achieves state-of-the-art performance on two standard 3D question answering benchmarks, demonstrating the effectiveness of 2D models for efficient and scalable 3D scene understanding."
Poster,AAAR-1.0: Assessing AI’s Potential to Assist Research,https://ICML.cc//virtual/2025/poster/45287,"Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, Congying Xia, Lifu Huang, Wenpeng Yin","Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; and (iii) PaperWeakness, identifying weaknesses in paper submissions. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will release the AAAR-1.0 and keep iterating it to new versions.","Artificial intelligence has shown great promise in helping with routine tasks like writing emails or answering questions, but its usefulness in supporting the work of scientific researchers remains unclear. For example, researchers often need to check the accuracy of equations, design effective experiments, and spot weaknesses in scientific papers—tasks that require in-depth expertise and careful reasoning. Our work introduces AAAR-1.0, a new benchmark designed to see how well modern AI language models, such as ChatGPT and its peers, can handle these demanding research duties. AAAR-1.0 focuses on three key activities that scientists regularly do: evaluating equations in papers, coming up with plans for experiments, and giving useful feedback on scientific drafts. We tested various AI models and found that, while they can sometimes offer helpful or creative ideas, they still struggle with the level of accuracy and insight needed for advanced research. Our benchmark aims to guide improvements in AI tools and help researchers use them thoughtfully, seeing them as supportive assistants rather than replacements."
Poster,A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints,https://ICML.cc//virtual/2025/poster/44536,"Michael Munn, Susan Wei","Recent advances in artificial intelligence have been fueled by the development of foundation models such as BERT, GPT, T5, and VisionTransformers. These models are first pretrained on vast and diverse datasets and then adapted to specific downstream tasks, often with significantly less data. However, the mechanisms behind the success of this ubiquitous pretrain-then-adapt paradigm remain underexplored, particularly the characteristics of pretraining checkpoints that enhance downstream adaptation. We introduce a Bayesian model selection criterion, called the downstream free energy, which quantifies a checkpoint’s adaptability by measuring the concentration of nearby favorable parameters for a downstream task. We demonstrate that this Bayesian model selection criterion can be effectively implemented without access to the downstream data or prior knowledge of the downstream task. Furthermore, we provide empirical evidence that the criterion reliably correlates with improved fine-tuning performance, offering a principled approach to predicting model adaptability.","Foundation models, the AI systems behind tools like ChatGPT and image generators, are typically trained on massive datasets and then fine-tuned for specific applications. However, selecting the best version (checkpoint) of the initial model to use is a significant challenge, especially without knowledge of or access to future task data. This research introduces ""downstream free energy,"" a Bayesian statistical measure, to predict a checkpoint's adaptability, and proposes using ""pretraining free energy"" which is calculable using only the initial training data. This approach offers a principled way to choose the best checkpoint, leading to AI models that adapt more effectively to diverse applications when future task details are limited."
Poster,Ab Initio Nonparametric Variable Selection for Scalable Symbolic Regression with Large $p$,https://ICML.cc//virtual/2025/poster/46212,"Shengbin Ye, Meng Li","Symbolic regression (SR) is a powerful technique for discovering symbolic expressions that characterize nonlinear relationships in data, gaining increasing attention for its interpretability, compactness, and robustness. However, existing SR methods do not scale to datasets with a large number of input variables (referred to as extreme-scale SR), which is common in modern scientific applications. This ""large $p$'' setting, often accompanied by measurement error, leads to slow performance of SR methods and overly complex expressions that are difficult to interpret. To address this scalability challenge, we propose a method called PAN+SR, which combines a key idea of ab initio nonparametric variable selection with SR to efficiently pre-screen large input spaces and reduce search complexity while maintaining accuracy. The use of nonparametric methods eliminates model misspecification, supporting a strategy called parametric-assisted nonparametric (PAN). We also extend SRBench, an open-source benchmarking platform, by incorporating high-dimensional regression problems with various signal-to-noise ratios. Our results demonstrate that PAN+SR consistently enhances the performance of 19 contemporary SR methods, enabling several to achieve state-of-the-art performance on these challenging datasets.","Scientists often want to understand how different factors relate to each other by finding clear, math-based rules in their data. Symbolic regression (SR) is a technique that does exactly this—it searches for equations that explain patterns, which can lead to new scientific insights. But SR struggles when there are too many variables, which is common in fields like biology, physics, or climate science. Too many inputs make the search very slow and the resulting equations hard to understand. Our method, called PAN+SR, helps SR focus on just the most important variables before trying to find equations. It does this using a model-free filtering step that’s flexible and avoids strong assumptions. We also built a new set of benchmark problems that reflect the messy, high-dimensional data real scientists deal with. PAN+SR improves the performance of many existing SR tools and helps them find better, simpler equations more quickly. This makes it easier for researchers to use symbolic regression in real-world science, where both accuracy and interpretability matter."
