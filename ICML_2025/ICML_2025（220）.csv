type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Permutation Equivariant Neural Networks for Symmetric Tensors,https://ICML.cc//virtual/2025/poster/44145,Edward Pearce-Crump,"Incorporating permutation equivariance into neural networks has proven to be useful in ensuring that models respect symmetries that exist indata. Symmetric tensors, which naturally appearin statistics, machine learning, and graph theory,are essential for many applications in physics,chemistry, and materials science, amongst others. However, existing research on permutationequivariant models has not explored symmetrictensors as inputs, and most prior work on learningfrom these tensors has focused on equivarianceto Euclidean groups. In this paper, we presenttwo different characterisations of all linear permutation equivariant functions between symmetricpower spaces of $\mathbb{R}^{n}$. We show on two tasks thatthese functions are highly data efficient comparedto standard MLPs and have potential to generalisewell to symmetric tensors of different sizes.","Many scientific fields use data in the form of symmetric tensors, but current machine learning models do not fully take advantage of their natural symmetries. This limits how well models can learn from such data.We developed a new way to design models that respect these symmetries by fully characterising all linear functions between symmetric tensors that are equivariant under permutations. To make this characterisation practical, we introduced a method that represents these functions without needing large amounts of memory to store weight matrices, making it adaptable to symmetric tensors of different sizes.Having been tested on two example problems, our method learns from less data and generalises better than standard neural networks to data that can be represented in the form of symmetric tensors. This opens up new possibilities for applying machine learning to scientific domains where symmetric tensors play a central role, such as physics, chemistry, and materials science."
Poster,Permutation-Free High-Order Interaction Tests,https://ICML.cc//virtual/2025/poster/44457,"Zhaolu Liu, Robert Peach, Mauricio Barahona","Kernel-based hypothesis tests offer a flexible, non-parametric tool to detect high-order interactions in multivariate data, beyond pairwise relationships. Yet the scalability of such tests is limited by the computationally demanding permutation schemes used to generate null approximations. Here we introduce a family of permutation-free high-order tests for joint independence and partial factorisations of $d$ variables. Our tests eliminate the need for permutation-based approximations by leveraging V-statistics and a novel cross-centring technique to yield test statistics with a standard normal limiting distribution under the null. We present implementations of the tests and showcase their efficacy and scalability through synthetic datasets. We also show applications inspired by causal discovery and feature selection, which highlight both the importance of high-order interactions in data and the need for efficient computational methods.","How can we understand complex relationships between multiple variables—beyond simple pairwise ones like correlation? These higher-order interactions are common in fields ranging from neuroscience to economics. Traditionally, detecting them has required computationally intensive permutation tests to check whether observed patterns are statistically meaningful.In our work, we introduce a much faster alternative using two mathematical tools: V-statistics and a novel cross-centring technique. This makes the tests permutation-free, resulting in speed-ups of over 100× without sacrificing accuracy.We demonstrate how this method can be used to uncover causal relationships and identify important features in machine learning quickly and accurately. This far more efficient testing method opens up new possibilities for exploring complex interactions in areas like financial markets, gene interactions, and brain activity."
Poster,Persistent Topological Features in Large Language Models,https://ICML.cc//virtual/2025/poster/43958,"Yuri Gardinazzi, Karthik Viswanathan, Giada Panerai, Alessio Ansuini, Alberto Cazzaniga, Matteo Biagetti","Understanding the decision-making processes of large language models is critical given their widespread applications. To achieve this, we aim to connect a formal mathematical framework—zigzag persistence from topological data analysis —with practical and easily applicable algorithms. Zigzag persistence is particularly effective for characterizing data as it dynamically transforms across model layers. Within this framework, we introduce topological descriptors that measure how topological features, $p$-dimensional holes, persist and evolve throughout the layers. Unlike methods that assess each layer individually and then aggregate the results, our approach directly tracks the full evolutionary path of these features. This offers a statistical perspective on how prompts are rearranged and their relative positions changed in the representation space, providing insights into the system’s operation as an integrated whole. To demonstrate the expressivity and applicability of our framework, we highlight how sensitive these descriptors are to different models and a variety of datasets. As a showcase application to a downstream task, we use zigzag persistence to establish a criterion for layer pruning, achieving results comparable to state-of-the-art methods while preserving the system-level perspective.","Large language models (LLMs), like those behind popular AI chatbots, can generate impressively human-like responses to text prompts, but how they actually process information inside remains mostly unknown. This lack of transparency, given the widespread use of these models in more important tasks, raises serious concerns in the scientific, and broader, community. Researchers also want to make these large models smaller and less resource-intensive, without losing their effectiveness.To tackle both issues, our work brings in tools from mathematics, specifically “topological data analysis,” which is good at describing complex shapes and relationships in data. We apply a mathematical approach called zigzag persistence to track how information evolves across each layer of an LLM, instead of looking at each layer separately. This lets us measure how groups of data points change and interact through the whole model. With this method, we’re able to spot different “phases” in how the model processes language inputs, and we can use our findings to suggest which model layers could be removed (pruned) to compress the model—without major performance loss.Our approach works on different models and datasets, offering a new window into how LLMs actually work, and paving the way to safer, more efficient AI systems."
Poster,PertEval-scFM: Benchmarking Single-Cell Foundation Models for Perturbation Effect Prediction,https://ICML.cc//virtual/2025/poster/43799,"Aaron Wenteler, Martina Occhetta, Nikhil Branson, Victor Curean, Magdalena Huebner, William Dee, William Connell, Siu Chung, Alex Hawkins-Hooker, Yasha Ektefaie, César Miguel Valdez Córdova, Amaya Gallagher-Syed","*In silico* modeling of transcriptional responses to perturbations is crucial for advancing our understanding of cellular processes and disease mechanisms. We present PertEval-scFM, a standardized framework designed to evaluate models for perturbation effect prediction. We apply PertEval-scFM to benchmark zero-shot single-cell foundation model (scFM) embeddings against baseline models to assess whether these contextualized representations enhance perturbation effect prediction. Our results show that scFM embeddings offer limited improvement over simple baseline models in the zero-shot setting, particularly under distribution shift. Overall, this study provides a systematic evaluation of zero-shot scFM embeddings for perturbation effect prediction, highlighting the challenges of this task and the limitations of current-generation scFMs. Our findings underscore the need for specialized models and high-quality datasets that capture a broader range of cellular states. Source code and documentation can be found at: https://github.com/aaronwtr/PertEval.","To develop new treatments, scientists study how cells respond when specific genes are changed. This is called a genetic perturbation experiment. Measuring these effects one by one is costly and slow, so researchers are exploring whether AI can predict them instead.Large machine learning models called single-cell foundation models (scFMs) are trained on massive datasets of RNA data. The hope is that they learn general principles of cell behavior to make informative predictions about cellular states. Our work introduces PertEval, a benchmark that tests whether the zero-shot embeddings produced by scFMs contain meaningful information for predicting perturbation effects. Given a pair of cells — one perturbed and one unperturbed — a simple model uses representations produced by the scFMs to predict how the cell changed.We evaluate five leading scFMs and find that, in the zero-shot setting, they often fail to accurately predict perturbation effects. Most models do not outperform simple baselines, particularly when evaluated on strong or atypical perturbations. PertEval offers a standard and rigorous way to test how well these models perform, highlighting the limitations of current approaches and helping guide the development of more robust tools."
Poster,Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44333,"Chi Zhang, Ziying Jia, George Atia, Sihong He, Yue Wang","Transfer reinforcement learning aims to derive a near-optimal policy for a target environment with limited data by leveraging abundant data from related source domains. However, it faces two key challenges: the lack of performance guarantees for the transferred policy, which can lead to undesired actions, and the risk of negative transfer when multiple source domains are involved. We propose a novel framework based on the pessimism principle, which constructs and optimizes a conservative estimation of the target domain’s performance. Our framework effectively addresses the two challenges by providing an optimized lower bound on target performance, ensuring safe and reliable decisions, and by exhibiting monotonic improvement with respect to the quality of the source domains, thereby avoiding negative transfer. We construct two types of conservative estimations, rigorously characterize their effectiveness, and develop efficient distributed algorithms with convergence guarantees. Our framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning.","We address a key challenge in reinforcement learning: how to transfer knowledge from multiple known source environments to an unseen target environment. Our method is grounded in the pessimism principle, enabling agents to make conservative but effective decisions in unknown environments.We construct conservative performance estimators that serve as lower bounds, offering provable performance guarantees. To further avoid negative transfer, we selectively aggregate only the most relevant source knowledge. More importantly, our framework supports a distributed and privacy-preserving setup, where each local agent shares only Q-function updates—not raw data—ensuring scalability and communication efficiency.This framework enables robust, zero-shot transfer learning with theoretical guarantees, paving the way for safer reinforcement learning deployment in real-world applications like robotics and autonomous driving."
Poster,PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting for Novel View Synthesis,https://ICML.cc//virtual/2025/poster/45052,"Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jisang Han, Jiaolong Yang, Chong Luo, Seungryong Kim","We consider the problem of novel view synthesis from unposed images in a single feed-forward. Our framework capitalizes on fast speed, scalability, and high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where we further extend it to offer a practical solution that relaxes common assumptions such as dense image views, accurate camera poses, and substantial image overlaps. We achieve this through identifying and addressing unique challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians across different views induce noisy or sparse gradients that destabilize training and hinder convergence, especially when above assumptions are not met. To mitigate this, we employ pre-trained monocular depth estimation and visual correspondence models to achieve coarse alignments of 3D Gaussians. We then introduce lightweight, learnable modules to refine depth and pose estimates from the coarse alignments, improving the quality of 3D reconstruction and novel view synthesis. Furthermore, the refined estimates are leveraged to estimate geometry confidence scores, which assess the reliability of 3D Gaussian centers and condition the prediction of Gaussian parameters accordingly. Extensive evaluations on large-scale real-world datasets demonstrate that PF3plat sets a new state-of-the-art across all benchmarks, supported by comprehensive ablation studies validating our design choices. We will make the code and weights publicly available.","Reconstructing 3D scenes and creating realistic novel views from just a few images is challenging, especially when the images lack precise camera positions. Existing methods either require large amounts of densely captured images (100+) or accurate camera measurements, making them impractical for casual use. We introduce PF3plat, a new approach that quickly generates high-quality 3D views without precise camera positions, even from sparse images captured from wide baselines. By identifying the existing limitations of 3D Gaussian Splatting, our method leverages advanced models initially trained to estimate depth and image correspondences, then fine-tunes these predictions with lightweight adjustments. By further assessing the reliability of these predictions, we ensure stable, accurate 3D reconstruction. Extensive experiments confirm PF3plat significantly outperforms previous techniques in speed, accuracy, and image quality across diverse indoor and outdoor settings. Our research brings us closer to making high-quality 3D scene capture accessible to everyone, even with just a few unposed photos from everyday cameras or smartphones."
Poster,Pfeife: Automatic Pipeline Parallelism for PyTorch,https://ICML.cc//virtual/2025/poster/45884,"Ho Young Jhoo, Chung-Kil Hur, Nuno P. Lopes","The memory requirements of machine learning (ML) models has been growing quickly. However, the memory capacity of GPUs has not kept pace. Despite significant research on reducing the memory usage of ML models, the larger models do not fit in a single device. A popular solution to the memory capacity issue is to use multiple devices in parallel. In this paper, we focus on a particular form of parallelism called pipelining, as it offers a good balance between cost and performance for many ML models. We present Pfeife, the first tool that integrates with PyTorch to provide automatic pipelining of ML models. Pfeife intercepts the execution of models and parallelizes them transparently, requiring no manual work. We show that Pfeife can execute large models that would otherwise not run due to not fitting in a single device. Moreover, Pfeife can pipeline non-sequential models such as Stable Diffusion, which are not supported by existing pipelining parallelism tools. Pfeife outperforms state-of-the-art tools by up to 22%.","Modern AI models contain billions of parameters, but current hardware devices don't have enough memory to store all of them. To overcome this limitation, AI researchers distribute these large models across multiple devices.Pipelining is one effective solution for this distribution challenge. It divides an AI model into several execution stages and assigns these stages to different devices. The input data then flows through these stages across devices like items on a conveyor belt.However, automatic generation of a pipeline for AI model training has been challenging because determining the optimal model partitioning for maximum training speed is complex.We have developed Pfeife, a tool that automatically slices large models and facilitates training through pipelining. Our research demonstrates that Pfeife can successfully train complex models that previously could not be automatically pipelined, while also operating up to 22% faster than existing pipelining tools."
Poster,PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation,https://ICML.cc//virtual/2025/poster/46018,"Albert Gong, Kamilė Stankevičiūtė, Chao Wan, Anmol Kabra, Raphael Thesmar, Johann Lee, Julius Klenke, Carla Gomes, Kilian Weinberger","High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities, respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities.","High-quality datasets are essential for comprehensive evaluation of large language models (LLMs), such as ChatGPT and Claude. However, as models learn from the whole of internet’s data, there is a risk that the models get exposed to such evaluation datasets and subsequently “cheat” by providing memorized answers instead of actually going through the “thinking” process.We present a methodology to programmatically generate unique datasets of articles and question-answer pairs. The nature of this process ensures that every time evaluation is needed, we can generate a new dataset instance that the model has not seen before (and therefore finds it hard to “cheat” on), while at the same time ensuring that the questions are difficult and the true answers are consistent with the data. We show that, as the number of documents and the question difficulty increase, frontier LLMs struggle to find the correct answers, demonstrating the limitations in their abilities to reason and find relevant information among a large set of documents."
Poster,Phase and Amplitude-aware Prompting for Enhancing Adversarial Robustness,https://ICML.cc//virtual/2025/poster/46201,"Yibo Xu, Dawei Zhou, Decheng Liu, Nannan Wang","Deep neural networks are found to be vulnerable to adversarial perturbations. The prompt-based defense has been increasingly studied due to its high efficiency. However, existing prompt-based defenses mainly exploited mixed prompt patterns, where critical patterns closely related to object semantics lack sufficient focus. The phase and amplitude spectra have been proven to be highly related to specific semantic patterns and crucial for robustness. To this end, in this paper, we propose a Phase and Amplitude-aware Prompting (PAP) defense. Specifically, we construct phase-level and amplitude-level prompts for each class, and adjust weights for prompting according to the model's robust performance under these prompts during training. During testing, we select prompts for each image using its predicted label to obtain the prompted image, which is inputted to the model to get the final prediction. Experimental results demonstrate the effectiveness of our method.","Deep learning models are often vulnerable to tiny changes in input images, where these changes are so small that humans can’t see them, but can still fool the model. Some recent defenses try to help make better predictions using extra “prompts”, which perform small adjustments to inputs. But most existing methods focus on mixed semantic information, missing the ones most critical for understanding what is in an image.Our method focuses on phase and amplitude spectra, which carry important semantic information for recognizing objects. We build unique prompts for each class using these spectra. During training, we automatically adjust the prompts based on how well they help the model stay accurate under attacks. At test time, the model chooses the right prompt based on what it sees for each instance.By focusing on the ""right"" parts of an image, this method helps the model make more robust predictions, making AI systems more reliable."
Poster,Phase transitions for the existence of unregularized M-estimators in single index models,https://ICML.cc//virtual/2025/poster/43592,"Takuya Koriyama, Pierre C Bellec","This paper studies phase transitions for the existence of unregularized M-estimators under proportional asymptotics where the sample size $n$ and feature dimension $p$ grow proportionally with $n/p \to \delta \in (1, \infty)$. We study the existence of M-estimators in single-index models where the response $y_i$ depends on covariates $x_i \sim N(0, I_p)$ through an unknown index ${w} \in \mathbb{R}^p$ and an unknown link function. An explicit expression is derived for the critical threshold $\delta_\infty$ that determines the phase transition for the existence of the M-estimator, generalizing the results of Candés & Sur (2020) for binary logistic regression to other single-index models.Furthermore, we investigate the existence of a solution to the nonlinear system of equations governing the asymptotic behavior of the M-estimator when it exists. The existence of solution to this system for $\delta > \delta_\infty$ remains largely unproven outside the global null in binary logistic regression. We address this gap with a proof that the system admits a solution if and only if $\delta > \delta_\infty$, providing a comprehensive theoretical foundation for proportional asymptotic results that require as a prerequisite the existence of a solution to the system.","This paper explores when certain statistical tools, called M-estimators, can be reliably used in high-dimensional settings—specifically, when the number of data points $n$ and the number of variables $p$ increase at the same rate. We focus on models where the outcome depends on many variables in an unknown way. We identify a precise threshold of the ratio $n/p$ that determines whether an M-estimator exists, extending previous results from specific cases like logistic regression to a broader class of models. We also prove a key mathematical result: a system of equations describing the estimator’s asymptotic behavior has a solution if and only if this threshold is exceeded. This resolves an open question and provides a stronger foundation for studying statistical methods in high dimensions."
