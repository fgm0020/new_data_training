type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,KoopSTD: Reliable Similarity Analysis between Dynamical Systems via Approximating Koopman Spectrum with Timescale Decoupling,https://ICML.cc//virtual/2025/poster/46598,"Shimin Zhang, Ziyuan Ye, Yinsong Yan, Zeyang Song, Yujie Wu, Jibin Wu","Determining the similarity between dynamical systems remains a long-standing challenge in both machine learning and neuroscience. Recent works based on Koopman operator theory have proven effective in analyzing dynamical similarity by examining discrepancies in the Koopman spectrum. Nevertheless, existing similarity metrics can be severely constrained when systems exhibit complex nonlinear behaviors across multiple temporal scales. In this work, we propose **KoopSTD**, a dynamical similarity measurement framework that precisely characterizes the underlying dynamics by approximating the Koopman spectrum with explicit timescale decoupling and spectral residual control. We show that KoopSTD maintains invariance under several common representation-space transformations, which ensures robust measurements across different coordinate systems. Our extensive experiments on physical and neural systems validate the effectiveness, scalability, and robustness of KoopSTD compared to existing similarity metrics. We also apply KoopSTD to explore two open-ended research questions in neuroscience and large language models, highlighting its potential to facilitate future scientific and engineering discoveries. Code is available at [link](https://github.com/ZhangShimin1/KoopSTD).","How can we tell if two complex dynamical systems behave similarly over time? This question is vital in fields such as neuroscience, where researchers compare different brain regions, and machine learning, where the goal is to understand how various AI models process information. Traditional methods often fall short when systems behave in nonlinear ways or operate across multiple timescales, including both short-term reactions and long-term trends.We developed KoopSTD, a new tool that captures the essence of how systems evolve by analyzing their behavior in a mathematical space called the Koopman spectrum. This approach allows us to disentangle patterns occurring at different timescales and filter out irrelevant modes that might otherwise lead to misleading conclusions. It is similar to breaking down a complex musical piece into distinct notes and rhythms to better understand its structure.We applied KoopSTD to a wide range of dynamical systems, including classical physical simulations, brain activity, and large language models, and found that it reliably identifies underlying similarities in dynamics. Remarkably, it revealed that brain regions with correlated anatomical features also exhibit similar functional patterns, and that larger AI models tend to have more stable internal dynamics. KoopSTD offers a powerful new way to explore and compare dynamic systems across science and engineering."
Poster,KV Shifting Attention Enhances Language Modeling,https://ICML.cc//virtual/2025/poster/45582,"Mingyu Xu, Bingning Wang, Weipeng Chen","Current large language models (LLMs) predominantly rely on decode-only transformer architectures, which exhibit exceptional in-context learning (ICL) capabilities. It is widely acknowledged that the cornerstone of their ICL ability lies in the induction heads mechanism, which necessitates at least two layers of attention. To more effectively harness the model's induction capabilities, we revisit the induction heads mechanism and provide theoretical proof that KV shifting attention reduces the model's dependency on the depth and width of the induction heads mechanism. Our experimental results confirm that KV shifting attention enhances the learning of induction heads and improves language modeling performance. This leads to superior performance or accelerated convergence, spanning from toy models to pre-trained models with over 10 billion parameters.","In order to better understand the contextual learning ability of LLM, we conducted research on its key mechanism, induction heads. And in order to unlock a better mechanism for in production heads, a convolution operation was proposed for the key and value of attention. Experimental results have shown that the addition of temporal convolution can effectively improve the language modeling ability of the model."
Poster,KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference,https://ICML.cc//virtual/2025/poster/43487,"Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Huiling Zhen, Yiwu Yao, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan","KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness.However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference.To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.","We theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. We can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks."
Poster,L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning,https://ICML.cc//virtual/2025/poster/44755,"Xiang Zhang, Run He, Chen Jiao, Di Fang, Ming Li, Ziqian Zeng, Cen Chen, HUIPING ZHUANG","Class-incremental learning (CIL) enables models to learn new classes continually without forgetting previously acquired knowledge. Multi-label CIL (MLCIL) extends CIL to a real-world scenario where each sample may belong to multiple classes, introducing several challenges: label absence, which leads to incomplete historical information due to missing labels, and class imbalance, which results in the model bias toward majority classes. To address these challenges, we propose Label-Augmented Analytic Adaptation (L3A), an exemplar-free approach without storing past samples. L3A integrates two key modules. The pseudo-label (PL) module implements label augmentation by generating pseudo-labels for current phase samples, addressing the label absence problem. The weighted analytic classifier (WAC) derives a closed-form solution for neural networks. It introduces sample-specific weights to adaptively balance the class contribution and mitigate class imbalance. Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms existing methods in MLCIL tasks. Our code is available at https://github.com/scut-zx/L3A.","Humans learn throughout their lives, building on past experiences without forgetting earlier knowledge. The class-incremental learning (CIL) research aims to teach AI models to do the same, to keep learning new classes in samples over time without needing to retrain from scratch.In many real-world scenarios, many samples contain multiple classes. This leads to a more complex challenge called multi-label class-incremental learning (MLCIL), where the AI models must learn to recognize many classes in a sample, even when not all of them are labeled during training. Over time, the model should be able to recognize all previously seen classes.We propose a new method called L3A, which learns new classes without forgetting the old ones and without needing to store past data. We use a technique to generate missing labels and assign different importance to each training sample, so the system doesn’t favor only the most common classes. This results in more accurate and fairer AI models that can continuously learn in complex environments."
Poster,Label Distribution Propagation-based Label Completion for Crowdsourcing,https://ICML.cc//virtual/2025/poster/46263,"Tong Wu, Liangxiao Jiang, Wenjun Zhang, Chaoqun Li","In real-world crowdsourcing scenarios, most workers often annotate a few instances only, which results in a significantly sparse crowdsourced label matrix and subsequently harms the performance of label integration algorithms. Recent work called worker similarity-based label completion (WSLC) has been proven to be an effective algorithm to addressing this issue. However, WSLC considers solely the correlation of the labels annotated by different workers on per individual instance while totally ignoring the correlation of the labels annotated by different workers among similar instances. To fill this gap, we propose a novel label distribution propagation-based label completion (LDPLC) algorithm. At first, we use worker similarity weighted majority voting to initialize a label distribution for each missing label. Then, we design a label distribution propagation algorithm to enable each missing label of each instance to iteratively absorb its neighbors’ label distributions. Finally, we complete each missing label based on its converged label distribution. Experimental results on both real-world and simulated crowdsourced datasets show that LDPLC significantly outperforms WSLC in enhancing the performance of label integration algorithms. Our codes and datasets are available at https://github.com/jiangliangxiao/LDPLC.","Crowdsourcing is an efficient and cost-effective method to rapidly obtain large volumes of annotated data. In real-world Crowdsourcing scenarios, most workers only annotate a small number of tasks, which leaves large missing labels in the data. This subsequently harms the performance of label integration algorithms (infer the true labels of tasks). A recent algorithm called worker similarity-based label completion (WSLC) completes missing labels by observing how different workers annotate the same tasks. However, we found that relying solely on this single perspective is insufficient to fully utilize the information in data. To address this, the proposed label Distribution propagation-based label completion (LDPLC) introduces a new perspective by observing how the same worker annotate different tasks. The new perspective allows each missing label to not only absorb the information from similar workers on the corresponding task but also absorb the information from the corresponding worker across similar tasks. Our finding have implications for the preprocessing of data, which can enhance the performance of label integration algorithms."
Poster,LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models,https://ICML.cc//virtual/2025/poster/45236,"Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, Yingyan (Celine) Lin","Recent advancements in Large Language Models (LLMs) have spurred interest in numerous applications requiring robust long-range capabilities, essential for processing extensive input contexts and continuously generating extended outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in LLMs escalates, creating a significant efficiency bottleneck.In this paper, we propose a new KV cache optimization paradigm called LaCache, a training-free method for efficient and accurate generative inference of LLMs. LaCache enables LLMs to simultaneously address both of the critical challenges in long-range modeling: robust long-range capabilities and continuous generation without running out-of-memory (OOM). Specifically, LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets.Experiments across various tasks, benchmarks, and LLM models consistently validate LaCache's effectiveness in enhancing LLMs' long-range capabilities. Our code is available at https://github.com/GATECH-EIC/LaCache.","Large Language Models (LLMs) encounter substantial challenges when handling extensive input contexts and continuously generating extended outputs. Specifically, the increasing number of Key-Value (KV) pairs during generation results in a significant efficiency bottleneck. In this paper, we propose a new KV cache optimization paradigm called LaCache, which enables LLMs with robust long-range capabilities and supports continuous generation without running out-of-memory (OOM). LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets."
Poster,LADA: Scalable Label-Specific CLIP Adapter for Continual Learning,https://ICML.cc//virtual/2025/poster/43751,"Mao-Lin Luo, Zi-Hao Zhou, Tong Wei, Min-Ling Zhang","Continual learning with vision-language models like CLIP offers a pathway toward scalable machine learning systems by leveraging its transferable representations. Existing CLIP-based methods adapt the pre-trained image encoder by adding multiple sets of learnable parameters, with each task using a partial set of parameters. This requires selecting the expected parameters for input images during inference, which is prone to error that degrades performance. To address this problem, we introduce LADA (**L**abel-specific **ADA**pter). Instead of partitioning parameters across tasks, LADA appends lightweight, label-specific memory units to the frozen CLIP image encoder, enabling discriminative feature generation by aggregating task-agnostic knowledge. To prevent catastrophic forgetting, LADA employs feature distillation for seen classes, preventing their features from being interfered with by new classes. Positioned after the image encoder, LADA prevents gradient flow to the frozen CLIP parameters, ensuring efficient training. Extensive results show that LADA achieves state-of-the-art performance in continual learning settings.  The implementation code is available at [https://github.com/MaolinLuo/LADA](https://github.com/MaolinLuo/LADA).","Continual learning enables AI systems to learn new tasks sequentially without forgetting previous knowledge. Existing methods for adapting vision-language models like CLIP require complex selection of task-specific parameters, which often leads to errors and degraded performance.We proposed LADA (Label-Specific Adapter), a simple yet powerful method that attaches compact, label-specific memory units to the fixed CLIP model. These units capture essential features from all tasks, allowing new tasks to be learned easily without interfering with prior knowledge.LADA significantly improves continual learning by eliminating the need for parameter selection, reducing errors, and ensuring strong performance across various tasks. Our method sets a new benchmark, helping AI models learn continually in a scalable and efficient way."
Poster,Ladder-Residual: Parallelism-Aware Architecture for Accelerating Large Model Inference with Communication Overlapping,https://ICML.cc//virtual/2025/poster/44749,"Muru Zhang, Mayank Mishra, Zhongzhu Zhou, William Brandon, Jue Wang, Yoon Kim, Jonathan Ragan-Kelley, Shuaiwen Song, Ben Athiwaratkun, Tri Dao","Large language model inference is both memory-intensive and time-consuming, often requiring distributed algorithms to efficiently scale. Various model parallelism strategies are used in multi-gpu training and inference to partition computation across multiple devices, reducing memory load and computation time. However, using model parallelism necessitates communication of information between GPUs, which has been a major bottleneck and limits the gains obtained by scaling up the number of devices. We introduce Ladder Residual, a simple architectural modification applicable to all residual-based models that enables straightforward overlapping that effectively hides the latency of communication. **Our insight is that in addition to systems optimization, one can also redesign the model architecture to decouple communication from computation.** While Ladder Residual can allow communication-computation decoupling in conventional parallelism patterns, we focus on Tensor Parallelism in this paper, which is particularly bottlenecked by its heavy communication. For a Transformer model with 70B parameters, applying Ladder Residual to all its layers can achieve 29% end-to-end wall clock speed up at inference time with TP sharding over 8 devices. We refer the resulting Transformer model as the Ladder Transformer. We train a 1B and 3B Ladder Transformer from scratch and observe comparable performance to a standard dense transformer baseline. We also show that it is possible to convert parts of the Llama-3.1 8B model to our Ladder Residual architecture with minimal accuracy degradation by only retraining for 3B tokens.","As foundation models continue to scale, multi-GPU inference is crucial. Tensor Parallelism, a widely adopted distributed inference approach, divides weights and computation across all devices, which helps with both memory efficiency and speed. However, the inter-GPU communication turns out to be a major bottleneck of the overall latency. For a 70B model running with TP on 8 GPUs, the communication can account for 38% of the total inference time. We introduce Ladder-residual, a simple architecture tweak that allows computation and communication to happen in parallel—reducing latency without needing custom kernels or hardware changes.Here's a quick summary of what Ladder-residual achieves:* ~30% speedup for LLaMA 3.1-70B (TP=8) and LLaMA 3.1-405B (TP=16), and almost doubled speedup when fast interconnect (NVLink) is not available. Comparable performance to standard Transformer.  * Can be applied to a pretrained model - we adapt LlaMA 3.1-8B and gained 23% speedup with no accuracy lost  * Pure PyTorch level modification, no custom CUDA kernels needed, work on any hardware."
Poster,LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models,https://ICML.cc//virtual/2025/poster/45771,"Fanfei Li, Thomas Klein, Wieland Brendel, Robert Geirhos, Roland S. Zimmermann","Out-of-distribution (OOD) robustness is a desired property of computer vision models. Improving model robustness requires high-quality signals from robustness benchmarks to quantify progress. While various benchmark datasets such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C corruption types are no longer OOD relative to today's large, web-scraped datasets, which already contain common corruptions such as blur or JPEG compression artifacts. Consequently, these benchmarks are no longer well-suited for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent models show saturating scores on ImageNet-era OOD benchmarks, indicating that it is unclear whether models trained on web-scale datasets truly become better at OOD generalization or whether they have simply been exposed to the test distortions during training. To address this, we introduce LAION-C as a benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion types specifically designed to be OOD, even for web-scale datasets such as LAION. In a comprehensive evaluation of state-of-the-art models, we find that the LAION-C dataset poses significant challenges to contemporary models, including MLLMs such as Gemini and GPT-4o. We additionally conducted a psychophysical experiment to evaluate the difficulty of our corruptions for human observers, enabling a comparison of models to lab-quality human robustness data. We observe a paradigm shift in OOD generalization: from humans outperforming models, to the best models now matching or outperforming the best human observers.","To make computer vision models more reliable in the real world, we want them to handle new situations that they haven’t seen during their training phase. In the past, researchers used special test sets to check how well models handle such situations. For example, models trained to recognize objects in normal images might fail on blurry ones, so we use test sets like ImageNet-C (for “corruptions”) to evaluate this. But modern models are  trained on huge collections of internet images, which already include many of those cases — making the old tests too easy.So, we built a new test set called LAION-C, with fresh, carefully designed images that today’s models haven’t already seen. When we tested top-performing AI models — including powerful systems like GPT-4o — we found that LAION-C was much harder for them. We also tested humans on the dataset and found that today’s best models are now approaching or even beating human-level performance on these difficult tasks. LAION-C offers a new and effective way to test how robust AI vision systems truly are."
Poster,LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation,https://ICML.cc//virtual/2025/poster/44935,"Chen-Chia Chang, Wan-Hsuan Lin, Yikang Shen, Yiran Chen, Xin Zhang","Automation of analog topology design is crucial due to customized requirements of modern applications with heavily manual engineering efforts. The state-of-the-art work applies a sequence-to-sequence approach and supervised finetuning on language models to generate topologies given user specifications.However, its circuit formulation is inefficient due to $O(|V|^2)$ token length and suffers from low precision sensitivity to numeric inputs.In this work, we introduce LaMAGIC2, a succinct float-input canonical formulationwith identifier (SFCI) for language model-based analog topology generation.SFCI addresses these challenges by improving component-type recognition through identifier-based representations, reducing token length complexity to $O(|V|)$, and enhancing numeric precision sensitivity for better performance under tight tolerances.Our experiments demonstrate that LaMAGIC2 achieves 34\% higher success rates under a tight tolerance 0.01 and 10X lower MSEs compared to a prior method. LaMAGIC2 also exhibits better transferability for circuits with more vertices with up to 58.5\% improvement.These advancements establish LaMAGIC2 as a robust framework for analog topology generation.","Automation of analog topology design is crucial due to customized requirements of modern applications with heavily manual engineering efforts. The state-of-the-art work applies a sequence-to-sequence approach and supervised finetuning on language models to generate topologies given user specifications. However, its circuit formulation is inefficient due to O(|V|^2) token length and suffers from low precision sensitivity to numeric inputs. Thus, we introduce LaMAGIC2, a succinct float-input canonical formulation with identifier (SFCI) for language model-based analog topology generation. SFCI addresses these challenges by improving component-type recognition through identifier-based representations, reducing token length complexity to O(|V|), and enhancing numeric precision sensitivity for better performance under tight tolerances. Our step-by-step analysis of circuit formulations provides valuable insights into graph generation with transformer models, advancing the field of topology generation and beyond."
