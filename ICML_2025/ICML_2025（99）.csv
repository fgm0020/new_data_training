type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,EvoControl: Multi-Frequency Bi-Level Control for High-Frequency Continuous Control,https://ICML.cc//virtual/2025/poster/45709,"Samuel Holt, Todor Davchev, Dhruva Tirumala, Ben Moran, Atil Iscen, Antoine Laurens, Yixin Lin, Erik Frey, Markus Wulfmeier, Francesco Romano, Nicolas Heess","High-frequency control in continuous action and state spaces is essential for practical applications in the physical world. Directly applying end-to-end reinforcement learning to high-frequency control tasks struggles with assigning credit to actions across long temporal horizons, compounded by the difficulty of efficient exploration. The alternative, learning low-frequency policies that guide higher-frequency controllers (e.g., proportional-derivative (PD) controllers), can result in a limited total expressiveness of the combined control system, hindering overall performance. We introduce *EvoControl*, a novel bi-level policy learning framework for learning both a slow high-level policy (using PPO) and a fast low-level policy (using Evolution Strategies) for solving continuous control tasks. Learning with Evolution Strategies for the lower-policy allows robust learning for long horizons that crucially arise when operating at higher frequencies. This enables *EvoControl* to learn to control interactions at a high frequency, benefitting from more efficient exploration and credit assignment than direct high-frequency torque control without the need to hand-tune PD parameters. We empirically demonstrate that *EvoControl* can achieve a higher evaluation reward for continuous-control tasks compared to existing approaches, specifically excelling in tasks where high-frequency control is needed, such as those requiring safety-critical fast reactions.","Robots often need to react in a split second when they bump into something or when conditions suddenly change. Today’s learning-based controllers struggle to make such lightning-fast decisions, while the hand-tuned “reflex” loops engineers add underneath can be hard to adjust and are not very flexible. We introduce EvoControl, a two-layer control system that behaves a bit like a human driver with quick reflexes and slower strategic thinking. The top layer plans only a few times per second, deciding roughly what the robot should do next. Beneath it, a second layer acts hundreds of times per second, handling the fine-grained muscle work. At first, this lower layer is a familiar, safe controller, but—as training progresses—our algorithm gradually “evolves” it into a learned neural reflex that can outperform the original hand-tuned version. Across a dozen simulated tasks and real-robot tests, EvoControl let machines move more smoothly, adapt faster to surprises, and required far less manual tweaking. This approach could make future assistive robots, factory arms, and autonomous vehicles safer and more reliable."
Poster,EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration,https://ICML.cc//virtual/2025/poster/44673,"Allen Nie, Yi Su, Bo Chang, Jonathan Lee, Ed Chi, Quoc Le, Minmin Chen","Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in)ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.","LLMs are in-context RL learners, but not great because they can’t explore well. How do we teach LLMs to explore better? 🤔Solution: Supervised fine-tuning on full exploration trajectories. Exploration is crucial for ICRL. LLMs need to explore in context, without the need to retrain. In the paper, we tried:- Summarized history - Few-shot demonstrations - Inference-time algorithm-guided support - Full exploration trajectory fine-tuningWith full exploration trajectory fine-tuning (we dubbed OFT -- Oracle Behavior Fine-tuning), we first let a smaller model (65.6%) (Gemini Flash -- similar to GPT-4o-mini) surpass Gemini Pro (similar to GPT-4o) (60.0%) on one task.Then, we almost closed the optimality gap between LLM and the theoretically optimal algorithm (LinUCB -- with no model misspecification) on another task (contextual bandit). There are many caveats on how to represent the exploration history in text, how to pick training data, and when to introduce inference-time support. Read the paper for more details!! The two tasks we built are inspired by a one-step RL problem: the bandit. This problem setup is used to rigorously study algorithms like UCB; now, it can be used to study LLM's ability to explore!"
Poster,Evolving Minds: Logic-Informed Inference from Temporal Action Patterns,https://ICML.cc//virtual/2025/poster/45341,"Chao Yang, Shuting Cui, Yang Yang, Shuang Li","Understanding human mental states—such as intentions and desires—is crucial for natural AI-human collaboration. However, this is challenging because human actions occur irregularly over time, and the underlying mental states that drive these actions are unobserved. To tackle this, we propose a novel framework that combines a logic-informed temporal point process (TPP) with amortized variational Expectation-Maximization (EM). Our key innovation is integrating logic rules as priors to guide the TPP’s intensity function, allowing the model to capture the interplay between actions and mental events while reducing dependence on large datasets. To handle the intractability of mental state inference, we introduce a discrete-time renewal process to approximate the posterior. By jointly optimizing model parameters, logic rules, and inference networks, our approach infers entire mental event sequences and adaptively predicts future actions. Experiments on both synthetic and real-world datasets show that our method outperforms existing approaches in accurately inferring mental states and predicting actions, demonstrating its effectiveness in modeling human cognitive processes.","To work well with humans, AI needs to understand what people are thinking—like their goals, intentions, or desires—but this is difficult because we can’t directly observe people’s thoughts, and their actions don’t always happen in predictable ways. Our research tackles this problem by developing a new method that helps AI make better guesses about what a person might be thinking based on how they behave over time. We combined a type of mathematical model that tracks when events happen with logic-based rules about human behavior, helping the AI make smarter predictions even with limited data. These logic rules act like common-sense guidelines, shaping the model’s understanding of how actions and thoughts are connected. Because it’s very hard to figure out people’s mental states directly, we also created an efficient way to approximate them in a way computers can manage. Our method learns to improve itself over time, using both observed actions and inferred mental states to make more accurate predictions about future behavior. This research brings us closer to AI systems that can truly understand and respond to human behavior in natural, intelligent ways."
Poster,"Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",https://ICML.cc//virtual/2025/poster/44299,"Jianyu Wang, Zhiqiang Hu, Lidong Bing","We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent ''gibberish'' can remarkably improve performance across diverse tasks. Notably, the ''gibberish'' always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature—such as symbiosis and self-organization—arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.","We use computers to create the most effective prompts to help large language models (LLMs) perform specific tasks more accurately. Traditionally, these prompts are written in natural language, using clear instructions and carefully selected examples—a method known as in-context learning. While this approach is effective, we ask a deeper question: is natural language truly the optimal way to communicate with LLMs?Through extensive experiments, we uncover a surprising new insight into in-context learning with LLMs. Concretely, we find that pruning random demonstrations into seemingly incoherent “gibberish” can surprisingly enhance task performance—consistently matching state-of-the-art prompt optimization results. The findings hold consistently across the LLMs we examined, irrespective of their alignment.To assist other researchers in exploring this idea—whether from a practical perspective, to develop better prompt optimization algorithms or stabilize in-context learning, or from a mechanistic perspective, to deepen our understanding of the surprising effectiveness of pruning, we have developed and released our algorithms (e.g., TAPruning and PromptQuine), which show good final task performance, decent runtime efficiency as well as promising scalability."
Poster,EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions,https://ICML.cc//virtual/2025/poster/44846,"Huayu Deng, Xiangming Zhu, Yunbo Wang, Xiaokang Yang","Graph neural networks have been a powerful tool for mesh-based physical simulation. To efficiently model large-scale systems, existing methods mainly employ hierarchical graph structures to capture multi-scale node relations. However, these graph hierarchies are typically manually designed and fixed, limiting their ability to adapt to the evolving dynamics of complex physical systems. We propose EvoMesh, a fully differentiable framework that jointly learns graph hierarchies and physical dynamics, adaptively guided by physical inputs. EvoMesh introduces anisotropic message passing, which enables direction-specific aggregation of dynamic features between nodes within each hierarchy, while simultaneously learning node selection probabilities for the next hierarchical level based on physical context. This design creates more flexible message shortcuts and enhances the model's capacity to capture long-range dependencies. Extensive experiments on five benchmark physical simulation datasets show that EvoMesh outperforms recent fixed-hierarchy message passing networks by large margins. The project page is available at https://hbell99.github.io/evo-mesh/.","Graph neural networks have proven to be powerful tools for mesh-based physical simulation, but most existing methods rely on static graph structures. We propose EvoMesh, a fully differentiable framework that jointly learns graph hierarchies and physical dynamics, adaptively guided by physical inputs. This enables better adaptation to the evolving dynamics of complex physical systems."
Poster,EvoPress: Accurate Dynamic Model Compression via Evolutionary Search,https://ICML.cc//virtual/2025/poster/44217,"Oliver Sieberling, Denis Kuznedelev, Eldar Kurtic, Dan Alistarh","The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by dynamic, non-uniform compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on estimating the ""importance"" of a given layer, implicitly assuming that layers contribute independently to the overall compression error. We begin from the motivating observation that this independence assumption does not generally hold for LLM compression: pruning a model further may even significantly recover performance.To address this, we propose EvoPress, a novel evolutionary framework for dynamic LLM compression. By formulating dynamic compression as a general optimization problem, EvoPress identifies optimal compression profiles in a highly efficient manner, and generalizes across diverse models and compression techniques. Via EvoPress, we achieve state-of-the-art performance for dynamic compression of Llama, Mistral, and Phi models, setting new benchmarks for structural pruning (block/layer dropping), unstructured sparsity, and quantization with dynamic bitwidths.","Machine learning models that generate text have become extremely large and expensive to run. This had led researchers to come up with ways to shrink these models. However, most of these methods compress all parts of the model by the same amount, even though some can be shrunk more than others.To fix this, we developed EvoPress, an evolutionary approach that figures out exactly how much to shrink each part of the model. EvoPress works in steps: it starts with a compressed model, then in each round it tests slightly modified versions (mutations), and picks the one that works best (selection). EvoPress can be used on top of any compression technique and on different text-generation models.Through this procedure, EvoPress sets a new benchmark for finding high-quality models that fit within a size limit. This means that large language models can run more efficiently, which makes AI tools more accessible to everyone."
Poster,Exactly Tight Information-theoretic Generalization Bounds via Binary Jensen-Shannon Divergence,https://ICML.cc//virtual/2025/poster/43929,"Yuxin Dong, Haoran Guo, Tieliang Gong, Wen Wen, Chen Li","Information-theoretic bounds, while achieving significant success in analyzing the generalization of randomized learning algorithms, have been criticized for their slow convergence rates and overestimation. This paper presents novel bounds that bridge the expected empirical and population risks through a binarized variant of the Jensen-Shannon divergence. Leveraging our foundational lemma that characterizes the interaction between an arbitrary and a binary variable, we derive hypothesis-based bounds that enhance existing conditional mutual information bounds by reducing the number of conditioned samples from $2$ to $1$. We additionally establish prediction-based bounds that surpass prior bounds based on evaluated loss mutual information measures. Thereafter, through a new binarization technique for the evaluated loss variables, we obtain exactly tight generalization bounds broadly applicable to general randomized learning algorithms for any bounded loss functions. Our results effectively address key limitations of previous results in analyzing certain stochastic convex optimization problems, without requiring additional stability or compressibility assumptions about the learning algorithm.","Machine learning models often perform well on the data they’re trained on, but the real challenge is ensuring they do just as well on new, unseen data. To understand and improve this “generalization” ability, researchers have developed mathematical tools called generalization bounds. These bounds try to measure how far off a model’s performance on training data might be from its performance on future data.However, existing tools sometimes give very loose estimates. This paper introduces a new way to get much sharper, more accurate estimates. We focus on a well-known information-theoretic measure, but simplify it using a “binary” version, breaking complex outcomes down to simpler yes/no signals. This makes the math more manageable and leads to better bounds.Moreover, through a new technique called “binarization”, we provide the first generalization bounds that are not only more accurate but exactly tight: matching the best possible performance for a wide range of machine learning methods, without needing extra assumptions. This makes our bounds particularly valuable for analyzing modern, randomized learning algorithms used in areas like deep learning and optimization.In short, this work improves our ability to trust machine learning models by offering stronger, more precise tools to measure how well they will generalize to new data."
Poster,Exact Recovery of Sparse Binary Vectors from Generalized Linear Measurements,https://ICML.cc//virtual/2025/poster/44544,"Arya Mazumdar, Neha Sangwan","We consider the problem of *exact* recovery of a $k$-sparse binary vector  from generalized linear measurements (such as *logistic regression*). We analyze the *linear estimation* algorithm (Plan, Vershynin, Yudovina, 2017), and also show information theoretic lower bounds on the number of required measurements. As a consequence of our results, for  noisy one bit quantized linear measurements ($\mathsf{1bCS}$), we obtain a sample complexity of $O((k+\sigma^2)\log{n})$, where $\sigma^2$ is the noise variance. This is shown to be optimal  due to the information theoretic lower bound. We also obtain tight sample complexity characterization for logistic regression.Since $\mathsf{1bCS}$ is a strictly harder problem than noisy linear measurements ($\mathsf{SparseLinearReg}$) because of added quantization, the same sample complexity is achievable for $\mathsf{SparseLinearReg}$. While this sample complexity can be obtained via the popular lasso algorithm, linear estimation is  computationally more efficient. Our lower bound  holds for any set of measurements for $\mathsf{SparseLinearReg}$ (similar bound was known for Gaussian measurement matrices) and is closely matched by the maximum-likelihood upper bound. For $\mathsf{SparseLinearReg}$,  it was conjectured in Gamarnik and Zadik, 2017 that there is a statistical-computational gap and the number of measurements should be at least $(2k+\sigma^2)\log{n}$ for efficient algorithms to exist. It is worth noting that our results imply that there is no such statistical-computational gap for $\mathsf{1bCS}$ and logistic regression.","We show that a simple linear estimator can exactly recover sparse binary signals from nonlinear and noisy measurements—like logistic regression or 1-bit compressed sensing—with an optimal number of samples. Surprisingly, this matches the best possible performance and suggests no fundamental gap between theory and efficient algorithms in these settings."
Poster,Exact risk curves of signSGD in High-Dimensions: quantifying preconditioning and noise-compression effects,https://ICML.cc//virtual/2025/poster/45544,"Kevin Xiao, Noah Marshall, Atish Agarwala, Elliot Paquette","In recent years, SignSGD has garnered interest as both a practical optimizer as well as a simple model to understand adaptive optimizers like Adam. Though there is a general consensus that SignSGD acts to precondition optimization and reshapes noise, quantitatively understanding these effects in theoretically solvable settings remains difficult. We present an analysis of SignSGD in a high dimensional limit, and derive a limiting SDE and ODE to describe the risk. Using this framework we quantify four effects of SignSGD: effective learning rate, noise compression, diagonal preconditioning, and gradient noise reshaping. Our analysis is consistent with experimental observations but moves beyond that by quantifying the dependence of these effects on the data and noise distributions. We conclude with a conjecture on how these results might be extended to Adam.","Despite being one of the most widely used algorithms in modern machine learning, Adam remains poorly understood from a theoretical perspective. To shed light on its behavior, we study signSGD—a special case of Adam—with the goal of gaining deeper insight into the algorithm. We model signSGD using a deterministic system of ordinary differential equations (ODEs) that accurately describe its dynamics in the high-dimensional limit. This approach allows us to replace the inherently stochastic process with a deterministic one. Our analysis uncovers precise preconditioning and noise compression effects that have long been hypothesized for signSGD and, by extension, Adam."
Poster,Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs,https://ICML.cc//virtual/2025/poster/44916,"Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura","We derive exact upper and lower bounds for the cumulative distribution function (cdf)  of the output of a neural network (NN) over its entire support subject to noisy (stochastic) inputs. The upper and lower bounds converge to the true cdf over its domain as the resolution increases. Our method applies to any feedforward NN using continuous monotonic piecewise twice continuously differentiable activation functions (e.g.,  ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and instrumental tool of our approach is to bound general NNs with ReLU NNs. The ReLU NN-based bounds are then used to derive the upper and lower bounds of the cdf of the NN output. Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches.","We developed a way to compute statistical bounds within which a neural network’s output falls when the inputs are uncertain or noisy.Our method works for many types of neural networks, including standard feedforward networks and convolutional networks, and it supports popular activation functions like ReLU, tanh, and softmax. This makes our approach more widely applicable than others currently available.A novel feature of our method is that we use simpler ReLU-based networks to approximate more complex networks. This allows us to reliably calculate upper and lower bounds for how the complex network behaves under uncertainty.Our method gives guaranteed and precise estimates of the network’s output behavior—something other approaches can’t fully provide."
