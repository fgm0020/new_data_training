type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference,https://ICML.cc//virtual/2025/poster/46297,"Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang","In vision-language models (VLMs), visual tokens usually consume a significant amount of computational overhead, despite their sparser information density compared to text tokens. To address this, most existing methods learn a network to prune redundant visual tokens and require additional training data. Differently, we propose an efficient training-free token optimization mechanism dubbed **SparseVLM** without extra parameters or fine-tuning costs. Concretely, given that visual tokens complement text tokens in VLMs for linguistic reasoning, we select visual-relevant text tokens to rate the significance of vision tokens within the self-attention matrix extracted from the VLMs. Then we progressively prune irrelevant tokens. To maximize sparsity while retaining essential information, we introduce a rank-based strategy to adaptively determine the sparsification ratio for each layer, alongside a token recycling method that compresses pruned tokens into more compact representations. Experimental results show that our SparseVLM improves the efficiency of various VLMs across a range of image and video understanding tasks. In particular, when LLaVA is equipped with SparseVLM, it achieves a 54\% reduction in FLOPs, lowers CUDA time by 37\%, and maintains an accuracy rate of 97\%. Our code is available at https://github.com/Gumpest/SparseVLMs.","Vision-language models (VLMs) combine images and text to perform tasks like answering questions about pictures or videos. However, processing visual information can be slow and inefficient because these models analyze every part of an image, even when many parts aren’t relevant.Our method, SparseVLM, makes VLMs faster and more efficient by automatically identifying and removing unnecessary visual details while keeping the important ones. Unlike other approaches that require extra training, SparseVLM works without any modifications to the original model. It uses the model’s attention patterns, how much the text ""focuses"" on different parts of the image, to decide which visual details can be safely removed. Additionally, it recycles removed details into a simpler form to save even more computation.Experiments show that SparseVLM can cut computation costs by over 50% and speed up processing by 37% while maintaining nearly the same accuracy. This makes VLMs more practical for real-world applications without sacrificing performance."
Poster,Sparsing Law: Towards Large Language Models with Greater Activation Sparsity,https://ICML.cc//virtual/2025/poster/45239,"Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Xiaojun Meng, Liqun Deng, Jiansheng Wei, Zhiyuan Liu, Maosong Sun","Activation sparsity denotes the existence of substantial weakly-contributed neurons within feed-forward networks of large language models (LLMs), providing wide potential benefits such as computation acceleration. However, existing works lack thorough quantitative studies on this useful property, in terms of both its measurement and influential factors. In this paper, we address three underexplored research questions: (1) How can activation sparsity be measured more accurately? (2) How is activation sparsity affected by the model architecture and training process? (3) How can we build a more sparsely activated and efficient LLM? Specifically, we develop a generalizable and performance-friendly metric, named CETT-PPL-1\%, to measure activation sparsity. Based on CETT-PPL-1\%, we quantitatively study the influence of various factors and observe several important phenomena, such as the convergent power-law relationship between sparsity and training data amount, the higher competence of ReLU activation than mainstream SiLU activation, the potential sparsity merit of a small width-depth ratio, and the scale insensitivity of activation sparsity. Finally, we provide implications for building sparse and effective LLMs, and demonstrate the reliability of our findings by training a 2.4B model with a sparsity ratio of 93.52\%, showing 4.1$\times$ speedup compared with its dense version. The codes and checkpoints are available at https://github.com/thunlp/SparsingLaw/.","We study activation sparsity, a widely existing phenomenon in most LLMs that benefits computation efficiency and interpretability. Three underexplored research questions are addressed in this work: (1) How can activation sparsity be measured more accurately? (2) How is activation sparsity affected by the model architecture and training process? (3) How can we build a more sparsely activated and efficient LLM?First, we propose a more general and performance-friendly metric for activation sparsity, named CETT-PPL-1\%. Next, comprehensive experiments are conducted to reveal the quantitative influence of four factors on activation sparsity: the amount of training data, the activation function, the width-depth ratio, and the parameter scale. Finally, we summarize the implications for building more sparsely-activated efficient LLMs, including the sparsity-promoting benefits of more training data, the ReLU activation (compared with SiLU), and a smaller width-depth ratio. The insensitiveness of sparsity to parameter scale is also a surprising and interesting observation.Our paper offers a more accurate paradigm for inspecting the sparsity level of an LLM. The empirical laws found in this work can provide instructional values for designing and pre-training an LLM with greater activation sparsity, which helps produce more efficient LLMs."
Poster,Spatial Reasoning with Denoising Models,https://ICML.cc//virtual/2025/poster/44634,"Christopher Wewer, Bartlomiej Pogodzinski, Bernt Schiele, Jan Eric Lenssen","We introduce Spatial Reasoning Models (SRMs), a framework to perform  reasoning over sets of continuous variables via denoising generative models. SRMs infer continuous representations on a set of unobserved variables, given observations on observed variables. Current generative models on spatial domains, such as diffusion and flow matching models, often collapse to hallucination in case of complex distributions. To measure this, we introduce a set of benchmark tasks that test the quality of complex reasoning in generative models and can quantify hallucination. The SRM framework allows to report key findings about importance of sequentialization in generation, the associated order, as well as the sampling strategies during training. It demonstrates, for the first time, that order of generation can successfully be predicted by the denoising network itself. Using these findings, we can increase the accuracy of specific reasoning tasks from <1% to >50%. Our [project website](https://geometric-rl.mpi-inf.mpg.de/srm/) provides additional videos, code, and the benchmark datasets.","Today’s image-generating AI can produce beautiful art, but often hallucinates details that don’t make sense. It might invent objects that cannot exist or do not follow simple logic. These mistakes become especially clear when we ask models to solve visual logic puzzles, like Sudoku. Most fail completely, because they try to fill in everything at once, without thinking through what’s plausible or consistent. In our work, we show that they can do much better by reasoning step by step: deciding which part of the image to complete first. We introduce a new method called Spatial Reasoning Models (SRMs) that treats image regions like puzzle pieces, and learns a smart order to fill them in, even when many clues are missing. We tested this on visual versions of our hard Sudoku puzzles and found that regular image generative models get nearly everything wrong. But when we let it decide the order based on uncertainty — filling in the most obvious parts first — it solves over half of the puzzles correctly. Our method turns artistic generators into better reasoners, and we believe this can help future AI systems that need to construct complex information, such as physics simulations, 3D scenes, or medical data."
Poster,SPD: Sync-Point Drop for Efficient Tensor Parallelism of Large Language Models,https://ICML.cc//virtual/2025/poster/46606,"Han-Byul Kim, Duc Hoang, Arnav Kundu, Mohammad Samragh, Minsik Cho","With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20\% overall inference latency reduction with < 1\% accuracy regression for LLaMA2-70B inference over 8 GPUs.","Serving Large Language Models (LLMs) requires distributing computation across multiple GPUs to handle their size and complexity efficiently. However, this distribution introduces delays due to frequent synchronization between devices during model execution. We developed a technique called Sync-Point Drop (SPD) that selectively removes unnecessary synchronization steps while running the model. This significantly reduces delay and speeds up response generation. SPD delivers these improvements without notable loss in model accuracy and without requiring any hardware changes. Our approach enables faster and more efficient deployment of large-scale AI systems, making them more practical and cost-effective in real-world applications."
Poster,Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions,https://ICML.cc//virtual/2025/poster/44478,"Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi","Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both *actionable* and *informative*---two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of $0.319$ in Attack Success Rate and $0.426$ in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.","Large language models (LLMs) are designed to follow safety guidelines that prevent harmful use. However, researchers have found ways to bypass these safeguards and generate dangerous content, a tactic known as ""jailbreaking."" While previous work has focused on technical methods for carrying out such attacks, we asked two new questions: First, are these harmful responses actually useful in helping someone carry out harmful actions? Second, can such responses be triggered through simple, everyday interactions?We found that the most harmful responses are both actionable (offering clear steps to follow) and informative (providing useful details). Surprisingly, these kinds of responses can be elicited using simple, non-technical methods. To better evaluate this risk, we develop HarmScore, a tool that measures how much a model response enables harmful actions. We also introduce Speak Easy, a simple jailbreak framework that uses natural, multi-step conversations across different languages to bypass safety measures. These findings highlight a critical vulnerability: even without advanced skills, users can exploit common interaction patterns to misuse LLMs. Recognizing this risk is an important step toward building safer and more responsible AI systems."
Poster,SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs,https://ICML.cc//virtual/2025/poster/45386,"Shibo Jie, Yehui Tang, Kai Han, Zhi-Hong Deng, Jing Han","Transformer-based large language models (LLMs) have already achieved remarkable results on long-text tasks, but the limited GPU memory (VRAM) resources struggle to accommodate the linearly growing demand for key-value (KV) cache as the sequence length increases, which has become a bottleneck for the application of LLMs on long sequences. Existing KV cache compression methods include eviction, merging, or quantization of the KV cache to reduce its size. However, compression results in irreversible information forgetting, potentially affecting the accuracy of subsequent decoding. In this paper, we propose SpeCache, which takes full advantage of the large and easily expandable CPU memory to offload the complete KV cache, and dynamically fetches KV pairs back in each decoding step based on their importance measured by low-precision KV cache copy in VRAM. To avoid inference latency caused by CPU-GPU communication, SpeCache speculatively predicts the KV pairs that the next token might attend to, allowing us to prefetch them before the next decoding step which enables parallelization of prefetching and computation. Experiments on LongBench and Needle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM usage while avoiding information forgetting for long sequences without re-training, even with a 10x high KV cache compression ratio.","The KV cache is one of the primary contributors to memory usage during inference in large language models (LLMs). We propose a training-free approach to reduce KV cache memory overhead. Specifically, we offload the original 16-bit KV cache to CPU RAM, and during inference, only fetch the top-k KV entries that are most relevant to the current decoding step into GPU memory.To enable parallelism between data loading and computation, we introduce a speculative token, which approximates the next output token and is decoded concurrently with the current token. Using this speculative token and its 1-bit or 2-bit precision KV cache replica, we predict the top-k KV entries likely to be required in the next step. This allows us to preload the necessary KV cache entries before they are actually needed, significantly reducing GPU memory usage without introducing noticeable latency overhead."
Poster,Spectral-Aware Reservoir Computing for Fast and Accurate Time Series Classification,https://ICML.cc//virtual/2025/poster/45987,"Shikang Liu, Chuyang Wei, Xiren Zhou, Huanhuan Chen","Analyzing inherent temporal dynamics is a critical pathway for time series classification, where Reservoir Computing (RC) exhibits effectiveness and high efficiency. However, typical RC considers recursive updates from adjacent states, struggling with long-term dependencies. In response, this paper proposes a Spectral-Aware Reservoir Computing framework (SARC), incorporating spectral insights to enhance long-term dependency modeling. Prominent frequencies are initially extracted to reveal explicit or implicit cyclical patterns. For each prominent frequency, SARC further integrates a Frequency-informed Reservoir Network (FreqRes) to adequately capture both sequential and cyclical dynamics, thereby deriving effective dynamic features. Synthesizing these features across various frequencies, SARC offers a multi-scale analysis of temporal dynamics and improves the modeling of long-term dependencies. Experiments on public datasets demonstrate that SARC achieves state-of-the-art results, while maintaining high efficiency compared to existing methods.","Time series data, which tracks changes over time like heartbeats or stock prices, often contains hidden cyclical patterns crucial for accurate classification. However, existing methods struggle to capture these patterns effectively. Our work introduces SARC, a new approach that identifies cyclical patterns across multiple time spans, like daily or seasonal cycles. Using a static model, SARC eliminates the need for heavy computational resources. Experiments on real-world datasets show that SARC achieves superior classification accuracy and higher efficiency than multiple existing methods. This makes it a practical tool for applications requiring quick, reliable decisions, such as detecting medical conditions by categorizing physiological signals."
Poster,"Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding",https://ICML.cc//virtual/2025/poster/44974,"Ziyao Wang, Muneeza Azmat, Ang Li, Raya Horesh, Mikhail Yurochkin","Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications. Our code has been released at https://github.com/ATP-1010/CoSD.","Large language models (LLMs) are good at many tasks, but each model often has strengths in different areas. This becomes a problem when one model alone can’t solve a task well because it lacks certain knowledge. To address this, we developed a new method called Collaborative Speculative Decoding (CoSD). It lets multiple LLMs work together during inference, without needing to retrain any of them. One model drafts a solution, and a lightweight rule decides when another model should step in to improve it. This approach is fast, works across domains, and is easy to interpret. We show that CoSD can improve accuracy by up to 10% on several benchmarks. Our work offers a practical way to combine the strengths of different models, making LLM-based systems more flexible and reliable."
Poster,Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation,https://ICML.cc//virtual/2025/poster/44706,"Jingyu Liu, Beidi Chen, Ce Zhang","Improving time-to-first-token (TTFT) is an essentially important objective in modern large language model (LLM) inference engines. Optimizing TTFT directly results in higher maximal QPS and meets the requirements of many critical applications. However, boosting TTFT is notoriously challenging since it is compute-bounded and the performance bottleneck shifts from the self-attention that many prior works focus on to the MLP part. In this work, we present SpecPrefill, a training free framework that accelerates the inference TTFT for both long and medium context queries based on the following insight: LLMs are generalized enough to preserve the quality given only a carefully chosen subset of prompt tokens. At its core, SpecPrefill leverages a lightweight model to speculate locally important tokens based on the context. These tokens, along with the necessary positional information, are then sent to the main model for processing. We evaluate SpecPrefill with a diverse set of tasks, followed by a comprehensive benchmarking of performance improvement both in a real end-to-end setting and ablation studies. SpecPrefill manages to serve Llama-3.1-405B-Instruct-FP8 with up to 7$\times$ maximal end-to-end QPS on real downstream tasks and 7.66$\times$ TTFT improvement.","Improving the inference performance of Large Language Models (LLMs) has been one of the most important research directions because it directly decides the accessibility and applicability of LLMs for different applications. One of the crucial problems is to optimize the response time after the arrival of the query and before the generation of the first token, which often gives users the first impression of the responsiveness of the serving system. Contrary to accelerating the generation phase, the query processing has drastically different bottlenecks, hence making it challenging. We introduce our method, Speculative Prefill, which introduces an easy-to-use method that builds upon a simple intuition that not all information of the prompt is necessary for answering the query. To figure out what the essential information is, we adopt a light-weight assistant model to estimate, the result of which, along with other properties, are then sent to the target model for further processing. Our method has several advantages: 1) it is a plug-in method that does not require further training or adaptation, 2) it can be used together with one of the most widely deployed decoding method called speculative decoding with the same assistant model, 3) it helps reduces the prompt processing latency by up to 7 times with minimal quality loss and even improved performance for certain tasks, and 4) it can be applied to high-throughput regime. Overall, our method can help make LLM inference much more efficient and accessible while remaining relatively easy to adopt to different systems."
Poster,Speeding up Policy Simulation in Supply Chain RL,https://ICML.cc//virtual/2025/poster/44952,"Vivek Farias, Joren Gijsbrechts, Aryan Khojandi, Tianyi Peng, Andrew Zheng","Simulating a single trajectory of a dynamical system under some state-dependent policy is a core bottleneck in policy optimization (PO) algorithms. The many inherently serial policy evaluations that must be performed in a single simulation constitute the bulk of this bottleneck. In applying PO to supply chain optimization (SCO) problems, simulating a single sample path corresponding to one month of a supply chain can take several hours. We present an iterative algorithm to accelerate policy simulation, dubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks to independent processes. Within an iteration, any given process evaluates the policy only on its assigned tasks while assuming a certain ‘cached’ evaluation for other tasks; the cache is updated at the end of the iteration. Implemented on GPUs, this scheme admits batched evaluation of the policy across a single trajectory. We prove that the structure afforded by many SCO problems allows convergence in a small number of iterations independent of the horizon. We demonstrate practical speedups of 400x on large-scale SCO problems even with a single GPU, and also demonstrate practical efficacy in other RL environments.","In the process of designing automated decision-making algorithms, a key bottleneck is simply simulating how well a candidate algorithm performs. In complex systems like supply chains, these simulations can be painfully slow—sometimes taking hours just to simulate one month. We present a new method called Picard Iteration that dramatically speeds up this process. Traditional simulations move step-by-step through time, which makes them hard to parallelize. Picard Iteration takes a different approach: it begins with a rough guess for the entire timeline of the simulation and then repeatedly improves that guess. Crucially, each round of refinement can be done in parallel (i.e., simultaneously) across time steps. This benefits from hardware like GPUs, which are designed to massively speed up parallel computations.Using this method, we achieve up to a 400-fold speedup for supply chain simulations—even on a single GPU—and see similar gains in other decision-making tasks."
