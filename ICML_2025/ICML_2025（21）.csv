type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A Non-isotropic Time Series Diffusion Model with Moving Average Transitions,https://ICML.cc//virtual/2025/poster/43547,"Chenxi Wang, Linxiao Yang, Zhixian Wang, Liang Sun, Yi Wang","Diffusion models, known for their generative ability, have recently been adapted to time series analysis. Most pioneering works rely on the standard isotropic diffusion, treating each time step and the entire frequency spectrum identically. However, it may not be suitable for time series, which often have more informative low-frequency components. We empirically found that direct application of standard diffusion to time series may cause gradient contradiction during training, due to the rapid decrease of low-frequency information in the diffusion process. To this end, we proposed a novel time series diffusion model, MA-TSD, which utilizes the moving average, a natural low-frequency filter, as the forward transition. Its backward process is accelerable like DDIM and can be further considered a time series super-resolution. Our experiments on various datasets demonstrated MA-TSD's superior performance in time series forecasting and super-resolution tasks.","Diffusion models, a kind of generative AI, are known for producing realistic images or videos. We wondered if such strong generative ability could then be utilized for time series analysis, like generating the possible scenes of future stock prices.The classical diffusion model works by repeated denoising a random noise until a clear image. Thus, training a diffusion model needs to teach the computer how to denoise a figure at different noisy levels. Delicately setting these different noise levels, i.e. making a noise schedule, is one key to helping train and make diffusion models work. When we directly applied the classical noise schedule for images to time series data, we found it unstable to train. We revealed that it’s because the noise schedule for images could polarize the noisy levels of time series, e.g. noisy time series at the first 25% levels are informative but the rest of them are almost pure noise.Therefore, we re-designed a diffusion framework where the trends of time series are extracted, and serve as common structural information at different noisy levels. We showed that our diffusion framework can stabilize the training process, and thus achieve higher-quality time series generation results."
Poster,An Online Adaptive Sampling Algorithm for Stochastic Difference-of-convex Optimization with Time-varying Distributions,https://ICML.cc//virtual/2025/poster/45324,"Yuhan Ye, Ying Cui, Jingyi Wang","We propose an online adaptive sampling algorithm for solving stochastic nonsmooth difference-of-convex (DC) problems under time-varying distributions. At each iteration, the algorithm relies solely on data generated from the current distribution and employs distinct adaptive sampling rates for the convex and concave components of the DC function, a novel design guided by our theoretical analysis. We show that, under proper conditions on the convergence of distributions, the algorithm converges subsequentially to DC critical points almost surely. Furthermore, the sample size requirement of our proposed algorithm matches the results achieved in the smooth case or when a measurable subgradient selector is available, both under static distributions. A key element of this analysis is the derivation of a novel $O(\sqrt{p/n})$ pointwise convergence rate (modulo logarithmic factors) for the sample average approximation of subdifferential mappings, where $p$ is the dimension of the variable and $n$ is the sample size -- a result of independent interest. Numerical experiments confirm that the proposed algorithm is both efficient and effective for addressing stochastic nonsmooth problems.","Many real-world problems—from resource allocation to machine learning—can be described using a flexible mathematical structure called difference-of-convex (DC) functions, which represent complex goals as the difference between two simpler parts. While powerful, these problems become especially difficult when data arrives continuously and is uncertain, as in many online systems.In this work, we introduce a new algorithm that adaptively tackles such problems in real time. The algorithm adjusts how much data to use for each part of the problem at every step, guided by our theoretical analysis. We also develop new mathematical tools to deal with the unique challenges caused by randomness and non-smoothness in these settings.Our algorithm is not only reliable under changing data distributions but also matches the best performance known in easier cases. Experiments show it is both robust and efficient for solving demanding online optimization tasks."
Poster,An Online Learning Approach to Prompt-based Selection of Generative Models and LLMs,https://ICML.cc//virtual/2025/poster/44140,"Xiaoyan Hu, Ho-fung Leung, Farzan Farnia","Selecting a sample generation scheme from multiple prompt-based generative models, including large language models (LLMs) and prompt-guided image and video generation models, is typically addressed by choosing the model that maximizes an averaged evaluation score. However, this score-based selection overlooks the possibility that different models achieve the best generation performance for different types of text prompts. An online identification of the best generation model for various input prompts can reduce the costs associated with querying sub-optimal models. In this work, we explore the possibility of varying rankings of text-based generative models for different text prompts and propose an online learning framework to predict the best data generation model for a given input prompt. The proposed PAK-UCB algorithm addresses a contextual bandit (CB) setting with shared context variables across the arms, utilizing the generated data to update kernel-based functions that predict the score of each model available for unseen text prompts. Additionally, we leverage random Fourier features (RFF) to accelerate the online learning process of PAK-UCB. Our numerical experiments on real and simulated text-to-image and image-to-text generative models show that RFF-UCB performs successfully in identifying the best generation model across different sample types. The code is available at: [github.com/yannxiaoyanhu/dgm-online-select](github.com/yannxiaoyanhu/dgm-online-select).","As the number of available generative AI models continues to grow, from large language models (LLMs) to text-to-image and video generators, selecting the right model for a given task has become increasingly important. Traditionally, model selection is performed by ranking models according to their average performance across a large set of prompts. However, this approach overlooks an important fact: different models often excel on different types of prompts. For example, one model may perform best on prompts about nature scenes, while another may be better at generating images of people.To address this limitation, we propose an *online learning approach* that adapts model selection to the specific prompt being used. Instead of relying on static rankings, our method learns in real time which models work best for which kinds of prompts, allowing it to make more informed, prompt-specific model selections.We formulate this problem as a *contextual bandit* task, where the *“context”* is the input prompt, and the *“arms”* are the available generative models. Our proposed algorithm, called *PAK-UCB*, uses kernel-based predictors to model how well each model is likely to perform on a new prompt, based on past generations. Our experiments suggest that this approach can quickly learn to assign prompts to the most suitable generative models across a range of tasks, including text-to-image and image-to-text generation. The result is more efficient use of generative models, reducing the cost of querying suboptimal models, and improved performance for end users."
Poster,An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints,https://ICML.cc//virtual/2025/poster/44534,"Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, Honghao Wei","Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret $\tilde{\mathcal{O}}(\sqrt{K})$ and strong constraint violation $\tilde{\mathcal{O}}(\sqrt{K})$ without relying on Slater’s condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments.","Modern AI systems—like those used in self-driving cars, robots, and cybersecurity—must learn to make good decisions while following safety rules. But in the real world, these safety rules can be unpredictable: they may change over time, be unclear, or even be set up in a way that tries to trick the system. This kind of hostile or adversarial behavior makes safe learning especially difficult.In this paper, we present a new learning algorithm that helps AI systems stay safe even when the safety constraints are changing in both adversarial and stochastic ways. Our method does not assume that the system already knows what is safe, and it does not require ideal conditions to work. Instead, it learns and adapts on the challenging and uncertain environments.We show that this approach can learn effectively while also minimizing safety violations over time. This work provides stronger and more realistic guarantees for using AI safely in high-risk settings."
Poster,A Novel Characterization of the Population Area Under the Risk Coverage Curve (AURC) and Rates of Finite Sample Estimators,https://ICML.cc//virtual/2025/poster/45606,"Han Zhou, dr. Jordy Van Landeghem, Teodora Popordanoska, Matthew B Blaschko","The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC  plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance.","When models are used in high-stakes situations such as healthcare, driving, or legal decisions, it is important not only to use the model for making predictions, but also to know when to trust their answers and when to back down. Selective classifiers are systems designed to do just that: only make a prediction when they’re confident, and otherwise remain silent to avoid costly mistakes.But how can we measure how well these systems balance safety with making useful decisions? In our research, we focus on an evaluation metric called the Area Under the Risk-Coverage Curve (AURC), which captures how effectively a system manages the trade-off between accuracy and caution.We developed a novel, statistical method to interpret and estimate this metric, even when data is limited. Our approach is not only statistically sound—becoming more accurate as more data is collected—but also practical. We tested our method on various datasets and models, showing it works reliably. This research helps make future AI systems more dependable when uncertainty really matters."
Poster,Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning Attack,https://ICML.cc//virtual/2025/poster/46150,"Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Joshua Kimball, Ling Liu","Safety aligned Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can break the LLMs's safety alignment. While several defenses have been proposed, our evaluation shows that existing defenses fail \textit{when some specific training hyper-parameters are chosen} -- a large learning rate or a large number of training epochs in the fine-tuning stage can easily invalidate the defense. To this end,  we propose Antidote, a post-fine-tuning stage solution, which remains \textbf{\textit{agnostic to the training hyper-parameters in the fine-tuning stage}}. Antidote relies on the philosophy that by removing the harmful parameters, the harmful model can be recovered from the harmful behaviors, regardless of how those harmful parameters are formed in the fine-tuning stage. With this philosophy, we introduce a one-shot pruning stage after harmful fine-tuning to remove the harmful weights that are responsible for the generation of harmful content. Despite its embarrassing simplicity, empirical results show that Antidote can reduce harmful score while maintaining accuracy on downstream tasks.","Large language models might go beyond control without proper safety alignment. A concrete example is that they may deliver harmful speech or  conduct even more seriously improper behavior.  Recent findings show that, while we are able to instruct the large language model to do good things with safety alignment, the safety alignment is too  fragile that it can easily be broken **if we fine-tune the model**.  Such a safety issue is named harmful fine-tuning attack. To counter harmful fine-tuning attack, our paper propose a solution, named Antidote, to recover the model from its harmful behavior. Our high level idea is straightforward -- we identify a few harmful parameters from the model and  and remove (i.e., zeroing out)  them from the model. Results show that such an embarrassingly simple method can recover the fine-tuned model from the harmful state back to the safe state."
Poster,any4: Learned 4-bit Numeric Representation for LLMs,https://ICML.cc//virtual/2025/poster/43785,"Mostafa Elhoushi, Jeff Johnson","We present any4, a learned 4-bit weight quantization solution for large language models (LLMs) providing arbitrary numeric representations without requiring pre-processing of weights or activations. any4 yields higher accuracy compared to other related 4-bit numeric representation types: int4, fp4 and nf4, as evaluated on a range of model sizes, generations and families (Llama 2, Llama 3, Mistral and Mixtral). While any4 does not require preprocessing of weights or activations, it is also competitive with orthogonal techniques that require such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3 and any2 and show competitiveness at lower bits. Additionally, we show that we can calibrate using a single curated diverse sample rather than hundreds of samples from a dataset as done in most quantization approaches. We also open source tinygemm, a latency optimized GPU matrix multiplication library for LLMs, that implements any4 using a GPU-efficient lookup table strategy along with other common quantization methods. We open source our code at https://github.com/facebookresearch/any4.","Large language models (LLMs), like ChatGPT or Llama, are powerful but very large—they have billions of numbers (called weights) that take up a lot of memory and make them slow and expensive to run. To speed them up and allow them to run on smaller devices, we need to make these numbers smaller in size without hurting performance.One common technique to do this is quantization, which means representing each number using fewer bits (the basic unit of data in computers). Normally, LLMs use 16 bits per weight. In this work, we focus on using only 4 bits per weight, which makes models much smaller and faster.The challenge with 4-bit quantization is keeping the model accurate. Our method, any4, learns the best way to represent these numbers, giving each row of weights its own custom mapping. This makes any4 more accurate than other 4-bit formats like int4, fp4, and nf4, and even competitive with more complex methods that need a lot of extra steps (like GPTQ and AWQ).We also tried using 3-bit and 2-bit versions (any3 and any2) and found them competitive. Uniquely, our method needs only a single well-chosen example to calibrate and tune the best mapping for each row of weights, while other quantization methods tend to require dozens or hundreds.Finally, we built and open-sourced tinygemm, a fast GPU library that runs any4 and other quantized models efficiently. Our code is freely available at: https://github.com/facebookresearch/any4."
Poster,AnyEdit: Edit Any Knowledge Encoded in Language Models,https://ICML.cc//virtual/2025/poster/44807,"Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Mingyang Wan, Guojun Ma, Xiang Wang, Xiangnan He, Tat-Seng Chua","Large language models (LLMs) often produce incorrect or outdated information, necessitating efficient and precise knowledge updates. Current model editing methods, however, struggle with long-form knowledge in diverse formats, such as poetry, code snippets, and mathematical derivations. These limitations arise from their reliance on editing a single token’s hidden state, a limitation we term as ``efficacy barrier''. To solve this, we propose \textbf{AnyEdit}, a new autoregressive editing paradigm. It decomposes long-form knowledge into sequential chunks and iteratively edits the key token in each chunk, ensuring consistent and accurate outputs. Theoretically, we ground AnyEdit in the Chain Rule of Mutual Information, showing its ability to update any knowledge within LLMs. Empirically, it outperforms strong baselines by 21.5\% on benchmarks including UnKEBench, AKEW, and our new \textbf{EditEverything} dataset for long-form diverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-play framework, enabling current editing methods to update knowledge with arbitrary length and format, significantly advancing the scope and practicality of LLM knowledge editing. Our code is available at: \url{https://github.com/jianghoucheng/AnyEdit}.","(1) Problem: Large Language Models (LLMs) often produce incorrect or outdated information. Current methods for updating their knowledge struggle with long and complex content, like poetry, code, or mathematical steps, because they typically only tweak a tiny part of the model's internal representation.(2) Solution: We introduce AnyEdit, a new method that addresses this. AnyEdit intelligently breaks down lengthy information into smaller, sequential pieces. It then edits the key element within each piece iteratively, ensuring each part is corrected in context.(3) Impact: This approach allows for accurate and consistent updates to complex knowledge of any length or format. AnyEdit significantly outperforms existing techniques and can serve as an add-on to improve them, making it much more practical to keep LLMs current and reliable."
Poster,Anytime-Constrained Equilibria in Polynomial Time,https://ICML.cc//virtual/2025/poster/46266,Jeremy McMahan,"We extend anytime constraints to the Markov game setting and the corresponding solution concept of anytime-constrained equilibrium (ACE). Then, we present a comprehensive theory of anytime-constrained equilibria that includes (1) a computational characterization of feasible policies, (2) a fixed-parameter tractable algorithm for computing ACE, and (3) a polynomial-time algorithm for approximately computing ACE. Since computing a feasible policy is NP-hard even for two-player zero-sum games, our approximation guarantees are the best possible so long as $P \neq NP$. We also develop the first theory of efficient computation for action-constrained Markov games, which may be of independent interest.","Fully autonomous vehicles are a core goal of many machine learning researchers. However, most standard frameworks for computing effective routes ignore two key factors: (1) routes must obey strict constraints including safety considerations, (2) routes must take into account the behavior of other vehicles on the road. We handle both considerations by adding strict ""anytime"" constraints and multiple agents on top of the usual Markov Decision Making Process (MDP) model often used for autonomous vehicles. Our contributions revolve around developing a rigorous, mathematical theory of multi-agent MDPs, also called Markov games (MG), under these anytime constraints. A significant component of our theory is the design of efficient approximation algorithms for the solution of such a constrained MG."
Poster,A Online Statistical Framework for Out-of-Distribution Detection,https://ICML.cc//virtual/2025/poster/46095,"Xinsong Ma, Xin Zou, Weiwei Liu","Out-of-distribution (OOD) detection  task  is significant   in  reliable and safety-critical applications.  Existing approaches primarily focus on developing  the powerful score function, but overlook the design of decision-making rules based on these score function. In contrast to prior studies, we rethink the OOD detection task from an perspective of online multiple hypothesis testing. We then propose a novel generalized  LOND (g-LOND) algorithm to solve the above problem. Theoretically, the g-LOND algorithm  controls false discovery rate  (FDR) at pre-specified level without the consideration for the dependence between the p-values. Furthermore, we prove that the false positive rate (FPR) of the g-LOND algorithm converges to zero in probability based on the  generalized Gaussian-like distribution family. Finally, the extensive experimental results verify the effectiveness of g-LOND algorithm for OOD detection.","Different from previous literature which focuses on score functions, this paper aims to provide a statistical decision-making framework with strong theoretical guarantee and well empirical performance."
