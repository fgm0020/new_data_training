type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models,https://ICML.cc//virtual/2025/poster/45910,"Saketh Bachu, Erfan Shayegani, Rohit Lal, Trishna Chakraborty, Arindam Dutta, Chengyu Song, Yue Dong, Nael Abu-Ghazaleh, Amit Roy-Chowdhury","Vision-language models (VLMs) have improved significantly in their capabilities, but their complex architecture makes their safety alignment challenging. In this paper, we reveal an uneven distribution of harmful information across the intermediate layers of the image encoder and show that skipping a certain set of layers and exiting early can increase the chance of the VLM generating harmful responses. We call it as “Image enCoder Early-exiT” based vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5, LLaVA-NeXT, and Llama 3.2 show that performing early exits from the image encoder significantly increases the likelihood of generating harmful outputs. To tackle this, we propose a simple yet effective modification of the Clipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing layer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO). We evaluate our L-PPO algorithm across three multi-modal datasets and show that it consistently reduces the harmfulness caused by early exits.","Vision-language models (VLMs) are systems that analyze images and respond to prompts. They consist of two modules: one for processing the input image and another for interpreting the question and generating a response. Each module has hierarchical layers that progressively filter input, much like a water filtration system. To ensure safety, VLMs are taught to refuse harmful requests, whether triggered by a dangerous image or a malicious question.Our paper presents a surprising result: even a safe VLM can produce harmful responses when someone uses the outputs from the early layers of the image analyzer module, even if the image itself is harmless. It is as if the VLM is stopped midway through its reasoning, before it applies its safety mechanisms.We also show that by explicitly teaching the VLM to refuse harmful questions using outputs from multiple layers of the image analyzer, through a modification of the Proximal Policy Optimization (PPO) algorithm, the VLM learns to say “no” not just at the final layer but across different stages of processing. This makes the model safer while still performing well on harmless tasks."
Poster,Layer-wise Quantization for Quantized Optimistic Dual Averaging,https://ICML.cc//virtual/2025/poster/45711,"Anh Duc Nguyen, Ilia Markov, Zhengqing Wu, Ali Ramezani-Kebrya, Kimon Antonakopoulos, Dan Alistarh, Volkan Cevher","Modern deep neural networks exhibit heterogeneity across  numerous layers of various types such as residuals, multi-head attention, etc., due to varying structures (dimensions, activation functions, etc.), distinct representation characteristics, which impact predictions. We develop a general layer-wise quantization framework with tight variance and code-length bounds, adapting to the heterogeneities over the course of training. We then apply a new layer-wise quantization technique within distributed variational inequalities (VIs), proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with adaptive learning rates, which achieves competitive convergence rates for monotone VIs. We empirically show that QODA achieves up to a $150$% speedup over the baselines in end-to-end training time for training Wasserstein GAN on $12+$ GPUs.","Training advanced AI models across many computers often stalls because of the huge amount of information that must be exchanged. We introduce a technique that squeezes the data shared during training by assigning different compression levels to each layer based on its importance. Key layers receive more precision, while others are represented with fewer bits. We integrate this into a training algorithm called Quantized Optimistic Dual Averaging (QODA), which works seamlessly with compressed data and skips extra synchronization steps. We rigorously prove that, despite the reduced communication, our method converges as reliably as standard uncompressed training. In experiments on image generation and large language models across dozens of GPUs, our approach more than doubles training speed while matching final accuracy. By cutting communication costs and speeding up each training round, our work makes distributed deep learning faster, more scalable, and energy-efficient."
Poster,LBI-FL: Low-Bit Integerized Federated Learning with Temporally Dynamic Bit-Width Allocation,https://ICML.cc//virtual/2025/poster/44187,"Li Ding, Hao Zhang, Wenrui Dai, Chenglin Li, Weijia Lu, ZHIFEI YANG, xiaodong Zhang, Xiaofeng Ma, Junni Zou, Hongkai Xiong","Federated learning (FL) is greatly challenged by the communication bottleneck and computation limitation on clients. Existing methods based on quantization for FL cannot simultaneously reduce the uplink and downlink communication cost and mitigate the computation burden on clients. To address this problem, in this paper, we propose the first low-bit integerized federated learning (LBI-FL) framework that quantizes the weights, activations, and gradients to lower than INT8 precision to evidently reduce the communication and computational costs. Specifically, we achieve dynamical temporal bit-width allocation for weights, activations, and gradients along the training trajectory via reinforcement learning. An agent is trained to determine bit-width allocation by comprehensively considering the states like current bit-width, training stage, and quantization loss as the state. The agent efficiently trained on small-scale datasets can be well generalized to train varying network architectures on non-independent and identically distributed datasets. Furthermore, we demonstrated in theory that federated learning with gradient quantization achieves an equivalent convergence rate to FedAvg. The proposed LBI-FL can  reduce the communication costs by 8 times compared to full-precision FL. Extensive experiments show that the proposed LBI-FL achieves a reduction of more than 50\% BitOPs per client on average for FL with less than 2\% accuracy loss compared to low-bit training with INT8 precision.","Training a shared AI model across many edge devices by sharing model without sharing data can protect privacy but require massive communication overheads of models and local model update on resource constrained edge devices.We introduce Low-Bit Integerized Federated Learning (LBI-FL), the first system that reduces all parts of the model — weights, activations, and gradients — from full-precision (32-bit) floating numbers to low-precision integer codes (fewer than eight bits). A lightweight reinforcement-learning agent watches the training process and assigns number of bits to each part iteration by iteration. It can dynamically allocate bit-widths based on the resulting accuracy loss. The agent is trained once on a toy dataset yet transfers effortlessly to new tasks and unbalanced data. LBI-FL can converge in training using low-precision integers as reliably as standard full-precision floating numbers. In practice, it cuts network traffic eight-fold and halves each device’s arithmetic work while keeping accuracy loss within two percentage. This makes privacy-preserving AI potentially feasible for phones, wearables, and other bandwidth-starved gadgets."
Poster,L-Diffusion: Laplace Diffusion for Efficient Pathology Image Segmentation,https://ICML.cc//virtual/2025/poster/46562,"Weihan Li, Linyun Zhou, YangJian, Shengxuming Zhang, Xiangtong Du, Xiuming Zhang, Jing Zhang, Chaoqing Xu, Mingli Song, Zunlei Feng","Pathology image segmentation plays a pivotal role in artificial digital pathology diagnosis and treatment. Existing approaches to pathology image segmentation are hindered by labor-intensive annotation processes and limited accuracy in tail-class identification, primarily due to the long-tail distribution inherent in gigapixel pathology images. In this work, we introduce the Laplace Diffusion Model, referred to as L-Diffusion, an innovative framework tailored for efficient pathology image segmentation. L-Diffusion utilizes multiple Laplace distributions, as opposed to Gaussian distributions, to model distinct components—a methodology supported by theoretical analysis that significantly enhances the decomposition of features within the feature space. A sequence of feature maps is initially generated through a series of diffusion steps. Following this, contrastive learning is employed to refine the pixel-wise vectors derived from the feature map sequence.  By utilizing these highly discriminative pixel-wise vectors, the segmentation module achieves a harmonious balance of precision and robustness with remarkable efficiency. Extensive experimental evaluations demonstrate that L-Diffusion attains improvements of up to  7.16\%,  26.74\%,  16.52\%, and  3.55\% on tissue segmentation datasets, and  20.09\%,  10.67\%,  14.42\%, and  10.41\% on cell segmentation datasets, as quantified by DICE, MPA, mIoU, and FwIoU metrics. The source are available at https://github.com/Lweihan/LDiffusion.","Accurately identifying different parts of tissue or cells in pathology images is a key step in helping doctors diagnose and treat diseases. However, this process usually takes a lot of time and effort because it depends on expert labeling, and it often struggles to recognize rare types of cells or tissue accurately.Our work presents a new method called L-Diffusion that makes this task much more efficient and reliable. Instead of using traditional approaches, L-Diffusion takes a different mathematical path to better understand the features inside these massive medical images. It gradually improves its understanding of the image in steps and then uses a smart way of comparing small parts of the image to sharpen its predictions.As a result, our method can more precisely detect and segment both common and rare parts of tissue or cells. In tests on multiple datasets, L-Diffusion showed major improvements in accuracy, confirming that it is both powerful and practical for real-world use in medical image analysis.You can find the code and more details at: https://github.com/Lweihan/LDiffusion"
Poster,LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models,https://ICML.cc//virtual/2025/poster/44220,"Jinho Chang, Jong Chul YE","With the emergence of diffusion models as a frontline generative model, many researchers have proposed molecule generation techniques with conditional diffusion models. However, the unavoidable discreteness of a molecule makes it difficult for a diffusion model to connect raw data with highly complex conditions like natural language. To address this, here we present a novel latent diffusion model dubbed LDMol for text-conditioned molecule generation. By recognizing that the suitable latent space design is the key to the diffusion model performance, we employ a contrastive learning strategy to extract novel feature space from text data that embeds the unique characteristics of the molecule structure. Experiments show that LDMol outperforms the existing autoregressive baselines on the text-to-molecule generation benchmark, being one of the first diffusion models that outperforms autoregressive models in textual data generation with a better choice of the latent domain.Furthermore, we show that LDMol can be applied to downstream tasks such as molecule-to-text retrieval and text-guided molecule editing, demonstrating its versatility as a diffusion model.","Designing new molecules with computers could one day revolutionize how we discover medicines or create new materials. One exciting way to do this is by using powerful AI models called diffusion models, which can learn to gradually generate data from pure noise. However, molecules are made up of discrete building blocks, which makes it hard for molecular diffusion models to understand complex instructions such as natural texts.To solve this, we created a new model called LDMol. It learns a better way to represent discrete molecules in an informative ""hidden"" space using a technique called contrastive learning, which helps the model understand how the structure of a molecule relates to the words we use to describe it.Our experiments show that LDMol performs better than previous methods in tasks where we turn plain text into molecules. Moreover, it can also be utilized in several other useful tasks, such as finding the right text for a given molecule, or even help tweak molecules based on textual instructions."
Poster,Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with Convergence Guarantees,https://ICML.cc//virtual/2025/poster/44323,"Thien Nguyen, Huy Nguyen","We introduce two complementary techniques for efficient optimization that reduce memory requirements while accelerating training oflarge-scale neural networks. The first technique, Subset-Norm step size, generalizes AdaGrad-Norm and AdaGrad(-Coordinate) through step-size sharing. Subset-Norm (SN) reduces AdaGrad's memory footprint from $O(d)$ to $O(\sqrt{d})$, where $d$ is the model size. For non-convex smooth objectives under coordinate-wise sub-gaussian noise, we show a noise-adapted high-probability convergence guarantee with improved dimensional dependence of SN over existing methods. Our second technique, Subspace-Momentum, reduces the momentum state's memory footprint by restricting momentum to a low-dimensional subspace while performing SGD in the orthogonal complement. We prove high-probability convergence rates for Subspace-Momentum under standard assumptions. Empirical evaluation on pre-training and fine-tuning LLMs  demonstrates the effectiveness of our methods. For instance, combining Subset-Norm with Subspace-Momentum achieves Adam's validation perplexity for LLaMA 1B in approximately half the trainingtokens (6.8B vs 13.1B) while reducing Adam's optimizer-states memory footprint by more than 80\% with minimal additional hyperparameter tuning.","Optimizers for training deep neural networks like Adam maintain internal states that consume substantial amount of memory for large models. We introduce novel algorithms that not only reduce memory consumptions but also achieve faster convergence than Adam and other recent memory efficient optimizers like GaLore and Adam, both experimentally and theoretically."
Poster,LEAPS: A discrete neural sampler via locally equivariant networks,https://ICML.cc//virtual/2025/poster/45761,"Peter Holderrieth, Michael Albergo, Tommi Jaakkola","We propose *LEAPS*, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call \textit{locally equivariant} functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics. We provide code in https://github.com/malbergo/leaps/.",We provide a numerically scalable method for learning to sample from a probability distribution over a discrete random variable when given access only to the unnormalized energy function of the distribution.
Poster,Learnable Spatial-Temporal Positional Encoding for Link Prediction,https://ICML.cc//virtual/2025/poster/45924,"Katherine Tieu, Dongqi Fu, Zihao Li, Ross Maciejewski, Jingrui He","Accurate predictions rely on the expressiveness power of graph deep learning frameworks like graph neural networks and graph transformers, where a positional encoding mechanism has become much more indispensable in recent state-of-the-art (SOTA) works to record the canonical position information. However, the current positional encoding limits in three aspects, at least: (1) most positional encodings are pre-defined, and fixed functions, which are inadequate to adapt to the complex attributed graphs; (2) a few pioneering works propose the learnable positional encoding but still limited to the structural information, leaving the real-world time-evolving topological and feature information untouched; (3) most positional encodings should be equipped with transformer's attention mechanism to fully release the power, where the dense or relational attention is often unaffordable on large-scale structured data.Hence, we study the possibility of Learnable Spatial-Temporal Positional Encoding in an effective and efficient manner and then propose a simple temporal link prediction model named L-STEP. Briefly, for L-STEP, we (1) prove the proposed positional learning scheme can preserve the graph property from the spatial-temporal spectral viewpoint, (2) verify that MLPs can fully exploit the expressiveness and reach Transformers' performance on that encoding, (3) change different initial positional encoding inputs to show robustness, (4) analyze the theoretical complexity and obtain less empirical running time than SOTA, and (5) demonstrate its temporal link prediction out-performance on 13 classic datasets and with 10 algorithms in both transductive and inductive settings using 3 different sampling strategies. Also, L-STEP obtains the leading performance in the newest large-scale TGB benchmark.","Accurate predictions on complex data structures like graphs, representing relationships in networks such as social platforms, or biological processes, depend on how well models can understand the relative positions and connections between nodes. Modern graph learning frameworks rely on positional encoding to capture this structural information. However, current methods fall short: they often rely on fixed formulas, overlook how graphs change over time, and are computationally expensive, especially for large datasets.To address these limitations, we developed L-STEP, a simple yet effective model for temporal link prediction. L-STEP introduces a learnable way to encode structural and time-evolving information. We show that this method preserves key graph properties from a spectral perspective, and surprisingly, simple Multi-Layer Perceptrons (MLPs) can match the performance of more complex Transformer models when using our encoding.Our work bridges the gap between accuracy and scalability in graph learning. By providing a learnable and efficient way to understand how graph structures evolve over time, L-STEP can lead to more reliable predictions in real-world scenarios, whether forecasting traffic patterns, or understanding evolving relationships in real-world data. Notably, our model achieves leading results on the different kinds of temporal networks, demonstrating its practical impact and potential in large-scale applications."
Poster,Learn Beneficial Noise as Graph Augmentation,https://ICML.cc//virtual/2025/poster/44340,"Siqi Huang, Yanchen Xu, Hongyuan Zhang, Xuelong Li","Although graph contrastive learning (GCL) has been widely investigated,  it is still a challenge to generate effective and stable graph augmentations. Existing methods often apply heuristic augmentation like random edge dropping, which may disrupt important graph structures and result in unstable GCL performance. In this paper, we propose **P**ositive-**i**ncentive **N**oise driven **G**raph **D**ata **A**ugmentation (PiNGDA), where positive-incentive noise (pi-noise) scientifically analyzes the beneficial effect of noise under the information theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian auxiliary variable to convert the loss function to information entropy. We prove that the standard GCL with pre-defined augmentations is equivalent to estimate the beneficial noise via the point estimation.  Following our analysis, PiNGDA is derived from learning the beneficial noise on both topology and attributes through a trainable noise generator for graph augmentations, instead of the simple estimation. Since the generator learns how to produce beneficial perturbations on graph topology and node attributes, PiNGDA is more reliable compared with the existing methods. Extensive experimental results validate the effectiveness and stability of PiNGDA.","Graphs are used to model complex systems like social networks or molecules. In graph contrastive learning, we train models to distinguish between different views of the same graph by applying data augmentation. However, existing methods often use random changes which can accidentally remove important information and harm performance.We propose PiNGDA, a new approach that replaces random edits with positive-incentive noise: learned, helpful changes based on principles from information theory. PiNGDA uses a trainable generator to add meaningful perturbations to both the graph's structure and node features. This guided augmentation helps the model learn more general and robust representations. Experiments show that PiNGDA leads to more stable and effective learning than traditional GCL methods."
Poster,Learn from Downstream and Be Yourself in Multimodal Large Language Models Fine-Tuning,https://ICML.cc//virtual/2025/poster/45894,"Wenke Huang, Jian Liang, Zekun Shi, Didi Zhu, Guancheng Wan, He Li, Bo Du, Dacheng Tao, Mang Ye","Multimodal Large Language Model (MLLM) has demonstrated strong generalization capabilities across diverse distributions and tasks, largely due to extensive pre-training datasets. Fine-tuning MLLM has become a common practice to improve performance on specific downstream tasks. However, during fine-tuning, MLLM often faces the risk of forgetting knowledge acquired during pre-training, which can result in a decline in generalization abilities. To balance the trade-off between generalization and specialization, we propose measuring the parameter importance for both pre-trained and fine-tuning distributions, based on frozen pre-trained weight magnitude and accumulated fine-tuning gradient values. We further apply an importance-aware weight allocation strategy, selectively updating relatively important parameters for downstream tasks. We conduct empirical evaluations on both image captioning and visual question-answering tasks using various MLLM architectures. The comprehensive experimental analysis demonstrates the effectiveness of the proposed solution, highlighting the efficiency of the crucial modules in enhancing downstream specialization performance while mitigating generalization degradation in MLLM Fine-Tuning.","Multimodal Large Language Model (MLLM) has demonstrated strong generalization capabilities across diverse distributions and tasks, largely due to extensive pre-training datasets. Fine-tuning MLLM has become a common practice to improve performance on specific downstream tasks. However, during fine-tuning, MLLM often faces the risk of forgetting knowledge acquired during pre-training, which can result in a decline in generalization abilities. To balance the trade-off between generalization and specialization, we propose measuring the parameter importance for both pre-trained and fine-tuning distributions, based on frozen pre-trained weight magnitude and accumulated fine-tuning gradient values. We further apply an importance-aware weight allocation strategy, selectively updating relatively important parameters for downstream tasks. We conduct empirical evaluations on both image captioning and visual question-answering tasks using various MLLM architectures. The comprehensive experimental analysis demonstrates the effectiveness of the proposed solution, highlighting the efficiency of the crucial modules in enhancing downstream specialization performance while mitigating generalization degradation in MLLM Fine-Tuning."
