type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"System-Aware Unlearning Algorithms: Use Lesser, Forget Faster",https://ICML.cc//virtual/2025/poster/45985,"Linda Lu, Ayush Sekhari, Karthik Sridharan","Machine unlearning addresses the problem of updating a machine learning model/system trained on a dataset $S$ so that the influence of a set of deletion requests $U \subseteq S$ on the unlearned model is minimized. The gold standard definition of unlearning demands that the updated model, after deletion, be nearly identical to the model obtained by retraining. This definition is designed for a worst-case attacker (one who can recover not only the unlearned model but also the remaining data samples, i.e., $S \setminus U$). Such a stringent definition has made developing efficient unlearning algorithms challenging. However, such strong attackers are also unrealistic. In this work, we propose a new definition, *system-aware unlearning*, which aims to provide unlearning guarantees against an attacker that can at best only gain access to the data stored in the system for learning/unlearning requests and not all of $S\setminus U$.  With this new definition, we use the simple intuition that if a system can store less to make its learning/unlearning updates, it can be more secure and update more efficiently against a system-aware attacker. Towards that end, we present an exact system-aware unlearning algorithm for linear classification using a selective sampling-based approach, and we generalize the method for classification with general function classes. We theoretically analyze the tradeoffs between deletion capacity, accuracy, memory, and computation time.","Machine learning models are often trained on private, sensitive data, which could be exposed during deployment. Due to these privacy concerns, some individuals may request for the influence of their data be removed from the model after deployment. Machine unlearning is the selective removal of specific training data after a model has been trained in a more efficient manner than retraining the entire model from scratch. The current definition of machine unlearning provides privacy guarantees against a worst-case attacker (one who can recover not only the unlearned model but also the remaining data samples); however, such strong attackers are unrealistic, and this stringent definition has made the development of efficient unlearning algorithms challenging. In this work, we propose a new definition, *system-aware unlearning*, which aims to provide unlearning guarantees against an attacker that can at best only gain access to the information stored in the learning system after unlearning. If less information is stored and used by the algorithm, then less information is exposed to the attacker, making it easier to provide privacy against such an attacker. Thus, algorithms that rely on less of their training data can unlearn more efficiently. Using this intuition, we use sample compression algorithms to design more efficient unlearning algorithms for classification."
Poster,T1: Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling,https://ICML.cc//virtual/2025/poster/43762,"Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong","Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks. However, existing approaches mainly rely on imitation learning and struggle to achieve effective test-time scaling. While reinforcement learning (RL) holds promise for enabling self-exploration, recent attempts yield modest improvements in complex reasoning. In this paper, we present T1 to scale RL by encouraging exploration and understand inference scaling. We first initialize the LLM using synthesized chain-of-thought data that integrates trial-and-error and self-verification. To scale RL training, we promote increased sampling diversity through over-sampling. We demonstrate that T1 with open LLMs as its base exhibits inference scaling behavior and achieves superior performance on challenging math reasoning benchmarks. More importantly, we present a simple strategy to examine inference scaling, where increased inference budgets directly lead to T1’s better performance without any additional verification. The model weights and training data are publicly available at https://github.com/THUDM/T1.","Current AI language models have limitations when tackling complex reasoning tasks like advanced mathematics. While these models can learn from examples, they typically don't improve their reasoning during the actual problem-solving process - similar to a student who can't think more deeply about a problem even when given extra time.We developed T1, a training method that helps AI models explore different approaches and learn from trial-and-error. Our approach includes three key strategies: first, we train the model using examples that show both failed attempts and successful solutions; second, we encourage the model to generate many diverse reasoning paths during training; and third, we help it spend more time considering harder problems.T1 shows a inference scaling pattern - the more time it spends ""thinking"" about a problem, the better its performance tends to become. On challenging mathematics competitions, T1 performs better than previous methods we tested. While there's still much work to be done, this suggests that giving AI more reasoning time can lead to improved solutions on difficult problems."
Poster,TabFlex: Scaling Tabular Learning to Millions with Linear Attention,https://ICML.cc//virtual/2025/poster/44649,"Yuchen Zeng, Tuan Dinh, Wonjun Kang, Andreas Mueller","Leveraging the in-context learning (ICL) capability of Large Language Models (LLMs) for tabular classification has gained significant attention for its training-free adaptability across diverse datasets. Recent advancements, like TabPFN, excel in small-scale tabular datasets but struggle to scale for large and complex datasets. Our work enhances the efficiency and scalability of TabPFN for larger datasets by incorporating linear attention mechanisms as a scalable alternative to complexity-quadratic self-attention. Our model, TabFlex, efficiently handles tabular datasets with thousands of features and hundreds of classes, scaling seamlessly to millions of samples. For instance, TabFlex processes the poker-hand dataset with over a million samples in just 5 seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a 2× speedup compared to TabPFN and a 1.5× speedup over XGBoost, outperforming 25 tested baselines in terms of efficiency across a diverse range of datasets. Furthermore, TabFlex remains highly effective on large-scale datasets, delivering strong performance with significantly reduced computational costs, especially when combined with data-efficient techniques such as dimensionality reduction and data sampling.","Recently, a new way of using large language models (LLMs) has gained attention: giving them a few examples to help make predictions on table-based tasks—where the data looks like a spreadsheet (e.g., a CSV file), and the goal is to predict one column based on others. This method is fast and does not require training the model. However, it only works well with a small number of examples; too many can slow things down and use a lot of memory, because of how LLMs are built. In this paper, we explore different model designs, find a better solution, and introduce a new model that handles more data with much less memory and faster processing—without losing accuracy."
Poster,TabFSBench: Tabular Benchmark for Feature Shifts in Open Environments,https://ICML.cc//virtual/2025/poster/44787,"Zijian Cheng, 贾 子怡, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo","Tabular data is widely utilized in various machine learning tasks. Current tabular learning research predominantly focuses on closed environments, while in real-world applications, open environments are often encountered, where distribution and feature shifts occur, leading to significant degradation in model performance. Previous research has primarily concentrated on mitigating distribution shifts, whereas feature shifts, a distinctive and unexplored challenge of tabular data, have garnered limited attention. To this end, this paper conducts the first comprehensive study on feature shifts in tabular data and introduces the first **tab**ular **f**eature-**s**hift **bench**mark (TabFSBench). TabFSBench evaluates impacts of four distinct feature-shift scenarios on four tabular model categories across various datasets and assesses the performance of large language models (LLMs) and tabular LLMs in the tabular benchmark for the first time. Our study demonstrates three main observations: (1) most tabular models have the limited applicability in feature-shift scenarios; (2) the shifted feature set importance has a linear relationship with model performance degradation; (3) model performance in closed environments correlates with feature-shift performance.  Future research direction is also explored for each observation.Benchmark: [LAMDASZ-ML/TabFSBench](https://github.com/LAMDASZ-ML/TabFSBench).","Tabular data is widely used in machine learning, but current research mainly focuses on closed environments. In real-world applications, however, data often comes from open environments where distribution shifts and feature shifts occur, significantly degrading model performance. While previous work has primarily addressed distribution shifts, feature shifts—a unique and understudied challenge in tabular data—have received little attention.  This paper presents the first comprehensive study on feature shifts in tabular data and introduces **TabFSBench**, the first benchmark for evaluating feature shifts in tabular learning. TabFSBench assesses the impact of four different feature-shift scenarios across multiple datasets and model categories, including large language models (LLMs) and tabular-specific LLMs, marking their first evaluation in this setting.  Key findings include:   - Most tabular models struggle with feature shifts.   - The importance of shifted features has a linear relationship with performance degradation.   - Model performance in closed environments correlates with feature-shift robustness.  Based on these insights, we outline future research directions. This work provides new perspectives on handling feature shifts in tabular data."
Poster,TabICL: A Tabular Foundation Model for In-Context Learning on Large Data,https://ICML.cc//virtual/2025/poster/46681,"Jingang QU, David Holzmüller, Gael Varoquaux, Marine Le Morvan","The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates. While TabPFNv2 foundation model excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables? We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 53 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data. Pretraining code, inference code, and pre-trained models are available at https://github.com/soda-inria/tabicl.","Spreadsheets—tables of rows and columns—sit at the heart of healthcare records, bank ledgers and countless business logs. For years, the go-to tools for learning from such tables have been a family of models called gradient-boosted decision trees, which must be trained separately on every new table and may take hours to tune. Our research asks a simple question: can we build one reusable model that instantly adapts to any new table, even very large ones? We introduce TabICL, a model that first learns general patterns from millions of computer-generated tables and then, at test time, makes predictions by simply “reading” the new data—all in a single pass, with no extra training. By releasing the code and pretrained models, we make rapid, energy-efficient data analysis available to everyone."
Poster,TabNAT: A Continuous-Discrete Joint Generative Framework for Tabular Data,https://ICML.cc//virtual/2025/poster/45006,"Hengrui Zhang, Liancheng Fang, Qitian Wu, Philip Yu","While autoregressive models dominate natural language generation, their application to tabular data remains limited due to two challenges: 1) tabular data contains heterogeneous types, whereas autoregressive next-token (distribution) prediction is designed for discrete data, and 2) tabular data is column permutation-invariant, requiring flexible generation orders. Traditional autoregressive models, with their fixed generation order, struggle with tasks like missing data imputation, where the target and conditioning columns vary. To address these issues, we propose Diffusion-nested Non-autoregressive Transformer (TabNAT), a hybrid model combining diffusion processes and masked generative modeling. For continuous columns, TabNAT uses a diffusion model to parameterize their conditional distributions, while for discrete columns, it employs next-token prediction with KL divergence minimization. A masked Transformer with bi-directional attention enables order-agnostic generation, allowing it to learn the distribution of target columns conditioned on arbitrary observed columns. Extensive experiments on ten datasets with diverse properties demonstrate TabNAT's superiority in both unconditional tabular data generation and conditional missing data imputation tasks.","AI models that are excellent at writing text often struggle to create or complete spreadsheet-style data because tables contain a mix of different data types, like numbers and text. These models also work in a fixed, step-by-step order, which is not ideal for tables where the column order can change. To address this, we developed a flexible new model called TabNAT. This hybrid system uses a special technique for generating numbers and a different one for categories, allowing it to handle the mixed data effectively. Because it doesn't rely on a fixed order, TabNAT can generate data for any column based on the others, making it perfect for filling in missing values. In extensive tests on various datasets, TabNAT proved to be significantly better at both creating new, realistic tables and completing existing ones than previous methods."
Poster,TabPFN Unleashed: A Scalable and Effective Solution to Tabular Classification Problems,https://ICML.cc//virtual/2025/poster/46442,"Si-Yang Liu, Han-Jia Ye","TabPFN has emerged as a promising in-context learning model for tabular data, capable of directly predicting the labels of test samples given labeled training examples. It has demonstrated competitive performance, particularly on small-scale classification tasks. However, despite its effectiveness, TabPFN still requires further refinement in several areas, including handling high-dimensional features, aligning with downstream datasets, and scaling to larger datasets.In this paper, we revisit existing variants of TabPFN and observe that most approaches focus either on reducing bias or variance, often neglecting the need to address the other side, while also increasing inference overhead. To fill this gap, we propose  Beta (**B**agging and **E**ncoder-based Fine-tuning for **T**abPFN **A**daptation), a novel and effective method designed to *minimize both bias and variance*. To reduce bias, we introduce a lightweight encoder to better align downstream tasks with the pre-trained TabPFN. By increasing the number of encoders in a lightweight manner, Beta mitigates variance, thereby further improving the model’s performance. Additionally, bootstrapped sampling is employed to further  reduce the impact of data perturbations on the model, all while maintaining computational efficiency during inference. Our approach enhances TabPFN’s ability to handle high-dimensional data and scale to larger datasets. Experimental results on over 200 benchmark classification datasets demonstrate that Beta either outperforms or matches state-of-the-art methods.","Modern deep learning models often struggle to perform well when working with spreadsheet-like data, especially when the number of features (columns) is large or when only a small amount of labeled data is available. One promising solution, called TabPFN, can make predictions by looking at a few examples—much like how humans can learn quickly from just a few cases. While TabPFN works well in many situations, it still faces challenges with larger and more complex datasets.In our work, we propose a new method called Beta that improves how TabPFN learns and makes predictions. Beta combines two key ideas: using simplified helper models (encoders) to better understand the data, and blending multiple training samples (a technique known as bagging) to make the final predictions more stable. We tested Beta on over 200 real-world datasets and found that it performs as well or better than current top methods, even when the data is high-dimensional."
Poster,"TabSDS: a Lightweight, Fully Non-Parametric, and Model Free Approach for Generating Synthetic Tabular Data",https://ICML.cc//virtual/2025/poster/45094,Elias Chaibub Neto,"The development of deep generative models for tabular data is currently a very active research area in machine learning. These models, however, tend to be computationally heavy and require careful tuning of multiple model parameters. In this paper, we propose TabSDS - a lightweight, non-parametric, and model free alternative to tabular deep generative models which leverages rank and data shuffling transformations for generating synthetic data which closely approximates the joint probability distribution of the real data. We evaluate TabSDS against multiple baselines implemented in the Synthcity Python library across several datasets. TabSDS showed very competitive performance against all baselines (including TabDDPM - a strong baseline model for tabular data generation). Importantly, the execution time of TabSDS is orders of magnitude faster than the deep generative baselines, and also considerably faster than other computationally efficient baselines such as adversarial random forests.","Creating synthetic versions of real-world data—like health records or financial information—is an important task in machine learning. These artificial datasets help researchers build and test models with lower privacy risks. However, most current methods for generating this kind of data rely on complex algorithms that require a lot of computing power and a lot of work to set up correctly. Here we propose a new method for generating synthetic tabular data (data organized in rows and columns, like a spreadsheet). It uses simple techniques like sorting and shuffling to recreate the patterns found in real data. Our evaluations on several datasets showed it performed just as well as the more complicated systems while running much faster. This makes it a practical and efficient tool for anyone who needs realistic synthetic data without the computational hassle."
Poster,Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation,https://ICML.cc//virtual/2025/poster/45686,"Hung-Chieh Fang, Po-Yi Lu, Hsuan-Tien (Tien) Lin","Universal Domain Adaptation (UniDA) addresses unsupervised domain adaptation where target classes may differ arbitrarily from source ones, except for a shared subset. A widely used approach, partial domain matching (PDM), aligns only shared classes but struggles in extreme cases where many source classes are absent in the target domain, underperforming the most naive baseline that trains on only source data. In this work, we identify that the failure of PDM for extreme UniDA stems from dimensional collapse (DC) in target representations. To address target DC, we propose to jointly leverage the alignment and uniformity techniques in self-supervised learning on the unlabeled target data to preserve the intrinsic structure of the learned representations. Our experimental results confirm that SSL consistently advances PDM and delivers new state-of-the-art results across a broader benchmark of UniDA scenarios with different portions of shared classes, representing a crucial step toward truly comprehensive UniDA. Project page: https://dc-unida.github.io/","When training AI models to work well across different environments, a common challenge is that the categories the model sees during training (source domain) may not fully match those in the new environment (target domain). This problem is known as Universal Domain Adaptation (UniDA). A popular method tries to match only the overlapping categories between domains. However, in extreme cases—where most categories in the training data don't exist in the new environment—this method actually performs worse than doing nothing at all.We found that this failure is caused by something called dimensional collapse, where the model's understanding of the new environment becomes overly simplified and loses important detail. To fix this, we borrow techniques from self-supervised learning to help the model preserve the structure of the new environment’s data, even without labels.Our method consistently improves performance in a wide range of UniDA scenarios, setting new state-of-the-art results. This brings us one step closer to building AI systems that can truly adapt to real-world environments with unknown or shifting categories."
Poster,Tackling View-Dependent Semantics in 3D Language Gaussian Splatting,https://ICML.cc//virtual/2025/poster/44052,"Jiazhong Cen, Xudong Zhou, Jiemin Fang, Changsong Wen, Lingxi Xie, xiaopeng zhang, Wei Shen, Qi Tian","Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D scene reconstruction from RGB images. Many studies extend this paradigm for language-driven open-vocabulary scene understanding. However, most of them simply project 2D semantic features onto 3D Gaussians and overlook a fundamental gap between 2D and 3D understanding: a 3D object may exhibit various semantics from different viewpoints—a phenomenon we term **view-dependent semantics**. To address this challenge, we propose **LaGa** (**La**nguage **Ga**ussians), which establishes cross-view semantic connections by decomposing the 3D scene into objects. Then, it constructs view-aggregated semantic representations by clustering semantic descriptors and reweighting them based on multi-view semantics. Extensive experiments demonstrate that LaGa effectively captures key information from view-dependent semantics, enabling a more comprehensive understanding of 3D scenes. Notably, under the same settings, LaGa achieves a significant improvement of **+18.7\% mIoU** over the previous SOTA on the LERF-OVS dataset. Our code is available at: https://github.com/https://github.com/SJTU-DeepVisionLab/LaGa.","When computers try to understand 3D scenes from multi-view images, they often face a key challenge: objects can look very different from different viewpoints. For example, a book may be easy to recognize from one angle but hard to identify from another. Many existing methods ignore this and simply project 2D semantic information from a single view onto the 3D scene.We introduce LaGa, a method that improves 3D understanding by recognizing that an object’s meaning can change with the viewpoint. LaGa first segments the 3D scene to get a set of 3D objects, and then collects semantic information from multiple views of each object to build a shared understanding across viewpoints. This helps computers better interpret complex scenes, especially when using natural language.Tested on a challenging benchmark, LaGa significantly outperforms previous approaches. It offers a step forward for applications like augmented reality, robotics, and virtual environments, where accurate 3D understanding is essential."
