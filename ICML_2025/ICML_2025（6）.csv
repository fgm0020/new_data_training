type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Active Reward Modeling: Adaptive Preference Labeling for Large Language Model Alignment,https://ICML.cc//virtual/2025/poster/45827,"Yunyi Shen, Hao Sun, Jean-Francois Ton","Building neural reward models from human preferences is a pivotal component in reinforcement learning from human feedback (RLHF) and large language model alignment research. Given the scarcity and high cost of human annotation, how to select the most informative pairs to annotate is an essential yet challenging open problem. In this work, we highlight the insight that an ideal comparison dataset for reward modeling should balance *exploration of the representation space* and make *informative comparisons* between pairs with moderate reward differences. Technically, challenges arise in quantifying the two objectives and efficiently prioritizing the comparisons to be annotated. To address this, we propose the Fisher information-based selection strategies, adapt theories from the *classical experimental design* literature, and apply them to the final linear layer of the deep neural network-based reward modeling tasks. Empirically, our method demonstrates remarkable performance, high computational efficiency, and stability compared to other selection methods from deep learning and classical statistical literature across multiple open-source LLMs and datasets. Further ablation studies reveal that incorporating cross-prompt comparisons in active reward modeling significantly enhances labeling efficiency, shedding light on the potential for improved annotation strategies in RLHF. Code and embeddings to reproduce all results of this paper are available at https://github.com/YunyiShen/ARM-FI/.","In training AI systems to align better with human values, especially large language models, researchers often rely on human feedback to teach the AI what good behavior looks like. But collecting this feedback is expensive and time-consuming, so it's important to carefully choose which examples we ask humans to label. This study explores how to pick the most useful pairs of AI responses for human comparison. Ideally, we want a diverse set of examples that are informative--but not too obvious or too subtle--so they teach the model the most about human preferences. To do this, we adapt ideas from classical statistical experimental design literature to select which examples to label, using Fisher information which measures how a data point could change our parameter estimation. Our method focuses on the final layer of the neural network that learns human preferences. The results show that our approach is not only accurate but also fast and stable, outperforming other methods on several open-source language models and datasets. We find that comparing answers to different prompts--not just the same prompt--can make the training process even more efficient, offering new directions for improving how we teach AI systems with human feedback."
Poster,Active Treatment Effect Estimation via Limited Samples,https://ICML.cc//virtual/2025/poster/43954,"Zhiheng Zhang, Haoxiang Wang, Haoxuan Li, Zhouchen Lin","Designing experiments for causal effect estimation remains an enduring topic in both machine learning and statistics. While much of the existing statistical literature focuses on using central limit theorems to analyze asymptotic properties of estimators, a parallel line of research has emerged around theoretical tools that provide finite-sample error bounds, offering performance on par with—or superior to—the asymptotic approaches. These finite-sample results are especially relevant in active sampling settings where the sample size is limited (for instance, under privacy or cost constraints). In this paper, we develop a finite-sample estimator with sample complexity analysis and extend its applicability to social networks. Through simulations and real-world experiments, we show that our method achieves higher estimation accuracy with fewer samples than traditional estimators endowed with asymptotic normality and other estimators backed by finite-sample guarantees.",Active Treatment Effect Estimation via Limited Samples
Poster,Actor-Critics Can Achieve Optimal Sample Efficiency,https://ICML.cc//virtual/2025/poster/46616,"Kevin Tan, Wei Fan, Yuting Wei","Actor-critic algorithms have become a cornerstone in reinforcement learning (RL), leveraging the strengths of both policy-based and value-based methods. Despite recent progress in understanding their statistical efficiency, no existing work has successfully learned an $\epsilon$-optimal policy with a sample complexity of $O(1/\epsilon^2)$ trajectories with general function approximation when strategic exploration is necessary. We address this open problem by introducing a novel actor-critic algorithm that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$ regret when the Bellman eluder dimension $d$ does not increase with $T$ at more than a $\log T$ rate. Here, $\mathcal{F}$ is the critic function class, and $\mathcal{A}$ is the action space. Our algorithm integrates optimism, off-policy critic estimation targeting the optimal Q-function, and rare-switching policy resets. We extend this to the setting of Hybrid RL, where we show that initializing the critic with offline data yields sample efficiency gains, and also provide a \textit{non-optimistic} provably efficient actor-critic algorithm, addressing another open problem in the literature. Numerical experiments support our theoretical findings.","Reinforcement learning is a type of machine learning where an agent learns by trying different actions and getting feedback, much like how people learn through trial and error. One popular paradigm within it combines two parts: one that decides what to do (the actor) and one that evaluates how good those decisions are (the critic). However, current actor-critic methods can be slow and inefficient, especially when the agent needs to explore and try new things informed by inexact critic estimates, while the critic has to continually evaluate an ever-changing actor. It has been an open question as to whether one can devise an actor-critic algorithm that converges at an optimal rate when the critic can be arbitrarily parameterized -- with deep neural nets, linear regressions, random forests, or some other class of machine learning algorithms. We provide a method that does so by exploring strategically, using past experience more effectively, and occasionally restarting the decision-making process to avoid getting stuck. We also show that using previously collected data, rather than only learning from scratch, can speed things up even more. Our approach not only improves learning efficiency but also answers a long-standing question in the field about whether these systems can be sample efficient or not."
Poster,AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism,https://ICML.cc//virtual/2025/poster/45049,"Zhepei Wei, Wei-Lin Chen, Xinyu Zhu, Yu Meng","Large language models (LLMs) are increasingly used for long-content generation (e.g., long Chain-of-Thought reasoning) where decoding efficiency becomes a critical bottleneck: Autoregressive decoding is inherently limited by its sequential token generation process, where each token must be generated before the next can be processed. This sequential dependency restricts the ability to fully leverage modern hardware’s parallel processing capabilities. Existing methods like speculative decoding and layer skipping offer potential speedups but have notable drawbacks: speculative decoding relies on an auxiliary “drafter” model, which can be challenging to acquire and increases memory overhead, while layer skipping may introduce discrepancies in the generated outputs due to the missing key-value cache at skipped layers. In this work, we propose AdaDecode, which accelerates LLM decoding without requiring auxiliary models or changes to the original model parameters, while ensuring output consistency. AdaDecode leverages the insight that many tokens—particularly simple or highly-predictable ones—can accurately be generated at intermediate layers, as further layers often do not significantly alter predictions once the model reaches a certain confidence. By adaptively generating tokens at intermediate layers when confidence is high, AdaDecode enables the next token’s computation to begin immediately. The remaining layer computations for early-predicted tokens are deferred and executed in parallel with subsequent tokens when needed, maximizing hardware utilization and reducing decoding latency. A final verification step ensures that early predictions match the results of standard autoregressive decoding, preserving output parity. Experiments across diverse generation tasks shows that AdaDecode consistently achieves superior decoding throughput compared to baselines with up to 1.73$\times$ speedup, while guaranteeing output parity with standard autoregressive decoding.","Large language models (LLMs) are increasingly used to produce long, detailed texts such as Chain-of-Thought reasoning. However, generating this kind of content can be slow because LLMs typically produce one word at a time, and each word must be completed before the next begins. This sequential process restricts the ability to fully leverage modern computer hardware’s parallel processing capabilities.In this work, we present AdaDecode, a new method that speeds up text generation without changing the original model parameters or introducing extra auxiliary models. The idea is simple: if the model is confident about a word early on, we make early predictions using only part of the model, then start working on the next word immediately. Any unfinished computation is done in parallel later, followed by a verification step to ensure the output quality. This approach makes better use of hardware and significantly reduces generation time. Our experiments show that AdaDecode can make generation up to 1.73x faster while keeping the output exactly the same as standard generation."
Poster,Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation,https://ICML.cc//virtual/2025/poster/45369,"Jintao Tong, Ran Ma, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li","Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few training samples are available for efficient finetuning. There are majorly two challenges in this task: (1) the domain gap and (2) finetuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and we find the model's inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the model's attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn knowledge specific to target domains. Extensive experiments demonstrate that our method surpasses the state-of-the-art method in CD-FSS significantly by 2.69% and 4.68% average MIoU in 1-shot and 5-shot scenarios, respectively.","Cross-domain few-shot segmentation (CD-FSS) aims to adapt models trained on one domain with ample data to new domains with only a few labeled examples. This task is challenged by domain shifts and the difficulty of fine-tuning with limited data. In this work, we revisit adapter-based methods and highlight a novel perspective: adapters not only facilitate efficient fine-tuning but also inherently decouple domain-specific information. Building on this insight, we propose the Domain Feature Navigator (DFN) to explicitly guide the model toward learning domain-agnostic representations. To prevent overfitting to source-domain biases during training—which could hinder adaptation—we further introduce SAM-SVN, a regularization technique that limits the DFN from capturing sample-specific noise. Together, DFN and SAM-SVN enable better generalization and more effective adaptation in CD-FSS settings. Experiments show that our method significantly outperforms prior approaches in both 1-shot and 5-shot segmentation scenarios."
Poster,Adapting Precomputed Features for Efficient Graph Condensation,https://ICML.cc//virtual/2025/poster/45162,"Yuan Li, Jun Hu, Zemin Liu, Bryan Hooi, Jia Chen, Bingsheng He","Graph Neural Networks (GNNs) face significant computational challenges when handling large-scale graphs. To address this, Graph Condensation (GC) methods aim to compress large graphs into smaller, synthetic ones that are more manageable for GNN training. Recently, trajectory matching methods have shown state-of-the-art (SOTA) performance for GC, aligning the model's training behavior on a condensed graph with that on the original graph by guiding the trajectory of model parameters. However, these approaches require repetitive GNN retraining during condensation, making them computationally expensive. To address the efficiency issue, we completely bypass trajectory matching and propose a novel two-stage framework. The first stage, a precomputation stage, performs one-time message passing to extract structural and semantic information from the original graph. The second stage, a diversity-aware adaptation stage, performs class-wise alignment while maximizing the diversity of synthetic features. Remarkably, even with just the precomputation stage, which takes only seconds, our method either matches or surpasses 5 out of 9 baseline results. Extensive experiments show that our approach achieves comparable or better performance while being 96× to 2,455× faster than SOTA methods, making it more practical for large-scale GNN applications. Our code and data are available at https://github.com/Xtra-Computing/GCPA.","Training deep learning models on large-scale graphs—such as those used in social networks or e-commerce platforms—can be extremely time-consuming and computationally intensive. To improve efficiency, researchers have developed techniques that compress these large graphs into smaller versions while preserving their most important information. The most effective existing methods typically rely on iterative training, where the model is repeatedly optimized to mimic the behavior of the original graph. While this can yield high accuracy, the process is often slow and, for some large graphs, can require substantial computational time. Our method offers a significantly faster alternative: rather than retraining multiple times, we extract useful representations from the large graph in a single step and use them to build a condensed version. This approach achieves up to a 2,455× speedup over SOTA methods, while matching or even surpassing their accuracy. It provides a practical and scalable solution for efficient learning on large graphs."
Poster,Adapting to Evolving Adversaries with Regularized Continual Robust Training,https://ICML.cc//virtual/2025/poster/45276,"Sihui Dai, Christian Cianfarani, Vikash Sehwag, Prateek Mittal, Arjun Bhagoji","Robust training methods typically defend against specific attack types, such as $\ell_p$ attacks with fixed budgets, and rarely account for the fact that defenders may encounter new attacks over time.  A natural solution is to adapt the defended model to new adversaries as they arise via fine-tuning, a method which we call continual robust training (CRT).  However, when implemented naively, fine-tuning on new attacks degrades robustness on previous attacks.  This raises the question: \textit{how can we improve the initial training and fine-tuning of the model to simultaneously achieve robustness against previous and new attacks?} We present theoretical results which show that the gap in a model's robustness against different attacks is bounded by how far each attack perturbs a sample in the model's logit space, suggesting that regularizing with respect to this logit space distance can help maintain robustness against previous attacks.Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and ImageNette) and over 100 attack combinations demonstrate that the proposed regularization improves robust accuracy with little overhead in training time. Our findings and open-source code lay the groundwork for the deployment of models robust to evolving attacks.","Adversarial examples are a phenomenon where neural networks are fooled by small imperceptible perturbations.  Many existing techniques for training neural networks to be robust against adversarial examples focus on a specific type of perturbations (i.e. those which lie within a specific bounded distance from the original image). These methods typically include examples of perturbed inputs during training to teach the model the kind of mistakes to avoid. However, the types of imperceptible perturbations to which models are vulnerable is large and over time, we may be able to generate perturbations (i.e. small transformations to the inputs) which were not considered when robustly training the model (researchers have come up with many clever transformations, including changing the color of pixels slightly!). In this work, we propose repeated training of the model against new attacks, using the previous iteration of the model as a starting point. We also add a term during training designed to prevent the model outputs from drifting too far away from each other for different perturbation types. This helps with robustness to new perturbations while not `forgetting' robustness against previous types.  Our approach takes steps towards achieving robust models that can be easily adapted to new attacks, which is important for applications in which robustness is critical and model retraining is expensive."
Poster,Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning,https://ICML.cc//virtual/2025/poster/45256,"Erchi Wang, Yuqing Zhu, Yu-Xiang Wang","This paper studies the problem of differentially private empirical risk minimization (DP-ERM) for binary linear classification.   We obtain an efficient $(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of $\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} + \frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and $\gamma$ is the margin of linear separation of the remaining data points (after $S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic terms. In the agnostic case, we improve the existing results when the number of outliers is small. Our algorithm is highly adaptive because it does not require knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We also derive a utility bound for the advanced private hyperparameter tuning algorithm.",This paper presents a differentially private algorithm for binary classification. It automatically adapts without knowing the data’s margin or outliers in advance and achieves good privacy-utility trade-offs.
Poster,Adapting While Learning: Grounding LLMs for Scientific Problems with Tool Usage Adaptation,https://ICML.cc//virtual/2025/poster/44034,"Bohan Lyu, Yadi Cao, Duncan Watson-Parris, Leon Bergen, Taylor Berg-Kirkpatrick, Rose Yu","Large Language Models (LLMs) demonstrate promising capabilities in solving scientific problems but often suffer from the issue of hallucination. While integrating LLMs with tools can mitigate this issue, models fine-tuned on tool usage become overreliant on them and incur unnecessary costs.    Inspired by how human experts assess problem complexity before selecting solutions, we propose a novel two-component fine-tuning method, *Adapting while Learning* (AWL). In the first component *World Knowledge Learning* (WKL), LLMs internalize scientific knowledge by learning from tool-generated solutions. In the second component *Tool Usage Adaptation* (TUA), we categorize problems as easy or hard based on the model's accuracy, and train it to maintain direct reasoning for easy problems while switching to tools for hard ones.    We validate our method on 6 scientific benchmark datasets across climate science, epidemiology, physics, and other domains. Compared to the original instruct model (8B), models post-trained with AWL achieve 29.11\% higher answer accuracy and 12.72\% better tool usage accuracy, even surpassing state-of-the-art models including GPT-4o and Claude-3.5 on 4 custom-created datasets. Our code is open-source at \url{https://github.com/Rose-STL-Lab/Adapting-While-Learning}.","Large language models (LLMs) are starting to help scientists tackle complex questions in fields like climate science and epidemiology. However, these models sometimes make up answers to hard questions or use expensive scientific tools even when a simple answer would do. This inefficiency can make them unreliable or costly to use for real scientific work.Inspired by how human experts decide when to use a calculator and when to solve something in their head, we developed a new training method called Adapting While Learning (AWL). Our approach teaches LLMs to first try to answer scientific questions on their own, and only use specialized tools when the question is truly difficult. We do this by splitting training into two parts: first, the model learns scientific knowledge from tool-generated solutions; second, it is trained to recognize which questions are easy (and can be answered directly) and which are hard (and need a tool).We tested our method on six scientific datasets, ranging from math and physics to climate and disease modeling. Our trained models not only became more accurate, but also learned to use tools more wisely, saving time and resources. In fact, our approach even outperformed some of the largest, most advanced AI models on new, challenging datasets. We hope this work leads to more trustworthy, efficient AI assistants for science."
Poster,Adaptive Data Collection for Robust Learning Across Multiple Distributions,https://ICML.cc//virtual/2025/poster/44118,"Chengbo Zang, Mehmet Turkcan, Gil Zussman, Zoran Kostic, Javad Ghaderi","We propose a framework for adaptive data collection aimed at robust learning in multi-distribution scenarios under a fixed data collection budget. In each round, the algorithm selects a distribution source to sample from for data collection and updates the model parameters accordingly. The objective is to find the model parameters that minimize the expected loss across all the data sources. Our approach integrates upper-confidence-bound (UCB) sampling with online gradient descent (OGD) to dynamically collect and annotate data from multiple sources. By bridging online optimization and multi-armed bandits, we provide theoretical guarantees for our UCB-OGD approach, demonstrating that it achieves a minimax regret of $O(T^{\frac{1}{2}}(K\ln T)^{\frac{1}{2}})$ over $K$ data sources after $T$ rounds. We further provide a lower bound showing that the result is optimal up to a $\ln T$ factor. Extensive evaluations on standard datasets and a real-world testbed for object detection in smart-city intersections validate the consistent performance improvements of our method compared to baselines such as random sampling and various active learning methods.","Data collection and annotation is essential for robust model performance in modern machine learning and deep learning systems. Our research introduces a novel method to adaptively decide where to collect and annotate data from multiple data sources, so that the model learns as efficiently as possible under a limited budget. In an iterative fashion, the system selects a data source, collects and annotates new sample from it, and updates the model accordingly. We propose an algorithm that combines techniques from optimization and reinforcement learning with robust mathematical guarantees. We further test its performance on both well-known datasets across multiple tasks and in a real-world smart-city testbed, demonstrating its effectiveness and flexibility."
