type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation,https://ICML.cc//virtual/2025/poster/45971,"Zhendong Wang, Max Li, Ajay Mandlekar, Zhenjia Xu, Jiaojiao Fan, Yashraj Narang, Jim Fan, Yuke Zhu, Yogesh Balaji, Mingyuan Zhou, Ming-Yu Liu, Yu Zeng","Diffusion models, praised for their success in generative tasks, are increasingly being applied to robotics, demonstrating exceptional performance in behavior cloning. However, their slow generation process stemming from iterative denoising steps poses a challenge for real-time applications in resource-constrained robotics setups and dynamically changing environments.In this paper, we introduce the One-Step Diffusion Policy (OneDP), a novel approach that distills knowledge from pre-trained diffusion policies into a single-step action generator, significantly accelerating response times for robotic control tasks. We ensure the distilled generator closely aligns with the original policy distribution by minimizing the Kullback-Leibler (KL) divergence along the diffusion chain, requiring only $2\%$-$10\%$ additional pre-training cost for convergence. We evaluated OneDP on 6 challenging simulation tasks as well as 4 self-designed real-world tasks using the Franka robot. The results demonstrate that OneDP not only achieves state-of-the-art success rates but also delivers an order-of-magnitude improvement in inference speed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing its potential for dynamic and computationally constrained robotic applications. A video demo is provided at our project page, and the code will be publicly available.","Robots are increasingly using advanced AI models to learn how to perform tasks by watching demonstrations, similar to how humans learn by imitation. One promising type of model, called a diffusion model, has shown great results in teaching robots complex behaviors. However, these models are typically slow, making them hard to use in real-time scenarios—like responding quickly in dynamic environments or running on less powerful hardware.To solve this, we developed the One-Step Diffusion Policy, a new method that speeds up these slow models without losing their performance. Our approach trains a lightweight version of the original model that can make decisions in just one step, rather than many. This was done by carefully guiding the simpler model to mimic the original’s behavior, adding only a small extra training cost.We tested our method on both simulated environments and real-world robot tasks, where it matched or exceeded previous performance while making decisions over 40 times faster. This breakthrough brings us closer to making fast, capable robots that can operate reliably in the real world."
Poster,One-Step Generalization Ratio Guided Optimization for Domain Generalization,https://ICML.cc//virtual/2025/poster/45152,"Sumin Cho, Dongwon Kim, Kwangsu Kim","Domain Generalization (DG) aims to train models that generalize to unseen target domains but often overfit to domain-specific features, known as undesired correlations. Gradient-based DG methods typically guide gradients in a dominant direction but often inadvertently reinforce spurious correlations. Recent work has employed dropout to regularize overconfident parameters, but has not explicitly adjusted gradient alignment or ensured balanced parameter updates. We propose GENIE (Generalization-ENhancing Iterative Equalizer), a novel optimizer that leverages the One-Step Generalization Ratio (OSGR) to quantify each parameter's contribution to loss reduction and assess gradient alignment. By dynamically equalizing OSGR via a preconditioning factor, GENIE prevents a small subset of parameters from dominating optimization, thereby promoting domain-invariant feature learning. Theoretically, GENIE balances convergence contribution and gradient alignment among parameters, achieving higher OSGR while retaining SGD's convergence rate. Empirically, it outperforms existing optimizers and enhances performance when integrated with various DG and single-DG methods.","Domain Generalization (DG) aims to train models that perform well on unseen domains. However, existing methods often overfit to domain-specific patterns, failing to generalize robustly. Our work introduces GENIE, a novel optimizer that balances the contribution of each model parameter to generalization. GENIE identifies which parameters help or hurt generalization using a theoretical metric called the One-Step Generalization Ratio (OSGR), and dynamically adjusts their updates during training. This prevents over-reliance on a small set of spurious features and promotes learning of domain-invariant representations. As a result, GENIE improves generalization across diverse domains and can be seamlessly integrated with existing domain generalization methods."
Poster,"One Stone, Two Birds: Enhancing Adversarial Defense Through the Lens of Distributional Discrepancy",https://ICML.cc//virtual/2025/poster/43991,"Jiacheng Zhang, Benjamin Rubinstein, Jingfeng Zhang, Feng Liu","*Statistical adversarial data detection* (SADD) detects whether an upcoming batch contains *adversarial examples* (AEs) by measuring the distributional discrepancies between *clean examples* (CEs) and AEs. In this paper, we explore the strength of SADD-based methods by theoretically showing that minimizing distributional discrepancy can help reduce the expected loss on AEs. Despite these advantages, SADD-based methods have a potential limitation: they discard inputs that are detected as AEs, leading to the loss of clean information within those inputs. To address this limitation, we propose a two-pronged adversarial defense method, named ***D***istributional-discrepancy-based ***A***dversarial ***D***efense (DAD). In the training phase, DAD first optimizes the test power of the *maximum mean discrepancy* (MMD) to derive MMD-OPT, which is *a stone that kills two birds*. MMD-OPT first serves as a *guiding signal* to minimize the distributional discrepancy between CEs and AEs to train a denoiser. Then, it serves as a *discriminator* to differentiate CEs and AEs during inference. Overall, in the inference stage, DAD consists of a two-pronged process: (1) directly feeding the detected CEs into the classifier, and (2) removing noise from the detected AEs by the distributional-discrepancy-based denoiser. Extensive experiments show that DAD outperforms current *state-of-the-art* (SOTA) defense methods by *simultaneously* improving clean and robust accuracy on CIFAR-10 and ImageNet-1K against adaptive white-box attacks. Codes are publicly available at: https://github.com/tmlr-group/DAD.","*Statistical adversarial data detection* (SADD) is a powerful method that leverages distributional discrepancy to defend against *adversarial examples* (AEs). However, they discard entire batches of samples if they are detected as AEs, leading to the loss of clean information within those samples. We aim to design an adversarial defense method that leverages the effectiveness of SADD-based methods, while at the same time, preserving all the data before feeding them into a classifier.In this paper, we first prove that minimizing distributional discrepancy helps reduce the expected loss on AEs, which motivates the design of our method. We propose ***D***istributional-discrepancy-based ***A***dversarial ***D***efense (DAD).  In the training phase, DAD first optimizes the test power of the *maximum mean discrepancy* (MMD) to derive MMD-OPT, which is *a stone that kills two birds*.MMD-OPT first serves as a *guiding signal* to minimize the distributional discrepancy between *clean examples* (CEs) and AEs to train a denoiser. Then, it serves as a *discriminator* to differentiate CEs and AEs during inference. Overall, in the inference stage, DAD consists of a two-pronged process: (1) directly feeding the detected CEs into the classifier, and (2) removing noise from the detected AEs by the distributional-discrepancy-based denoiser.DAD combines the strengths of SADD-based and denoiser-based methods while addressing their limitations: DAD separates CEs and AEs in the inference phase, thereby keeping the accuracy for CEs nearly unaffected (i.e., the utility of the model is unaffected). At the same time, AEs can be properly handled by the denoiser (i.e., the robustness of the model is improved). Furthermore, DAD is very efficient and can generalize well to unseen attacks, which can be deployed into real-world systems to defend against adversarial attacks."
Poster,One Wave To Explain Them All: A Unifying Perspective On Feature Attribution,https://ICML.cc//virtual/2025/poster/44099,"Gabriel Kasmi, Amandine Brunetto, Thomas Fel, Jayneel Parekh","Feature attribution methods aim to improve the transparency of deep neural networks by identifying the input features that influence a model's decision. Pixel-based heatmaps have become the standard for attributing features to high-dimensional inputs, such as images, audio representations, and volumes. While intuitive and convenient, these pixel-based attributions fail to capture the underlying structure of the data. Moreover, the choice of domain for computing attributions has often been overlooked. This work demonstrates that the wavelet domain allows for informative and meaningful attributions. It handles any input dimension and offers a unified approach to feature attribution. Our method, the **W**avelet **A**ttribution **M**ethod (WAM), leverages the spatial and scale-localized properties of wavelet coefficients to provide explanations that capture both the *where* and *what* of a model's decision-making process. We show that WAM quantitatively matches or outperforms existing gradient-based methods across multiple modalities, including audio, images, and volumes. Additionally, we discuss how WAM bridges attribution with broader aspects of model robustness and transparency. Project page: https://gabrielkasmi.github.io/wam/","Modern machine learning models, especially deep neural networks, are very accurate but lack transparency, which can be a problem in critical applications.To overcome this limitations, feature attribution methods have been introduced to highlight which individual input variables contribute the most to the model's decision. When dealing with input data such as images, feature attribution method usually consist in computing a heatmap, which highlights which pixels or image regions contributed the most to the model's decision. However, these ""pixel-based"" methods do not inform cannot indicate whether it is the shape or the texture of the object that mattered for the model's prediction and the pixel space is only convenient for representing images.Our work argues that attributing the model's decision to the wavelet decompisition of the input signal provides more informative and more generalizable model explanations.Wavelets are a mathematical tool that decomposes an input into different scales, allowing to isolate textures or edges on images, or bursts of sound on waveforms.We demonstrate that our approach, the Wavelet Attribution Method provides more meaningful explanations than existing techniques. It also opens up new ways to explore how reliable and robust a model’s decisions are, contributing to AI transparency."
Poster,On Exact Bit-level Reversible Transformers Without Changing Architecture,https://ICML.cc//virtual/2025/poster/43627,"Guoqiang Zhang, John Lewis, W. Bastiaan Kleijn","In this work we present the BDIA-transformer, which is an exact bit-level reversible transformer that uses an unchanged standard architecture for inference. The basic idea is to first treat each transformer block as the Euler integration approximation for solving an ordinary differential equation (ODE) and then incorporate the technique of bidirectional integration approximation (BDIA) (originally designed for diffusion inversion) into the neural architecture, together with activation quantization to make it exactly bit-level reversible. In the training process, we let a hyper-parameter $\gamma$ in BDIA-transformer randomly take one of the two values $\{0.5, -0.5\}$ per training sample per transformer block for averaging every two consecutive integration approximations. As a result, BDIA-transformer can be viewed as training an ensemble of ODE solvers parameterized by a set of binary random variables,  which regularizes the model and results in improved validation accuracy. Lightweight side information is required to be stored in the forward process to account for binary quantization loss to enable exact bit-level reversibility.  In the inference procedure, the expectation $\mathbb{E}(\gamma)=0$ is taken to make the resulting architecture identical to transformer up to activation quantization. Our experiments in  natural language generation, image classification, and language translation show that BDIA-transformers outperform their conventional counterparts significantly in terms of validation performance while also requiring considerably less training memory. Thanks to the regularizing effect of the ensemble, the BDIA-transformer is particularly suitable for fine-tuning with limited data. Source-code can be found via \href{https://github.com/guoqiang-zhang-x/BDIA-Transformer}{this link}.","Nowadays, almost all popular large language models (LLMs) use the so-called transformer (consisting of a sequence of blocks) architecture to learn from data. Fine-tuning a pre-trained transformer-based LLM for downstream tasks usually involves a small or medium-sized dataset and is prone to overfitting, implying that the LLM attempts to memorize the data rather than understand it.Our work proposes a new technique named __bidirectional integration approximation__ (BDIA) to assist fine-tuning a transformer-based LLM to reduce overfitting.  The basic idea is to fine-tune an ensemble of transformer-based LLMs parameterised by a set of binary random variables, which essentially enforces those LLMs in the ensemble to understand data rather than memorize it. After finishing fine-tuning, we take the average of all the LLMs in the ensemble as the final LLM to be employed in practice.If needed, BDIA can also be implemented to save GPU memory in the fine-tuning process. To do so, we perform quantization on the output of each block of each transformer model in the ensemble when feeding input data to the model. With BDIA and quantization, it becomes feasible to update each block in each model in the ensemble on-the-fly.Experiments on natural language generation, translation, and image classification, confirm that our new BDIA technique can indeed reduce over-fitting, promoting the transformer model to understand data."
Poster,On Explaining Equivariant Graph Networks via Improved Relevance Propagation,https://ICML.cc//virtual/2025/poster/45859,"Hongyi Ling, Haiyang Yu, Zhimeng Jiang, Na Zou, Shuiwang Ji","We consider explainability in equivariant graph neural networks for 3D geometric graphs. While many XAI methods have been developed for analyzing graph neural networks, they predominantly target 2D graph structures. The complex nature of 3D data and the sophisticated architectures of equivariant GNNs present unique challenges. Current XAI techniques either struggle to adapt to equivariant GNNs or fail to effectively handle positional data and evaluate the significance of geometric features adequately.   To address these challenges, we introduce a novel method, known as EquiGX, which uses the Deep Taylor decomposition framework to extend the layer-wise relevance propagation rules tailored for spherical equivariant GNNs. Our approach decomposes prediction scores and back-propagates the relevance scores through each layer to the input space. Our decomposition rules provide a detailed explanation of each layer’s contribution to the network’s predictions, thereby enhancing our understanding of how geometric and positional data influence the model’s outputs.   Through experiments on both synthetic and real-world datasets, our method demonstrates its capability to identify critical geometric structures and outperform alternative baselines. These results indicate that our method provides significantly enhanced explanations for equivariant GNNs. Our code has been released as part of the AIRS library (https://github.com/divelab/AIRS/).","Understanding how machine learning models make decisions is crucial, especially when they analyze complex 3D structures like molecules or physical systems. While there are many tools to explain decisions made by models working on simpler, 2D data, these tools often fall short when applied to advanced models that work with 3D information. In this work, we focus on a special type of model called equivariant graph neural networks, which are designed to handle 3D geometric data in a way that respects its spatial structure. We develop a new explanation method called EquiGX that helps us see how these models arrive at their predictions. EquiGX works by breaking down the model’s output into contributions from each layer, tracing this back to the input data, and showing which 3D features matter most.We tested our method on both artificial and real-world datasets and found that it gives clearer and more accurate explanations than existing approaches. This work helps open the “black box” of 3D deep learning models, making them more transparent and trustworthy."
Poster,On Expressive Power of Looped Transformers: Theoretical Analysis and Enhancement via Timestep Encoding,https://ICML.cc//virtual/2025/poster/45800,"Kevin Xu, Issei Sato","Looped Transformers provide advantages in parameter efficiency, computational capabilities, and generalization for reasoning tasks. However, their expressive power regarding function approximation remains underexplored. In this paper, we establish the approximation rate of Looped Transformers by defining the modulus of continuity for sequence-to-sequence functions. This reveals a limitation specific to the looped architecture. That is, the analysis prompts the incorporation of scaling parameters for each loop, conditioned on timestep encoding. Experiments validate the theoretical results, showing that increasing the number of loops enhances performance, with further gains achieved through the timestep encoding.","Transformers typically apply multiple layers once to the input, but what happens if we reuse the same layer multiple times? We explore how looping, a core concept in computer science, can benefit neural networks.In this work, we show a surprising result: even with a fixed number of parameters in a single layer, performance improves as we increase the number of loops. In contrast, conventional Transformers require increasing the parameter count to achieve better results.We further analyze a fundamental limitation of this looped design: after a point, adding more loops no longer helps. To address this, we propose a simple solution—injecting timestep information into each loop, which overcomes the barrier and boosts performance further.Our findings provide a deeper theoretical understanding of Looped Transformers and suggest a new modeling paradigm: achieving high performance through parameter-efficient architectures. This opens the door to more compact, generalizable, and resource-friendly models."
Poster,On Fine-Grained Distinct Element Estimation,https://ICML.cc//virtual/2025/poster/45716,"Ilias Diakonikolas, Daniel Kane, Jasper Lee, Thanasis Pittas, David Woodruff, Samson Zhou","We study the problem of distributed distinct element estimation, where $\alpha$ servers each receive a subset of a universe $[n]$ and aim to compute a $(1+\varepsilon)$-approximation to the number of distinct elements using minimal communication. While prior work establishes a worst-case bound of $\Theta\left(\alpha\log n+\frac{\alpha}{\varepsilon^2}\right)$ bits, these results rely on assumptions that may not hold in practice. We introduce a new parameterization based on the number $C = \frac{\beta}{\varepsilon^2}$ of pairwise collisions, i.e., instances where the same element appears on multiple servers, and design a protocol that uses only $O\left(\alpha\log n\log\log n+\frac{\sqrt{\beta}}{\varepsilon^2} \log n\right)$ bits, breaking previous lower bounds when $C$ is small. We further improve our algorithm under assumptions on the number of distinct elements or collisions and provide matching lower bounds in all regimes, establishing $C$ as a tight complexity measure for the problem. Finally, we consider streaming algorithms for distinct element estimation parameterized by the number of items with frequency larger than $1$. Overall, our results offer insight into why statistical problems with known hardness results can be efficiently solved in practice.","When data is spread across multiple servers, it's often important to estimate how many unique items there are without moving all the data to one place. Previous methods for this task focused on worst-case scenarios and required a lot of communication between servers, which can be inefficient and impractical. We develop a new approach that looks at how often the same item appears on multiple servers—a quantity we call ""collisions""—and uses this to guide the communication strategy. This leads to much lower communication in typical cases, making the process faster and more scalable. Our results also help explain why these estimation tasks are easier in practice than worst-case theory suggests."
Poster,On Learning Parallel Pancakes with Mostly Uniform Weights,https://ICML.cc//virtual/2025/poster/44305,"Ilias Diakonikolas, Daniel Kane, Sushrut Karmalkar, Jasper Lee, Thanasis Pittas","We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on $\mathbb R^d$. This task is known to have complexity $d^{\Omega(k)}$ in full generality. To circumvent this exponential lower bound on the number of components, research has focused on learning families of GMMs satisfying additional structural properties. A natural assumption posits that the component weights are not exponentially small and that the components have the same unknown covariance. Recent work gave a $d^{O(\log(1/w_{\min}))}$-time algorithm for this class of GMMs, where $w_{\min}$ is the minimum weight. Our first main result is a Statistical Query (SQ) lower bound showing that this quasi-polynomial upper bound is essentially best possible, even for the special case of uniform weights. Specifically, we show that it is SQ-hard to distinguish between such a mixture and the standard Gaussian. We further explore how the distribution of weights affects the complexity of this task. Our second main result is a quasi-polynomial upper bound for the aforementioned testing task when most of the weights are uniform while a small fraction of the weights are potentially arbitrary.","Gaussian mixtures are often used to describe data that comes from several overlapping groups. These models are widely used in fields like image processing and genetics to make sense of complex data. However, learning the underlying structure, especially when the number of groups grows, quickly becomes very hard, often taking an impractical amount of computation.To make progress, past works have studied simpler versions of the problem. For example, they might assume that all groups are shaped similarly and that no group is too small in size. Recent work showed that, under these assumptions, the problem can be solved faster, though still not efficiently enough for very large datasets. A natural question is whether this can be further.Our research shows that even when all the groups are equally sized, the problem remains hard, and the above complexity is unavoidable. We then investigate what happens when most of the groups are equally sized but a small number are allowed to be much smaller. In this case, we show that there is an algorithm whose complexity lies between the two extremes: the case where all groups are equally sized and the case where group sizes are completely arbitrary."
Poster,On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms,https://ICML.cc//virtual/2025/poster/43671,"Ekaterina Borodich, Alexander Gasnikov, Dmitry Kovalev","We revisit the smooth convex-concave bilinearly-coupled saddle-point problem of the form $\min_x\max_y f(x) + \langle y,\mathbf{B} x\rangle - g(y)$. In the highly specific case where function $f(x)$ is strongly convex and function $g(y)$ is affine, or both functions are affine, there exist lower bounds on the number of gradient evaluations and matrix-vector multiplications required to solve the problem, as well as matching optimal algorithms. A notable aspect of these algorithms is that they are able to attain linear convergence, i.e., the number of iterations required to solve the problem is proportional to $\log(1/\epsilon)$. However, the class of bilinearly-coupled saddle-point problems for which linear convergence is possible is much wider and can involve general smooth non-strongly convex functions $f(x)$ and $g(y)$. Therefore, *we develop the first lower complexity bounds and matching optimal linearly converging algorithms for this problem class*. Our lower complexity bounds are much more general, but they cover and unify the existing results in the literature. On the other hand, our algorithm implements the separation of complexities, which, for the first time, enables the simultaneous achievement of both optimal gradient evaluation and matrix-vector multiplication complexities, resulting in the best theoretical performance to date.","In this paper, we revisit iterative first-order, i.e., gradient algorithms for solving a class of minimax optimization problems of the form $\min_x \max_y f(x) + \langle y, \mathbf{B} x\rangle - g(y)$. We investigate the following question: ""How many computations of the gradients $\nabla f(x)$, $\nabla g(y)$, and matrix-vector multiplications with the matrix $\mathbf{B}$ are necessary to find an approximate solution to the problem up to a given precision?"" We provide an exhaustive answer to this question: (1) we establish mathematical lower bounds on these numbers; (2) we develop a state-of-the-art algorithm for solving the problem, which, for the first time in the literature, can simultaneously match these lower bounds."
