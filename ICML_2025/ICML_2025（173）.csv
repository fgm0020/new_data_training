type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,LLaVA-ReID: Selective Multi-image Questioner for Interactive Person Re-Identification,https://ICML.cc//virtual/2025/poster/44702,"Yiding Lu, Mouxing Yang, Dezhong Peng, Peng Hu, Yijie Lin, Xi Peng","Traditional text-based person ReID assumes that person descriptions from witnesses are complete and provided at once. However, in real-world scenarios, such descriptions are often partial or vague. To address this limitation, we introduce a new task called interactive person re-identification (Inter-ReID). Inter-ReID is a dialogue-based retrieval task that iteratively refines initial descriptions through ongoing interactions with the witnesses. To facilitate the study of this new task, we construct a dialogue dataset that incorporates multiple types of questions by decomposing fine-grained attributes of individuals. We further propose LLaVA-ReID, a question model that generates targeted questions based on visual and textual contexts to elicit additional details about the target person. Leveraging a looking-forward strategy, we prioritize the most informative questions as supervision during training. Experimental results on both Inter-ReID and text-based ReID benchmarks demonstrate that LLaVA-ReID significantly outperforms baselines.","Imagine you’re trying to help security staff find someone you saw earlier on a busy street or in a shopping mall. You might say, “He was tall, wearing a plaid shirt, and carrying a bag.” But such descriptions are often vague and incomplete, making it hard for computer systems to identify the person in surveillance footage.Our research proposes a more interactive solution. Instead of relying on a one-time description, our system asks follow-up questions to refine your memory, much like a helpful assistant. It might ask, “What color were his pants?” or “Was he carrying anything else?”This approach helps systems find the right person more accurately and efficiently in real-world settings like malls, transit hubs, or office buildings."
Poster,LLM Alignment as Retriever Optimization: An Information Retrieval Perspective,https://ICML.cc//virtual/2025/poster/45659,"Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan Arik","Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative.In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.","Large language models (LLMs) like ChatGPT have transformed how we interact with technology — they can reason, write code, and hold conversations. But ensuring these models behave reliably and ethically is a major challenge. Without proper safeguards, they can produce misinformation, show bias, or behave unpredictably.Traditionally, aligning these models to behave well involves complicated training techniques. In our work, we introduce a much simpler and more effective way to align LLMs by borrowing ideas from how search engines work.Just like a search engine finds and ranks relevant information, our method trains LLMs to “rank” better responses — rewarding the ones that are more helpful or accurate. We call this approach LarPO, short for LLM Alignment as Retriever Preference Optimization.Our results show that this strategy significantly improves how well the models perform on challenging tasks. By connecting language model training with ideas from information retrieval, we offer a new and practical path toward safer and more trustworthy AI systems."
Poster,LLM-Assisted Semantically Diverse Teammate Generation for Efficient Multi-agent Coordination,https://ICML.cc//virtual/2025/poster/45055,"Lihe Li, lei yuan, Pengsen Liu, Tao Jiang, Yang Yu","Training with diverse teammates is the key for learning generalizable agents. Typical approaches aim to generate diverse teammates by utilizing techniques like randomization, designing regularization terms, or reducing policy compatibility, etc. However, such teammates lack semantic information, resulting in inefficient teammate generation and poor adaptability of the agents. To tackle these challenges, we propose Semantically Diverse Teammate Generation (SemDiv), a novel framework leveraging the capabilities of large language models (LLMs) to discover and learn diverse coordination behaviors at the semantic level. In each iteration, SemDiv first generates a novel coordination behavior described in natural language, then translates it into a reward function to train a teammate policy. Once the policy is verified to be meaningful, novel, and aligned with the behavior, the agents train a policy for coordination. Through this iterative process, SemDiv efficiently generates a diverse set of semantically grounded teammates, enabling agents to develop specialized policies, and select the most suitable ones through language-based reasoning to adapt to unseen teammates. Experiments show that SemDiv generates teammates covering a wide range of coordination behaviors, including those unreachable by baseline methods. Evaluation across four MARL environments, each with five unseen representative teammates, demonstrates SemDiv's superior coordination and adaptability. Our code is available at https://github.com/lilh76/SemDiv.","How can we train learning agents to effectively generalize and adapt to unseen teammates? We address this question by introducing a novel framework called Semantically Diverse Teammate Generation (SemDiv), which leverages large language models (LLMs) to discover and learn diverse coordination behaviors at the semantic level.  Our paper presents two key findings. First, we demonstrate that traditional approaches to teammate diversity—such as randomization, regularization, or policy compatibility reduction—often fail to produce semantically meaningful behaviors, limiting both the efficiency of teammate generation and the adaptability of trained agents. This is surprising because prior work assumed that procedural diversity (e.g., varying policy parameters) was sufficient for generalization. Second, we show that by grounding teammate generation in natural language descriptions of coordination behaviors—and then translating these into reward functions—SemDiv produces a richer, more interpretable set of teammates than previously possible.  Our results have implications for how we design multi-agent learning systems, suggesting that semantic diversity, rather than purely algorithmic variation, is crucial for developing agents that can reason about and adapt to novel partners. Experiments across four multi-agent environments confirm that SemDiv generates a broader range of coordination behaviors than baseline methods, enabling superior generalization to unseen teammates."
Poster,LLM-Augmented Chemical Synthesis and Design Decision Programs,https://ICML.cc//virtual/2025/poster/45488,"Haorui Wang, Jeff Guo, Lingkai Kong, Rampi Ramprasad, Philippe Schwaller, Yuanqi Du, Chao Zhang","Retrosynthesis, the process of breaking down a target molecule into simpler precursors through a series of valid reactions, stands at the core of organic chemistry and drug development. Although recent machine learning (ML) research has advanced single-step retrosynthetic modeling and subsequent route searches, these solutions remain restricted by the extensive combinatorial space of possible pathways. Concurrently, large language models (LLMs) have exhibited remarkable chemical knowledge, hinting at their potential to tackle complex decision-making tasks in chemistry. In this work, we explore whether LLMs can successfully navigate the highly constrained, multi-step retrosynthesis planning problem. We introduce an efficient scheme for encoding reaction pathways and present a new route-level search strategy, moving beyond the conventional step-by-step reactant prediction. Through comprehensive evaluations, we show that our LLM-augmented approach excels at retrosynthesis planning and extends naturally to the broader challenge of synthesizable molecular design.","Designing a new drug is pointless if chemists can’t figure out how to build it. Retrosynthesis—the backwards puzzle of breaking a complex molecule into buyable pieces is still a challenging task for today's AI.We turned to large language models, asking them to write the whole recipe at once instead of guessing one move at a time. By inventing a compact “sentence” that encodes an entire reaction pathway and coupling it with a smart search routine, our system explores the maze efficiently.On benchmark tests, it delivers shorter, more practical syntheses than leading planners—and can even suggest novel, synthesizable molecules—promising faster, cheaper paths from idea to real-world medicines and materials."
Poster,LLM Data Selection and Utilization via Dynamic Bi-level Optimization,https://ICML.cc//virtual/2025/poster/46544,"Yang Yu, Kai Han, Hang Zhou, Yehui Tang, Kaiqi Huang, Yunhe Wang, Dacheng Tao","While large-scale training data is fundamental for developing capable large language models (LLMs), strategically selecting high-quality data has emerged as a critical approach to enhance training efficiency and reduce computational costs. Current data selection methodologies predominantly rely on static, training-agnostic criteria, failing to account for the dynamic model training and data interactions. In this paper, we propose a new Data Weighting Model (DWM) to adjust the weight of selected data within each batch to achieve a dynamic data utilization during LLM training. Specially, to better capture the dynamic data preference of the trained model, a bi-level optimization framework is implemented to update the weighting model. Our experiments demonstrate that DWM enhances the performance of models trained with randomly-selected data, and the learned weighting model can be transferred to enhance other data selection methods and models of different sizes. Moreover, we further analyze how a model’s data preferences evolve throughout training, providing new insights into the data preference of the model during training.","Training powerful language models requires vast amounts of data, but using all available data can be inefficient, costly, and environmentally taxing — especially when much of it is low quality. While recent methods try to select better data before training begins, they often ignore how a model’s preferences change as it learns.Our work introduces a new model called the Data Weighting Model (DWM), which dynamically adjusts how much influence each piece of data has during training. Instead of treating all data in a batch equally, DWM learns to emphasize the most helpful examples at each training stage. This system is trained using a novel two-step optimization approach that tracks the model’s evolving preferences. We show that even when starting with randomly chosen data, DWM can improve training efficiency and performance — even outperforming some hand-picked datasets. The trained weighting model also works well across different model sizes and can boost other data selection techniques. Our research offers a more adaptive, cost-effective way to train large language models."
Poster,LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification,https://ICML.cc//virtual/2025/poster/45114,"Hang Gao, Huang Wenxuan, Fengge Wu, Zhao Junsuo, Changwen Zheng, Huaping Liu","The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module.","Graph neural networks (GNNs) are powerful tools for analyzing complex relationships in data, but improving how they represent these relationships is still an ongoing challenge. In this study, we explore how large language models (LLMs) can be used to enhance the features that GNNs rely on for better performance. We created a synthetic dataset with controlled causal relationships to better understand how LLMs and GNNs interact. By experimenting with these data, we were able to reveal hidden patterns in the way these two systems work together. Based on our findings, we developed a new optimization tool that helps improve the way information is shared between LLMs and GNNs. Our experiments show that this tool can significantly boost the performance of GNNs on various tasks. This work provides valuable insights into the inner workings of LLM-enhanced GNNs and presents a practical solution for improving their capabilities."
Poster,LLMScan: Causal Scan for LLM Misbehavior Detection,https://ICML.cc//virtual/2025/poster/45556,"Mengdi Zhang, Goh Kiat, Peixin Zhang, Jun Sun, Lin Rose, Hongyu Zhang","Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMScan, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMScan systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's `brain' behaves differently when generating harmful or untruthful responses. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMScan effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.","Large language models (LLMs) can generate fluent, human-like text across many domains. But sometimes, they also produce responses that are untruthful, harmful, or even manipulated, raising risks in sensitive settings like healthcare, finance, or education. How can we tell when a model is misbehaving?In this work, we introduce LLMScan, a tool designed to monitor and detect such misbehavior by analyzing how the model ""thinks"" internally. Instead of just looking at what the model says, LLMScan examines how different parts of the model contribute to the answer it generates by causality analysis. With the generated causal map, we can detect patterns that distinguish normal behavior from harmful or deceptive ones.Through extensive experiments across tasks and models, LLMScan shows that it can reliably identify when a model is going off track. This opens the door to safer and more trustworthy language models that can be monitored and corrected in real time."
Poster,LLMs Can Reason Faster Only If We Let Them,https://ICML.cc//virtual/2025/poster/43727,"Bilgehan Sel, Lifu Huang, Naren Ramakrishnan, Ruoxi Jia, Ming Jin","Large language models (LLMs) are making inroads into classical AI problems such as automated planning, yet key shortcomings continue to hamper their integration. Chain-of-Thought (CoT) struggles in complex multi-step reasoning, and Tree-of-Thoughts requires multiple queries that increase computational overhead. Recently, Algorithm-of-Thoughts (AoT) have shown promise using in-context examples, at the cost of significantly longer solutions compared to CoT.  Aimed at bridging the solution length gap between CoT and AoT, this paper introduces AoT-O3, which combines supervised finetuning on AoT-style plans with a reinforcement learning (RL) framework designed to reduce solution length. The RL component uses a reward model that favors concise, valid solutions  while maintaining planning accuracy. Empirical evaluations indicate that AoT-O3 shortens solution length by up to 80\% compared to baseline AoT while maintaining or surpassing prior performance. These findings suggest a promising pathway for more efficient, scalable LLM-based planning.","Large language models (LLMs) can solve complex problems better when they are guided in smarter ways. The paper introduces a new method called AoT-O3 that helps these models plan more efficiently by giving rewards for shorter, accurate solutions. This approach significantly cuts down on the steps needed to reach a solution—by up to 80\%—without sacrificing quality. As a result, it also reduces energy use and makes AI more scalable and environmentally friendly."
Poster,LLMs can see and hear without any training,https://ICML.cc//virtual/2025/poster/44693,"Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar","We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple, training-free approach, to imbue multimodal capabilities intoyour favorite LLM. Leveraging their innate ability to perform multi-step reasoning, MILS prompts the LLM to generate candidate outputs, each of which are scored and fed back iteratively, eventually generating a solution to the task. This enables various applications that typically require training specialized models on task-specific data. In particular, we establish a new state-of-the-art on emergent zero-shot image, video and audio captioning. MILS seamlessly applies to media generation as well, discovering prompt rewrites to improve text-to-image generation, and even edit prompts for style transfer! Finally, being a gradient-free optimization approach, MILS can invert multimodal embeddings into text, enabling applications like cross-modal arithmetic.","This work introduces MILS, a method that allows large language models (LLMs) to interpret images, videos, and audio—without any additional training or task-specific data. MILS connects an LLM with a scoring model that evaluates how well each proposed caption or description matches a given input, such as an image or audio clip. The LLM generates multiple candidate responses, receives feedback from the scorer, and refines its outputs iteratively.MILS demonstrates strong zero-shot performance across a wide range of tasks: captioning visual and audio inputs, enhancing text-to-image generation, performing style transfer, and even combining information across modalities. It does all this using only pre-trained models and test-time reasoning, avoiding any fine-tuning or supervised training.By leveraging the native reasoning ability of LLMs and the representational power of multimodal models, MILS showcases that powerful multimodal understanding and generation can emerge without explicit supervision. Its simplicity and flexibility open new possibilities for building general-purpose AI systems that operate across modalities."
Poster,LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws,https://ICML.cc//virtual/2025/poster/45734,"Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel","Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute.More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance and generalization.In this work, we investigate which factors most strongly influence loss-to-loss scaling.Our experiments reveal that the pretraining data determines the scaling trend.In contrast, model size, optimization hyperparameters, tokenizer and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, generally have limited impact.Consequently, practitioners should carefully curate pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.","Our work indicates that if two models with different training setups (architecture, context length, tokenizer, etc.) but trained on the same data—achieve similar training losses, they will exhibit closely matched downstream test performance."
