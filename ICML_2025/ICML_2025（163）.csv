type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Learning Gaussian  DAG Models without Condition Number Bounds,https://ICML.cc//virtual/2025/poster/44495,"Constantinos Daskalakis, Vardis Kandiros, Rui Yao","We study the problem of learning the topology of a directed Gaussian Graphical Model under the equal-variance assumption, where the graph has $n$ nodes and maximum in-degree $d$. Prior work has established that $O(d \log n)$ samples are sufficient for this task. However, an important factor that is often overlooked in these analyses is the dependence on the condition number of the covariance matrix of the model. Indeed, all algorithms from prior work require a number of samples that grows polynomially with this condition number. In many cases this is unsatisfactory, since the condition number could grow polynomially with $n$, rendering these prior approaches impractical in high-dimensional settings. In this work, we provide an algorithm that recovers the underlying graph and prove that the number of samples required is independent of the condition number. Furthermore, we establish lower bounds that nearly match the upper bound up to a $d$-factor, thus providing an almost tight characterization of the true sample complexity of the problem. Moreover, under a further assumption that all the variances of the variables are bounded, we design a polynomial-time algorithm that recovers the underlying graph, at the cost of an additional polynomial dependence of the sample complexity on $d$. We complement our theoretical findings with simulations on synthetic datasets that confirm our predictions.","We study the problem of learning a specific type of Bayesian network, a statistical model used to model the cause-and-effect relationships in various areas. There are prior works that tackle this problem, but the number of samples used by prior works depends on the condition number (the ratio of the largest to the smallest eigenvalue) of the covariance matrix. This condition number bounds are not optimal, so we provide a new algorithm that finds the Bayesian network to reduce the number of samples. We prove that the number of samples our algorithm requires is independent of the condition number. Additionally, we provide examples to show that the upper bound of the samples complexity of our algorithm is almost tight. Moreover, under certain additional assumptions, we design a polynomial-time algorithm that recovers the underlying graph, which does not depend on the condition number. Finally, we run simulations on synthetic datasets that confirm our predictions."
Poster,Learning Imbalanced Data with Beneficial Label Noise,https://ICML.cc//virtual/2025/poster/46163,"Guangzheng Hu, Feng Liu, Mingming Gong, Guanghui Wang, Liuhua Peng","Data imbalance is a common factor hindering classifier performance. Data-level approaches for imbalanced learning, such as resampling, often lead to information loss or generative errors. Building on theoretical studies of imbalance ratio in binary classification, it is found that adding suitable label noise can adjust biased decision boundaries and improve classifier performance. This paper proposes the Label-Noise-based Re-balancing (LNR) approach to solve imbalanced learning by employing a novel design of an asymmetric label noise model. In contrast to other data-level methods, LNR alleviates the issues of informative loss and generative errors and can be integrated seamlessly with any classifier or algorithm-level method. We validated the superiority of LNR on synthetic and real-world datasets. Our work opens a new avenue for imbalanced learning, highlighting the potential of beneficial label noise.","Machine learning struggles when one category (like fraudulent transactions) is vastly outnumbered by another (like normal transactions). Traditional fixes—deleting common examples or creating fake rare ones—often lose critical information or produce unrealistic data.We propose LNR, a simple but effective solution:  we intentionally mislabel a small number of common examples as rare, labeling some suspicious ""normal transactions"" as ""fraudulent"" to stop the model from ignoring genuine fraud patterns. Unlike other methods, LNR preserves all original features, avoids information loss, and unrealistic samples.Tests across binary tabular data classification and multi-class image recognition tasks show LNR consistently improves rare-class recognition. Surprisingly, it proves that not all ""label errors"" are harmful—when applied strategically, they can enhance fairness. LNR's plug-and-play design makes it universally applicable to imbalance challenges in healthcare, finance, computer vision, and more, offering an easier way to help machine learning models see the ""unseen."""
Poster,Learning Imperfect Information Extensive-form Games with Last-iterate Convergence under Bandit Feedback,https://ICML.cc//virtual/2025/poster/45856,"Canzhe Zhao, Yutian Cheng, Jing Dong, Baoxiang Wang, Shuai Li","We investigate learning approximate Nash equilibrium (NE) policy profiles in two-player zero-sum imperfect information extensive-form games (IIEFGs) with last-iterate convergence guarantees. Existing algorithms either rely on full-information feedback or provide only asymptotic convergence rates. In contrast, we focus on the bandit feedback setting, where players receive feedback solely from the rewards associated with the experienced information set and action pairs in each episode. Our proposed algorithm employs a negentropy regularizer weighted by a ""virtual transition"" over the information set-action space to facilitate an efficient approximate policy update. Through a carefully designed virtual transition and leveraging the entropy regularization technique, we demonstrate finite-time last-iterate convergence to the NE with a rate of $\widetilde{\mathcal{O}}(k^{-\frac{1}{8}})$ under bandit feedback in each episode $k$. Empirical evaluations across various IIEFG instances show its competitive performance compared to baseline methods.","In competitive games like poker, players must make decisions without knowing their opponent’s moves—a scenario called imperfect information. Finding optimal strategies (Nash equilibria) in such games is challenging, especially when players only learn from their own gameplay (bandit feedback), rather than observing the full game dynamics.Our work introduces a new algorithm that helps players efficiently improve their strategies over time, ensuring they converge to a near-optimal solution with guaranteed performance. Unlike prior methods, which either require full knowledge of the game or lack convergence guarantees, our approach uses a carefully designed regularization technique to balance exploration and exploitation. We prove that our method converges reliably, even with limited feedback, and demonstrate its effectiveness across various game scenarios. This could have applications in AI for games, strategic decision-making, and even real-world negotiations where information is incomplete.In short: we propose a practical way for AI (or humans) to learn strong strategies in competitive, hidden-information settings—with theoretical guarantees and strong empirical results."
Poster,Learning In-context $n$-grams with Transformers: Sub-$n$-grams Are Near-Stationary Points,https://ICML.cc//virtual/2025/poster/45448,"Aditya Vardhan Varre, Gizem Yüce, Nicolas Flammarion","In this article, we explore the loss landscape of next-token prediction with transformers. Specifically, we focus on learning in-context n-gram language models with cross-entropy loss using a simplified two-layer transformer. We design a series of transformers that represent $k$-grams (for $k \leq n$) for which the gradient of the population loss approaches zero in the limit of both infinite sequence length and infinite parameter norm. This construction reveals a key property of the loss landscape: \emph{$k$-grams are stationary points of the population cross-entropy loss}, offering theoretical insights for widely observed empirical phenomena such as stage-wise learning dynamics and emergent phase transitions. These insights are further supported by comprehensive numerical experiments that illustrate the dynamics of learning $n$-grams, characterized by jumps between stationary points.","When training AI language models, the learning happens in clear steps - like levels in a game. At each level, the model picks up new skills. To investigate this, we analyze the training dynamics of a simplified transformer model applied to a basic yet mathematically well-characterized n-gram language model. We demonstrate the existence of non-trivial partial solutions where the gradient vanishes, inhibiting further training progress and thereby producing the observed step-wise learning behavior."
Poster,Learning Initial Basis Selection for Linear Programming via Duality-Inspired Tripartite Graph Representation and Comprehensive Supervision,https://ICML.cc//virtual/2025/poster/44992,"Anqi Lu, Junchi Yan","For the fundamental linear programming (LP) problems, the simplex method remains popular, which usually requires an appropriate initial basis as a warm start to accelerate the solving process. Predicting an initial basis close to an optimal one can often accelerate the solver, but a closer initial basis does not always result in greater acceleration. To achieve better acceleration, we propose a GNN model based on a tripartite graph representation inspired by LP duality. This approach enables more effective feature extraction for general LP problems and enhances the expressiveness of GNNs. Additionally, we introduce novel loss functions targeting basic variable selection and basis feasibility, along with data preprocessing schemes, to further improve learning capability. In addition to achieving high prediction accuracy, we enhance the quality of the initial basis for practical use. Experimental results show that our approach greatly surpasses the state-of-the-art method in predicting initial basis with greater accuracy and in reducing the number of iterations and solving time of the LP solver.","Linear programming is a key mathematical tool used to solve many real-world optimization problems, like scheduling or resource allocation. One popular method to solve these problems, called the simplex method, works faster if it starts from a good initial guess, but finding that guess is tricky. Simply having a guess closer to the best solution doesn’t always speed things up.We developed a new machine learning model using graph neural networks (GNNs) that better understands the structure of these problems by representing them in a novel way inspired by mathematical theory. Our model learns to predict better starting points for the simplex method, improving not only the accuracy of the guess but also making it more useful in practice.In tests, our approach outperformed existing methods by predicting initial solutions more precisely, which led to faster problem-solving and reduced computing time. This advancement can help speed up many optimization tasks in industries relying on linear programming."
Poster,Learning Input Encodings for Kernel-Optimal Implicit Neural Representations,https://ICML.cc//virtual/2025/poster/46033,"Zhemin Li, Liyuan Ma, Hongxia Wang, Yaoyun Zeng, 晓龙 韩","Implicit Neural Representations (INRs) rely heavily on architectural choices for good generalization. Developing theoretically grounded approaches for architecture design remains an active area of research. Via theoretical analysis of the infinite-width limit, we establish a methodology that characterizes INR's generalization by means of kernel alignment. We first formulate the optimal kernel that minimizes pointwise expected squared error, then demonstrate that the Neural Tangent Kernel of the composed function (INR with input encoding) can approximate any positive semidefinite dot-product kernels through input feature mapping adjustments. Building upon these insights, we propose a Kernel Alignment Regularizer (KAR) that naturally integrates with existing INR systems to enhance kernel alignment. We further develop Plug-in Encoding for Aligned Kernels (PEAK) to refine INR models with KAR using learnable input encoding. This work contributes to the ongoing research efforts in bridging theory and practice for principled INR architecture design. Code is available at https://github.com/lizhemin15/KAR.","Implicit Neural Representations (INRs) need careful architecture design to generalize well, but lack strong theoretical guidance. We analyze how INR generalization relates to kernel alignment, i.e., how well the model matches the optimal kernel. This leads us to create:1.KAR: A regularizer that boosts kernel alignment in existing INRs.2.PEAK: An algorithm to refine INR models with KAR using learnable input encoding.By connecting theory with practice, our work improves INR's generalization."
Poster,Learning Invariant Causal Mechanism from Vision-Language Models,https://ICML.cc//virtual/2025/poster/45848,"Zeen Song, Siyu Zhao, Xingyu Zhang, Jiangmeng Li, Changwen Zheng, Wenwen Qiang","Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success, but its performance can degrade when fine-tuned in out-of-distribution (OOD) scenarios. We model the prediction process using a Structural Causal Model (SCM) and show that the causal mechanism involving both invariant and variant factors in training environments differs from that in test environments. In contrast, the causal mechanism with solely invariant factors remains consistent across environments. We theoretically prove the existence of a linear mapping from CLIP embeddings to invariant factors, which can be estimated using interventional data. Additionally, we provide a condition to guarantee low OOD risk of the invariant predictor. Based on these insights, we propose the Invariant Causal Mechanism of CLIP (CLIP-ICM) framework. CLIP-ICM involves collecting interventional data, estimating a linear projection matrix, and making predictions within the invariant subspace. Experiments on several OOD datasets show that CLIP-ICM significantly improves the performance of CLIP. Our method offers a simple but powerful enhancement, boosting the reliability of CLIP in real-world applications.","Large-scale vision-language models like CLIP (which learns to match images and text) have proven remarkably good at identifying images without needing to be trained on specific tasks. However, when these models are fine-tuned for new, real-world tasks, they often struggle to generalize — especially when the test data looks different from what the model saw during training.Our research investigates why this happens and how to fix it. We take a causal perspective, modeling how different ""hidden factors"" influence CLIP’s predictions. Some factors are consistent across different environments (like an animal’s shape), while others vary (like lighting or background). We show that if a model relies too much on the variable factors, its predictions can break down in new situations. But if it uses only the consistent ones, it can make reliable predictions even when the environment changes.We then prove that these consistent factors can be recovered from CLIP’s internal features using a simple linear transformation — but only if we have access to carefully designed “intervention” data (like changing only one thing in an image at a time). Based on this, we introduce a new framework, CLIP-ICM, that projects CLIP’s features into an ""invariant"" space before making predictions. This process doesn’t require retraining CLIP itself, just a light extra step.Across several challenging benchmarks, our approach improves accuracy significantly. It helps CLIP maintain its zero-shot power (handling new categories it has never seen) while becoming more reliable when the environment shifts — a crucial step for deploying AI systems in the real world."
Poster,Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models,https://ICML.cc//virtual/2025/poster/44381,"Armin Kekić, Sergio Hernan Garrido Mejia, Bernhard Schölkopf","Estimating causal effects of joint interventions on multiple variables is crucial in many domains, but obtaining data from such simultaneous interventions can be challenging. Our study explores how to learn joint interventional effects using only observational data and single-variable interventions. We present an identifiability result for this problem, showing that for a class of nonlinear additive outcome mechanisms, joint effects can be inferred without access to joint interventional data. We propose a practical estimator that decomposes the causal effect into confounded and unconfounded contributions for each intervention variable. Experiments on synthetic data demonstrate that our method achieves performance comparable to models trained directly on joint interventional data, outperforming a purely observational estimator.","Understanding how multiple actions work together to influence an outcome is crucial in many fields, from marketing campaigns to medical treatments. However, running experiments that test every possible combination of actions is often prohibitively expensive and time-consuming—the number of required experiments grows exponentially with each additional variable.We developed a mathematical approach that allows researchers to predict the effects of combining multiple interventions using simpler experiments where only one variable is changed at a time, plus observational data. Our method works when the outcome can be understood as a sum of separate contributions from each action, even if those individual contributions are complex and nonlinear. We created a practical algorithm that decomposes causal effects into components that can be learned from these simpler data sources.Our approach could dramatically reduce experimental costs across many domains. A company optimizing marketing across multiple channels could understand joint effects without testing every channel combination. Medical researchers could predict how multiple treatments work together without running every possible clinical trial. By making it possible to learn about complex multi-variable effects from simpler experiments, this work enables more efficient and cost-effective decision-making in situations where comprehensive experimentation would be impractical."
Poster,Learning Latent Graph Structures and their Uncertainty,https://ICML.cc//virtual/2025/poster/45179,"Alessandro Manenti, Daniele Zambon, Cesare Alippi","Graph neural networks use relational information as an inductive bias to enhance prediction performance. Not rarely, task-relevant relations are unknown and graph structure learning approaches have been proposed to learn them from data. Given their latent nature, no graph observations are available to provide a direct training signal to the learnable relations. Therefore, graph topologies are typically learned on the prediction task alongside the other graph neural network parameters.In this paper, we demonstrate that minimizing point-prediction losses does not guarantee proper learning of the latent relational information and its associated uncertainty. Conversely, we prove that suitable loss functions on the stochastic model outputs simultaneously grant solving two tasks: (i) learning the unknown distribution of the latent graph and (ii) achieving optimal predictions of the target variable. Finally, we propose a sampling-based method that solves this joint learning task. Empirical results validate our theoretical claims and demonstrate the effectiveness of the proposed approach.","Some Deep Learning models, particularly Graph Neural Networks, utilize relational information as an effective inductive bias. However, task-relevant relations are often unknown. While approaches exist to learn these hidden connections as part of the model's main task, accurately learning these latent relationships and their associated uncertainty remains a major challenge, especially without direct supervision of the true underlying structures.Learning latent relationships can shed light on hidden structures and support more informed decision-making. For example, this can be valuable for understanding how information spreads through a social or physical network, or for analyzing complex biological systems where interactions between components are not always observable.In this paper, we show that learning accurate probabilistic relationships requires the use of specific loss functions. In particular:1. We demonstrate that commonly used loss functions - even if probabilistic - do not ensure accurate learning of latent relational structures, when they focus solely on point predictions.  2. We show that a different, yet broad, class of loss functions offers stronger guarantees while maintaining accurate point predictions.  Empirical analyses support these theoretical findings."
Poster,Learning Likelihood-Free Reference Priors,https://ICML.cc//virtual/2025/poster/46468,"Nick Bishop, Daniel Jarne Ornia, Joel Dyer, Anisoara Calinescu, Michael Wooldridge","Simulation modeling offers a flexible approach to constructing high-fidelity synthetic representations of complex real-world systems. However, the increased complexity of such models introduces additional complications, for example when carrying out statistical inference procedures. This has motivated a large and growing literature on *likelihood-free* or *simulation-based* inference methods, which approximate (e.g., Bayesian) inference without assuming access to the simulator's intractable likelihood function.  A hitherto neglected problem in the simulation-based Bayesian inference literature is the challenge of constructing minimally informative *reference priors* for complex simulation models. Such priors maximise an expected Kullback-Leibler distance from the prior to the posterior, thereby influencing posterior inferences minimally and enabling an ``objective'' approach to Bayesian inference that does not necessitate the incorporation of strong subjective prior beliefs. In this paper, we propose and test a selection of likelihood-free methods for learning reference priors for simulation models, using variational approximations to these priors and a variety of mutual information estimators. Our experiments demonstrate that good approximations to reference priors for simulation models are in this way attainable, providing a first step towards the development of likelihood-free objective Bayesian inference procedures.","Scientists often use simulators to better understand large-scale complex systems such as epidemics and financial markets. However, setting up a simulator that perfectly mirrors the real world at the right level of abstraction is difficult. In Bayesian statistics, we attempt to resolve our uncertainty by using data to update our beliefs about the real world. Through the process of collecting data and adjusting our beliefs, we can tweak a simulator until it matches the real-world system we seek to emulate. However, to update our beliefs upon seeing new data, we must first possess beliefs to begin with. In Bayesian statistics, this is known as the problem of prior specification. In cases where we are in a state of relative ignorance about the behaviour of a real world system, we would like our prior beliefs, before having seen any data, to capture our complete lack of knowledge. In particular, we don't want our initial beliefs to have a heavy influence on our stance in the future, once we have seen some data. However, forming such prior beliefs can be difficult especially for complicated phenomena. For example, consider running a simulation of the US stock market. It is difficult to write down all the *plausible ways* in which the stock market could evolve, and eliminate all impossibilities from consideration. To address this problem we propose two methods that learn an appropriate mathematical representation of uninformed beliefs, known as the reference prior, by repeatedly running computer simulations. Through these methods, scientists and practitioners may obtain uninformed beliefs which they can update upon seeing data, without concerns of initial bias clouding their judgement."
