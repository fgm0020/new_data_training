type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model and Input View Curation,https://ICML.cc//virtual/2025/poster/46356,"Junlin Han, Jianyuan Wang, Andrea Vedaldi, Phil Torr, Filippos Kokkinos","Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications.Existing methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality.To address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging an arbitrary number of high-quality input views. The first stage consists of a candidate view generation and curation pipeline. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models.","Creating 3D assets from text descriptions or a single picture is a challenging task. Current AI methods often struggle because they usually rely on a few generated 2D multi-view images to build the final 3D asset. If these generated multi-view images are low-quality, inconsistent, or don't show enough angles, the final 3D model can look unrealistic or incomplete.Our system, Flex3D, tackles this problem with a two-stage approach. First, it generates a large and diverse pool of candidate 2D multi-view images. Then, it curates these, selecting an optimal subset of high-quality, consistent multi-view images. These curated views are fed into our novel Flexible Reconstruction Model (FlexRM), which effectively processes an arbitrary number of input views to reconstruct a detailed 3D asset.This method allows Flex3D to generate significantly higher-quality 3D assets. Our results show it outperforms current methods, with users preferring its results over 92% of the time. This research makes it easier to create realistic 3D content for applications like video games and virtual reality, making 3D creation tools more powerful and accessible."
Poster,FlexControl: Computation-Aware Conditional Control with Differentiable Router for Text-to-Image Generation,https://ICML.cc//virtual/2025/poster/45383,"Zheng Fang, Lichuan Xiang, Xu Cai, Kaicheng Zhou, Hongkai Wen","Spatial conditioning control offers a powerful way to guide diffusion‐based generative models. Yet, most implementations (e.g., ControlNet) rely on ad-hoc heuristics to choose which network blocks to control — an approach that varies unpredictably with different tasks. To address this gap, we propose FlexControl, a novel framework that equips all diffusion blocks with control signals during training and employs a trainable gating mechanism to dynamically select which control signal to activate at each denoising step. By introducing a computation-aware loss, we can encourage the control signal to activate only when it benefits the generation quality. By eliminating manual control unit selection, FlexControl enhances adaptability across diverse tasks and streamlines the design pipeline with computation-aware training loss in an end-to-end training manner. Through comprehensive experiments on both UNet and DiT architectures on different control methods, we show that our method can upgrade existing controllable generative models in certain key aspects of interest. As evidenced by both quantitative and qualitative evaluations, FlexControl preserves or enhances image fidelity while also reducing computational overhead by selectively activating the most relevant blocks to control. These results underscore the potential of a flexible, data‐driven approach for controlled diffusion and open new avenues for efficient generative model design. The code will soon be available at https://github.com/Daryu-Fan/FlexControl.","Spatial condition (e.g. depth map, canny edge, sketch) compensates the visual expression ability of text description, and effectively guides the diffusion-based generative model to generate the images imagined by users. Yet, most implementations rely on ad-hoc heuristics to choose which network blocks to inject conditional control, which varies unpredictably with different tasks. To address this gap, we propose FlexControl — a novel framework which automatically adjust the diffusion blocks needed to be controlled depend on specific-sample and timestep. Specifically, we employ a trainable gating mechanism — router unit, to dynamically select control signals to activate. By introducing a computation-aware loss, we can encourage the control signal to activate only when it benefits the generation quality. By eliminating manual control unit selection, FlexControl enhances adaptability across diverse tasks and streamlines the design pipeline with computation-aware training loss in an end-to-end training manner.Through comprehensive experiments on different architectures and control methods, we show that our method can upgrade existing controllable generative models in certain key aspects of interest. As evidenced by both quantitative and qualitative evaluations, FlexControl preserves or enhances image fidelity while also reducing computational overhead by selectively activating the most relevant blocks to control. These results underscore the potential of a flexible, data‐driven approach for controlled diffusion and open new avenues for efficient generative model design."
Poster,Flexibility-conditioned protein structure design with flow matching,https://ICML.cc//virtual/2025/poster/46289,"Vsevolod Viliuga, Leif Seute, Nicolas Wolf, Simon Wagner, Arne Elofsson, Jan Stuehmer, Frauke Gräter","Recent advances in geometric deep learning and generative modeling have enabled the design of novel proteins with a wide range of desired properties. However, current state-of-the-art approaches are typically restricted to generating proteins with only static target properties, such as motifs and symmetries. In this work, we take a step towards overcoming this limitation by proposing a framework to condition structure generation on flexibility, which is crucial for key functionalities such as catalysis or molecular recognition. We first introduce BackFlip, an equivariant neural network for predicting per-residue flexibility from an input backbone structure. Relying on BackFlip, we propose FliPS, an SE(3)-equivariant conditional flow matching model that solves the inverse problem, that is, generating backbones that display a target flexibility profile. In our experiments, we show that FliPS is able to generate novel and diverse protein backbones with the desired flexibility, verified by Molecular Dynamics (MD) simulations.","One of the key features of functional proteins, large biomolecules with complex structures, is their inherent structural flexibility - they wiggle, thumble and change shape. We asked a question whether one could design proteins with a custom flexibility from scratch.We build a model that learns how to generate proteins such that their structures are flexible to a given extent at a given position. We show that the model can generate proteins with the desired flexibility patterns, even for patterns that are uncommon in natural proteins. Our work is a step towards the challenging goal of designing new proteins for applications where flexibility is required, such as enzyme catalysts."
Poster,Flexible and Efficient Grammar-Constrained Decoding,https://ICML.cc//virtual/2025/poster/45613,"Kanghee Park, Timothy Zhou, Loris D&#x27;Antoni","Large Language Models (LLMs) are often asked to generate structured outputs that obey precise syntactic rules, such as code snippets or formatted data. Grammar-constrained decoding (GCD) can guarantee that LLM outputs matches such rules by masking out tokens that will provably lead to outputs that do not belong to a specified context-free grammar (CFG). To guarantee soundness, GCD algorithms have to compute how a given LLM subword tokenizer can ``align'' with the tokens used by a given context-free grammar and compute token masks based on this information. Doing so efficiently is challenging and existing GCD algorithms require tens of minutes to preprocess common grammars. We present a new GCD algorithm together with an implementation that offers 17.71x faster offline preprocessing than existing approaches while preserving state-of-the-art efficiency in online mask computation.","Large language models (LLMs) are often used to produce structured text, such as computer code or data in specific formats. But getting them to always follow strict rules — like grammar in programming languages — can be tricky.To help with this, researchers use a technique called grammar-constrained decoding (GCD), which ensures that the model only generates text that follows the rules. However, existing methods for doing this take a long time to prepare — often tens of minutes — before they can be used.Our work introduces a faster and more efficient approach. We developed a new algorithm and tool that can prepare these grammar rules much more quickly — significantly reducing setup time — while keeping the actual text generation just as fast and accurate as before.This improvement makes it easier and more practical to use LLMs in real-world applications where following strict formats is essential, such as writing code or generating structured data for websites or databases."
Poster,"Flexible, Efficient, and Stable Adversarial Attacks on Machine Unlearning",https://ICML.cc//virtual/2025/poster/44738,"Zihan Zhou, Yang Zhou, Zijie Zhang, Lingjuan Lyu, Da Yan, Ruoming Jin, Dejing Dou","Machine unlearning (MU) aims to remove the influence of specific data points from trained models, enhancing compliance with privacy regulations. However, the vulnerability of basic MU models to malicious unlearning requests in adversarial learning environments has been largely overlooked. Existing adversarial MU attacks suffer from three key limitations: inflexibility due to pre-defined attack targets, inefficiency in handling multiple attack requests, and instability caused by non-convex loss functions. To address these challenges, we propose a Flexible, Efficient, and Stable Attack (DDPA). First, leveraging Carathéodory's theorem, we introduce a convex polyhedral approximation to identify points in the loss landscape where convexity approximately holds, ensuring stable attack performance. Second, inspired by simplex theory and John's theorem, we develop a regular simplex detection technique that maximizes coverage over the parameter space, improving attack flexibility and efficiency. We theoretically derive the proportion of the effective parameter space occupied by the constructed simplex. We evaluate the attack success rate of our DDPA method on real datasets against state-of-the-art machine unlearning attack methods. Our source code is available at https://github.com/zzz0134/DDPA.","Modern privacy regulations, such as the European Union’s General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), enforce ""the right to be forgotten"", i.e., individuals can request that their personal data be deleted.Machine unlearning techniques aim to enable data owners to proactively remove their data and eliminate its influence from already trained machine learning model upon requests.For example, Stability AI announced that it would allow artists to remove their work from the training data used for the Stable Diffusion 3.0 release.However, these unlearning models are vulnerable to malicious requests in adversarial environments, where attackers try to exploit the system. In this work, we present a method that can effectively attack unlearning models in a flexible, efficient, and stable way. Our framework can inspire new defensive techniques for a wide variety of privacy-critical applications that usually require near-zero tolerance of data leaking, such as financial and health data analyses, where even minimal data leaks are unacceptable."
Poster,Flexible Tails for Normalizing Flows,https://ICML.cc//virtual/2025/poster/44878,"Tennessee Hickling, Dennis Prangle","Normalizing flows are a flexible class of probability distributions, expressed as transformations of a simple base distribution. A limitation of standard normalizing flows is representing distributions with heavy tails, which arise in applications to both density estimation and variational inference. A popular current solution to this problem is to use a heavy tailed base distribution. We argue this can lead to poor performance due to the difficulty of optimising neural networks, such as normalizing flows, under heavy tailed input. We propose an alternative, ""tail transform flow'' (TTF), which uses a Gaussian base distribution and a final transformation layer which can produce heavy tails. Experimental results show this approach outperforms current methods, especially when the target distribution has large dimension or tail weight.","Modern machine learning methods often represent uncertainty by learning how to turn simple random inputs into outputs that resemble observed data. These methods are very flexible, but they systematically underestimate the chance of extreme outcomes. Yet in many real world scenarios, extreme events are crucial, for example, severe weather or financial shocks.Some recent proposals solve this problem by including extreme events in the random input. But this can introduce new problems, as models struggle to handle extreme inputs effectively. We take a different approach: we add a dedicated step at the end of the process, designed to allow models to generate extreme events from non-extreme inputs. This simple change improves the modelling of data that contains extreme observations."
Poster,FlexiClip: Locality-Preserving Free-Form Character Animation,https://ICML.cc//virtual/2025/poster/43560,Anant Khandelwal,"Animating clipart images with seamless motion while maintaining visual fidelity and temporal coherence presents significant challenges. Existing methods, such as AniClipart, effectively model spatial deformations but often fail to ensure smooth temporal transitions, resulting in artifacts like abrupt motions and geometric distortions. Similarly, text-to-video (T2V) and image-to-video (I2V) models struggle to handle clipart due to the mismatch in statistical properties between natural video and clipart styles. This paper introduces FlexiClip, a novel approach designed to overcome these limitations by addressing the intertwined challenges of temporal consistency and geometric integrity. FlexiClip extends traditional Bézier curve-based trajectory modeling with key innovations: temporal Jacobians to correct motion dynamics incrementally, continuous-time modeling via probability flow ODEs (pfODEs) to mitigate temporal noise, and a flow matching loss inspired by GFlowNet principles to optimize smooth motion transitions. These enhancements ensure coherent animations across complex scenarios involving rapid movements and non-rigid deformations. Extensive experiments validate the effectiveness of FlexiClip in generating animations that are not only smooth and natural but also structurally consistent across diverse clipart types, including humans and animals. By integrating spatial and temporal modeling with pre-trained video diffusion models, FlexiClip sets a new standard for high-quality clipart animation, offering robust performance across a wide range of visual content. Project Page: https://creative-gen.github.io/flexiclip.github.io/","Animating simple drawings, like clipart images, has traditionally required artists to create each frame by hand. Automated tools often produce jerky movements or distort the original artwork, especially during rapid motions.**FlexiClip** introduces a new method to animate still clipart images smoothly, preserving the character's original style and structure. By calculating small, continuous movements for each part of the drawing, FlexiClip ensures fluid transitions between frames without sudden jumps or unnatural distortions.Key innovations include:* **Continuous Motion Modeling**: Treating animation as a seamless flow rather than separate steps, reducing flickering and stuttering.* **Learning from Examples**: Mimicking natural movement patterns observed in hand-drawn animations to enhance realism.* **Detail Preservation**: Maintaining essential features like facial expressions and clothing details throughout the animation.Tests on various clipart styles demonstrated that FlexiClip produces more natural and faithful animations compared to existing tools. Observers noted smoother movements and better preservation of the characters' original appearances.By simplifying the animation process, FlexiClip empowers educators, small businesses, and content creators to bring their illustrations to life efficiently. However, users should apply this technology responsibly to avoid creating misleading or deceptive content.*For more details, refer to the project page: [https://creative-gen.github.io/flexiclip.github.io/](https://creative-gen.github.io/flexiclip.github.io/).*"
Poster,FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification,https://ICML.cc//virtual/2025/poster/44630,"Zhen Sun, Lei Tan, Yunhang Shen, Chengmao Cai, Xing Sun, Pingyang Dai, Liujuan Cao, Rongrong Ji","Multimodal person re-identification (Re-ID) aims to match pedestrian images across different modalities. However, most existing methods focus on limited cross-modal settings and fail to support arbitrary query-retrieval combinations, hindering practical deployment. We propose FlexiReID, a flexible framework that supports seven retrieval modes across four modalities: RGB, infrared, sketches, and text. FlexiReID introduces an adaptive mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality features and a cross-modal query fusion module to enhance multimodal feature extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a unified dataset extending four popular Re-ID datasets to include all four modalities. Extensive experiments demonstrate that FlexiReID achieves state-of-the-art performance and offers strong generalization in complex scenarios.","Identifying people across different types of images—such as color photos, infrared images, sketches, or even text descriptions—is a difficult task, especially in real-world settings where information may be incomplete. This paper introduces a new system called FlexiReID, which can flexibly search for a person using any mix of these information sources. For example, if only a sketch and a short description are available, the system can still find the matching photo. The key idea is to combine expertise from different specialized models and adapt based on what information is available. To support this system, the authors also created a new dataset that includes four types of data. Experiments show that FlexiReID works better than previous systems, especially in complex or low-quality situations. This research makes person search more practical and reliable in real-world applications like public safety and smart cities."
Poster,FlexTok: Resampling Images into 1D Token Sequences of Flexible Length,https://ICML.cc//virtual/2025/poster/45993,"Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin Ali, Amir Zamir, Afshin Dehghan","We introduce FlexTok, a tokenizer that projects 2D images into variable-length, ordered 1D token sequences. For example, a 256x256 image can be resampled into anywhere from 1 to 256 discrete tokens, hierarchically and semantically compressing its information. By training a rectified flow model as the decoder and using nested dropout, FlexTok produces plausible reconstructions regardless of the chosen token sequence length. We evaluate our approach in an autoregressive generation setting using a simple GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to 128 tokens, outperforming TiTok and matching state-of-the-art methods with far fewer tokens. We further extend the model to support to text-conditioned image generation and examine how FlexTok relates to traditional 2D tokenization. A key finding is that FlexTok enables next-token prediction to describe images in a coarse-to-fine ""visual vocabulary"", and that the number of tokens to generate depends on the complexity of the generation task.","Generating high-quality images with artificial intelligence typically requires representing images in simplified forms that computers can handle more efficiently. Traditionally, images are split into grids of small patches, but this method doesn't adapt well to images of varying complexity.We developed FlexTok, a new method that converts images into a flexible number of simple building blocks, or ""tokens,"" based on how detailed or complex an image is. For instance, a complex image could be broken into more tokens for greater detail, while a simpler image might need fewer tokens. FlexTok effectively compresses information by using fewer tokens without losing crucial visual details.By training image generation models with FlexTok, we achieved a quality comparable to leading methods but with significantly fewer tokens, making the process faster and more efficient. Additionally, FlexTok can describe images from coarse to detailed features, similar to how humans gradually perceive visual information. This flexible and efficient method could greatly enhance applications where rapid, high-quality image generation is important, like long-horizon video generation, as well as visual understanding and reasoning."
Poster,FlipAttack: Jailbreak LLMs via Flipping,https://ICML.cc//virtual/2025/poster/45738,"Yue Liu, Xiaoxin He, Miao Xiong, Jinlan Fu, Shumin Deng, YINGWEI MA, Jiaheng Zhang, Bryan Hooi","This paper proposes a simple yet effective jailbreak attack named FlipAttack against black-box LLMs. First, from the autoregressive nature, we reveal that LLMs tend to understand the text from left to right and find that they struggle to comprehend the text when the perturbation is added to the left side. Motivated by these insights, we propose to disguise the harmful prompt by constructing a left-side perturbation merely based on the prompt itself, then generalize this idea to 4 flipping modes. Second, we verify the strong ability of LLMs to perform the text-flipping task and then develop 4 variants to guide LLMs to understand and execute harmful behaviors accurately. These designs keep FlipAttack universal, stealthy, and simple, allowing it to jailbreak black-box LLMs within only 1 query. Experiments on 8 LLMs demonstrate the superiority of FlipAttack. Remarkably, it achieves $\sim$78.97\% attack success rate across 8 LLMs on average and $\sim$98\% bypass rate against 5 guard models on average.","- We reveal LLMs' understanding mechanism and find that left-side perturbation weaken their understanding ability on text, keeping the attack universally applicable. - We disguise the harmful request by adding left-side perturbation iteratively based on the request itself and generalizing it to four flipping modes, keeping attack stealthy. - We design a flipping guidance module to teach LLMs to recover, understand, and execute the disguised prompt, jailbreaking black-box LLMs within one query easily.    - We conduct extensive experiments to demonstrate the superiority and efficiency of FlipAttack."
