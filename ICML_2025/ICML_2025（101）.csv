type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Explicit Preference Optimization: No Need for an Implicit Reward Model,https://ICML.cc//virtual/2025/poster/44352,"Xiangkun Hu, Lemin Kong, Tong He, David Wipf","The generated responses of large language models (LLMs) are often fine-tuned to human preferences through a process called reinforcement learning from human feedback (RLHF).  As RLHF relies on a challenging training sequence, whereby a separate reward model is independently learned and then later applied to LLM policy updates, ongoing research effort has targeted more straightforward alternatives. In this regard, direct preference optimization (DPO) and its many offshoots circumvent the need for a separate reward training step.  Instead, through the judicious use of a reparameterization trick that induces an implicit reward, DPO and related methods consolidate learning to the minimization of a single loss function.  And yet despite demonstrable success in some real-world settings, we prove that DPO-based objectives are nonetheless subject to sub-optimal regularization and counter-intuitive interpolation behaviors, underappreciated  artifacts of the reparameterizations upon which they are based.  To this end, we introduce an explicit preference optimization framework termed EXPO that requires no analogous reparameterization to achieve an implicit reward.  Quite differently, we merely posit intuitively-appealing regularization factors from scratch that transparently avoid the potential pitfalls of key DPO variants, provably satisfying regularization desiderata that prior methods do not.  Empirical results serve to corroborate our analyses and showcase the efficacy of EXPO.","Large language models are often fine-tuned to match human preferences by first training a separate reward model and then using it to guide subsequent refinement steps, a process that can be complex, unstable, and slow. Direct preference optimization (DPO) simplifies this pipeline by using a mathematical reparameterization to create an implicit reward without the requirement of a separate explicit reward model; however, this shortcut can introduce unintended, counterintuitive behaviors and degeneracies. To address these issues, we propose EXPO, which directly forms the training objective using intuitive penalty factors and no dependency on subtle reparameterizations.  This transparent approach avoids some DPO shortcomings, and across empirical testing, matches or surpasses its performance in aligning models with human preferences."
Poster,Exploiting Curvature in Online Convex Optimization with Delayed Feedback,https://ICML.cc//virtual/2025/poster/44622,"Hao Qiu, Emmanuel Esposito, Mengxiao Zhang","In this work, we study the online convex optimization problem with curved losses and delayed feedback.When losses are strongly convex, existing approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is the maximum delay and $T$ is the time horizon. However, in many cases, this guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the total delay.We bridge this gap by proposing a variant of follow-the-regularized-leader that obtains regret of order $\min\\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\\}$, where $\sigma_{\max}$ is the maximum number of missing observations.We then consider exp-concave losses and extend the Online Newton Step algorithm to handle delays with an adaptive learning rate tuning, achieving regret $\min\\{d_{\max} n\ln T, \sqrt{d_{\mathrm{tot}}}\\}$ where $n$ is the dimension.To our knowledge, this is the first algorithm to achieve such a regret bound for exp-concave losses.We further consider the problem of unconstrained online linear regression and achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth forecaster with a clipping trick.Finally, we implement our algorithms and conduct experiments under various types of delay and losses, showing an improved performance over existing methods.","Sequential decision-making often involves delays before outcomes become known: imagine scenarios like financial markets or recommendation systems, where decisions are made continuously but feedback arrives late. Importantly, these scenarios usually have special loss structures, such as strong convexity or exponential concavity. Our goal is to leverage this ""curvature"" information to significantly improve decision-making performance under the delayed feedback.Specifically, for strongly convex losses, we designed an algorithm that adapts to both the maximum number of missing observations and total accumulated delays, achieving better theoretical guarantees compared to previous methods. For exp-concave losses, we extend the Online Newton Step algorithm by incorporating an adaptive learning rate, allowing it to effectively adapt to different delay regimes. We further extend the Vovk-Azoury-Warmuth forecaster with a carefully designed clipping scheme and a learning rate choice to achieve similar curvature adaptive guarantees for unconstrained online linear regression. Extensive experiments across various settings further validate the superior performance of our algorithms compared to exists ones."
Poster,Exploiting Presentative Feature Distributions for Parameter-Efficient Continual Learning of Large Language Models,https://ICML.cc//virtual/2025/poster/46354,"Xin Cheng, Jiabo Ye, Haiyang Xu, Ming Yan, Ji Zhang, Feng Liu, Fei Huang, Lei Feng","Endowing large language models (LLMs) with continual learning (CL) capacities is practically important, which enables them to dynamically acquire new knowledge over time. Although many effective methods have been proposed for CL of LLMs, they did not consider online scenarios, thereby sharing a common problem: information leakage (IL), where the task-related information of learned tasks is accessed or reused again. IL not only imposes potential risks on data privacy protection but also significantly hinders the deployment of LLMs in real-world scenarios. To avoid IL while maintaining outstanding CL performance, we propose a novel CL method for LLMs, which first characterizes a parameter-efficient fine-tuning (PEFT) block by a presentative feature distribution, and then dynamically selects the appropriate PEFT blocks for each instance based on its similarity with the presentative feature distributions. Extensive experiments validate the effectiveness of our method on the CL of LLM, showcasing its potential to enhance both privacy and adaptability in practical applications.","Providing large language models (LLMs) with continual learning abilities is crucial for them to dynamically acquire new knowledge over time. We analysed current advanced continual learning methods for LLMs in online scenarios and found that they either perform poorly or have the information leakage problem. We aim to develop a method that avoids information leakage while maintaining strong continual learning performance.Our method leveraged well-developed pre-trained LLMs to describe the distribution of each streaming task. We discovered that by evaluating the distance between samples and distributions, we can effectively achieve good continual learning performance. Additionally, our method can also flexibly expand the knowledge trained with the same model framework, achieving plug-in continual learning.Our in-depth study of information leakage in online continual learning can help better deploy  LLMs with continual learning capabilities in real-world scenarios."
Poster,Exploiting Similarity for Computation and Communication-Efficient Decentralized Optimization,https://ICML.cc//virtual/2025/poster/45166,"Yuki Takezawa, Xiaowen Jiang, Anton Rodomanov, Sebastian Stich","Reducing communication complexity is critical for efficient decentralized optimization. The proximal decentralized optimization (PDO) framework is particularly appealing, as methods within this framework can exploit functional similarity among nodes to reduce communication rounds. Specifically, when local functions at different nodes are similar, these methods achieve faster convergence with fewer communication steps. However, existing PDO methods often require highly accurate solutions to subproblems associated with the proximal operator, resulting in significant computational overhead. In this work, we propose the Stabilized Proximal Decentralized Optimization (SPDO) method, which achieves state-of-the-art communication and computational complexities within the PDO framework. Additionally, we refine the analysis of existing PDO methods by relaxing subproblem accuracy requirements and leveraging average functional similarity. Experimental results demonstrate that SPDO significantly outperforms existing methods.","Many different organizations and devices, such as hospitals and mobile phones, possess valuable data for training machine learning models. However, sharing this data directly is often not possible due to privacy concerns. One solution is to use a method called decentralized optimization. With this approach, each client keeps its own data and trains models with other clients by communicating only model parameters. However, there are several challenges. Since communication between clients can be unstable or slow, reducing the communication costs is crucial. Additionally, reducing the computational cost on clients is also important, especially for those with very limited resources, such as mobile phones. In this paper, we propose a new algorithm, called Accelerated Stabilized Proximal Decentralized Optimization. This algorithm can run with lowest communication and computation costs among existing algorithms, making decentralized optimization more suitable for real-world use."
Poster,ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts,https://ICML.cc//virtual/2025/poster/45411,"Samar Khanna, Medhanie Irgau, David Lobell, Stefano Ermon","Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation (LoRA) can effectively adapt large pre-trained foundation models to downstream tasks using only a small fraction (0.1%-10%) of the original trainable weights. An under-explored question of PEFT is in extending the pre-training phase without supervised labels; that is, can we adapt a pre-trained foundation model to a new domain via efficient self-supervised pre-training on this domain? In this work, we introduce ExPLoRA, a highly effective technique to improve transfer learning of pre-trained vision transformers (ViTs) under domain shifts. Initializing a ViT with pre-trained weights on large, natural-image datasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised pre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks and tuning all other layers with LoRA. We then fine-tune the resulting model only with LoRA on this new domain for supervised learning. Our experiments demonstrate state-of-the-art results on satellite imagery, even outperforming fully pre-training and fine-tuning ViTs. Using the DinoV2 training objective, we demonstrate up to 8% improvement in linear probing top-1 accuracy on downstream tasks while using <10% of the number of parameters that are used in prior fully-tuned state-of-the art approaches. Our ablation studies confirm the efficacy of our approach over other baselines such as PEFT. Code is available at: https://samar-khanna.github.io/ExPLoRA/","Training powerful AI models to analyze specialized images—like satellite photos for environmental monitoring or medical scans for disease detection—requires enormous computational resources that most researchers cannot afford. Current AI models trained on everyday photos perform poorly on these specialized domains, forcing scientists to build entirely new models at costs exceeding thousands of hours on specialized computer hardware (eg: GPUs) and significant environmental impact.We developed ExPLoRA, an efficient method that takes existing AI models trained on natural images and adapts them to new domains by updating only a small fraction of the model's components. Instead of building new models from scratch, our approach lets the model continue to learn patterns from specialized images on its own—without needing humans to label examples—while preserving most of the original knowledge the model learned from natural, everyday photos.Our method achieves better performance than training models from scratch while using 10 times less computing power and producing 8 times fewer carbon emissions. This makes cutting-edge AI accessible to researchers with limited resources, enabling more scientists to develop tools for climate monitoring, medical diagnosis, and agricultural assessment."
Poster,Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards,https://ICML.cc//virtual/2025/poster/43464,"Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, Chiyuan Zhang","It is now common to evaluate Large Language Models (LLMs) by having humans manually vote to evaluate model outputs, in contrast to typical benchmarks that evaluate knowledge or skill at some particular task. Chatbot Arena, the most popular benchmark of this type, ranks models by asking users to select the better response between two randomly selected models (without revealing which model was responsible for the generations).  These platforms are widely trusted as a fair and accurate measure of LLM capabilities. In this paper, we show that if bot protection and other defenses are not implemented, these voting-based benchmarks are potentially vulnerable to adversarial manipulation. Specifically, we show that an attacker can alter the leaderboard (to promote their favorite model or demote competitors) at the cost of roughly a thousand votes (verified in a simulated, offline version of Chatbot Arena). Our attack consists of two steps: first, we show how an attacker can determine which model was used to generate a given reply with more than $95\%$ accuracy; and then, the attacker can use this information to consistently vote for (or against) a target model. Working with the Chatbot Arena developers, we identify, propose, and implement mitigations to improve the robustness of Chatbot Arena against adversarial manipulation, which, based on our analysis, substantially increases the cost of such attacks. Some of these defenses were present before our collaboration, such as bot protection with Cloudflare, malicious user detection, and rate limiting. Others, including reCAPTCHA and login are being integrated to strengthen the security in Chatbot Arena.","The field of natural language processing has long relied on domain-specific, easy-to-implement evaluation metrics. But  dramatic  advances  in  LLM  performance  challenges traditional evaluation practices. As we show in this paper, moving from evaluations that use an objective source of truth to evaluations that utilize human inputs introduces the potential for new types of evaluation difficulties. We focus on this paper in validating one straightforward attack:  by identifying and selectively voting for (or against) a particular model, an adversary can significantly alter the ordering of the best models.Mitigating this attack is feasible, and we are actively collaborating with the Chatbot Arena team to make Chatbot Arena more robust. We also encourage the community to explore and adopt mitigation strategies, such as voter authentication, rate limits, and more robust mechanisms for detecting malicious activities."
Poster,Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning,https://ICML.cc//virtual/2025/poster/44163,"Puning Yang, Qizhou Wang, Zhuo Huang, Tongliang Liu, Chengqi Zhang, Bo Han","Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance---the former indicates that those insufficiently optimized data should be emphasized, while the latter stresses some critical data that are most influential for loss minimization. To study their usefulness, we design specific reweighting strategies for each goal and evaluate their respective effects on unlearning. We conduct extensive empirical analyses on well-established benchmarks, and summarize some important observations as follows:(i) Saturation enhances efficacy more than importance-based reweighting, and their combination can yield additional improvements.(ii) Saturation typically allocates lower weights to data with lower likelihoods, whereas importance-based reweighting does the opposite.(iii) The efficacy of unlearning is also largely influenced by the smoothness and granularity of the weight distributions.Based on these findings, we propose SatImp, a simple reweighting method that combines the advantages of both saturation and importance.Empirical results on extensive datasets validate the efficacy of our method, potentially bridging existing research gaps and indicating directions for future research.Our code is available at https://github.com/tmlr-group/SatImp.","(1) Current explorations in LLM unlearning remain insufficient, particularly lacking a clear understanding of the fundamental question: ""How do existing reweighting strategies actually work?"" This paper systematically investigates and summarizes existing approaches, filling important gaps in the field of LLM unlearning research.(2) This paper finds that existing reweighting methods all fall under the saturation-based paradigm, while the importance-based paradigm has been largely overlooked. Moreover, key training details—such as smoothness, hard-soft weighting, and granularity—have not been adequately discussed or explored.(3) We annotated key tokens in the TOFU dataset to explore the importance-based paradigm. Considering the impracticality of manual annotation on large-scale data, we further investigated the correlation between weights and likelihoods. Interestingly, saturation-based methods tend to assign large weights to low-likelihood tokens, whereas importance-based methods show the opposite trend. Additionally, we extensively discussed training details and ultimately incorporated them into the proposed SatImp method.(4) In summary, this paper makes several fundamental contributions: new data annotations, a systematic review of existing methods, and findings regarding training detail choices. These contributions are both important and timely."
Poster,Exploring Invariance in Images through One-way Wave Equations,https://ICML.cc//virtual/2025/poster/45773,"Yinpeng Chen, Dongdong Chen, Xiyang Dai, Mengchen Liu, Yinan Feng, Youzuo Lin, Lu Yuan, Zicheng Liu","In this paper, we empirically demonstrate that natural images can be reconstructed with high fidelity from compressed representations using a simple first-order norm-plus-linear autoregressive (FINOLA) process—without relying on explicit positional information. Through systematic analysis, we observe that the learned coefficient matrices ($\mathbf{A}$ and $\mathbf{B}$) in FINOLA are typically invertible, and their product, $\mathbf{AB}^{-1}$, is diagonalizable across training runs. This structure enables a striking interpretation: FINOLA’s latent dynamics resemble a system of one-way wave equations evolving in a compressed latent space. Under this framework, each image corresponds to a unique solution of these equations. This offers a new perspective on image invariance, suggesting that the underlying structure of images may be governed by simple, invariant dynamic laws. Our findings shed light on a novel avenue for understanding and modeling visual data through the lens of latent-space dynamics and wave propagation.","We explored a surprising and elegant way to reconstruct images using only a small amount of compressed information — without needing to know the exact positions of pixels. Our method, called FINOLA, learns to rebuild images using simple mathematical rules that work like ""ripples"" spreading out from a central point.What’s fascinating is that, when we look closely at how this method works under the hood, we see patterns that resemble wave equations — the same kind used to describe sound or light. It turns out that all images might follow a shared set of these wave-like rules, with each specific image corresponding to its own unique “ripple pattern.”This discovery gives us a new way to think about how images are structured. Instead of seeing images as just grids of color, we can start to understand them as dynamic patterns governed by underlying laws. Our findings may open up new possibilities for how machines understand, compress, and generate visual data."
Poster,Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling,https://ICML.cc//virtual/2025/poster/45873,"Walid Bendada, Guillaume Salha-Galvan, Romain Hennequin, Théo Bontempelli, Thomas Bouabca, Tristan Cazenave","This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable method for exploring large action sets in reinforcement learning problems where hyperspherical embedding vectors represent these actions. vMF-exp involves initially sampling a state embedding representation using a von Mises-Fisher distribution, then exploring this representation's nearest neighbors, which scales to virtually unlimited numbers of candidate actions.We show that, under theoretical assumptions, vMF-exp asymptotically maintains the same probability of exploring each action as Boltzmann Exploration (B-exp), a popular alternative that, nonetheless, suffers from scalability issues as it requires computing softmax values for each action.Consequently, vMF-exp serves as a scalable alternative to B-exp for exploring large action sets with hyperspherical embeddings. Experiments on simulated data, real-world public data, and the successful large-scale deployment of vMF-exp on the recommender system of a global music streaming service empirically validate the key properties of the proposed method.","This paper presents a new way to help algorithms make better decisions—decisions that include an element of randomness, which is useful for exploring different options—when they have many choices to consider. The method, called von Mises-Fisher exploration (vMF-exp), is designed to work efficiently even when there are millions of possible choices—like recommending songs to users on a music app. It does this by first randomly selecting a direction that seems promising and then exploring options that are similar to it, rather than checking every single choice. The researchers show that this method performs as well as older techniques but is much faster and more practical for large systems. They tested it with both simulated and real-world data, including on a major music streaming platform, and found that it works effectively at scale."
Poster,Exploring Representations and Interventions in Time Series Foundation Models,https://ICML.cc//virtual/2025/poster/44453,"Michal Wilinski, Mononito Goswami, Willa Potosnak, Nina Żukowska, Artur Dubrawski","Time series foundation models (TSFMs) promise to be powerful tools for a wide range of applications. However, their internal representations and learned concepts are still not well understood. In this study, we investigate the structure and redundancy of representations across various TSFMs, examining the self-similarity of model layers within and across different model sizes. This analysis reveals block-like redundancy in the representations, which can be utilized for informed pruning to improve inference speed and efficiency. We also explore the concepts learned by these models, such as periodicity and trends. We demonstrate how conceptual priors can be derived from TSFM representations and leveraged to steer its outputs toward concept-informed predictions. Our work bridges representational analysis from language and vision models to TSFMs, offering new methods for building more computationally efficient and transparent TSFMs.","Everyday things like the weather, stock prices, heart-rate readings or traffic counts are all examples of time-series data, i.e. numbers that arrive one after another over time. Researchers have recently begun training very large, general-purpose AI models on many kinds of these sequences. Because a single such model can “found” (i.e., serve as the starting point for) many different forecasting tasks, scientists call them foundation models — similar to how ChatGPT is a single text model used for lots of language tasks.In this study we opened that black box. First, we peered inside several leading time-series foundation models and found that many of their layers were doing almost identical work. By safely trimming these duplicates we cut the models’ size and made their predictions up to 50% faster, without harming accuracy.Next, we asked what the models actually know. We discovered they naturally learn simple concepts people care about, such as whether a signal is flat, rising, falling or repeating in cycles. Finally, we showed that we can gently turn the model’s internal “knobs” to inject these concepts on demand: for example, turning a flat forecast into one that rises, or adding a seasonal ups-and-downs pattern. Crucially, this steering happens after training, so users don’t need to retrain the model or supply extra data.Together, these findings make large time-series AI models cheaper to run, easier to understand and easier to control, paving the way for safer, more efficient tools in areas from finance to healthcare."
