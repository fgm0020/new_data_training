type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Hardware and Software Platform Inference,https://ICML.cc//virtual/2025/poster/44242,"Cheng Zhang, Hanna Foerster, Robert Mullins, Yiren Zhao, Ilia Shumailov","It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce ***hardware and software platform inference (HSPI)*** -- a method for identifying the underlying GPU architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various GPU architectures and compilers to distinguish between different GPU types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the GPU used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring GPU type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different GPUs with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy.","It is now a common business practice to buy access to large language model (LLM) inference rather than self-host because of the high-cost of running LLMs. However, buyers lack the ability to verify if the service is actually served using the hardware and software stack claimed by the providers. Our method, HSPI, offers a method for identifying the underlying GPU architecture and software stack of deep learning models solely by sending inputs and collecting outputs.We first split the application senarios into white-box and black-box, where white-box means the model checkpoint is publicly available like open-sourced Llama and black-box means the model is close-sourced like ChatGPT. We propose two approaches, one is HPSI-BI, that learns model inputs on the decision boundary of the model (border input). For example, a border image may be predicted as ""cat"" by a ResNet18 on NVIDIA H100, but prediced as ""dog"" by the same ResNet18 on AMD MI300X. Our second approach is to train an SVM to classify the distribution difference in the output logits.We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different GPUs with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy."
Poster,HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration,https://ICML.cc//virtual/2025/poster/45499,"Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang","Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives-*aligned predicted noise vs. high-quality images*-between training and inference. These two discrepancies compromise both performance and efficiency.To this end, we *harmonize* training and inference with a novel learning-based *caching* framework dubbed **HarmoniCa**. It first incorporates *Step-Wise Denoising Training* (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an *Image Error Proxy-Guided Objective* (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\\%$ latency reduction (*i.e.*, $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our *image-free* approach reduces training time by $25\\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.","Generative AI models called Diffusion Transformers can create stunning images, but they are painfully slow because they repeat similar calculations tens of times during each generation. Our team asked: *Can we reuse those repeated computations instead of recalculating them, without hurting image quality?* We built **HarmoniCa**, a “feature-caching” system that learns when to store and when to recall intermediate results inside the model. To train this cache intelligently, we introduced two new techniques: *Step-Wise Denoising Training*, which lets the model practise using its cache across the full generation process, and an *Image-Error Proxy*, which teaches the model to protect final image quality while still maximising speed. In tests on eight state-of-the-art diffusion models, HarmoniCa cut inference time by up to $40\\%$ while slightly improving image quality scores. Overall, HarmoniCa paves the way for real-time, high-resolution generative media on everyday hardware."
Poster,Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres,https://ICML.cc//virtual/2025/poster/46186,"Muskan Dosi, Chiranjeev Chiranjeev, Kartik Thakral, Mayank Vatsa, Richa Singh","Do contemporary diffusion models preserve the class geometry of hyperspherical data? Standard diffusion models rely on isotropic Gaussian noise in the forward process, inherently favoring Euclidean spaces. However, many real-world problems involve non-Euclidean distributions, such as hyperspherical manifolds, where class-specific patterns are governed by angular geometry within hypercones. When modeled in Euclidean space, these angular subtleties are lost, leading to suboptimal generative performance. To address this limitation, we introduce \textbf{HyperSphereDiff} to align hyperspherical structures with directional noise, preserving class geometry and effectively capturing angular uncertainty. We demonstrate both theoretically and empirically that this approach aligns the generative process with the intrinsic geometry of hyperspherical data, resulting in more accurate and geometry-aware generative models. We evaluate our framework on four object datasets and two face datasets, showing that incorporating angular uncertainty better preserves the underlying hyperspherical manifold.","Diffusion models have recently become a popular choice for generating high-quality images, thanks to their ability to learn from data by gradually adding noise and then learning to reverse this process. However, most existing diffusion models are designed for Euclidean spaces, where data relationships are defined by straight-line distances. This design choice assumes that all variations in data can be modeled through additive Gaussian noise, which works well in many cases, but not all.In several real-world problems, especially those involving classification and recognition tasks, the data naturally resides on non-Euclidean structures such as hyperspheres. For example, in face recognition or semantic embeddings, class-specific structures are not defined by distance alone but by angular relationships, how the direction of one data point relates to another. Modeling such data using traditional Gaussian-based diffusion can distort these angular relationships, leading to blurred boundaries between classes and reduced generation quality.To solve this problem, we introduce \textbf{HyperSphereDiff}, a diffusion framework tailored for hyperspherical data. Instead of using Gaussian noise, we use the von Mises-Fisher (vMF) distribution, a directional noise model that respects the geometry of the sphere. This allows us to capture and preserve class-specific angular patterns, where each class is modeled as a “hypercone” (a region defined by direction and angular spread).Our method modifies both the forward and reverse steps of the diffusion process. In the forward direction, data is perturbed with directional noise that pushes it toward a uniform distribution on the sphere while maintaining its angular structure. In the reverse direction, a score-based network guides the generation back toward class-specific hypercones, ensuring that samples remain faithful to their class geometry.We also introduce two new metrics: \textbf{Hypercone Coverage Ratio (HCR)} and \textbf{Hypercone Difficulty Skew (HDS)}, to evaluate how well a generative model preserves the angular structure and generates challenging, diverse samples.Through theoretical insights and empirical results on six datasets (four object categories and two face datasets), we demonstrate that HyperSphereDiff not only better aligns with the natural structure of hyperspherical data but also generates higher-quality, class-consistent samples. Compared to traditional methods, our approach improves robustness, sample diversity, and overall fidelity in class-conditional generation tasks.By integrating directional uncertainty and hyperspherical geometry awareness into diffusion models, HyperSphereDiff opens a new direction for generative modeling that more faithfully captures the geometric nature of complex data."
Poster,Harnessing Heterogeneous Statistical Strength for Personalized Federated Learning via Hierarchical Bayesian Inference,https://ICML.cc//virtual/2025/poster/44831,"Mahendra Singh Thapa, Rui Li","Personalized federated learning (PFL) based on Bayesian approach tackle the challenges from statistical heterogeneity of client data by computing a personalized posterior distribution over the parameters of each client's local model and constructing a global distribution by aggregating the parameters of these personalized posteriors. However, the heuristic aggregation methods introduce strong biases and result in global models with poor generalization. We thus propose a novel hierarchical Bayesian inference framework for PFL by specifying a conjugate hyper-prior over the parameters of the personalized posteriors. This allows us to jointly compute a global posterior distribution for aggregation and the personalized ones at local level. This hierarchical Bayesian inference framework achieves elegant balance between local personalization and global model robustness. Extensive empirical study shows that by effectively sharing the heterogeneous statistical strength across the local models while retaining their distinctive characteristics, our framework yields state-of-the-art performance. We also show that existing Bayesian PFLs are special cases of our framework.","Federated Learning (FL) is a way to train machine learning models across many users’ devices, such as smartphones or laptops, without needing to share their private data with anyone. Each user trains a personalized model locally using only their private data. These personalized models are then combined to build a global model. However, since users’ data can vary significantly, combining all these personalized models into a reliable global model while preserving the unique characteristics of each user’s data remains a major challenge. In our work, instead of simply averaging the personalized models to form the global model, we model the relationship between the personalized and global models using a hierarchical Bayesian framework. This framework allows us to jointly find the distribution parameters for both the global and personalized models, achieving an elegant balance between local personalization and global model robustness. We show that many existing personalized FL methods are special cases of our approach. By framing personalized federated learning using a principled Bayesian framework, we offer a fresh perspective on how to improve FL systems. Our work has the potential to make a meaningful impact in fields like healthcare and finance, where both model performance and data privacy are critically important."
Poster,HashAttention: Semantic Sparsity for Faster Inference,https://ICML.cc//virtual/2025/poster/45928,"Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E Gonzalez, Ion Stoica","Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to $16\times$ with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to $32\times$ through task-specific fine-tuning. On A100 GPU, at $32\times$ sparsity, incorporating HashAttention reduces attention latency by up to $4.3\times$ in GPT-FAST and $2.54\times$ in FlashDecode, and achieves up to $3.12\times$ higher throughput for GPT-FAST.","Modern AI systems like chatbots, image generators and code assistants, etc. rely on a mechanism called “attention” to decide which parts of the input are most important — but this process becomes slow and memory-intensive as inputs get longer. We noticed that not every word or token contributes equally; only a few really matter.Our method, HashAttention, finds and focuses only on these important tokens. We discovered this could be done by treating the problem like a recommendation system — similar to how Netflix suggests shows based on your preferences. Using clever mathematical tricks and learned functions, we represent the tokens in a compact format that allows fast comparisons using simple bitwise operations.HashAttention speeds up attention without hurting accuracy. It can reduce the number of tokens processed by up to 32×, while keeping the output quality nearly the same. This leads to faster, more efficient AI models — helping them handle longer inputs, think more and produce more text with less computing power and lower costs."
Poster,Haste Makes Waste: A Simple Approach for Scaling Graph Neural Networks,https://ICML.cc//virtual/2025/poster/45271,"Rui Xue, Tong Zhao, Neil Shah, Xiaorui Liu","Graph neural networks (GNNs) have demonstrated remarkable success in graph representation learning and various sampling approaches have been proposed to scale GNNs to applications with large-scale graphs. A class of promising GNN training algorithms take advantage of historical embeddings to reduce the computation and memory cost while maintaining the model expressiveness of GNNs. However, they incur significant computation bias due to the stale feature history. In this paper, we provide a comprehensive analysis of their staleness and inferior performance on large-scale problems. Motivated by our discoveries, we propose a simple yet highly effective training algorithm (REST) to effectively reduce feature staleness, which leads to significantly improved performance and convergence across varying batch sizes, especially when staleness is predominant. The proposed algorithm seamlessly integrates with existing solutions, boasting easy implementation, while comprehensive experiments underscore its superior performance and efficiency on large-scale benchmarks. Specifically, our improvements to state-of-the-art historical embedding methods result in a 2.7\% and 3.6\% performance enhancement on the ogbn-papers100M and ogbn-products dataset respectively, accompanied by notably accelerated convergence. The code can be found at https://github.com/RXPHD/REST.","Contemporary datasets —from social-media interactions to molecular structures and power-grid topologies—are most naturally modeled as graphs, collections of nodes connected by edges. Graph Neural Networks (GNNs) provide a powerful framework for analyzing such data, yet their training is computationally intensive. A common remedy is to sample smaller subgraphs, but this inevitably discards information from un-sampled nodes. To avoid this loss, the widely adopted historical-embedding approach caches intermediate node representations for reuse. Unfortunately, these cached embeddings quickly become stale, forcing the model to rely on outdated signals and thereby undermining both accuracy and convergence speed.To address this staleness problem, our work first offers a comprehensive analysis that pinpoints its root causes and then introduces REST—a simple yet effective training framework that mitigates the issue. REST decouples forward and backward passes and executes them at different frequencies, allowing historical embeddings to be refreshed more frequently. Experiments on multiple large-scale benchmarks show that REST improves prediction accuracy, accelerates convergence, and preserves computational efficiency, delivering a promising solution for diverse real-world applications."
Poster,Heads up! Large Language Models Can Perform Tasks Without Your Instruction via Selective Attention Head Masking,https://ICML.cc//virtual/2025/poster/43598,"Senyu Han, Hongchuan Zeng, Kai Yu, Lu Chen","Large language models (LLMs) consist of numerous Transformer modules, and while the models can perform various functions, it remains an open question of how these modules are combined to elicit distinct inherent functionalities. In this paper, we investigate the modules inside LLMs and demonstrate that, by simply masking or retaining specific attention heads during inference, LLMs can exhibit specific task functionalities without requiring explicit instructions or modifications to the model parameters. Experiments across various models and tasks reveal that LLMs inherently encode ``functional pathways'', the structured groups of interdependent attention heads that are crucial for executing specific tasks. These pathways not only govern the model's functional behaviors but also enhance parameter efficiency, as suppressing attention heads outside the pathway can improve task performance. The code is available in this repository: [https://github.com/OpenDFM/HeadsUp](https://github.com/OpenDFM/HeadsUp).","Researchers are trying to understand how large language models (LLMs)—like those that power AI chatbots—perform different tasks using the same architecture. The core issue is that while these models consist of many parts called “attention heads,” it’s unclear how specific combinations of them contribute to different functions. In this study, we found that you can control which functions the model performs by turning on or off certain attention heads, without changing the model’s settings or giving it special instructions. Through experiments, we discovered that these models contain hidden “functional pathways”—groups of attention heads that naturally work together to handle specific tasks. Surprisingly, removing parts of the model that aren’t in use for a particular task can actually improve performance. This insight shows that LLMs are more organized internally than previously thought. Our findings could help make AI models more efficient, faster, and easier to understand."
Poster,HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation,https://ICML.cc//virtual/2025/poster/45007,"Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Song xiaohui, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi","We present **HealthGPT**, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation **(H-LoRA)** technique, which is complemented by a tailored hierarchical visual perception **(HVP)** approach and a three-stage learning strategy **(TLS)**. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called **VL-Health**. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.","In general applications, AI models have been developed to both understand and generate different types of data, such as recognizing objects in images or generating pictures based on text descriptions. However, such unified models have not yet been explored or developed in the field of medical imaging. This is largely due to a lack of large-scale, high-quality medical data and the distinct requirements of understanding and generation tasks in healthcare.To address this, we introduce **HealthGPT**, the first large vision-language model designed to unify both understanding and generation tasks in the medical domain. We use a quantization technique to represent medical images as sequences of ""tokens"" similar to text, allowing the model to process and generate both images and text in a consistent way. We also propose a novel model architecture that stores task-specific knowledge separately and combines it using a multi-stage training process.To support this approach, we built a new dataset called **VL-Health**, which includes a range of medical image tasks covering both understanding and generation. Our experiments show that HealthGPT achieves strong performance on both types of tasks. We open-source our dataset and model weights to encourage further research on unified AI models in healthcare."
Poster,HEAP: Hyper Extended A-PDHG Operator for Constrained High-dim PDEs,https://ICML.cc//virtual/2025/poster/44499,"Mingquan Feng, Weixin Liao, Yixin Huang, Yifan Fu, Qifu Zheng, Junchi Yan","Neural operators have emerged as a promising approach for solving high-dimensional partial differential equations (PDEs). However, existing neural operators often have difficulty in dealing with constrained PDEs, where the solution must satisfy additional equality or inequality constraints beyond the governing equations. To close this gap, we propose a novel neural operator, Hyper Extended Adaptive PDHG (HEAP) for constrained high-dim PDEs, where the learned operator evolves in the parameter space of PDEs. We first show that the evolution operator learning can be formulated as a quadratic programming (QP) problem, then unroll the adaptive primal-dual hybrid gradient (APDHG) algorithm as the QP-solver into the neural operator architecture. This allows us to improve efficiency while retaining theoretical guarantees of the constrained optimization. Empirical results on a variety of high-dim PDEs show that HEAP outperforms the state-of-the-art neural operator model.","Partial differential equations (PDEs) describe many complex processes in science, engineering, and finance. Solving high-dimensional PDEs—especially with practical constraints like temperature limits or financial rules—is challenging and often computationally intensive. We introduce HEAP, an innovative machine learning method that efficiently solves these constrained PDEs by translating the problem into an optimization task. HEAP quickly finds accurate solutions while strictly meeting necessary constraints, outperforming current methods in accuracy and efficiency. This advancement enables faster and more reliable solutions for practical problems, such as calculating heat distributions and pricing financial derivatives, significantly broadening the applications of machine learning in crucial fields."
Poster,Heavy-Tailed Linear Bandits: Huber Regression with One-Pass Update,https://ICML.cc//virtual/2025/poster/46246,"Jing Wang, Yu-Jie Zhang, Peng Zhao, Zhi-Hua Zhou","We study the stochastic linear bandits with heavy-tailed noise. Two principled strategies for handling heavy-tailed noise, truncation and median-of-means, have been introduced to heavy-tailed bandits. Nonetheless, these methods rely on specific noise assumptions or bandit structures, limiting their applicability to general settings. The recent work [Huang et al.2024] develop a soft truncation method via the adaptive Huber regression to address these limitations. However, their method suffers undesired computational cost: it requires storing all historical data and performing a full pass over these data at each round. In this paper, we propose a \emph{one-pass} algorithm based on the online mirror descent framework. Our method updates using only current data at each round, reducing the per-round computational cost from $\mathcal{O}(t \log T)$ to $\mathcal{O}(1)$ with respect to current round $t$ and the time horizon $T$, and achieves a near-optimal and variance-aware regret of order $\widetilde{\mathcal{O}}\big(d T^{\frac{1-\varepsilon}{2(1+\varepsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\varepsilon}{2(1+\varepsilon)}}\big)$ where $d$ is the dimension and $\nu_t^{1+\varepsilon}$ is the $(1+\varepsilon)$-th central moment of reward at round $t$.","In online decision-making with streaming data, such as in financial markets and online advertising, observations are often affected by heavy-tailed noise, making learning and decision-making particularly challenging. Existing methods rely on storing and processing all past data at each step to achieve strong performance, which is time-consuming and inefficient. This paper introduces a new algorithm that makes decisions using only the current observation, drastically reducing computational cost while maintaining high performance—both theoretically proven and empirically validated."
