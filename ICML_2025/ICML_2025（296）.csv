type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback,https://ICML.cc//virtual/2025/poster/46149,"Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, Yu Cheng","Large language models (LLMs) have presented impressive performance but often lack the flexibility to adapt to human preferences quickly without retraining. Inspired by the recent efforts on test-time scaling, we make the first attempt to propose Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, eliminating the need to update model parameters. Instead of relying on purely numerical rewards, TPO translates reward signals into \emph{textual} critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth of the inference process. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly.","Large language models (LLMs) like ChatGPT often need retraining to better follow human preferences—a costly and inflexible process. Our method, Test-time Preference Optimization (TPO), sidesteps this by improving responses on the fly, without changing the model itself. TPO works by first generating multiple answers to a question (parallel sampling), then using a separate reward model to pick the best and worst ones. The model reflects on their strengths and weaknesses and rewrites a better version, much like editing an essay based on feedback. Repeating this process just once or twice significantly improves model alignment with human values. TPO combines the breadth of sampling with the depth of iterative revision, enabling models to adapt quickly and intelligently at test time. This makes AI systems both more helpful and more efficient—no retraining required."
Poster,Test-Time Selective Adaptation for Uni-Modal Distribution Shift in Multi-Modal Data,https://ICML.cc//virtual/2025/poster/46389,"MingCai Chen, Baoming Zhang, Zongbo Han, Wenyu Jiang, Yanmeng Wang, Shuai Feng, Yuntao Du, Bingkun BAO","Modern machine learning applications are characterized by the increasing size of deep models and the growing diversity of data modalities.  This trend underscores the importance of efficiently adapting pre-trained multi-modal models to the test distribution in real time, i.e., multi-modal test-time adaptation.  In practice, the magnitudes of multi-modal shifts vary because multiple data sources interact with the impact factor in diverse manners.  In this research, we investigate the the under-explored practical scenario *uni-modal distribution shift*, where the distribution shift influences only one modality, leaving the others unchanged. Through theoretical and empirical analyses, we demonstrate that the presence of such shift impedes multi-modal fusion and leads to the negative transfer phenomenon in existing test-time adaptation techniques.  To flexibly combat this unique shift, we propose a selective adaptation schema that incorporates multiple modality-specific adapters to accommodate potential shifts and a ``router'' module that determines which modality requires adaptation. Finally, we validate the effectiveness of our proposed method through extensive experimental evaluations.Code available at https://github.com/chenmc1996/Uni-Modal-Distribution-Shift.","In real-world applications like self-driving cars or medical imaging, machines often face data that looks different from what they’ve trained on—a problem called “distribution shift.” Traditional methods assume all types of data (like images and sounds) change together, but in reality, only one type of data (e.g., just camera images or just audio) might shift. This “uni-modal” shift can confuse models, making them unreliable.To address this, we developed a selective adaptation method that lets models focus on fixing only the shifted data type. Imagine a self-driving car where the camera sees snowy roads (shifted data), but the LiDAR (another data type) works normally. Our approach uses a “router” to detect which data type needs adjustment and a lightweight “adapter” to fix just that part, leaving the rest of the model unchanged. This prevents the model from overreacting to stable data, like misadjusting the LiDAR when only the camera is affected.Through experiments on video and audio datasets, we showed our method outperforms existing techniques in handling such shifts. By isolating and adapting only the affected data type, our approach ensures machines stay accurate and reliable in dynamic real-world scenarios, where different data types may change independently. This is crucial for building robust AI systems that can adapt safely without retraining, especially in high-stakes fields like healthcare or autonomous driving."
Poster,Test-Time Training Provably Improves Transformers as In-context Learners,https://ICML.cc//virtual/2025/poster/44720,"Halil Alperen Gozeten, Muhammed Emrullah Ildiz, Xuechen Zhang, Mahdi Soltanolkotabi, Marco Mondelli, Samet Oymak","Test-time training (TTT) methods explicitly update the weights of a model to adapt to the specific test instance, and they have found success in a variety of settings, including most recently language modeling and reasoning. To demystify this success, we investigate a gradient-based TTT algorithm for in-context learning, where we train a transformer model on the in-context demonstrations provided in the test prompt. Specifically, we provide a comprehensive theoretical characterization of linear transformers when the update rule is a single gradient step. Our theory (i) delineates the role of alignment between pretraining distribution and target task, (ii) demystifies how TTT can alleviate distribution shift, and (iii) quantifies the sample complexity of TTT including how it can significantly reduce the eventual sample size required for in-context learning. As our empirical contribution, we study the benefits of TTT for TabPFN, a tabular foundation model. In line with our theory, we demonstrate that TTT significantly reduces the required sample size for tabular classification (3 to 5 times fewer) unlocking substantial inference efficiency with a negligible training cost.","Modern machine learning models, like large language models, can perform well across many tasks but might still struggle when encountering unfamiliar ones. A technique called ""in-context learning"" addresses this by providing relevant examples (demonstrations) of a task to the model, helping models better understand the new task. However, we are looking for ways to further leverage these examples by quickly updating the model's parameters using the provided examples themselves. To do this, our work explores a method called test-time training (TTT), which involves quickly adapting the model through one learning step based on the examples of a new task given at test-time. We develop a theoretical framework showing how, even with just one learning step, TTT can enhance the model’s ability to learn from the given demonstrations. Adapting the model to a new task using TTT significantly reduces the number of demonstrations needed, which makes inference data-efficient by enabling the model to achieve high performance from fewer examples. We confirm these in a real-world tabular foundation model, designed for table-like data. Using TTT, the tabular model maintains high performance while needing three to five times fewer examples."
Poster,TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation,https://ICML.cc//virtual/2025/poster/44110,"Tianyi Liang, Jiangqi Liu, Yifei Huang, Shiqi Jiang, Jianshen Shi, Changbo Wang, Chenhui Li","Text-to-image (T2I) generation has made remarkable progress in producing high-quality images, but a fundamental challenge remains: creating backgrounds that naturally accommodate text placement without compromising image quality. This capability is non-trivial for real-world applications like graphic design, where clear visual hierarchy between content and text is essential.Prior work has primarily focused on arranging layouts within existing static images, leaving unexplored the potential of T2I models for generating text-friendly backgrounds.We present TextCenGen, a training-free approach that actively relocates objects before optimizing text regions, rather than directly reducing cross-attention which degrades image quality. Our method introduces: (1) a force-directed graph approach that detects conflicting objects and guides them relocation using cross-attention maps, and (2) a spatial attention constraint that ensures smooth background generation in text regions. Our method is plug-and-play, requiring no additional training while well balancing both semantic fidelity and visual quality.Evaluated on our proposed text-friendly T2I benchmark of 27,000 images across three seed datasets, TextCenGen outperforms existing methods by achieving 23\% lower saliency overlap in text regions while maintaining 98\% of the original semantic fidelity measured by CLIP score and our proposed Visual-Textual Concordance Metric (VTCM).","When you add text to an image, like putting a caption on a photo or adding information to a poster, the text needs to be clearly visible. However, images created by AI often have important objects or busy patterns that make text hard to read when placed on top. This is a significant problem for designers who need backgrounds that work well with text.Our research introduces TextCenGen, a new method that creates images specifically designed to accommodate text. Unlike previous approaches that try to fit text onto existing images, our method actually modifies how the AI generates the image in the first place.TextCenGen works by identifying which objects in the image would conflict with the planned text area and gently moving these objects to other parts of the image. It then ensures the text area has a smooth, clean background. This creates a harmonious balance between the image content and the space reserved for text.Our method requires no additional training and can work with existing AI image generation tools. When tested against other approaches, TextCenGen created images that were 23% better at keeping important objects out of text areas while maintaining 98% of the image quality and meaning.This technology could help designers, marketers, and everyday users create more effective visual communications where text and images work together seamlessly, such as for social media posts, advertisements, or mobile app interfaces."
Poster,Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models,https://ICML.cc//virtual/2025/poster/46007,"Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian","Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal.However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects.Besides, the rendering process from parametric sequences to visual objects is many-to-one.Therefore, both sequential and visual signals are critical for effective training.In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated.These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals.Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively.","The Computer-Aided Design (CAD) objects can be represented by sequences reflect the design history. Generating CAD from written instructions involve model training on only sequential data pairs, which posed problems on learning efficiency and final performance.We developed a text-to-CAD system on not only sequential learning but also visual feedback. This paradigm improved the model performance by integrating CAD's visual quality into the pipeline. This helps the system produce cleaner, more natural designs with smooth edges and correct holes. Our work improves the text-to-CAD generation quality and can help speed up the early stages of product design, saving time and reducing the need for expertise."
Poster,Text-to-LoRA: Instant Transformer Adaption,https://ICML.cc//virtual/2025/poster/43471,"Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Lange","While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices.To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets.Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks.This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements.Our code is available at https://github.com/SakanaAI/text-to-lora","This paper introduces Text-to-LoRA (T2L), a method to make adapting Language Models for specific tasks much easier and more accessible. Traditionally, customizing these models requires gathering large datasets and performing expensive, time-consuming fine-tuning for specific applications. T2L bypasses this by training a special model (a hypernetwork) that can instantly generate the necessary task-specific adaptations (called LoRAs) in a single, inexpensive step, based solely on a simple textual description of the task. This approach is a step towards dramatically lowering the technical and computational barriers, allowing non-technical users to specialize foundation models using plain language, rather than needing deep technical expertise or large compute resources."
Poster,Textual Unlearning Gives a False Sense of Unlearning,https://ICML.cc//virtual/2025/poster/44277,"Jiacheng Du, Zhibo Wang, Jie Zhang, Xiaoyi Pang, Jiahui Hu, Kui Ren","Language Models (LMs) are prone to ''memorizing'' training data, including substantial sensitive user information. To mitigate privacy risks and safeguard the right to be forgotten, machine unlearning has emerged as a promising approach for enabling LMs to efficiently ''forget'' specific texts. However, despite the good intentions, is textual unlearning really as effective and reliable as expected? To address the concern, we first propose Unlearning Likelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing method, and find that unlearned texts can still be detected with very high confidence after unlearning. Further, we conduct an in-depth investigation on the privacy risks of textual unlearning mechanisms in deployment and present the Textual Unlearning Leakage Attack (TULA), along with its variants in both black- and white-box scenarios. We show that textual unlearning mechanisms could instead reveal more about the unlearned texts, exposing them to significant membership inference and data reconstruction risks. Our findings highlight that existing textual unlearning actually gives a false sense of unlearning, underscoring the need for more robust and secure unlearning mechanisms.","Machine unlearning is a method designed to make artificial intelligence (AI) models ""forget"" specific pieces of information. This is especially important for protecting sensitive data and complying with privacy laws. However, our research shows that unlearning doesn't work as well as it seems in language models (the AI behind tools like chatbots). We found that current unlearning techniques often fail to fully erase the targeted information. Using a rigorous auditing approach, we were still able to detect traces of the supposedly forgotten data. Even more concerning, we discovered that trying to unlearn data can backfire: it can actually make it easier for attackers to figure out what you want to forget by comparing models before and after unlearning. Our work highlights serious flaws in current machine unlearning practices and emphasizes the need for safer, more reliable methods to truly protect user privacy in AI systems."
Poster,Textural or Textual: How Vision-Language Models Read Text in Images,https://ICML.cc//virtual/2025/poster/44522,"Hanzhang Wang, Qingyuan Ma","Typographic attacks are often attributed to the ability of multimodal pre-trained models to fuse textual semantics into visual representations, yet the mechanisms and locus of such interference remain unclear. We examine whether such models genuinely encode textual semantics or primarily rely on texture-based visual features. To disentangle orthographic form from meaning, we introduce the ToT dataset, which includes controlled word pairs that either share semantics with distinct appearances (synonyms) or share appearance with differing semantics (paronyms). A layer-wise analysis of Intrinsic Dimension (ID) reveals that early layers exhibit competing dynamics between orthographic and semantic representations. In later layers, semantic accuracy increases as ID decreases, but this improvement largely stems from orthographic disambiguation. Notably, clear semantic differentiation emerges only in the final block, challenging the common assumption that semantic understanding is progressively constructed across depth. These findings reveal how current vision-language models construct text representations through texture-dependent processes, prompting a reconsideration of the gap between visual perception and semantic understanding. The code is available at: https://github.com/Ovsia/Textural-or-Textual","Can AI models that read text in images really understand what the words mean? Or are they simply recognizing shapes and patterns, like reading handwriting without knowing the language?We investigated this question and found that, in most layers of these models, words are treated as visual textures rather than meaningful language. As the model compresses these visual features, its ability to recognize words improves. However, this improvement is still rooted in how the text looks, not what it means. Only in the final block, across its last few layers, does the model begin to show signs of actual language understanding.This discovery led us to a simple but effective solution. By fine-tuning this final part of the model, we can help it better distinguish between meaningless text patterns and meaningful words. This makes the system more robust against typographic attacks that try to confuse it with misleading or irrelevant text."
Poster,TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization,https://ICML.cc//virtual/2025/poster/45181,"Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia","Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work  establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.","Large language models like ChatGPT learn to generate helpful responses by being trained using reinforcement learning from human feedback. A key part of this process involves teaching the model which responses are better, often using numerical rewards. Traditionally, these rewards are given for the entire response, but recent advances in a reinforcement learning algorithm called Proximal Policy Optimization show that giving rewards for each individual word or token can help models learn more effectively. However, this idea doesn’t easily fit with another popular method called Direct Preference Optimization (DPO), which focuses on learning from entire responses.Our work bridges this gap. We developed a way to break down the learning process so that it operates at the token level, allowing them to guide the model’s learning with token-specific feedback. This new approach allows the model to adjust each word it generates based on how good or bad that word is considered. As a result, the model learns to generate better responses overall.Experiments show that our method significantly improves performance over existing preference optimization methods on standard benchmarks used to evaluate how well AI models follow instructions."
Poster,The Batch Complexity of Bandit Pure Exploration,https://ICML.cc//virtual/2025/poster/44354,"Adrienne Tuynman, Rémy Degenne","In a fixed-confidence pure exploration problem in stochastic multi-armed bandits, an algorithm iteratively samples arms and should stop as early as possible and return the correct answer to a query about the arms distributions.We are interested in batched methods, which change their sampling behaviour only a few times, between batches of observations.We give an instance-dependent lower bound on the number of batches used by any sample efficient algorithm for any pure exploration task.We then give a general batched algorithm and prove upper bounds on its expected sample complexity and batch complexity.We illustrate both lower and upper bounds on best-arm identification and thresholding bandits.","With access to a set of options, we want to answer a question about these options: in a clinical trial setting, those questions could be: which treatment is the best? Which is efficient enough when compared to an efficiency threshold? To answer the question, we can sample the options. To quickly learn, it can be a good idea to adapt to past observations, to for example not resample a very bad option if we are looking for the best one. However, adapting at every single time step is computationally expensive and not always practically doable. How, then, does one balance learning speed and the number of adaptivity rounds (how many times one has to recompute and reevaluate)?We look at how many rounds of adaptivity are necessary for a given learning speed. We also give a general algorithm that can answer any possible question with guarantees on its speed and its number of adaptivity rounds."
