type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Outsourced Diffusion Sampling: Efficient Posterior Inference in Latent Spaces of Generative Models,https://ICML.cc//virtual/2025/poster/46251,"Siddarth Venkatraman, Mohsin Hasan, Minsu Kim, Luca Scimeca, Marcin Sendera, Yoshua Bengio, Glen Berseth, Nikolay Malkin","Any well-behaved generative model over a variable $\mathbf{x}$ can be expressed as a deterministic transformation of an exogenous (‘*outsourced'*) Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_\theta(\mathbf{z})$. In such a model (*eg*, a VAE, GAN, or continuous-time flow-based model), sampling of the target variable $\mathbf{x} \sim p_\theta(\mathbf{x})$ is straightforward, but sampling from a posterior distribution of the form $p(\mathbf{x}\mid\mathbf{y}) \propto p_\theta(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint function depending on an auxiliary variable $\mathbf{y}$, is generally intractable.We propose to amortize the cost of sampling from such posterior distributions with diffusion models that sample a distribution in the noise space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement learning algorithms to enforce that the transformed samples $f_\theta(\mathbf{z})$ are distributed according to the posterior in the data space ($\mathbf{x}$). For many models and constraints, the posterior in noise space is smoother than in data space, making it more suitable for amortized inference. Our method enables conditional sampling under unconditional GAN, (H)VAE, and flow-based priors, comparing favorably with other inference methods. We demonstrate the proposed ___outsourced diffusion sampling___ in several experiments with large pretrained prior models: conditional image generation, reinforcement learning with human feedback, and protein structure generation.","The paper introduces a novel approach for steering the output distribution of a generative model toward samples that achieve high reward under a given objective. Instead of modifying the generative model directly, a separate diffusion model is trained using reinforcement learning to sample noise vectors that when passed through the generative model, produce high-reward outputs."
Poster,Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner,https://ICML.cc//virtual/2025/poster/46576,"Chunhui Zhang, Zhongyu Ouyang, Kwonjoon Lee, Nakul Agarwal, Sean Houlihan, Soroush Vosoughi, Shao-Yuan Lo","Theory-of-mind (ToM) enables humans to infer mental states—such as beliefs, desires, and intentions—forming the foundation of social cognition. Existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning but struggle with scalability in multimodal environments. They remain trapped within the gravitational pull of multi-step planning complexity, failing to generalize as task demands increase. To overcome these limitations, we propose a scalable Bayesian ToM planner. It breaks down ToM complexity into stepwise Bayesian updates. Meanwhile, weak-to-strong control specializes smaller LMs to refine ToM-specific likelihood estimation, transferring their ToM reasoning behavior to larger LMs (7B to 405B) for social and world knowledge integration. This synergistic approach enables scalability, aligning large-model inference with human mental states with Bayesian principles. Extensive experiments demonstrate a 4.6% improvement in accuracy over state-of-the-art methods on multimodal ToM benchmarks, including unseen scenarios, establishing a new standard for modeling human mental states in complex environments.","Humans understand others' thoughts and intentions by watching what they do and where they do it—whether someone reaches into a basket or walks toward a cabinet. However, it is challenging to equip machines with this ""theory-of-mind"" skill. Current AI methods either struggle to reason through multiple steps or require costly retraining for each new situation. In our work, we present a two-part solution. The first part breaks down complex reasoning into simple, step-by-step updates. The second part uses a small, task-tuned model to gently guide a larger, world-knowledge model during inference. This combination keeps the system both lightweight and broadly informed, so it remains highly accurate even as tasks become more complex. In simulations that combine video and text, our approach worked better than previous methods. This creates a path for AI to better understand human goals and beliefs in the real world."
Poster,Overcoming Non-monotonicity in Transducer-based Streaming Generation,https://ICML.cc//virtual/2025/poster/45770,"Zhengrui Ma, Yang Feng, Min zhang","Streaming generation models are utilized across fields, with the Transducer architecture being popular in industrial applications. However, its input-synchronous decoding mechanism presents challenges in tasks requiring non-monotonic alignments, such as simultaneous translation. In this research, we address this issue by integrating Transducer's decoding with the history of input stream via a learnable monotonic attention. Our approach leverages the forward-backward algorithm to infer the posterior probability of alignments between the predictor states and input timestamps, which is then used to estimate the monotonic context representations, thereby avoiding the need to enumerate the exponentially large alignment space during training. Extensive experiments show that our MonoAttn-Transducer effectively handles non-monotonic alignments in streaming scenarios, offering a robust solution for complex generation tasks. Code is available at https://github.com/ictnlp/MonoAttn-Transducer.","Real-time speech recognition and translation systems face a critical challenge: they must start generating translations or transcriptions before the speaker finishes talking. Existing methods typically either use attention-based models, which require external policies to balance quality and latency, or Transducer models, which efficiently synchronize input and output but struggle when translations involve reordering words.Our research addresses the limitation of Transducer models by introducing a new method that combines them with a dynamic attention mechanism, allowing the model to better handle sentences where words don't align in the same order across languages. Specifically, we developed an efficient training algorithm that enables the model to attend to previously spoken words without needing to consider an overwhelming number of alignment possibilities.By applying our approach to simultaneous translation tasks, we demonstrated that it significantly improves the quality of real-time translations without sacrificing speed. This advancement is particularly beneficial when dealing with languages or speech scenarios that require more flexible word ordering, making real-time communication smoother and more accurate."
Poster,Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan,https://ICML.cc//virtual/2025/poster/46663,"Jaemoo Choi, Jaewoong Choi, Dohyun Kwon","We address the convergence problem in learning the Optimal Transport (OT) map, where the OT Map refers to a map from one distribution to another while minimizing the transport cost. Semi-dual Neural OT, a widely used approach for learning OT Maps with neural networks, often generates spurious solutions that fail to transfer one distribution to another accurately. We identify a sufficient condition under which the max-min solution of Semi-dual Neural OT recovers the true OT Map. Moreover, to address cases when this sufficient condition is not satisfied, we propose a novel method, OTP, which learns both the OT Map and the Optimal Transport Plan, representing the optimal coupling between two distributions. Under sharp assumptions on the distributions, we prove that our model eliminates the spurious solution issue and correctly solves the OT problem. Our experiments show that the OTP model recovers the optimal transport map where existing methods fail and outperforms current OT-based models in image-to-image translation tasks. Notably, the OTP model can learn stochastic transport maps when deterministic OT Maps do not exist, such as one-to-many tasks like colorization.","Optimal Transport (OT) is a powerful mathematical framework for mapping one distribution to another, widely used in machine learning tasks such as image translation and domain adaptation. While neural networks have recently been applied to learn these OT maps, many popular methods often produce spurious solutions — results that satisfy the learning objectives but fail to correctly transform the source distribution into the target distribution.We analyzed this issue and identified a mathematical condition that guarantees these neural OT methods can recover the correct OT map. However, this condition is often violated in practice. To address this, we developed a new method — called the Optimal Transport Plan (OTP) model — that reliably learns the correct transformation even when the condition does not hold.Our approach smooths the source data distribution to avoid pathological cases, learns the OT plan from the smoothed data, and then gradually returns to the original data. This allows the model to recover both deterministic and stochastic transport plans, where previous methods fail. OTP demonstrates strong performance on synthetic and real-world tasks, providing a robust foundation for neural OT in complex settings."
Poster,Overcoming the Curse of Dimensionality in Reinforcement Learning Through Approximate Factorization,https://ICML.cc//virtual/2025/poster/44811,"Chenbei Lu, Laixi Shi, Zaiwei Chen, Chenye Wu, Adam Wierman","Factored Markov Decision Processes (FMDPs) offer a promising framework for overcoming the curse of dimensionality in reinforcement learning (RL) by decomposing high-dimensional MDPs into smaller and independently evolving components. Despite their potential, existing studies on FMDPs face three key limitations: reliance on perfectly factorizable models, suboptimal sample complexity guarantees for model-based algorithms, and the absence of model-free algorithms. To address these challenges, we introduce approximate factorization, which extends FMDPs to handle imperfectly factored models. Moreover, we develop a model-based algorithm and a model-free algorithm (in the form of variance-reduced Q-learning), both achieving the first near-minimax sample complexity guarantees for FMDPs. A key novelty in the design of these two algorithms is the development of a graph-coloring-based optimal synchronous sampling strategy. Numerical simulations based on the wind farm storage control problem corroborate our theoretical findings.","Solving large-scale Markov Decision Processes (MDPs) is computationally expensive due to the high-dimensional state-action spaces and the large amount of data required to model transition dynamics accurately. Traditional reinforcement learning methods often suffer from high sample complexity and slow convergence when applied to such settings. We introduce an approximate factorization framework that decomposes the MDP's transition kernel into independent or weakly dependent components. This decomposition enables more efficient learning by reducing the dimensionality of the problem and leveraging structural properties of the system. Our approach integrates this factorization with variance-reduced Q-learning, ensuring both computational efficiency and robust convergence. By exploiting the structure of MDPs, our method significantly reduces the sample complexity and accelerates convergence. In a wind farm storage control problem, our approach achieved a 19.3% reduction in penalty costs compared to baseline methods, using just one year of operational data. This framework is broadly applicable across domains where MDPs are used, offering a principled way to balance computational efficiency and solution quality in large-scale decision-making problems."
Poster,Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling,https://ICML.cc//virtual/2025/poster/45311,"Haebin Shin, Lei Ji, Xiao Liu, Yeyun Gong","Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.","Large language models (LLMs) often pass down their knowledge to smaller models through a process called distillation. However, even as more powerful LLMs emerge, this process remains challenging, mainly because different models often use different vocabularies. If the models ""speak"" in different units, like using different sets of words, it's hard for one to teach the other effectively.VocAgnoLM introduces a new way to overcome this challenge. Instead of relying on shared vocabularies, it uses character-level alignment to extract and transfer meaningful information. This allows smaller models to learn effectively even when their vocabulary doesn't overlap with that of the teacher model. For example, when a large model demonstrates how to solve a math problem, VocAgnoLM helps the smaller model understand and rephrase it using its own language system.Experiments show that VocAgnoLM outperforms conventional distillation methods, even in cases where the teacher and student models share very little vocabulary. This research helps bridge the gap between different LLM natures and supports the development of smaller, more efficient AI systems."
Poster,Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination’s Impact on Machine Translation,https://ICML.cc//virtual/2025/poster/45530,"Muhammed Yusuf Kocyigit, Eleftheria Briakou, Daniel Deutsch, Jiaming Luo, Colin Cherry, Markus Freitag","Data contamination—the accidental consumption of evaluation examples within the pre-training data—can undermine the validity of evaluation benchmarks. In this paper, we present a rigorous analysis of the effects of contamination on language models at 1B and 8B scales on the machine translation task. Starting from a carefully decontaminated train-test split, we systematically introduce contamination at various stages, scales, and data formats to isolate its effect and measure its impact on performance metrics. Our experiments reveal that contamination with both source and target substantially inflates BLEU scores, and this inflation is 2.5 times larger (up to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and target-only contamination generally produce smaller, less consistent over-estimations. Finally, we study how the temporal distribution and frequency of contaminated samples influence performance over-estimation across languages with varying degrees of data resources.","Sometimes test examples end up in the training data making models look better than they really are. This paper studies how that “cheating” affects medium (1B) and large (8B) models. Starting from a clean train and test dataset, we deliberately reintroduced test examples in different formats and varying number of copies into the training set. This allowed us to compare the effect of seeing these test example during pre-training as opposed to not seeing them. We did this specifically for the machine translation task where we have a source sentence and want to translate it into a different language. We found that leaking source and translation together boosts BLEU scores(a measure of how good the translation is) by up to 30 points for the 8B model and around 8 BLEU scores for the 1B model. Leaking only one side gave smaller, inconsistent gains. We also showed that the frequency and recency of leaks can impact the amount of score inflation. We also show that effects can vary depending on how much data for a language exists in our training data which is called resource level of a language."
Poster,Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling,https://ICML.cc//virtual/2025/poster/44467,"Hongzhi Huang, Defa Zhu, Banggu Wu, Zeng, Ya Wang, Qiyang Min, zhou Xun","Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs.","Large language models (LLM) like ChatGPT rely on a step called tokenization, breaking text into small pieces with a vocabulary before they can understand and generate language. But we still don’t fully understand how this step affects a model’s performance as it grows larger.In our research, we decouples input and output vocabulary, allowing the model to process a more diverse and fine-grained vocabulary at input. By experimenting with different vocabulary sizes, we found that using larger input vocabulary leads to consistently better performance, even without making the model itself any bigger.This discovery shows that choices in how we carefully break down language can unlock more power from existing models. Our work provides new insights for building the next generation of LLMs."
Poster,Overtrained Language Models Are Harder to Fine-Tune,https://ICML.cc//virtual/2025/poster/44907,"Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, Graham Neubig, Aditi Raghunathan","Large language models are pre-trained on ever-growing token budgets under the assumption that better pre-training performance translates to improved downstream models. In this work, we challenge this assumption and show that extended pre-training can make models harder to fine-tune, leading to degraded final performance. We term this phenomenon \textbf{catastrophic overtraining}. For example, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to over 2\% worse performance on multiple standard LLM benchmarks than its 2.3T token counterpart. Through controlled experiments and theoretical analysis, we show that catastrophic overtraining arises from a systematic increase in the broad sensitivity of pre-trained parameters to modifications, including but not limited to fine-tuning. Our findings call for a critical reassessment of pre-training design that considers the downstream adaptability of the model.","Recent improvements in artificial intelligence language models have largely been driven by increasing the amount of resources spent training them. Typically, this involves making models larger and giving them more data to learn from. Recently, however, researchers have begun training models far beyond the usual amount of data, aiming to keep models smaller and more efficient for later use. In our research, we identify an unexpected downside of this approach. Although training these models longer continuously improves their basic learning performance, we found that excessively trained models become surprisingly harder to adapt later for specific tasks. Specifically, models trained on very large amounts of data perform worse after adaptation---both on familiar tasks and new ones---compared to those trained more moderately. We also provide theoretical insights to explain this behavior in simplified settings. Overall, our findings suggest an unexpected trade-off: using more resources to train a model initially may actually make it less effective when it needs to be adapted later."
Poster,OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition,https://ICML.cc//virtual/2025/poster/44927,"Zheng Lian, Haiyang Sun, Licai Sun, Haoyu Chen, Lan Chen, Hao Gu, Zhuofan Wen, Shun Chen, Zhang Siyuan, Hailiang Yao, Bin Liu, Rui Liu, Shan Liang, Ya Li, Jiangyan Yi, Jianhua Tao","Multimodal Emotion Recognition (MER) is a critical research area that seeks to decode human emotions from diverse data modalities. However, existing machine learning methods predominantly rely on predefined emotion taxonomies, which fail to capture the inherent complexity, subtlety, and multi-appraisal nature of human emotional experiences, as demonstrated by studies in psychology and cognitive science. To overcome this limitation, we advocate for introducing the concept of *open vocabulary* into MER. This paradigm shift aims to enable models to predict emotions beyond a fixed label space, accommodating a flexible set of categories to better reflect the nuanced spectrum of human emotions. To achieve this, we propose a novel paradigm: *Open-Vocabulary MER (OV-MER)*, which enables emotion prediction without being confined to predefined spaces. However, constructing a dataset that encompasses the full range of emotions for OV-MER is practically infeasible; hence, we present a comprehensive solution including a newly curated database, novel evaluation metrics, and a preliminary benchmark. By advancing MER from basic emotions to more nuanced and diverse emotional states, we hope this work can inspire the next generation of MER, enhancing its generalizability and applicability in real-world scenarios. Code and dataset are available at: https://github.com/zeroQiaoba/AffectGPT.","Recognizing human emotions is a key challenge in AI, but current methods often rely on limited, predefined emotion categories that don’t fully capture the complexity of how people feel. Therefore, we propose a new approach called *Open-Vocabulary Multimodal Emotion Recognition (OV-MER)*, which allows AI models to predict emotions beyond fixed labels, enabling a more flexible and nuanced understanding of human feelings.However, creating a dataset that covers the full spectrum of human emotions is nearly impossible. To address this, we introduce a new framework, including a curated dataset, novel evaluation metrics, and a benchmark. Our work aims to push the boundaries of emotion recognition, making it more adaptable and useful in real-world applications. By moving beyond rigid emotion categories, we hope to inspire the next generation of AI systems that better understand and respond to human emotions."
