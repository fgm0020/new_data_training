type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs,https://ICML.cc//virtual/2025/poster/43884,"Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun","Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.","Large language models (LLMs) like ChatGPT are powerful but require a lot of computing power, making them hard to use in real-world applications. To solve this, researchers use a process called ""distillation,"" which helps create smaller, faster models that try to mimic the abilities of larger ones. However, most previous distillation methods used the same training strategy for all types of model outputs, missing out on the chance to better tailor training for different situations.Our work introduces DistiLLM-2, a new approach that uses a ""contrastive"" training strategy. Instead of treating all model responses equally, DistiLLM-2 separately encourages the student model to learn from good answers provided by the teacher model, while also learning from its own mistakes. By using different training techniques for each type of answer, we help the smaller model learn more effectively.We tested DistiLLM-2 on a wide variety of tasks, including following instructions, solving math problems, and writing code. The results show that our method creates small language models that perform better than those trained with older techniques. Beyond language, our method also improves models that can understand images and text together, and helps make AI run faster and more efficiently. We believe this work will help make AI more accessible and practical for everyone."
Poster,Distinguishing Cause from Effect with Causal Velocity Models,https://ICML.cc//virtual/2025/poster/44472,"Johnny Xi, Hugh Dance, Peter Orbanz, Benjamin Bloem-Reddy","Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a *causal velocity* by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location-scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non–identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation.","Causal relationships manifest as statistical dependence, but statistical models are not able to distinguish between when a variable causes the other, and when a variable is caused by the other. Causal discovery refers to the problem of determining the causal direction with only passively observed data from the system, without performing experimental intervention. This paper follows up on recent work that selects the causal direction to be the one that best fits some presupposed hypothesis, formulated as a restricted statistical model. Here, this is done by introducing a new way of formulating such a hypothesis via the incremental effects of the cause on the effect, which we call causal velocity models. This formulation greatly enlarges the types of hypotheses that can be made about the causal relationship. Although simple models for the causal effect are used here, they perform very well for causal relationships that are not well-explained by existing methods."
Poster,Distributed Conformal Prediction via Message Passing,https://ICML.cc//virtual/2025/poster/45295,"Haifeng Wen, Hong XING, Osvaldo Simeone","Post-hoc calibration of pre-trained models is critical for ensuring reliable inference, especially in safety-critical domains such as healthcare. Conformal Prediction (CP) offers a robust post-hoc calibration framework, providing distribution-free statistical coverage guarantees for prediction sets by leveraging held-out datasets. In this work, we address a decentralized setting where each device has limited calibration data and can communicate only with its neighbors over an arbitrary graph topology. We propose two message-passing-based approaches for achieving reliable inference via CP: quantile-based distributed conformal prediction (Q-DCP) and histogram-based distributed conformal prediction (H-DCP). Q-DCP employs distributed quantile regression enhanced with tailored smoothing and regularization terms to accelerate convergence, while H-DCP uses a consensus-based histogram estimation approach. Through extensive experiments, we investigate the trade-offs between hyperparameter tuning requirements, communication overhead, coverage guarantees, and prediction set sizes across different network topologies. The code of our work is released on: https://github.com/HaifengWen/Distributed-Conformal-Prediction.","AI models are increasingly used in critical areas like healthcare, in which ensuring reliable predictions is important. However, this problem becomes challenging when the data needed to calibrate these models is scattered across many devices. We introduce two new methods that enable devices to collaborate and enhance their AI's trustworthiness, even if they can only communicate with their neighbors in a network. One approach allows them to quickly reach an agreement on a safety margin for AI predictions by sharing only a little information. The other involves cooperatively constructing a more detailed margin of the AI's potential errors. We help to build more reliable AI systems by calibrating them using decentralized data. We also provide the practical trade-offs of each method, such as communication needs and ease of setup, guiding better choices for real-world applications."
Poster,Distributed Differentially Private Data Analytics via Secure Sketching,https://ICML.cc//virtual/2025/poster/46582,"Jakob Burkhardt, Hannah Keller, Claudio Orlandi, Chris Schwiegelshohn","We introduce the *linear-transformation model*, a distributed model of differentially private data analysis. Clients have access to a trusted platform capable of applying a public matrix to their inputs. Such computations can be securely distributed across multiple servers using simple and efficient secure multiparty computation techniques. The linear-transformation model serves as an intermediate model between the highly expressive *central model* and the minimal *local model*. In the central model, clients have access to a trusted platform capable of applying any function to their inputs. However, this expressiveness comes at a cost, as it is often expensive to distribute such computations, leading to the central model typically being implemented by a single trusted server. In contrast, the local model assumes no trusted platform, which forces clients to add significant noise to their data. The linear-transformation model avoids the single point of failure for privacy present in the central model, while also mitigating the high noise required in the local model. We demonstrate that linear transformations are very useful for differential privacy, allowing for the computation of linear sketches of input data. These sketches largely preserve utility for tasks such as private low-rank approximation and private ridge regression, while introducing only minimal error, critically independent of the number of clients.","In machine learning or data analysis tasks, a common and useful goal is to compute and publish some accurate model or group statistics, while preventing leakage of any single training datapoint. This goal is formalised by differential privacy; however, increasing privacy comes at the cost of a less accurate result. Without any trust assumptions, this loss in accuracy will scale with the number of users. If individuals are willing to share their private input with a third party whom they trust to compute the private statistics, the resulting error is reduced. Some intermediate models make some limited trust assumptions. In this work, we show that the relatively weak assumption of a trusted linear transformations can result in low error for the tasks of private low rank approximation and regularized linear regression. Our choice of trust assumption is motivated by existing cryptographic tools for computing such functions efficiently in a distributed setting."
Poster,Distributed Event-Based Learning via ADMM,https://ICML.cc//virtual/2025/poster/43945,"Guner Dilsad ER, Sebastian Trimpe, Michael Muehlebach","We consider a distributed learning problem, where agents minimize a global objective function by exchanging information over a network. Our approach has two distinct features: (i) It substantially reduces communication by triggering communication only when necessary, and (ii) it is agnostic to the data-distribution among the different agents. We can therefore guarantee convergence even if the local data-distributions of the agents are arbitrarily distinct. We analyze the convergence rate of the algorithm both in convex and nonconvex settings and derive accelerated convergence rates in a convex setting. We also characterize the effect of communication failures and demonstrate that our algorithm is robust to communication failures. The article concludes by presenting numerical results from distributed learning tasks on the MNIST and CIFAR-10 datasets. The experiments underline communication savings of 35\% or more due to the event-based communication strategy, show resilience towards heterogeneous data-distributions, and highlight that our approach outperforms common baselines such as FedAvg, FedProx, SCAFFOLD and FedADMM.","In our increasingly connected world, many smart devices like phones, sensors, and computers have data spread across different locations and aim to train the best global model. However, gathering all the data into one central server is not always possible due to privacy concerns or memory limitations.A common strategy is to let each device train its own local model and periodically share updates to form a shared global model (often to a central server). But constantly exchanging these updates across the network can be costly. This paper introduces a more efficient alternative where devices communicate only when significant changes occur. This technique, known as event-based communication, allows devices to skip unnecessary updates while still maintaining learning accuracy. We, then, integrate this idea into a widely used optimization method, ADMM, which helps devices converge to the best model, even when each has different data. Experiments on popular image datasets (MNIST and CIFAR-10) show that our method reduces communication cost by 35\% without losing accuracy and is robust to communication failures.This makes our technique well-suited for large-scale, resource-constrained systems, such as networks of sensors, wearable technology, or smart city infrastructure where saving bandwidth and energy is critical."
Poster,Distributed Nonparametric Estimation: from Sparse to Dense Samples per Terminal,https://ICML.cc//virtual/2025/poster/46141,"Deheng Yuan, Tao Guo, Zhongyi Huang","Consider the communication-constrained problem of nonparametric function estimation, in which each distributed terminal holds multiple i.i.d. samples. Under certain regularity assumptions, we characterize the minimax optimal rates for all regimes,  and identify phase transitions of the optimal rates as the samples per terminal vary from sparse to dense. This fully solves the problem left open by previous works, whose scopes are limited to regimes with either dense samples or a single sample per terminal. To achieve the optimal rates, we design a layered estimation protocol by exploiting protocols for the parametric density estimation problem. We show the optimality of the protocol using information-theoretic methods and strong data processing inequalities, and incorporating the classic balls and bins model. The optimal rates are immediate for various special cases such as density estimation, Gaussian, binary, Poisson and heteroskedastic regression models.","In many distributed learning systems, samples are collected by different devices and need to be transmitted to a central processor for training a model. The communication bandwidth for the transmission is often limited, which directly affects the accuracy of the model. The goal of our work is to mathematically characterize the trade-off between the accuracy and the bandwidth. We focus on a unified distributed nonparametric estimation framework, where the accuracy is quantified by the optimal convergence rate. Different from previous works, we allow the number of samples at each device to change freely without putting any restrictions. We determine the optimal rates in this more realistic and complex setting, hence unify and extend prior fragmented results. An interesting finding is that the optimal estimation rate undergoes sharp phase transitions, as the samples for each device vary from sparse to dense. On the technical side, we develop a novel layered estimation protocol to achieve the optimal rate, and show its optimality by incorporating tools from information theory and classic probabilistic models. The understanding of the trade-off in our work can provide guidance to the development of distributed systems and algorithms."
Poster,Distributed Parallel Gradient Stacking(DPGS): Solving Whole Slide Image Stacking Challenge in Multi-Instance Learning,https://ICML.cc//virtual/2025/poster/43811,"Boyuan Wu, wang, Xianwei Lin, Jiachun Xu, Jikai Yu, Zhou Shicheng, Hongda Chen, Lianxin Hu","Whole Slide Image (WSI) analysis is framed as a Multiple Instance Learning (MIL) problem, but existing methods struggle with non-stackable data due to inconsistent instance lengths, which degrades performance and efficiency. We propose a Distributed Parallel Gradient Stacking (DPGS) framework with Deep Model-Gradient Compression (DMGC) to address this. DPGS enables lossless MIL data stacking for the first time, while DMGC accelerates distributed training via joint gradient-model compression. Experiments on Camelyon16 and TCGA-Lung datasets demonstrate up to 31× faster training, up to a 99.2% reduction in model  communication size at convergence, and up to a 9.3% improvement in accuracy compared to the baseline. To our knowledge, this is the first work to solve non-stackable data in MIL while improving both speed and accuracy.","Whole Slide Images (WSIs) are large medical images used in cancer diagnosis. They are split into many patches and analyzed using Multiple Instance Learning (MIL). But since each image has a different number of patches, current methods can’t train in batches, making them slow and less accurate. We propose DPGS, a method that enables fast, parallel training on uneven data. We also introduce DMGC, which cuts communication costs by over 99%. Tested on cancer datasets, our method sped up training by up to 31× and improved accuracy by 9.3%, making AI tools for pathology faster and more reliable."
Poster,Distributed Retraction-Free and Communication-Efficient Optimization on the Stiefel Manifold,https://ICML.cc//virtual/2025/poster/45108,"Yilong Song, Peijin Li, Bin Gao, Kun Yuan","Optimization problems on the Stiefel manifold, ranging from principal component analysis to enhancing neural network robustness, are ubiquitous in machine learning. The Landing algorithm avoids computationally expensive retraction operations on manifolds, making it highly competitive for large-scale problems. This paper extends this method to distributed settings, introducing *EF-Landing*, the first retraction-free and communication-efficient algorithm for distributed stochastic optimization on the Stiefel manifold. By incorporating communication compression and error feedback, EF-Landing ensures convergence and constraint feasibility while significantly reducing communication overhead. We provide sharp convergence guarantees, demonstrating that EF-Landing achieves the same asymptotic linear speedup convergence rate as existing methods without communication compression. Furthermore, our analysis is highly versatile, applying to both deterministic and stochastic settings and encompassing algorithms based on gradient descent or momentum-based gradient descent. We also generalize EF-Landing to operate on block-wise Stiefel manifolds, enabling greater flexibility for structured constraints. Extensive numerical experiments validate our theoretical results.","Many machine learning tasks involve solving optimization problems on the Stiefel manifold, which can be regarded as an orthogonal constraint for matrices. Problems like principal component analysis need this constraint, and adding extra orthogonal constraint for training neural networks can also improve their robustness. Traditional Riemannian methods for solving these problems often require costly computations to retract the iteration back onto the manifold. The Landing algorithm avoids these expensive steps, making it efficient for large-scale problems.In this paper, we introduce *EF-Landing*, the first distributed optimization method on the Stiefel manifold that eliminates costly computations while also reducing communication between machines. With error correcting technique, EF-Landing maintains the same accuracy while significantly cutting down communication costs—a crucial advantage for large-scale distributed learning.Our convergence analysis shows that EF-Landing converges as accurately as existing methods, even with compressed communication, and it works well in both deterministic and stochastic settings. Additionally, we extend EF-Landing to handle block-wise constraints, making it more flexible for structured problems. Experiments confirm that our method is both practical and effective."
Poster,Distributional Diffusion Models with Scoring Rules,https://ICML.cc//virtual/2025/poster/45519,"Valentin De Bortoli, Alexandre Galashov, J Swaroop Guntupalli, Guangyao Zhou, Kevin Murphy, Arthur Gretton, Arnaud Doucet","Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to  data until fully corrupted. The corresponding reverse process progressively ``denoises"" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to speed up sample generation by learning the posterior distribution of  clean data samples  given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is  accomplished by replacing the standard regression loss used to estimate conditional means with a  scoring rule. We validate our method on  image and robot trajectory generation, where we  consistently outperform standard diffusion models at few discretization steps.",We propose a new loss for diffusion models based on scoring rules. This new loss allows us to learn a full posterior distribution of clean data given noisy data. Using this learned posterior distribution in DDIM sampler allows us to achieve better quality of samples in few steps regime.
Poster,Distributionally Robust Active Learning for Gaussian Process Regression,https://ICML.cc//virtual/2025/poster/43779,"Shion Takeno, Yoshito Okura, Yu Inatsu, Tatsuya Aoyama, Tomonari Tanaka, Satoshi Akahane, Hiroyuki Hanada, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, Ichiro Takeuchi","Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction. Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem. However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution. Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult. Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates. We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.","Collecting data often requires a significant amount of time and financial resources. In such cases, it is important to collect data so that the machine learning model has good predictive performance with as little data as possible. Active learning is a methodology for collecting data for such a purpose. If the target to be predicted is known, it is effective to collect data to reduce the prediction error for that target. However, in general, it is unclear how the target to be predicted is specified. For example, multiple users may apply a predictive model to different targets.In this study, we propose active learning methods that aim to improve prediction performance for all given candidates of target probability distributions that generate the target to be predicted. Furthermore, we demonstrate the theoretical validity of the proposed methods for a widely used machine learning model called Gaussian process regression. Therefore, our proposed methods enable the construction of a general-purpose and accurate machine learning model for various target probability distributions with reduced data collection costs."
