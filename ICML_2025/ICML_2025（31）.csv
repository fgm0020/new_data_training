type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders,https://ICML.cc//virtual/2025/poster/45658,"Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher Manning, Christopher Potts","Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering,  we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.","Imagine trying to teach a voice-assistant to avoid spoilers, speak politely, or explain ideas at a child-friendly level every single time it answers. Researchers use two main tricks to guide these systems today: (1) writing clever prompts and (2) re-training the model on lots of new examples. A third family of methods—tweaking what happens *inside* the model’s hidden layers—has attracted growing interest because it promises faster, more targeted control. Yet no one has had a single testbed for judging which approach actually works best.Our work introduces **AxBench**, the first large-scale benchmark designed to compare all three strategies on two everyday challenges:1. **Steering** – getting the model to talk *about* or *avoid* a chosen topic.2. **Concept detection** – quickly spotting whether a passage already contains that topic.Running AxBench on open-source Gemma models (2-billion and 9-billion parameters), we found:* Well-crafted prompts still give the most reliable steering, with full model retraining close behind.* For detecting concepts, simple statistical checks inside the model outperform everything else.* A popular interpretability tool, sparse autoencoders, surprisingly lags on both tasks.Finally, we present **ReFT-r1**, a lightweight way to nudge the model’s internal representations. It competes with the best methods on both steering and detection while remaining transparent about *why* it works. To help others build on this, we are releasing AxBench, our evaluation code, and ready-to-use feature dictionaries for the community."
Poster,Backdoor Attacks in Token Selection of Attention Mechanism,https://ICML.cc//virtual/2025/poster/44868,"Yunjuan Wang, Raman Arora","Despite the remarkable success of large foundation models across a range of tasks, they remain susceptible to security threats such as backdoor attacks. By injecting poisoned data containing specific triggers during training, adversaries can manipulate model predictions in a targeted manner. While prior work has focused on empirically designing and evaluating such attacks, a rigorous theoretical understanding of when and why they succeed is lacking. In this work, we analyze backdoor attacks that exploit the token selection process within attention mechanisms--a core component of transformer-based architectures. We show that single-head self-attention transformers trained via gradient descent can interpolate poisoned training data. Moreover, we prove that when the backdoor triggers are sufficiently strong but not overly dominant, attackers can successfully manipulate model predictions. Our analysis characterizes how adversaries manipulate token selection to alter outputs and identifies the theoretical conditions under which these attacks succeed. We validate our findings through experiments on synthetic datasets.","Large foundation models have achieved remarkable success, but they remain vulnerable to backdoor attacks. In these attacks, adversaries inject poisoned data during training to secretly manipulate the model’s behavior. The poisoned data contains a special pattern or trigger, like a specific word, image patch, or other signal. After training, the model performs as expected most of the time. However, when the trigger appears in new input data, the model behaves differently and produces incorrect predictions, just as the attacker intended. Understanding these vulnerabilities is crucial because it helps researchers and practitioners identify potential weaknesses in modern AI systems. While many studies have focused on designing novel backdoor attacks, there is limited understanding of why and when these attacks are effective. To address this gap, we examined how these attacks exploit the attention mechanism—a key part of transformer models that helps decide which words or data points are most important. We discovered that attackers can trick transformers into giving special attention to certain patterns, altering predictions when those patterns appear. Our research shows that even simple transformer models can learn to respond to these hidden triggers during training, especially if the trigger is strong enough to be noticed but not so obvious that it dominates the data."
Poster,BackSlash: Rate Constrained Optimized Training of Large Language Models,https://ICML.cc//virtual/2025/poster/45543,"Jun Wu, jiangtao wen, Yuxing Han","The rapid advancement of large-language models (LLMs) has driven extensive research into parameter compression after training has been completed, yet compression during the training phase remains largely unexplored. In this work, we introduce Rate-Constrained Training (BackSlash), a novel training-time compression approach based on rate-distortion optimization (RDO). BackSlash enables a flexible trade-off between model accuracy and complexity, significantly reducing parameter redundancy while preserving performance. Experiments in various architectures and tasks demonstrate that BackSlash can reduce memory usage by 60\% - 90\% without accuracy loss and provides significant compression gain compared to compression after training. Moreover, BackSlash proves to be highly versatile: it enhances generalization with small Lagrange multipliers, improves model robustness to pruning (maintaining accuracy even at 80\% pruning rates), and enables network simplification for accelerated inference on edge devices.","In this work, we introduces a new method called Rate-Constrained Training (BackSlash) that makes AI models smaller and more efficient while they are being trained, rather than after. Unlike traditional approaches that shrink models post-training, RCT optimizes the balance between model size and performance from the start. Tests show it can cut memory usage by 60–90% without losing accuracy, while also making models more adaptable, robust to pruning (e.g., removing 80% of unnecessary parts), and faster for edge devices. This could help deploy advanced AI on low-power gadgets more easily."
Poster,BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing,https://ICML.cc//virtual/2025/poster/45689,"Dongliang Guo, Mengxuan Hu, Zihan Guan, Thomas Hartvigsen, Sheng Li","Large multi-modal models inevitably decay over time as facts update and previously learned information becomes outdated. Traditional approaches such as fine-tuning are often impractical for updating these models due to their size and complexity. Instead, direct knowledge editing within the models presents a more viable solution. Current model editing techniques, however, typically overlook the unique influence ranges of different facts, leading to compromised model performance in terms of both generality and locality. To address this issue, we introduce the concept of the generality-locality trade-off in multi-modal model editing. We develop a new model editing dataset named OKEDIT, specifically designed to effectively evaluate this trade-off. Building on this foundation, we propose \textbf{BalancEdit}, a novel method for balanced model editing that dynamically achieves an optimal balance between generality and locality. BalancEdit utilizes a unique mechanism that generates both positive and negative samples for each fact to accurately determine its influence scope and incorporates these insights into the model's latent space using a discrete, localized codebook of edits, without modifying the underlying model weights. To our knowledge, this is the first approach explicitly addressing the generality-locality trade-off in multi-modal model editing. Our comprehensive results confirm the effectiveness of BalancEdit, demonstrating minimal trade-offs while maintaining robust editing capabilities. Our code and dataset are available at https://github.com/donglgcn/BalancEdit/tree/MMOKVQA.","Large language models and vision-language models often struggle to keep up with evolving facts over time. Traditional small-scale model updates, such as fine-tuning, usually require significant computational resources and data. In contrast, model editing offers a more efficient solution by locally modifying model weights to inject new knowledge.However, existing methods tend to overemphasize improving generality after editing, while overlooking the preservation of locality. This ""overgeneralization"" may cause collateral damage to other correct knowledge, ultimately degrading the model's overall performance.To address this, we propose BalancEdit, a model editing method that dynamically balances generality and locality. By constructing both supporting and counterfactual examples for the target fact, BalancEdit efficiently estimates the influence boundary of the edit, allowing the model to focus on the intended change without interfering with unrelated knowledge. Moreover, we introduce a codebook module to store and adjust the modified weights, enabling repeated edits with minimal cumulative impact.Experiments on both existing datasets and our newly constructed dataset demonstrate that BalancEdit effectively injects new knowledge while preserving the model’s original capabilities."
Poster,Balanced Learning for Domain Adaptive  Semantic Segmentation,https://ICML.cc//virtual/2025/poster/46247,"Wangkai Li, Rui Sun, Bohao Liao, Zhaoyang Li, Tianzhu Zhang","Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain.Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains.To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift.First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits.Subsequently, we introduce a post-hoc approach to align the  logits distributions across different classes using shared anchor distributions.To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function.Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains.Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods.","AI models for image segmentation often perform poorly when applied to new environments, especially for classes that are harder to recognize, like bicycles or traffic signs. This happens because models tend to favor ""easier"" or more frequent classes, leading to unbalanced learning. We propose BLDA, a new method that detects and corrects this imbalance by analyzing the model’s own prediction patterns—without needing prior knowledge about the new domain. BLDA adjusts the learning process on the fly to ensure fair treatment of all classes. This leads to more balanced and accurate predictions, especially in challenging real-world scenarios like autonomous driving."
Poster,Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality,https://ICML.cc//virtual/2025/poster/44678,"Joshua Southern, Yam Eitan, Guy Bar Shalom, Michael Bronstein, Haggai Maron, Fabrizio Frasca","Subgraph GNNs have emerged as promising architectures that overcome the expressiveness limitations of Graph Neural Networks (GNNs) by processing bags of subgraphs. Despite their compelling empirical performance, these methods are afflicted by a high computational complexity: they process bags whose size grows linearly in the number of nodes, hindering their applicability to larger graphs. In this work, we propose an effective and easy-to-implement approach to dramatically alleviate the computational cost of Subgraph GNNs and unleash broader applications thereof. Our method, dubbed HyMN, leverages walk-based centrality measures to sample a small number of relevant subgraphs and drastically reduce the bag size. By drawing a connection to perturbation analysis, we highlight the strength of the proposed centrality-based subgraph sampling, and further prove that these walk-based centralities can be additionally used as Structural Encodings for improved discriminative power. A comprehensive set of experimental results demonstrates that HyMN provides an effective synthesis of expressiveness, efficiency, and downstream performance, unlocking the application of Subgraph GNNs to dramatically larger graphs. Not only does our method outperform more sophisticated subgraph sampling approaches, it is also competitive, and sometimes better, than other state-of-the-art approaches for a fraction of their runtime.","Graph Neural Networks (GNNs) are powerful tools used to analyze data that’s structured like a network—think social networks, molecules, or transportation systems. However, standard GNNs often fall short when it comes to capturing the full complexity of these structures. A newer class of models, called Subgraph GNNs, improves this by analyzing smaller, overlapping parts of the graph. The problem? These models can be computationally expensive, especially as the graphs grow larger, because they analyze many subgraphs per node. This work introduces HyMN, a simple yet effective way to cut down the computational cost. Instead of blindly analyzing lots of subgraphs, HyMN uses measures based on how ""important"" or ""central"" nodes are—based on how walks through the graph behave—to focus only on the most relevant parts. These centrality-based measures also turn out to help the model better understand the structure of the graph. Experiments show that HyMN is both faster and often more accurate than more complex alternatives, making it possible to apply Subgraph GNNs to much larger datasets than before."
Poster,Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach,https://ICML.cc//virtual/2025/poster/43725,"Jin Zhu, Jingyi Li, Hongyi Zhou, Yinan Lin, Zhenhua Lin, Chengchun Shi","This paper focuses on the design of spatial experiments to optimize the amount of information derived from the experimental data and enhance the accuracy of the resulting causal effect estimator. We propose a surrogate function for the mean squared error (MSE) of the estimator, which facilitates the use of classical graph cut algorithms to learn the optimal design. Our proposal offers three key advances: (1) it accommodates moderate to large spatial interference effects; (2) it adapts to different spatial covariance functions; (3) it is computationally efficient.  Theoretical results and numerical experiments based on synthetic environments and a dispatch simulator that models a city-scale ridesharing market, further validate the effectiveness of our design. A python implementation of our method is available at https://github.com/Mamba413/CausalGraphCut.","To assess how a policy affects outcomes, we need to run randomized controlled trials or online experiments. However, when experiments take place across different locations, changes made in one area can influence nearby areas—a phenomenon called spatial interference. At the same time, observed outcomes may be spatially correlated (e.g., like weather patterns). Both interference and spatial correlation can challenge the estimation of the policy’s impact.Our work introduces the causal graph cut algorithm, a tool designed for experiments in scenarios where locations interfere with each other and are spatially connected. It handles both interference effects and adapts to different patterns of spatial connections. Additionally, the algorithm is efficient for analyzing large numbers of spatial regions, making it suitable for large-scale experiments across many locations. Using a city-level model—built with real data from a ridesharing company to realistically mimic driver and passenger behavior—we demonstrate that our method outperforms existing approaches."
Poster,Balancing Model Efficiency and Performance: Adaptive Pruner for Long-tailed Data,https://ICML.cc//virtual/2025/poster/46627,"Zhe Zhao, HaiBin Wen, Pengkun Wang, ShuangWang, Zhenkun Wang, Qingfu Zhang, Yang Wang","Long-tailed distribution datasets are prevalent in many machine learning tasks, yet existing neural network models still face significant challenges when handling such data. This paper proposes a novel adaptive pruning strategy, LTAP (Long-Tailed Adaptive Pruner), aimed at balancing model efficiency and performance to better address the challenges posed by long-tailed data distributions. LTAP introduces multi-dimensional importance scoring criteria and designs a dynamic weight adjustment mechanism to adaptively determine the pruning priority of parameters for different classes. By focusing on protecting parameters critical for tail classes, LTAP significantly enhances computational efficiency while maintaining model performance. This method combines the strengths of long-tailed learning and neural network pruning, overcoming the limitations of existing approaches in handling imbalanced data. Extensive experiments demonstrate that LTAP outperforms existing methods on various long-tailed datasets, achieving a good balance between model compression rate, computational efficiency, and classification accuracy. This research provides new insights into solving model optimization problems in long-tailed learning and is significant for improving the performance of neural networks on imbalanced datasets. The code is available at https://github.com/DataLab-atom/LT-VOTE.","In the real world, AI models struggle with imbalance; they learn to see common things like cats and dogs but overlook rare ones like endangered species. This problem is often worsened when we try to make models more efficient through ""pruning,"" a process that can accidentally erase the very knowledge needed to identify these rare cases.We developed an intelligent pruning strategy called LTAP. It acts with surgical precision on the model’s knowledge, first identifying which parts are essential for recognizing rare classes and then carefully protecting them. This ensures that only true redundancy is trimmed away.The result is an AI that is not only much smaller and faster but also better at its job, especially on rare categories. This breakthrough allows us to create compact, fair, and dependable AI for devices with limited power, enabling them to handle critical tasks like spotting unusual hazards for self-driving cars or finding minute flaws in manufacturing."
Poster,Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing,https://ICML.cc//virtual/2025/poster/45478,"Zhuoying Li, Zhu Xu, Yuxin Peng, Yang Liu","Instruction-based image editing, which aims to modify the image faithfully towards instruction while preserving irrelevant content unchanged, has made advanced progresses. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high costs concerning human evaluation, which hinders large-scale evaluation, or adapt from other tasks and lose specified concerns, failing to comprehensively evaluate the modification of instruction and the preservation of irrelevant regions, resulting in biased evaluation. To tackle it, we introduce a new metric Balancing Preservation Modification (BPM), that tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve the editing quality, manifesting its wild application. We verify the effectiveness of BPM metric on comprehensive instruction-editing data, and the re- sults show that we yield the highest alignment with human evaluation compared to existing metrics, indicating efficacy. The code is available at https://joyli-x.github.io/BPM/.","Automatic AI image editing based on text instructions—like “make the sky look cloudy”—has seen big improvements. But judging how well these edits are done is still a challenge. Current evaluation methods are either expensive, relying on human reviewers, or not tailored to this specific task, failing to comprehensively evaluate editing quality with full utilization of crucial information from original image, edited image and intruction. To address this, we introduce a new evaluation method. It breaks the image into parts that should be changed based on the instruction and parts that shouldn’t. Then, it checks whether the edits were done in the right place and whether the final result matches the instruction, while ensuring the untouched parts remain consistent. Our method Balancing Preservation and Modification (BPM) gives a more accurate and interpretable way to assess edits—and even helps improve the editing process itself. Tests show BPM matches human judgment better than existing tools, making it a valuable resource for building and evaluating smarter image editing systems."
Poster,Balancing the Scales: A Theoretical and Algorithmic Framework for Learning from Imbalanced Data,https://ICML.cc//virtual/2025/poster/44448,"Corinna Cortes, Anqi Mao, Mehryar Mohri, Yutao Zhong","Class imbalance remains a major challenge in machine learning, especially in multi-class problems with long-tailed distributions. Existing methods, such as data resampling, cost-sensitive techniques, and logistic loss modifications, though popular and often effective, lack solid theoretical foundations. As an example, we demonstrate that cost-sensitive methods  are not Bayes-consistent. This paper introduces a novel theoretical framework for analyzing generalization in imbalanced classification. We propose a new class-imbalanced margin loss function for both binary and multi-class settings, prove its strong $H$-consistency, and derive corresponding learning guarantees based on empirical loss and a new notion of class-sensitive Rademacher complexity. Leveraging these theoretical results, we devise novel and general learning algorithms, IMMAX (*Imbalanced Margin Maximization*), which incorporate confidence margins and are applicable to various hypothesis sets. While our focus is theoretical, we also present extensive empirical results demonstrating the effectiveness of our algorithms compared to existing baselines.","Imagine you're training a learning algorithm to identify different types of animals in photos, but your dataset has 1,000 pictures of cats for every one picture of a rare leopard. A learning algorithm trained on this data will become an expert at spotting cats, but it will likely fail to recognize the leopard, simply because it's so rare. This ""class imbalance"" problem is a major challenge in machine learning, appearing in fields from medical diagnosis (rare diseases) to fraud detection (rare fraudulent activities). When the stakes are high, failing to identify the rare case can have serious consequences.Many current techniques try to solve this by either duplicating the rare data or telling the learning algorithm to pay extra attention to it. While these methods can sometimes help, they are more like patches than real solutions. They lack strong theoretical foundations, meaning we don't fully understand why they work or when they might fail. In fact, we show that some of these popular methods can be fundamentally flawed and may not lead to the best possible predictions, even with infinite data.This research builds a new, solid foundation for training learning algorithms on imbalanced data. We went back to the drawing board and designed a new learning method from scratch, specifically for these situations. Our approach, called IMMAX (Imbalanced Margin Maximization), teaches the learning algorithm to be confident in its predictions for all classes, not just the common ones.Crucially, we have proven mathematically that our method is reliable and will guide the learning algorithm toward the best possible performance. While our work is primarily theoretical, we also conducted experiments showing that algorithms based on our framework outperform existing methods in practice. This provides a more principled and effective way to build machine learning systems that can handle the ""long tail"" of rare but important events that are common in the real world."
