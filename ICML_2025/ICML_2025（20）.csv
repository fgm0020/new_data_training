type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A New Approach to Backtracking Counterfactual Explanations: A Unified Causal Framework for Efficient Model Interpretability,https://ICML.cc//virtual/2025/poster/46336,"Pouria Fatemi, Ehsan Sharifian, Mohammad Hossein Yassaee","Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method called BRACE based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.","In many everyday decisions—like whether to grant a loan or recommend a medical test—AI models make accurate predictions but seldom explain why they reach a given outcome or what could be changed to alter it. To help people understand and trust these “black-box” systems, we present BRACE (Backtracking Recourse and Actionable Counterfactual Explanations), a fast, unified framework that uses simple cause-and-effect links among inputs (for example, how income and debt interact) and efficiently searches for the smallest, most realistic tweaks needed to flip a model’s prediction. In experiments on a standard loan-risk dataset, BRACE consistently generated actionable recommendations, such as modestly reducing the loan amount and the repayment period, which were more insightful than existing methods. By delivering these concrete “what-if” scenarios, BRACE empowers users to understand and influence AI decisions, enhancing trust and transparency in high-stakes settings."
Poster,A New Concentration Inequality for Sampling Without Replacement and Its Application for Transductive Learning,https://ICML.cc//virtual/2025/poster/45498,Yingzhen Yang,"We introduce a new tool, Transductive Local Complexity (TLC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable and novel changes compared to the analysis of typical LRC methods in the inductive setting. While LRC has been widely used as a powerful tool in the analysis of inductive models  with sharp generalization bounds for classification and minimax rates for nonparametric regression, it remains an open problem whether a localized version of Rademacher complexity based tool can be designed and applied to transductive learning and gain sharp bound for transductive learning which is consistent with the inductive excess risk bound by (LRC). We give a confirmative answer to this open problem by TLC. Similar to the development of LRC, we build TLC by first establishing a novel and sharp concentration inequality for supremum of empirical processes for the gap between test and training loss in the setting of sampling uniformly without replacement. Then a peeling strategy and a new surrogate variance operator are used to derive the following excess risk bound in the transductive setting, which is consistent with that of the classical LRC based excess risk bound in the inductive setting.As an application of TLC, we use the new TLC tool to analyze the Transductive Kernel Learning (TKL) model, and derive sharper excess risk bound than that by the current state-of-the-art. As a result of independent interest, the concentration inequality for the test-train process is used to derive a sharp concentration inequality for the general supremum of empirical process involving random variables in the setting of sampling uniformly without replacement, with comparison to current concentration inequalities.","When we train machine learning models, we usually assume we have one dataset to train on and a separate, unseen dataset to test on. But in many real-world scenarios, we already know the test data, we just don’t have their labels yet. This setting is called transductive learning.Our work introduces a new mathematical tool called Transductive Local Complexity (TLC) to better understand and improve how models generalize in this setting. This builds on a successful idea from traditional learning theory called Local Rademacher Complexity, but adapting it to transductive learning posed major challenges that hadn’t been solved — until now.We not only answer this open question, but also apply TLC to analyze a popular algorithm called Transductive Kernel Learning, showing it can perform better than previous methods. Along the way, we also developed sharper mathematical techniques that could benefit other areas of machine learning and statistics."
Poster,An Expressive and Self-Adaptive Dynamical System for Efficient Function Learning,https://ICML.cc//virtual/2025/poster/45640,"Chuan Liu, Chunshu Wu, Ruibing Song, Ang Li, Ying Nian Wu, Tong Geng","Function learning forms the foundation of numerous scientific and engineering tasks. While modern machine learning (ML) methods model complex functions effectively, their escalating complexity and computational demands pose challenges to efficient deployment. In contrast, natural dynamical systems exhibit remarkable computational efficiency in representing and solving complex functions. However, existing dynamical system approaches are limited by low expressivity and inefficient training. To this end, we propose EADS, an Expressive and self-Adaptive Dynamical System capable of accurately learning a wide spectrum of functions with extraordinary efficiency. Specifically, (1) drawing inspiration from biological dynamical systems, we integrate hierarchical architectures and heterogeneous dynamics into EADS, significantly enhancing its capacity to represent complex functions. (2) We propose an efficient on-device training method that leverages intrinsic electrical signals to update parameters, making EADS self-adaptive at negligible cost. Experimental results across diverse domains demonstrate that EADS achieves higher accuracy than existing works, while offering orders-of-magnitude speedups and energy efficiency over traditional neural network solutions on GPUs for both inference and training, showcasing its broader impact in overcoming computational bottlenecks across various fields.","Many scientific and engineering tasks require learning functions that map inputs (like measurements or signals) to outputs (such as predictions or control actions). Modern machine-learning models can approximate these functions effectively, but they’re becoming ever more complex, slow, and energy-hungry.We introduce EADS, a novel machine-learning paradigm that builds upon an extremely computationally efficient electronic dynamical system. EADA has layered processing stages and varied dynamics, enabling it to represent a wide variety of functions. It employs an on-device training mechanism that leverages the system’s own electrical signals to update its parameters efficiently.We evaluate EADS across diverse function-learning tasks. EADS not only delivers superior accuracy but also achieves orders-of-magnitude faster runtime and dramatically lower energy consumption compared to conventional methods running on GPUs."
Poster,Angle Domain Guidance: Latent Diffusion Requires Rotation Rather Than Extrapolation,https://ICML.cc//virtual/2025/poster/45992,"Cheng Jin, Zhenyu Xiao, Chutao Liu, Yuantao Gu","Classifier-free guidance (CFG) has emerged as a pivotal advancement in text-to-image latent diffusion models, establishing itself as a cornerstone technique for achieving high-quality image synthesis. However, under high guidance weights, where text-image alignment is significantly enhanced, CFG also leads to pronounced color distortions in the generated images. We identify that these distortions stem from the amplification of sample norms in the latent space. We present a theoretical framework that elucidates the mechanisms of norm amplification and anomalous diffusion phenomena induced by classifier-free guidance. Leveraging our theoretical insights and the latent space structure, we propose an Angle Domain Guidance (ADG) algorithm. ADG constrains magnitude variations while optimizing angular alignment, thereby mitigating color distortions while preserving the enhanced text-image alignment achieved at higher guidance weights. Experimental results demonstrate that ADG significantly outperforms existing methods, generating images that not only maintain superior text alignment but also exhibit improved color fidelity and better alignment with human perceptual preferences.","Diffusion model is a powerful AI technique for generating images from text descriptions. A key component in this process is Classifier-Free Guidance (CFG), which helps the model better align images with the input text. However, when the guidance weight is set high, the generated images often break down—appearing overly saturated or distorted. We analyzed why this happens and found that CFG tends to push image features too strongly in a linear fashion. To address this, we propose a new method called Angle-Domain Guidance (ADG), which provides more stable control by adjusting the direction of updates rather than their intensity. This leads to more reliable and visually consistent images, improving the usability of AI-generated content in real-world applications."
Poster,An Improved Clique-Picking Algorithm for Counting Markov Equivalent DAGs via Super Cliques Transfer,https://ICML.cc//virtual/2025/poster/44137,"Lifu Liu, Shiyuan He, Jianhua Guo","Efficiently counting Markov equivalent directed acyclic graphs (DAGs) is crucial in graphical causal analysis. Wienöbst et al. (2023) introduced a polynomial-time algorithm, known as the Clique-Picking algorithm, to count the number of Markov equivalent DAGs for a given completed partially directed acyclic graph (CPDAG). This algorithm iteratively selects a root clique, determines fixed orientations with outgoing edges from the clique, and generates the unresolved undirected connected components (UCCGs). In this work, we propose a more efficient approach to UCCG generation by utilizing previously computed results for different root cliques. Our method introduces the concept of super cliques within rooted clique trees, enabling their efficient transfer between trees with different root cliques. The proposed algorithm effectively reduces the computational complexity of the Clique-Picking method, particularly when the number of cliques is substantially smaller than the number of vertices and edges.","In the study of causal inference, Directed Acyclic Graphs (DAGs) are fundamental tools for representing cause-and-effect relationships among variables. However, observational data alone often cannot identify a unique DAG. Instead, we can only determine a **Markov equivalence class (MEC)**—a set of DAGs that encode the same conditional independence relations. Understanding the size of a MEC, i.e., the number of DAGs it contains, is crucial for evaluating causal uncertainty and designing informative interventions. Yet, counting the number of DAGs in a MEC is computationally challenging, particularly as the size and complexity of the graph grow.To address this, the **Clique-Picking algorithm** (Wienöbst et al., 2023) was recently proposed as an efficient polynomial-time method for MEC size computation. This method explores different cliques as roots in a clique tree and recursively computes the connected components of oriented subgraphs. While effective, this approach still incurs high computational cost due to repeated processing of similar graph structures when different root cliques are selected.This paper presents an improved solution by introducing the concept of **super cliques** and developing a new **Super Cliques Transfer Algorithm**. Super cliques capture groups of related cliques in the clique tree and allow the reuse of previously computed results. The algorithm exploits overlapping structures among different root selections, enabling efficient transfer of information between clique trees rooted at different cliques.As a result, the proposed method significantly reduces the computational complexity of a key step in the original Clique-Picking algorithm—from $\mathcal{O}(m(|V|+|E|))$ to $\mathcal{O}(m^2)$, where $m$ is the number of cliques. The paper further provides both intuitive (tree-based) and theoretical (sequence-based) formulations of super cliques, which support the algorithm's correctness and generalizability.Extensive experiments demonstrate substantial speedup of the improved method (referred to as ICP) over the original algorithm, especially for large or dense graphs. This advancement makes MEC size computation more practical for high-dimensional causal analysis tasks and opens up broader applications in fields such as epidemiology, policy modeling, and automated reasoning."
Poster,An in depth look at the Procrustes-Wasserstein distance: properties and barycenters,https://ICML.cc//virtual/2025/poster/44717,"Davide Adamo, Marco Corneli, Manon Vuillien, Emmanuelle Vila","Due to its invariance to rigid transformations such as rotations and reflections, Procrustes-Wasserstein (PW) was introduced in the literature as an optimal transport (OT) distance, alternative to Wasserstein and more suited to tasks such as the alignment and comparison of point clouds. Having that application in mind, we carefully build a space of discrete probability measures and show that over that space PW actually *is* a distance. Algorithms to solve the PW problems already exist, however we extend the PW framework by discussing and testing several initialization strategies. We then introduce the notion of PW barycenter and detail an algorithm to estimate it from the data. The result is a new method to compute representative shapes from a collection of point clouds. We benchmark our method against existing OT approaches, demonstrating superior performance in scenarios requiring precise alignment and shape preservation. We finally show the usefulness of the PW barycenters in an archaeological context. Our results highlight the potential of PW in advancing 2D and 3D point cloud analysis for machine learning and computational geometry applications.","Can we compute a “mean shape” from a set of objects, such as handwritten digits or 3D scans of bones, in such a way to preserve their finest geometric details, while ignoring differences solely due to rotations, reflections or rigid transformations in general? And can such a “mean” be used for wider purposes such as tracking a shape's evolution in time?Traditional machine learning methods often struggle with shapes appearing in different poses or orientations, such as rotated or mirrored. To address this problem, we formalize the Procrustes-Wasserstein (PW) distance as an optimal transport metric for measuring the similarity between discrete distributions, such as shapes represented as point clouds. Unlike previous approaches, PW is both mathematically rigorous (we prove that it is a true distance) and fits naturally to recover rigid transformations in the Euclidean space. We also introduce PW barycenters as a novel technique for computing interpolations from a set of point clouds in an isometric invariant way. Our method provides faithful and interpretable representations and highlights better geometric details than existing optimal transport tools. We demonstrate its effectiveness in real-world tasks, including shape clustering and tracking evolutionary changes in ancient animal bones. By making point cloud comparison more accurate and robust, our research has the potential to open new perspectives in fundamental research in zoo-archaeology and, more generally, in the the analysis of complex 2D and 3D data."
Poster,An Instrumental Value for Data Production and its Application to Data Pricing,https://ICML.cc//virtual/2025/poster/44174,"Rui Ai, Boxiang Lyu, Zhaoran Wang, Zhuoran Yang, Haifeng Xu","We develop a framework for capturing the instrumentalvalue of data production processes, whichaccounts for two key factors: (a) the context ofthe agent’s decision-making; (b) how much dataor information the buyer already possesses. We""micro-found"" our data valuation function by establishingits connection to classic notions of signalsand information design in economics. Wheninstantiated in Bayesian linear regression, ourvalue naturally corresponds to information gain.Applying our proposed data value in Bayesian linearregression for monopoly pricing, we show thatif the seller can fully customize data production,she can extract the first-best revenue (i.e., full surplus)from any population of buyers, i.e., achievingfirst-degree price discrimination. If data canonly be constructed from an existing data pool,this limits the seller’s ability to customize, andachieving first-best revenue becomes generallyimpossible. However, we design a mechanismthat achieves seller revenue at most $\log(\kappa)$ lessthan the first-best, where $\kappa$ is the condition numberassociated with the data matrix. As a corollary,the seller extracts the first-best revenue in themulti-armed bandits special case.","How do we determine the value of data to an agent? It depends on the problem the agent is facing and the amount of information they already possess. From the perspective of rational agent decision-making, we propose an instrumental value framework that characterizes valid data valuation. Notably, we show that in the case of Bayesian linear regression, this value coincides with information gain. We then apply our instrumental value framework to a monopoly data pricing setting. We find that when the seller can perfectly customize data production, the buyer's surplus is zero, leading to severe market asymmetry and unfairness. In contrast, under limited customization, we derive an upper bound on the buyer's surplus. This prompts broader reflections on how to price such novel products in the data era and the resulting concerns about market fairness."
Poster,An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks,https://ICML.cc//virtual/2025/poster/44672,"Valentyn Boreiko, Alexander Panfilov, Václav Voráček, Matthias Hein, Jonas Geiping","A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. These methods largely succeed in coercing the target output in their original settings, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods.Our threat model checks if a given jailbreak is likely to occur in the distribution of text. For this, we build an N-gram language model on 1T tokens, which, unlike model-based perplexity, allows for an LLM-agnostic, nonparametric, and inherently interpretable evaluation. We adapt popular attacks to this threat model, and, for the first time, benchmark these attacks on equal footing with it. After an extensive comparison, we find attack success rates against safety-tuned modern models to be lower than previously presented and that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent bigrams, either selecting the ones absent from real-world text or rare ones, e.g., specific to Reddit or code datasets.","Jailbreaking attacks are techniques that trick a language model into ignoring safety rules and answering disallowed questions such as ""How do I make a bomb?"". However, such attacks are not easily comparable, slowing the development of defenses against them. We propose tools to compare different attack types and allow for additional insights, making attacks and models more interpretable. We observe that effective attacks exploit infrequent words, either picking those absent from ordinary text or choosing rare, domain-specific ones (e.g., from Reddit or code)."
Poster,Annealing Flow Generative Models Towards Sampling High-Dimensional and Multi-Modal Distributions,https://ICML.cc//virtual/2025/poster/44357,"Dongze Wu, Yao Xie","Sampling from high-dimensional, multi-modal distributions remains a fundamental challenge across domains such as statistical Bayesian inference and physics-based machine learning. In this paper, we propose Annealing Flow (AF), a method built on Continuous Normalizing Flows (CNFs) for sampling from high-dimensional and multi-modal distributions. AF is trained with a dynamic Optimal Transport (OT) objective incorporating Wasserstein regularization, and guided by annealing procedures, facilitating effective exploration of modes in high-dimensional spaces. Compared to recent NF methods, AF significantly improves training efficiency and stability, with minimal reliance on MC assistance. We demonstrate the superior performance of AF compared to state-of-the-art methods through extensive experiments on various challenging distributions and real-world datasets, particularly in high-dimensional and multi-modal settings. We also highlight AF’s potential for sampling the least favorable distributions.","Sampling is a key tool in science and engineering—it helps us understand complex systems by generating representative samples from statistical distributions. However, when the distributions are complex and high-dimensional, such as in quantum systems, or Bayesian statistics, standard methods often struggle. Our paper introduces Annealing Flow, a new machine learning method designed to efficiently sample from these complex distributions, especially when they have many distinct patterns (called ""modes"") or live in high-dimensional spaces.The method works by gradually transforming samples from a simple distribution into the complex ones of interest, similar to how metals are cooled slowly to reach a stable state—an idea known as annealing. This transformation is learned using neural networks and a mathematical framework called optimal transport, ensuring stability and efficiency.Annealing Flow outperforms existing methods across a wide range of tests, offering faster, more reliable sampling. It also enables better handling of rare but important scenarios, improving both scientific insight and practical applications."
Poster,A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations,https://ICML.cc//virtual/2025/poster/44968,"Junwei Su, Chuan Wu","This paper investigates the convergence behavior of score-based graph generative models (SGGMs). Unlike common score-based generative models (SGMs) that are governed by a single stochastic differential equation (SDE), SGGMs utilize a system of dependent SDEs, where the graph structure and node features are modeled separately, while accounting for their inherent dependencies. This distinction makes existing convergence analyses from SGMs inapplicable for SGGMs. In this work, we present the first convergence analysis for SGGMs, focusing on the convergence bound (the risk of generative error) across three key graph generation paradigms: (1) feature generation with a fixed graph structure, (2) graph structure generation with fixed node features, and (3) joint generation of both graph structure and node features. Our analysis reveals several unique factors specific to SGGMs (e.g., the topological properties of the graph structure) which significantly affect the convergence bound. Additionally, we offer theoretical insights into the selection of hyperparameters (e.g., sampling steps and diffusion length) and advocate for techniques like normalization to improve convergence. To validate our theoretical findings, we conduct a controlled empirical study using a synthetic graph model. The results in this paper contribute to a deeper theoretical understanding of SGGMs and offer practical guidance for designing more efficient and effective SGGMs.","This paper examines how accurately score-based graph generative models (SGGMs) can create realistic graph data, such as social networks or molecular structures. Unlike traditional methods, SGGMs handle graph connections and node features separately while still accounting for their interactions, making existing analysis methods unsuitable. The authors provide the first theoretical analysis of these models, evaluating their accuracy in different graph-generation tasks: creating node features for existing graph structures, generating graph structures based on given features, and simultaneously generating both. The study highlights critical factors—such as network shape and connectivity—that influence model performance. Additionally, the paper offers practical recommendations for selecting model parameters, helping improve the effectiveness of these generative models."
