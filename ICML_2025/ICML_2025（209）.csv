type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,On the Power of Context-Enhanced Learning in LLMs,https://ICML.cc//virtual/2025/poster/45811,"Xingyu Zhu, Abhishek Panigrahi, Sanjeev Arora","We formalize a new concept for LLMs, **context-enhanced learning**. It involves standard gradient-based learning on text except that the context is enhanced with additional data on which no auto-regressive gradients are computed. This setting is a gradient-based analog of usual in-context learning (ICL) and appears in some recent works.Using a multi-step reasoning task, we prove in a simplified setting that context-enhanced learning can be **exponentially more sample-efficient** than standard learning when the model is capable of ICL. At a mechanistic level, we find that the benefit of context-enhancement arises from a more accurate gradient learning signal.We also experimentally demonstrate that it appears hard to detect or recover learning materials that were used in the context during training. This may have implications for data security as well as copyright.","Kids use open textbooks for homework. Can LLM training benefit from ""helpful textbooks"" in context with no gradients computed on these tokens? The answer is yes, and we call it Context-Enhanced Learning (CEL) – it can exponentially accelerate training while avoiding verbatim memorization of “textbooks”.To systematically study context enhanced learning, we design a multi-step translation (MLT) task as a synthetic testbed, where a token sequence undergoes 𝑑 translation steps via 𝑑 phrasebooks + circular shifts. Without having the phrasebooks in context, vanilla SFT from input-output pairs alone provably requires Ω(exp(𝑑)) samples. However, with phrasebooks provided in context and slowly removed with a simple dropout curriculum, a 3B Llama model that is capable of in-context learning can learn the task with 20x less data.Based on the understanding of the CEL-trained Llama model, we construct a mechanistically similar surrogate model to theoretically analyze CEL. We show that if the model has certain ICL capability, CEL can reduce the sample complexity to just O(𝑑 log 𝑑) compared to Ω(exp(𝑑))!We conduct further mechanistic experiments on optimization gradients and show that additional context enhancement provides more accurate learning signal in gradients. We also note that while having phrasebooks significantly accelerated learning of the task, since no autoregressive gradients were computed on them, they are not verbatim memorized (we’re unable to recover them by querying the model). This may have implications for data security as well as copyright."
Poster,On the Power of Learning-Augmented Search Trees,https://ICML.cc//virtual/2025/poster/43806,"Jingbang Chen, Xinyuan Cao, Alicia Stepin, Li Chen","We study learning-augmented binary search trees (BSTs) via Treaps with carefully designed priorities.The result is a simple search tree in which the depth of each item $x$ is determined by its predicted weight $w_x$.Specifically, each item $x$ is assigned a composite priority of $-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform random variable. By choosing $w_x$ as the relative frequency of $x$, the resulting search trees achieve static optimality.This approach generalizes the recent learning-augmented BSTs [Lin-Luo-Woodruff ICML`22], which only work for Zipfian distributions, by extending them to arbitrary input distributions.Furthermore, we demonstrate that our method can be generalized to a B-Tree data structure using the B-Treap approach [Golovin ICALP'09]. Our search trees are also capable of leveraging localities in the access sequence through online self-reorganization, thereby achieving the working-set property. Additionally, they are robust to prediction errors and support dynamic operations, such as insertions, deletions, and prediction updates. We complement our analysis with an empirical study, demonstrating that our method outperforms prior work and classic data structures.","Search trees are a fundamental tool in how computers store data. In real world applications--like databases or recommendation systems--we often don’t know in advance which items will be accessed most or what the access patterns will look like.Our work presents a new search tree, based on a classical data structure called a ""Treap"", which uses machine learning predictions to optimize data storage in the search tree. By predicting which items will be accessed more often, the tree allows quicker overall accesses to these items by moving these frequently-referenced elements to ""earlier"" parts of the data structure.We show that our method achieves \textbf{static optimality}, meaning it performs as well as the best possible tree tailored to the true access frequencies--assuming we had advanced knowledge of this true distribution.This paper generalizes previous work that relied on assumptions about data access patterns and we show how we can achieve similar speedups for a wider pattern of access frequencies, and we maintain strong guarantees even with noisy predictions. We also extend our method to disk-based systems using B-Trees.Finally, we back our theoretical guarantees with experimental results, demonstrating that our learning-augmented search trees consistently outperform traditional data structures in practice across a wide range of patterns."
Poster,On the Private Estimation of Smooth Transport Maps,https://ICML.cc//virtual/2025/poster/44057,"Clément Lalanne, Franck Iutzeler, Loubes Jean-Michel, Julien Chhor","Estimating optimal transport maps between two distributions from respective samples is an important element for many machine learning methods. To do so, rather than extending discrete transport maps, it has been shown that estimating the Brenier potential of the transport problem and obtaining a transport map through its gradient is near minimax optimal for smooth problems. In this paper, we investigate the private estimation of such potentials and transport maps with respect to the distribution samples.We propose a differentially private transport map estimator with $L^2$ error at most $n^{-1} \vee n^{-\frac{2 \alpha}{2 \alpha - 2 + d}} \vee (n\epsilon)^{-\frac{2 \alpha}{2 \alpha + d}} $ up do polylog terms where $n$ is the sample size, $\epsilon$ is the desired level of privacy, $\alpha$ is the smoothness of the true transport map, and $d$ is the dimension of the feature space. We also provide a lower bound for the problem.","Imagine you need to rearrange piles of sand from one shape to another using the least effort possible—this is the essence of optimal transport, a powerful concept used in fields like machine learning, economics, and data analysis. When we only have a handful of sand grains (samples) to work with, and those grains represent sensitive information—like personal data—we must protect privacy while still finding an efficient way to move the sand. This paper introduces a new method to tackle this challenge, ensuring both accuracy in rearranging the sand and strong privacy safeguards for the data, paving the way for safer and smarter data-driven solutions."
Poster,On the Provable Separation of Scales in Maximal Update Parameterization,https://ICML.cc//virtual/2025/poster/44662,"Letong Hong, Zhangyang “Atlas” Wang","Maximal Update Parameterization ($\mu$P) has shown significant promise in allowing zero-shot hyperparameter transfer across neural network scales, reducing the prohibitive cost of hyperparameter tuning for large models. However, the theoretical foundation behind the observed approximate transferability of hyperparameters remains underexplored. Relying on a width-dominance regime, which ensures that as width grows, certain terms of the learning dynamics dominate, we establish the first fundamental separation of scales in $\mu$P between macro-variables (e.g. loss landscapes) and micro-variables (e.g. individual weights). Our formulation explains why hyperparameter tuning can be effectively performed in early training stages, i.e., \textit{early statistics effectively approximate global hyperparameter optima}, implying the potential to further reduce the training costs required for searching optimal hyperparameters. We further apply our main theory to explain an empirical deep learning phenomenon discovered independently by prior work.","We prove that very wide neural networks learn on two distinct time-scales: overall performance indicators stabilise almost immediately, while individual weights change much more slowly. Because the “big-picture” settles so fast, hyper-parameters chosen on a small or early-stage model remain valid when the model is scaled up, explaining the success of μP’s zero-shot transfer. This two-speed view also predicts the real-world lag you see between changing the learning-rate and the loss curve’s response."
Poster,On the Query Complexity of Verifier-Assisted Language Generation,https://ICML.cc//virtual/2025/poster/46206,"Edoardo Botta, Yuchen Li, Aashay Mehta, Jordan Ash, Cyril Zhang, Andrej Risteski","Recently, a plethora of works have proposedinference-time algorithms (e.g. best-of-n), whichincorporate verifiers to assist the generation process. Their quality-efficiency trade-offs have beenempirically benchmarked on a variety of constrained generation tasks, but the algorithmic design landscape is still largely poorly understood.In this paper, we develop a mathematical framework for reasoning about constrained generationusing a pre-trained language model generator oracle and a process verifier—which can decidewhether a prefix can be extended to a string whichsatisfies the constraints of choice. We show thateven in very simple settings, access to a verifiercan render an intractable problem (information-theoretically or computationally) to a tractableone. In fact, we show even simple algorithms,like tokenwise rejection sampling, can enjoy significant benefits from access to a verifier. Empirically, we show that a natural modification oftokenwise rejection sampling, in which the sampler is allowed to ""backtrack"" (i.e., erase the finalfew generated tokens) has robust and substantivebenefits over natural baselines (e.g. (blockwise)rejection sampling, nucleus sampling)—both interms of computational efficiency, accuracy anddiversity.","In this paper, we develop a mathematical framework for reasoning about constrained generation using a pre-trained language model generator oracle and a process verifier--which can decide whether a prefix can be extended to a string which satisfies the constraints of choice. We show that even in very simple settings, access to a verifier can render an intractable problem (information-theoretically or computationally) to a tractable one. In fact, we show even simple algorithms, like tokenwise rejection sampling, can enjoy significant benefits from access to a verifier. Empirically, we show that a natural modification of tokenwise rejection sampling, in which the sampler is allowed to ""backtrack"" (i.e., erase the final few generated tokens) has robust and substantive benefits over natural baselines (e.g. (blockwise) rejection sampling, nucleus sampling)--both in terms of computational efficiency, accuracy and diversity."
Poster,On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents,https://ICML.cc//virtual/2025/poster/44721,"Jen-Tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Michael Lyu, Maarten Sap","Large language model-based multi-agent systems have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, the impact of clumsy or even malicious agents—those who frequently make errors in their tasks—on the overall performance of the system remains underexplored. This paper investigates: (1) What is the resilience of various system structures (e.g., A$\rightarrow$B$\rightarrow$C, A$\leftrightarrow$B$\leftrightarrow$C) under faulty agents, on different downstream tasks? (2) How can we increase system resilience to defend against these agents? To simulate faulty agents, we propose two approaches—AutoTransform and AutoInject—which introduce mistakes into the agents' responses. Experiments on four downstream tasks using six systems show that the ""hierarchical"" structure, i.e., A$\rightarrow$(B$\leftrightarrow$C), exhibits superior resilience with the lowest performance drop of 5.5%, compared to 10.5% and 23.7% of other two structures. To further improve resilience, we introduce (1) Challenger, that introduces a mechanism for each agent to challenge others' outputs, and (2) Inspector, an additional agent to review and correct messages, recovering up to 96.4% errors made by faulty agents. Our code and data are available at https://github.com/CUHK-ARISE/MAS-Resilience.","Teams of AI “agents” built on large language models can solve coding, maths and translation tasks, but one careless—or malicious—agent can poison the discussion and drag down the whole team. We create two automated stress-tests: AutoTransform, which rewrites an agent’s role so it secretly adds mistakes, and AutoInject, which slips errors directly into its messages. Using them, we explore how different multi-agent structures (linear chains, flat peer groups and human-like hierarchies) and different tasks suffer from these faulty agents. A hierarchical structure—one “boss” overseeing peer agents—proved most robust, losing only ≈ 5% accuracy, while a simple chain collapsed by ≈ 24%. Adding two simple safeguards—a “Challenger” ability that lets agents question each other and an independent “Inspector” reviewer—recovered up to 96% of the lost performance. Our open-source toolkit lets researchers and companies quickly gauge and harden the resilience of their AI agents before deploying in the wild."
Poster,On the Robustness of Reward Models for Language Model Alignment,https://ICML.cc//virtual/2025/poster/45164,"Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, James Thorne","The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss as one-way classifiers are prone to over-optimization, losing generalizability to unseen inputs. In this paper, we study the cause of over-optimization and its downstream effects on the RLHF procedure, highlighting the importance of robustness in RMs. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Correspondingly, we propose batch-wise sum-to-zero regularization (BSR) that enforces reward sum for each batch to be zero-centered, constraining the rewards with abnormally large magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness on unseen inputs. Then, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5\% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0, with reducing generation length by 40\% while adding a 7\% increase in win rate, further highlights that robustness in RMs induces robustness in RLHF training.","Reward models (RMs) are crucial tools that help large language models (LLMs) align with human preferences by assigning scores to their outputs. Typically, these models learn by comparing pairs of responses, preferring one over another, to better match human choices. However, these models can overly focus on their training data, becoming less effective when encountering new or slightly different responses.This paper identifies a major cause of this problem: RMs trained in this way tend to produce uneven scores due to large variations in the internal features (hidden states) they use. When these variations become too large, models become overly confident in their scores, causing them to lose their ability to generalize. To address this, the authors propose a straightforward solution, batch-wise sum-to-zero regularization (BSR), that encourages the scores within each training batch to be balanced around zero. This prevents extreme scoring, stabilizing the model’s internal representations and significantly improving its ability to handle new data.The paper tests this solution rigorously across various scenarios, demonstrating that RMs trained with this regularization consistently outperform traditional approaches. Moreover, when these improved RMs are used for reinforcement learning in language models (i.e., RLHF), they lead to more aligned, less verbose, and more robust language outputs. Comprehensively, the paper provides a systematic analysis of the propagation of RM's over-optimization from reward modeling to RLHF training, highlighting the significance of improving the robustness of RMs in the reward modeling phase."
Poster,On the Role of Label Noise in the Feature Learning Process,https://ICML.cc//virtual/2025/poster/44229,"Andi Han, Wei Huang, Zhanpeng Zhou, Gang Niu, Wuyang Chen, Junchi Yan, Akiko Takeda, Taiji Suzuki","Deep learning with noisy labels presents significant challenges. In this work, we theoretically characterize the role of label noise from a feature learning perspective. Specifically, we consider a signal-noise data distribution, where each sample comprises a label-dependent signal and label-independent noise, and rigorously analyze the training dynamics of a two-layer convolutional neural network under this data setup, along with the presence of label noise. Our analysis identifies two key stages. In Stage I, the model perfectly fits all the clean samples (i.e., samples without label noise) while ignoring the noisy ones (i.e., samples with noisy labels). During this stage, the model learns the signal from the clean samples, which generalizes well on unseen data. In Stage II, as the training loss converges, the gradient in the direction of noise surpasses that of the signal, leading to overfitting on noisy samples. Eventually, the model memorizes the noise present in the noisy samples and degrades its generalization ability. Furthermore, our analysis provides a theoretical basis for two widely used techniques for tackling label noise: early stopping and sample selection. Experiments on both synthetic and real-world setups validate our theory.","Training deep learning models with incorrect (noisy) labels is a major challenge, often leading to poor performance. In this work, we take a closer look at how such models learn when some of the training labels are wrong. We study a simple yet representative neural network and show that the learning process happens in two distinct stages.In the first stage, the model learns useful patterns from correctly labeled data while largely ignoring the incorrect ones—this leads to good performance on new, unseen data. But if training continues too long, the model enters a second stage where it starts to learn from the wrong labels. This causes it to ""memorize the noise,"" ultimately reducing its ability to generalize.Our analysis also sheds light on why two popular strategies—early stopping (ending training before the second stage) and sample selection (focusing on more reliable data)—are effective ways to handle label noise. We support our findings with both simulated and real-world experiments."
Poster,On the Similarities of Embeddings in Contrastive Learning,https://ICML.cc//virtual/2025/poster/46045,"Chungpa Lee, Sehee Lim, Kibok Lee, Jy-yong Sohn","Contrastive learning (CL) operates on a simple yet effective principle: embeddings of positive pairs are pulled together, while those of negative pairs are pushed apart. Although various forms of contrastive loss have been proposed and analyzed from different perspectives, prior works lack a comprehensive framework that systematically explains a broad class of these objectives. In this paper, we present a unified framework for understanding CL, which is based on analyzing the cosine similarity between embeddings of positive and negative pairs. In full-batch settings, we show that perfect alignment of positive pairs is unattainable when similarities of negative pairs fall below a certain threshold, and that this misalignment can be alleviated by incorporating within-view negative pairs. In mini-batch settings, we demonstrate that smaller batch sizes incur stronger separation among negative pairs within batches, which leads to higher variance in similarities of negative pairs. To address this limitation of mini-batch CL, we introduce an auxiliary loss term that reduces the variance of similarities of negative pairs in CL. Empirical results demonstrate that incorporating the proposed loss consistently improves the performance of CL methods in small-batch training.","Contrastive learning (CL) operates on a simple yet effective principle: embeddings of positive pairs are pulled together, while those of negative pairs are pushed apart. Although various forms of contrastive loss have been proposed and analyzed from different perspectives, prior works lack a comprehensive framework that systematically explains a broad class of these objectives. In this paper, we present a unified framework for understanding CL, which is based on analyzing the cosine similarity between embeddings of positive and negative pairs. In full-batch settings, we show that perfect alignment of positive pairs is unattainable when similarities of negative pairs fall below a certain threshold, and that this misalignment can be alleviated by incorporating within-view negative pairs. In mini-batch settings, we demonstrate that smaller batch sizes incur stronger separation among negative pairs within batches, which leads to higher variance in similarities of negative pairs. To address this limitation of mini-batch CL, we introduce an auxiliary loss term that reduces the variance of similarities of negative pairs in CL. Empirical results demonstrate that incorporating the proposed loss consistently improves the performance of CL methods in small-batch training."
Poster,On the Statistical Mechanisms of Distributional Compositional Generalization,https://ICML.cc//virtual/2025/poster/46551,"Jingwen Fu, Nanning Zheng","Distributional Compositional Generalization (DCG) refers to the ability to tackle tasks from new distributions by leveraging the knowledge of concepts learned from supporting distributions. In this work, we aim to explore the statistical mechanisms of DCG, which have been largely overlooked in previous studies. By statistically formulating the problem, this paper seeks to address two key research questions: 1) Can a method to one DCG problem be applicable to another? 2) What statistical properties can indicate a learning algorithm's capacity for knowledge composition in DCG tasks? \textbf{To address the first question}, an invariant measure is proposed to provide a dimension where all different methods converge. This measure underscores the critical role of data in enabling improvements without trade-offs. \textbf{As for the second question}, we reveal that by decoupling the impacts of insufficient data and knowledge composition, the ability of the learning algorithm to compose knowledge relies on the compatibility and sensitivity between the learning algorithm and the composition rule. In summary, the statistical analysis of the generalization mechanisms provided in this paper deepens our understanding of compositional generalization, offering a complementary evidence on the importance of data in DCG task.","(1) This study investigates the statistical mechanisms of Distributional Compositional Generalization (DCG), an area that has received limited attention in prior research.(2) We focus on two central research questions: (a) Can a solution to one DCG task be effectively transferred to another? (b) What statistical characteristics reveal a learning algorithm’s ability to perform knowledge composition in DCG scenarios?(3) Our findings highlight the pivotal role of data in enabling performance gains without trade-offs. Furthermore, a learning algorithm's capacity for knowledge composition depends on the compatibility and sensitivity between the algorithm and the underlying composition rule."
