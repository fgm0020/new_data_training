type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Conformal Tail Risk Control for Large Language Model Alignment,https://ICML.cc//virtual/2025/poster/45795,"Catherine Chen, Jingyan Shen, Xinyu Yang, Lihua Lei","Recent developments in large language models (LLMs) have led to their widespread usage for various tasks. The prevalence of LLMs in society implores the assurance on the reliability of their performance. In particular, risk-sensitive applications demand meticulous attention to unexpectedly poor outcomes, i.e., tail events, for instance, toxic answers, humiliating language, and offensive outputs. Due to the costly nature of acquiring human annotations, general-purpose scoring models have been created to automate the process of quantifying these tail events. This phenomenon introduces potential human-machine misalignment between the respective scoring mechanisms. In this work, we present a lightweight calibration framework for blackbox models that ensures the alignment of humans and machines with provable guarantees. Our framework provides a rigorous approach to controlling any distortion risk measure that is characterized by a weighted average of quantiles of the loss incurred by the LLM with high confidence. The theoretical foundation of our method relies on the connection between conformal risk control and a traditional family of statistics, i.e., L-statistics. To demonstrate the utility of our framework, we conduct comprehensive experiments that address the issue of human-machine misalignment.","Recent developments in large language models (LLMs) have led to their widespread usage for various tasks. The prevalence of LLMs in society implores the assurance on the reliability of their performance. In particular, risk-sensitive applications demand meticulous attention to unexpectedly poor outcomes, i.e., tail events, for instance, toxic answers, humiliating language, and offensive outputs. Due to the costly nature of acquiring human annotations, machine scoring models have been created to automate the process of quantifying these tail events. This phenomenon introduces potential human-machine misalignment between the respective scoring mechanisms. In this work, we present a lightweight framework that ensures the alignment of humans and machines with provable guarantees. Our method provides a rigorous approach to controlling any tail risk incurred by the LLM with high confidence. The theoretical foundation of our method relies on the connection between conformal risk control and a traditional family of statistics. This approach not only ensures the alignment of human and machine scores, but also the safety of LLM outputs."
Poster,Conformity Score Averaging for Classification,https://ICML.cc//virtual/2025/poster/45362,"Rui Luo, Zhixin Zhou","Conformal prediction provides a robust framework for generating prediction sets with finite-sample coverage guarantees, independent of the underlying data distribution. However, existing methods typically rely on a single conformity score function, which can limit the efficiency and informativeness of the prediction sets. In this paper, we present a novel approach that enhances conformal prediction for multi-class classification by optimally averaging multiple conformity score functions. Our method involves assigning weights to different score functions and employing various data splitting strategies. Additionally, our approach bridges concepts from conformal prediction and model averaging, offering a more flexible and efficient tool for uncertainty quantification in classification tasks. We provide a comprehensive theoretical analysis grounded in Vapnik–Chervonenkis (VC) theory, establishing finite-sample coverage guarantees and demonstrating the efficiency of our method. Empirical evaluations on benchmark datasets show that our weighted averaging approach consistently outperforms single-score methods by producing smaller prediction sets without sacrificing coverage.","We present a novel approach for improving conformal prediction in multi-class classification by optimally combining multiple uncertainty scoring functions rather than relying on a single one. Conformal prediction generates prediction sets with guaranteed coverage probability, but existing single-score methods can produce inefficiently large sets. Our method assigns optimal weights to different scoring functions using four data splitting strategies (VFCP, EFCP, DLCP, DLCP+) that balance coverage guarantees with prediction set efficiency. We provide theoretical analysis using Vapnik-Chervonenkis theory establishing finite-sample coverage guarantees and demonstrating near-optimal efficiency as dataset size increases. Experiments on benchmark datasets (CIFAR-10/100, MNIST, Fashion-MNIST, ImageNet) show our weighted averaging approach consistently produces smaller prediction sets than single-score baselines while maintaining required coverage, making it particularly valuable for applications requiring precise uncertainty quantification with more informative predictions."
Poster,Confounder-Free Continual Learning via Recursive Feature Normalization,https://ICML.cc//virtual/2025/poster/46299,"Yash Shah, Camila Gonzalez, MohammadHassan Abbasi, Qingyu Zhao, Kilian M Pohl, Ehsan Adeli","Confounders are extraneous variables that affect both the input and the target, resulting in spurious correlations and biased predictions. There are recent advances in dealing with or removing confounders in traditional models, such as metadata normalization (MDN), where the distribution of the learned features is adjusted based on the study confounders. However, in the context of continual learning, where a model learns continuously from new data over time without forgetting, learning feature representations that are invariant to confounders remains a significant challenge. To remove their influence from intermediate feature representations, we introduce the Recursive MDN (R-MDN) layer, which can be integrated into any deep learning architecture, including vision transformers, and at any model stage. R-MDN performs statistical regression via the recursive least squares algorithm to maintain and continually update an internal model state with respect to changing distributions of data and confounding variables. Our experiments demonstrate that R-MDN promotes equitable predictions across population groups, both within static learning and across different stages of continual learning, by reducing catastrophic forgetting caused by confounder effects changing over time.","When we train an AI model, it can often be influenced by certain factors, which we call confounders, that affect both the input to the model as well as the outcome that the model predicts. This can cause the model to learn false associations from the data and make biased predictions. For example, a model trained to diagnose neurodegenerative disorders from brain MRI scans acquired from two different clinical sites, where healthy patients come from one site and diseased patients come from the other, can start learning site-specific patterns for prediction as opposed to actual signs of the disease from the collected scans. While recently proposed techniques such as metadata normalization (MDN) have been developed to help fix this issue, the setting assumed is that when we have all data collected prior to training the model. However, in most settings, data is collected over long periods of time--often months or years--and we would like to keep training the model when new data comes in as opposed to starting from scratch. This setting is called continual learning. We propose a new method called Recursive MDN to combat biased learning in continual learning settings by using a technique from statistics called recursive least squares. Our model continually tracks changes in the distribution of confounders and learns in real time to avoid making misleading predictions. Through our tests, we find that our model makes equitable predictions across various population groups."
Poster,ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization,https://ICML.cc//virtual/2025/poster/44867,"Hee Suk Yoon, Eunseop Yoon, Mark Hasegawa-Johnson, Sungwoong Kim, Chang Yoo","We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead.","Teaching large language models to generate responses that align with human preferences is a key challenge. Current training methods are often inefficient because they treat every word—from simple fillers like ""the"" and ""a"" to critical keywords—as equally important for learning. This wastes resources and can limit how much the AI's helpfulness improves.We developed a method called ConfPO that intelligently focuses the learning process. ConfPO monitors the AI's own confidence as it generates a response; when the model is less confident about a particular word, it signals that this word is likely an important decision point. Our method directs all the training updates only to these few, high-impact words.This focused strategy makes the learning process more efficient, resulting in higher-quality, more helpful AI responses. Crucially, ConfPO achieves this performance boost without requiring any additional complex models or extra computational power, offering a simple and effective tool for building better language technologies."
Poster,Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret,https://ICML.cc//virtual/2025/poster/45953,"Bingshan Hu, Zhiming Huang, Tianyue Zhang, Mathias Lécuyer, Nidhi Hegde","We address differentially private stochastic bandit problems by leveraging  Thompson Sampling with Gaussian priors  and Gaussian differential privacy (GDP).  We propose DP-TS-UCB, a novel parametrized private  algorithm that enables trading off privacy and regret. DP-TS-UCB satisfies $ \tilde{O} \left(T^{0.25(1-\alpha)}\right)$-GDP and achieves $O \left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bounds, where $K$ is the number of arms, $ \Delta$ is the sub-optimality gap, $T$ is the learning horizon, and $\alpha \in [0,1]$ controls the trade-off between privacy and regret. Theoretically,  DP-TS-UCB relies on anti-concentration bounds for the Gaussian distributions, linking the exploration mechanisms of Thompson Sampling and Upper Confidence Bound, which may be of independent research interest.","Since machine learning algorithms without  consideration of differential privacy  constantly expose private information, this paper works on developing efficient differentially private sequential learning algorithms under bandit feedback model.  It is centered around  understanding how to trade off privacy and regret, and finding the fundamental limits that prevent private sequential learning algorithms from having a good trade-off."
Poster,Consensus Based Stochastic Optimal Control,https://ICML.cc//virtual/2025/poster/45038,"Liyao Lyu, Jingrun Chen","We propose a *gradient-free* deep reinforcement learning algorithm to solve *high-dimensional*, finite-horizon stochastic control problems. Although the recently developed deep reinforcement learning framework has achieved great success in solving these problems, direct estimation of policy gradients from Monte Carlo sampling often suffers from high variance.  To address this, we introduce the Momentum Consensus-Based Optimization (M-CBO) and Adaptive Momentum Consensus-Based Optimization (Adam-CBO) frameworks. These methods optimize policies using Monte Carlo estimates of the value function, rather than its gradients. Adjustable Gaussian noise supports efficient exploration, helping the algorithm converge to optimal policies in complex, nonconvex environments. Numerical results confirm the accuracy and scalability of our approach across various problem dimensions and show the potential for extension to mean-field control problems. Theoretically, we prove that M-CBO can converge to the optimal policy under some assumptions.","This paper presents a new approach to solving reinforcement learning problems, where agents learn to make decisions over time without a predefined model. Our method is more efficient than existing ones and can be extended to a variety of complex domains. By improving the agent's ability to explore different solutions, our approach enhances its performance, helping it identify optimal strategies more quickly. This method holds great potential for addressing advanced problems and shows promise in real-world applications such as robotics, finance, and more."
Poster,Consensus Is All You Get: The Role of Attention in Transformers,https://ICML.cc//virtual/2025/poster/44961,"Alvaro Rodriguez Abella, João Pedro Silvestre, Paulo Tabuada","A key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token along the layers of a  transformer. In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transformers. Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature. Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model.","The self-attention mechanism is the key component of transformers, the main architecture of modern Large Language Models (LLMs). What is the role that self-attention plays in transformers? The previous literature suggests that self-attention forces tokens (words in a sentence or characters in a word) to collapse, that is, all the tokens became the same as the depth of a transformer increases.In this work, we mathematically prove that tokens collapse. Our results are empirically shown to hold, even when our modeling assumptions are not satisfied, using the GPT-2 and the GPT-Neo models.Having established that tokens collapse, our paper suggests the need to study the optimal transformer depth: transformers that are too short do not adequately use the context provided by a prompt whereas transformers that are too long ignore the prompt since all the tokens became the same."
Poster,Conservative Offline Goal-Conditioned Implicit V-Learning,https://ICML.cc//virtual/2025/poster/46406,"Ke Kaiqiang, qian lin, Zongkai Liu, Shenghong He, Chao Yu","Offline goal-conditioned reinforcement learning (GCRL) learns a goal-conditioned value function to train policies for diverse goals with pre-collected datasets. Hindsight experience replay addresses the issue of sparse rewards by treating intermediate states as goals but fails to complete goal-stitching tasks where achieving goals requires stitching different trajectories. While cross-trajectory sampling is a potential solution that associates states and goals belonging to different trajectories, we demonstrate that this direct method degrades performance in goal-conditioned tasks due to the overestimation of values on unconnected pairs. To this end, we propose Conservative Goal-Conditioned Implicit Value Learning (CGCIVL), a novel algorithm that introduces a penalty term to penalize value estimation for unconnected state-goal pairs and leverages the quasimetric framework to accurately estimate values for connected pairs. Evaluations on OGBench, a benchmark for offline GCRL, demonstrate that CGCIVL consistently surpasses state-of-the-art methods across diverse tasks.","When some tasks involve situations where the starting point and the goal are from different experiences, known as ""goal-stitching"" tasks, agents need to sample states and goals from different trajectories. However, this naive cross-trajectory sampling often leads to inaccurate value estimates.In our study, we found that the values of unconnected state-goal pairs may be overestimated, where ""unconnected"" means that no path in the dataset links the state and the goal. This happens because intermediate steps or transitions are missing from the dataset. Furthermore, the errors caused by overestimating values can spread to other state-goal pairs through bootstrapping (which occurs when an agent updates its values based on other estimates). To address this, we introduce a penalty in the learning process that prevents the overestimation of values for unconnected state-goal pairs.Our work is the first to highlight the value estimation issue caused by cross-trajectory sampling and offers a practical solution. This helps agents learn to combine different skills, like humans, to tackle more complex tasks."
Poster,Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability,https://ICML.cc//virtual/2025/poster/43864,"Michael Crawshaw, Blake Woodworth, Mingrui Liu","Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous objectives requires stepsizes $\eta \leq 1/K$ where $K$ is the communication interval, which ensures monotonic decrease of the objective. In contrast, we analyze Local Gradient Descent for logistic regression with separable, heterogeneous data using any stepsize $\eta > 0$. With $R$ communication rounds and $M$ clients, we show convergence at a rate $\mathcal{O}(1/\eta K R)$ after an initial unstable phase lasting for $\widetilde{\mathcal{O}}(\eta K M)$ rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general smooth, convex objectives. Our analysis parallels the single machine analysis of Wu et al. (2024) in which instability is caused by extremely large stepsizes, but in our setting another source of instability is large local updates with heterogeneous objectives.","Machine learning is generally very powerful, but also very expensive in terms of resources such as computer power and data. To mitigate these resource requirements, it is common to train machine learning models in parallel across many devices, such as mobile phones, which helps by leveraging compute power and data from many users. In this paper, we study a classic algorithm for training machine learning models in this distributed manner, and prove that this algorithm can train certain machine learning models much faster than previously understood. Essentially, this acceleration comes from allowing the algorithm to be ""unstable"", which is usually considered inadvisable, but in our case the instability actually creates acceleration."
Poster,Constrain Alignment with Sparse Autoencoders,https://ICML.cc//virtual/2025/poster/46128,"Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang","The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often experience computational inefficiencies and training instability. In this paper, we propose \textbf{F}eature-level constrained \textbf{P}reference \textbf{O}ptimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves an above 5\% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.","Problem: Large Language Models, the AI behind chatbots, need ""alignment"" to ensure their answers are helpful and safe, matching human preferences. However, current alignment methods can be slow, expensive, and sometimes unstable during training. It's difficult to make them both efficient and precisely controllable.Solution: We introduce Feature-level Preference Optimization (FPO), a new method using Sparse Autoencoders (SAEs). SAEs help us understand the core concepts or ""features"" the model uses. FPO guides the model at this deeper feature level, rather than just the words (tokens) it produces. It also pre-calculates data, making training faster and less memory-intensive.Impact: FPO makes LLM alignment more efficient and stable. Our tests show it outperforms top methods while using fewer resources. Importantly, it allows fine-grained control, letting us adjust specific AI behaviors—like improving safety or managing its use of different languages—without harming overall performance. This makes creating well-behaved and reliable AI more practical."
