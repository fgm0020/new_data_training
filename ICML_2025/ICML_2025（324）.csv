type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,VerbalTS: Generating Time Series from Texts,https://ICML.cc//virtual/2025/poster/45631,"Shuqi Gu, Chuyue Li, Baoyu Jing, Kan Ren","Time series synthesis has become a foundational task in modern society, underpinning decision-making across various scenes. Recent approaches primarily generate time series from structured conditions, such as attribute-based metadata. However, these methods struggle to capture the full complexity of time series, as the predefined structures often fail to reflect intricate temporal dynamics or other nuanced characteristics. Moreover, constructing structured metadata requires expert knowledge, making large-scale data labeling costly and impractical. In this paper, we introduce VerbalTS, a novel framework for generating time series from unstructured textual descriptions, offering a more expressive and flexible solution to time series synthesis. To bridge the gap between unstructured text and time series data, VerbalTS employs a multi-focal alignment and generation framework, effectively modeling their complex relationships. Experiments on two synthetic and four real-world datasets demonstrate that VerbalTS outperforms existing methods in both generation quality and semantic alignment with textual conditions.","Imagine you’re planning a vacation and want to understand how the weather might change over time. Or maybe you’re a traffic manager trying to figure out how traffic flows through a city during the day. To do this, people often rely on time series—patterns that show how things change step by step. But creating these patterns is tricky. Most existing tools require experts to provide structured labels, which is slow, expensive, and often misses important details.So we asked: what if we could just verbally describe what we want in natural language—like “a sudden drop, then a slow rise”—and have a computer bring that pattern to life? That’s exactly what our new system, VerbalTS, does. It takes simple, natural descriptions and turns them into realistic patterns that behave like the real thing. We tested it across many different scenarios, and it consistently outperformed other tools. This could make it easier for people in many fields to explore “what if” questions, without needing to be data experts."
Poster,Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible,https://ICML.cc//virtual/2025/poster/44815,"Lin-Han Jia, Wen-Chao Hu, Jie-Jing Shao, Lan-Zhe Guo, Yu-Feng Li","The current Neuro-Symbolic (NeSy) Learning paradigm suffers from an over-reliance on labeled data, so if we completely disregard labels, it leads to less symbol information, a larger solution space, and more shortcuts—issues that current Nesy systems cannot resolve. This paper introduces a novel learning paradigm, Verification Learning (VL), which addresses this challenge by transforming the label-based reasoning process in Nesy into a label-free verification process. VL achieves excellent learning results solely by relying on unlabeled data and a function that verifies whether the current predictions conform to the rules. We formalize this problem as a Constraint Optimization Problem (COP) and propose a Dynamic Combinatorial Sorting (DCS) algorithm that accelerates the solution by reducing verification attempts, effectively lowering computational costs and introduce a prior alignment method to address potential shortcuts. Our theoretical analysis points out which tasks in Nesy systems can be completed without labels and explains why rules can replace infinite labels for some tasks, while for others the rules have no effect. We validate the proposed framework through several fully unsupervised tasks including addition, sort, match, and chess, each showing significant performance and efficiency improvements.","Current neuro-symbolic systems typically require a large amount of annotated data to train models. However, in real-world scenarios, acquiring annotations for reasoning tasks is often challenging. This paper proposes Verification Learning (VL), a novel learning paradigm that significantly alleviates issues associated with limited information, large symbolic spaces, and the prevalence of shortcuts in the absence of supervision. The VL problem is formulated as an optimization problem, which can be efficiently solved using a low-complexity ranking algorithm under specific conditions. To further address the shortcut issue, we introduce a distribution alignment strategy. On the theoretical side, we analyze the errors introduced by knowledge and data in unsupervised settings. Experimentally, we demonstrate the superiority of the VL paradigm on four benchmark datasets."
Poster,VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data,https://ICML.cc//virtual/2025/poster/44223,"Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, HYUNG IL KOO, Kannan Ramchandran, Dimitris Papailiopoulos, Kangwook Lee","Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce ***VersaPRM***, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline–surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.","Recent advances have shown that Large Language Models (like ChatGPT) solve math problems more effectively if they explain their thinking process step by step before giving a final answer. This ability can be further improved with Process Reward Models—models that check and grade each step of the reasoning process for correctness.However, previous work on Process Reward Models has mostly focused on math problems. We find that these models don’t perform well on questions from other areas, such as Law or Biology. To address this, we introduce a new Process Reward Model called VersaPRM, which is trained on a more diverse set of reasoning tasks. As a result, VersaPRM can help Large Language Models reason better across a wider range of subjects—not just math."
Poster,Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach,https://ICML.cc//virtual/2025/poster/43738,"Minting Pan, Yitao Zheng, Jiajian Li, Yunbo Wang, Xiaokang Yang","Offline reinforcement learning (RL) enables policy optimization using static datasets, avoiding the risks and costs of extensive real-world exploration. However, it struggles with suboptimal offline behaviors and inaccurate value estimation due to the lack of environmental interaction. We present Video-Enhanced Offline RL (VeoRL), a model-based method that constructs an interactive world model from diverse, unlabeled video data readily available online. Leveraging model-based behavior guidance, our approach transfers commonsense knowledge of control policy and physical dynamics from natural videos to the RL agent within the target domain. VeoRL achieves substantial performance gains (over 100% in some cases) across visual control tasks in robotic manipulation, autonomous driving, and open-world video games. Project page: https://panmt.github.io/VeoRL.github.io.","Training AI systems to perform real-world tasks often requires risky and costly trial-and-error in physical environments. While existing methods use pre-recorded datasets to avoid this, they face a critical limitation: robots or AI agents trained this way often make poor decisions because they can’t interact with the real world to test and refine their understanding. VeoRL solves this by letting AI learn from available online videos—like robot demonstrations, car dashcam footage, or gameplay streams—to build a “virtual playground.” By analyzing video patterns, VeoRL automatically learns real-world physics and control strategies. This “common sense” helps AI avoid dangerous mistakes. For example, robots master tasks after “watching” human-like movements in videos, while self-driving systems improve safety by practicing with traffic footage. Tested in robotics, driving, and gaming, VeoRL doubled performance in some cases compared to standard methods—proving AI can learn complex skills by observing first, acting later, just like humans do."
Poster,VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models,https://ICML.cc//virtual/2025/poster/43541,"Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, Shelly Sheynin","Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence.To address this, we introduce **VideoJAM**, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn *a joint appearance-motion representation*. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce **Inner-Guidance**, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal.Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model.VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations.These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation.","Text-based video generation is an intensely studied problem in modern computer vision. However, even large-scale proprietary models trained on millions—or even billions—of high-quality videos still struggle to faithfully model the temporal axis. This often results in severe temporal incoherence: limbs appearing or disappearing arbitrarily, objects defying basic physical laws, or extreme distortions.In this work, we investigate this prominent issue and find that it can be attributed to the pixel reconstruction training objective commonly used to train these models. Intuitively, pixel reconstruction favors appearance-based features—such as colors, shapes, and outlines—over temporal features like motion. As a result, models tend to display limited sensitivity to temporal information.To address this issue, we propose a framework dubbed **VideoJAM**. VideoJAM requires the model to explicitly learn temporal information by incorporating a motion-based objective. During training, we modify the loss to predict not only pixels but also their corresponding motion. This compels the model to represent temporal information, which is necessary to reconstruct the motion signal.At inference time, we introduce **Inner-Guidance**, a mechanism wherein the motion signal predicted by the model is used to guide generation toward temporally coherent results. This allows the model to steer itself using predictions from previous generation steps.We benchmark our models against their ""base"" counterparts, which do not employ our framework, as well as against a range of state-of-the-art proprietary models such as OpenAI’s Sora and Kling. In all cases, our framework significantly improves motion coherence without compromising other aspects of generation, such as aesthetic quality and prompt alignment."
Poster,Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations,https://ICML.cc//virtual/2025/poster/44705,"Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen","Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data.In experiments, VPP achieves a 18.6\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\% increase in success rates for complex real-world dexterous manipulation tasks. For your convenience, videos can be found at https://video-prediction-policy.github.io/","In this work, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from Video diffusion models.  VPP implicitly learns inverse dynamics conditioned on these predictive representations, leading to consistent performance gains in both simulated and real-world environments. We also demonstrate the benefit of utilizing physical knowledge embedded in pre-trained video generation models and large-scale Internet manipulation datasets.Our results underscore the potential of video models in enabling physical intelligence and highlight their value in embodied robotic tasks."
Poster,VideoRoPE: What Makes for Good Video Rotary Position Embedding?,https://ICML.cc//virtual/2025/poster/43783,"Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, Dahua Lin","While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge.This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work.As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH.The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors.Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships.VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing.VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination.Our code and model weights will be publicly released.","Videos have complex structures that make it hard for models to understand long sequences of information. Adapting previous methods designed for one-dimensional data (like text) to videos has been a challenge due to the video’s spatio-temporal nature. Our research introduces a new method called VideoRoPE that improves how models handle video by considering both time and space in a more effective way. We find that existing methods fail when distractors (unrelated elements) are added to video tasks, so we design VideoRoPE to reduce errors and handle these distractions better. This method works better than older ones across various video-related tasks, like searching for video clips or understanding scenes. Our approach helps improve how machines understand videos, making them smarter and more reliable."
Poster,video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model,https://ICML.cc//virtual/2025/poster/43555,"Guangzhi Sun, Yudong Yang, Jimin Zhuang, Changli Tang, Yixuan Li, Wei Li, Zejun MA, Chao Zhang","While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding. This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities.","We develop a new AI system called video-SALMONN-o1 that is the first open-source audio-visual large language model that can perform reasoning to help understand videos better. While past efforts to improve model reasoning ability mainly focused on solving math problems or analyzing images, video-SALMONN-o1 is one of the first to focus on more general video content—like scenes from comedy shows, lectures, or detecting fake (synthetic) videos.To train the system, we created a special dataset filled with complex audio-visual questions and step-by-step answers. We also introduced a new training method called pDPO, which helps the the model learn to reason more effectively by rewarding it for choosing better steps in its thought process.We also built RivaBench, a new benchmark to test how well AI systems can reason about videos. On this benchmark, video-SALMONN-o1 performed significantly better than existing models—improving accuracy by 3–8%. It even showed abilities to detect fake videos without being specifically trained for that task.In short, video-SALMONN-o1 takes a big step forward in helping AI understand and reason about complex video content more like humans do."
Poster,VinePPO: Refining Credit Assignment in RL Training of LLMs,https://ICML.cc//virtual/2025/poster/45526,"Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux","Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a common reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, recent approaches achieve strong results without it, raising questions about the efficacy of value networks in practice. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they often produce poor estimate of expected return and barely outperform a random baseline when comparing alternative steps. This motivates our key question: Can improved credit assignment enhance RL training for LLMs? To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates. Our method consistently outperforms PPO and other baselines across MATH and GSM8K datasets in less wall-clock time (up to 3.0x).  Crucially, it achieves higher test accuracy for a given training accuracy, capturing more generalization signal per sample. These results emphasize the importance of accurate credit assignment in RL training of LLM.","Large language models (LLMs) like ChatGPT learn complex tasks through trial-and-error training called reinforcement learning. During that process, the model may generate many reasoning steps before producing an answer, but only a few steps actually matter. Figuring out which steps are truly helpful, known as the “credit assignment” problem, is crucial, yet the standard methods such as (PPO or GRPO) either completely treat all steps equally or rely on a helper model that often guesses incorrectly.We found that this helper, called the value network, frequently fails to recognize which reasoning steps contribute to success. This may explain why recent simplified approaches that ignore step-by-step evaluation still perform surprisingly well. Instead of discarding credit assignment, we propose VinePPO, which makes it accurate by directly measuring the usefulness of each step through re-simulation.Since language models can easily restart from any intermediate point by re-feeding context, VinePPO uses this property to get reliable, unbiased feedback, without needing to train a separate network. VinePPO outperforms PPO and shortcut methods on math reasoning tasks, achieving better accuracy with less total training time. Our results show that smarter credit assignment can still drive better LLMs."
Poster,Vintix: Action Model via In-Context Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44459,"Andrei Polubarov, Nikita Lyubaykin, Alexander Derevyagin, Ilya Zisman, Denis Tarasov, Alexander Nikulin, Vladislav Kurenkov","In-Context Reinforcement Learning (ICRL) represents a promising paradigm for developing generalist agents that learn at inference time through trial-and-error interactions, analogous to how large language models adapt contextually, but with a focus on reward maximization. However, the scalability of ICRL beyond toy tasks and single-domain settings remains an open challenge. In this work, we present the first steps toward scaling ICRL by introducing a fixed, cross-domain model capable of learning behaviors through in-context reinforcement learning. Our results demonstrate that Algorithm Distillation, a framework designed to facilitate ICRL, offers a compelling and competitive alternative to expert distillation to construct versatile action models. These findings highlight the potential of ICRL as a scalable approach for generalist decision-making systems.","What if AI agents could learn new tasks on the fly—just by interacting with their environment—without ever being retrained? In-Context Reinforcement Learning (ICRL) promises exactly that, but until now, it hasn’t scaled beyond toy examples or single-domain settings. Our work takes a leap forward: we introduce a single model that learns from inference-time trial and error across a wide variety of tasks and environments. No task-specific tuning. Just adaptation in real time. This is a glimpse into the future of generalist AI—agents that learn the way we do: by doing."
