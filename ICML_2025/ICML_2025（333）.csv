type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,XAttention: Block Sparse Attention with Antidiagonal Scoring,https://ICML.cc//virtual/2025/poster/45650,"Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, Song Han","Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements.In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference.Across comprehensive evaluations on demanding long-context benchmarks—including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation—XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications.","AI models that process long documents or videos, known as Long-Context Transformer Models (LCTMs), are powerful but face a major hurdle: they are computationally expensive, largely due to a component called the attention mechanism. The cost of this mechanism grows quadratically with the length of the information, creating a significant bottleneck.To solve this, researchers have developed ""block-sparse attention,"" an approach that saves time by having the AI focus only on the most critical blocks of information instead of every single detail. However, existing methods have struggled because the process of figuring out which blocks are important can itself be slow and inefficient, canceling out the benefits.Our paper introduces XAttention, a new framework that makes these models much faster without sacrificing accuracy. The key discovery is that summing up values along antidiagonals (lines running from the lower-left to the upper-right) within the model's attention grid is a surprisingly simple and effective way to measure a block's importance. This ""antidiagonal scoring"" method allows XAttention to quickly identify and prune away non-essential computations.Evaluated on demanding tasks involving long-form language, video analysis, and video generation, XAttention demonstrated performance comparable to the original, slower models while providing significant speed-ups. For instance, it achieved up to a 13.5x acceleration in the core attention computation. These results show that XAttention makes powerful AI for long-form content more practical and efficient for real-world applications."
Poster,XAttnMark: Learning Robust Audio Watermarking with Cross-Attention,https://ICML.cc//virtual/2025/poster/43452,"Yixin Liu, Lie Lu, Jihui Jin, Lichao Sun, Andrea Fanelli","The rapid proliferation of generative audio synthesis and editing technologies has raised significant concerns about copyright infringement, data provenance, and the spread of misinformation through deepfake audio. Watermarking offers a proactive solution by embedding imperceptible, identifiable, and traceable marks into audio content. While recent neural network-based watermarking methods like WavMark and AudioSeal have improved robustness and quality, they struggle to achieve both robust detection and accurate attribution simultaneously. This paper introduces the Cross-Attention Robust Audio Watermark (XAttnMark), which bridges this gap by leveraging partial parameter sharing between the generator and the detector, a cross-attention mechanism for efficient message retrieval, and a temporal conditioning module for improved message distribution. Additionally, we propose a psychoacoustic-aligned temporal-frequency masking loss that captures fine-grained auditory masking effects, enhancing watermark imperceptibility. Our approach achieves state-of-the-art performance in both detection and attribution, demonstrating superior robustness against a wide range of audio transformations, including challenging generative editing with strong editing strength. This work represents a significant step forward in protecting intellectual property and ensuring the authenticity of audio content in the era of generative AI.","People are now able to edit or generate music and speech with artificial-intelligence tools, but this freedom brings risks: copied songs can be passed off as new, and fake audio can spread online without a clear way to track who made it. Our study introduces a new “audio watermark” that hides an invisible code inside sound while leaving what you hear unchanged. The code can be read even after the audio is compressed, trimmed, mixed, or put through powerful AI editors—tasks that defeat earlier watermarks. We achieve this by letting the part that inserts the code and the part that reads it share helpful clues and by teaching the system to tuck the code into places where the human ear is naturally less sensitive. In tests across many types of music and speech, the watermark stayed intact and the sound quality stayed high, offering a practical step toward protecting creators and spotting fakes."
Poster,X-Hacking: The Threat of Misguided AutoML,https://ICML.cc//virtual/2025/poster/46106,"Rahul Sharma, Sumantrak Mukherjee, Andrea Šipka, Eyke Hüllermeier, Sebastian Vollmer, Sergey Redyuk, David A Selby","Explainable AI (XAI) and interpretable machine learning methods help to build trust in model predictions and derived insights, yet also present a perverse incentive for analysts to manipulate XAI metrics to support pre-specified conclusions. This paper introduces the concept of X-hacking, a form of p-hacking applied to XAI metrics such as Shap values. We show how easily an automated machine learning pipeline can be adapted to exploit model multiplicity at scale: searching a set of ‘defensible’ models with similar predictive performance to find a desired explanation. We formulate the trade-off between explanation and accuracy as a multi-objective optimisation problem, and illustrate empirically on familiar real-world datasets that, on average, Bayesian optimisation accelerates X-hacking 3-fold for features susceptible to it, versus random sampling. We show the vulnerability of a dataset to X-hacking can be determined by information redundancy among features. Finally, we suggest possible methods for detection and prevention, and discuss ethical implications for the credibility and reproducibility of XAI.","Artificial Intelligence (AI) models are increasingly used to make critical decisions in fields like healthcare and justice. To build trust and ensure accountability in these powerful systems, we rely on ""Explainable AI"" to understand how they arrive at their conclusions. However, our research has uncovered a critical vulnerability we call ""X-hacking."" This is when an AI's explanation can be skewed or misrepresented to support a specific outcome, even if that outcome is misleading or doesn't reflect the AI's true reasoning. This can happen intentionally, but also inadvertently by those who are inexperienced or pressed for time. X-hacking is possible because many different AI models can produce equally good results, even if their underlying logic and the explanations they provide differ. We found that automated tools for building AI (called AutoML) make it surprisingly easy to find and leverage these differences to ""X-hack"" explanations on a large scale. Our findings show that using a smart search method can even speed up this manipulation by three times! We also discovered that some types of data are more prone to X-hacking, especially when the information within that data is very similar or redundant. To combat this, we propose ways to detect X-hacking. For example, we can check if a given explanation seems unusually far-fetched compared to other reasonable possibilities, and show this on a graph. Our work aims to alert everyone to this significant risk, encouraging the creation of AI systems that are truly transparent and trustworthy."
Poster,xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference,https://ICML.cc//virtual/2025/poster/45587,"Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Richard Kurle, Patrick Blies, Günter Klambauer, Sebastian Böck, Sepp Hochreiter","Recent breakthroughs in solving reasoning, math and coding problems with Large Language Models (LLMs) have been enabled by investing substantial computation budgets at inference time. Therefore, inference speed is one of the most critical properties of LLM architectures, and there is a growing need for LLMs that are efficient and fast at inference. Recently, LLMs built on the xLSTM architecture have emerged as a powerful alternative to Transformers, offering linear compute scaling with sequence length and constant memory usage, both highly desirable properties for efficient inference. However, such xLSTM-based LLMs have yet to be scaled to larger models and assessed and compared with respect to inference speed and efficiency. In this work, we introduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM’s architectural benefits with targeted optimizations for fast and efficient inference. Our experiments demonstrate that xLSTM 7B achieves performance on downstream tasks comparable to other similar-sized LLMs, while providing significantly faster inference speeds and greater efficiency compared to Llama- and Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most efficient 7B LLM, offering a solution for tasks that require large amounts of test-time computation. Our work highlights xLSTM’s potential as a foundational architecture for methods building on heavy use of LLM inference. Our model weights, model code and training code are open-source.Model: https://huggingface.co/NX-AI/xLSTM-7bCode: https://github.com/NX-AI/xlstm andhttps://github.com/NX-AI/xlstm-jax.","Recent Large Language Models (LLMs) are trained to think through problems before they respond. This thinking process can improve performance especially on hard reasoning problems. Since LLMs generate long texts during this thought process, the required context length becomes very large.The dominant Transformer architecture scales unfavorably quadratically in terms of compute cost with this increased context lengths.We show that the recently introduced recurrent xLSTM architecture can be adapted to build an LLM at the seven billion (7B) parameter model size. As it scales linearly in compute, and due to our speed optimization, it is now the fastest model at this scale, matching the performance of other models.This shift from quadratic Transformer to linear xLSTM models makes LLMs more memory-, compute- and energy-efficient without compromising the quality of the outputs."
Poster,X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP,https://ICML.cc//virtual/2025/poster/46255,"Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, James Bailey","As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce **X-Transfer**, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as **super transferability**—a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through **surrogate scaling**, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models.","AI systems like CLIP learn to understand images and text together, and they’re now widely used in AI tools, such as chatbots. But these systems can be tricked. By slightly altering an image in a way that’s invisible to humans, attackers can make the AI misinterpret it completely.In this work, we introduce X-Transfer, a powerful new method that creates such a “visual trick”—a single, tiny change that confuses many different CLIP models across various image types, tasks, and applications. We call this rare and powerful ability super transferability because one small change works almost everywhere.X-Transfer achieves this using an efficient technique we call surrogate scaling, which smartly picks the right models to “train against” without needing to run expensive computations on all possible models.Our results show that X-Transfer is far more effective than past method. It sets a new standard for evaluating the security of AI systems that use CLIP, raising important concerns for their real-world deployment."
Poster,You Always Recognize Me (YARM): Robust Texture Synthesis Against Multi-View Corruption,https://ICML.cc//virtual/2025/poster/44107,"Weihang Ran, Wei Yuan, Yinqiang Zheng","Damage to imaging systems and complex external environments often introduce corruption, which can impair the performance of deep learning models pretrained on high-quality image data. Previous methods have focused on restoring degraded images or fine-tuning models to adapt to out-of-distribution data. However, these approaches struggle with complex, unknown corruptions and often reduce model accuracy on high-quality data. Inspired by the use of warning colors and camouflage in the real world, we propose designing a robust appearance that can enhance model recognition of low-quality image data. Furthermore, we demonstrate that certain universal features in radiance fields can be applied across objects of the same class with different geometries. We also examine the impact of different proxy models on the transferability of robust appearances. Extensive experiments demonstrate the effectiveness of our proposed method, which outperforms existing image restoration and model fine-tuning approaches across different experimental settings, and retains effectiveness when transferred to models with different architectures. Code will be available at https://github.com/SilverRAN/YARM.","Artificial intelligence technologies can learn visual features for classification using clean images and labels. However, in real-world application scenarios, the imaging system may capture low-quality images due to environmental factors, motion intensity, or hardware issues. We refer to such images as *corrupted images*. Previous studies have observed that severe corruption can significantly degrade the accuracy of AI models and have proposed various solutions to address this issue, such as restoring the corrupted images or fine-tuning models on datasets containing corrupted data. In contrast to these approaches, we draw inspiration from the camouflage patterns found in nature and propose to address the problem from the perspective of **data appearance**. Our proposed framework enables the redesign of an object’s original appearance, so that the images captured by the imaging system naturally exhibit resistance to corruption."
Poster,You Get What You Give: Reciprocally Fair Federated Learning,https://ICML.cc//virtual/2025/poster/44841,"Aniket Murhekar, Jiaxin Song, Parnian Shahkar, Bhaskar Ray Chaudhury, Ruta Mehta","Federated learning (FL) is a popular collaborative learning paradigm, whereby agents with individual datasets can jointly train an ML model. While higher data sharing improves model accuracy and leads to higher payoffs, it also raises costs associated with data acquisition or loss of privacy, causing agents to be strategic about their data contribution. This leads to undesirable behavior at a Nash equilibrium (NE) such as *free-riding*, resulting in sub-optimal fairness, data sharing, and welfare.To address this, we design $\mathcal{M}^{Shap}$, a budget-balanced payment mechanism for FL, that admits Nash equilibria under mild conditions, and achieves *reciprocal fairness*: where each agent's payoff equals her contribution to the collaboration, as measured by the Shapley share. In addition to fairness, we show that the NE under $\mathcal{M}^{Shap}$ has desirable guarantees in terms of accuracy, welfare, and total data collected.We validate our theoretical results through experiments, demonstrating that $\mathcal{M}^{Shap}$ outperforms baselines in terms of fairness and efficiency.","(1) Federated learning (FL) is a popular collaborative learning paradigm, but the cost of data sharing causes agents to be strategic about their data contribution. (2) We design MShap -- a budget-balanced payment mechanism for FL that admits Nash equilibria under mild conditions. (3) Our mechanism achieves reciprocal fairness and also has desirable guarantees in terms of accuracy, welfare, and total data collected. The theoretical results are validated on real-world datasets."
Poster,Zebra: In-Context Generative Pretraining for Solving Parametric PDEs,https://ICML.cc//virtual/2025/poster/46609,"Louis Serrano, Armand Kassaï Koupaï, Thomas Wang, Pierre ERBACHER, patrick gallinari","Solving time-dependent parametric partial differential equations (PDEs) is challenging for data-driven methods, as these models must adapt to variations in parameters such as coefficients, forcing terms, and initial conditions. State-of-the-art neural surrogates perform adaptation through gradient-based optimization and meta-learning to implicitly encode the variety of dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context example trajectories. As a generative model, Zebra can be used to generate new trajectories and allows quantifying the uncertainty of the predictions. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.","Solving complex equations that describe how things change over time, like weather patterns or fluid flow (called time-dependent parametric PDEs), is hard for computer models, especially when factors like initial conditions or forces can vary. Existing advanced computer models try to adjust to these changes by ""learning"" from observations, but this can make them slow to use.Inspired by how large language models can understand new situations without being explicitly re-trained, we developed a new model called Zebra. Zebra is a generative model, meaning it can create new solutions. It learns to adapt to different situations by looking at examples of how the PDE behaved in similar contexts, both during its initial training and when it's solving a new problem. This means Zebra doesn't need to be retrained for each new scenario, making it much faster. Zebra can also generate multiple possible solutions, which helps us understand the uncertainty in its predictions. We have tested Zebra on many difficult PDE problems, and it has shown to be adaptable, reliable, and better than current methods."
Poster,ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning,https://ICML.cc//virtual/2025/poster/43835,"Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, Yejin Choi","We investigate the logical reasoning capabilities of Large Language Models (LLMs) and their scalability across complex deductive tasks. Using ZebraLogic, a newly developed benchmark dataset of logic grid puzzles derived from constraint satisfaction problems (CSPs), we systematically evaluate LLM performance. ZebraLogic spans a broad range of search space complexities and incorporates diverse logical constraints, providing a controlled environment to assess reasoning abilities. Our results reveal a significant decline in accuracy as problem complexity increases—a phenomenon we term the “curse of complexity.” Notably, this limitation persists even with scaling model size and inference-time computation, suggesting fundamental constraints in current LLM reasoning capabilities. Additionally, we explore strategies such as Best-of-N sampling, backtracking mechanisms, and self-verification prompts to enhance logical reasoning performance. Our findings provide critical insights into the scaling behavior of LLMs, highlight their limitations, and outline potential directions for advancing their reasoning capabilities.","Large language models (LLMs), like those powering chatbots, are great at many tasks, but can they solve complex logic puzzles? We created ZebraLogic, a set of 1,000 logic grid puzzles, to test how well these models handle pure logical reasoning, similar to solving a brain teaser about who lives in which house with specific clues. Our puzzles range from simple to extremely challenging, allowing us to see how model performance changes as difficulty grows. We found that even the biggest and most advanced models struggle when puzzles get very complex—a problem we call the “curse of complexity.” Simply making models larger or giving them more tries doesn’t fully solve this. However, models that “think” step-by-step, using a process called chain-of-thought, perform better. Our work shows that teaching AI to reason more like humans, with careful backtracking, could improve their ability to tackle tough logic problems, benefiting real-world tasks like planning and scheduling."
Poster,ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think,https://ICML.cc//virtual/2025/poster/44360,"Tao Feng, Wei Li, Didi Zhu, Hangjie Yuan, Wendi Zheng, Dan Zhang, Jie Tang","Backpropagation provides a generalized configuration for overcoming catastrophic forgetting. Optimizers such as SGD and Adam are commonly used for weight updates in continual learning and continual pre-training. However, access to gradient information is not always feasible in practice due to black-box APIs, hardware constraints, or non-differentiable systems, a challenge we refer to as the gradient bans. To bridge this gap, we introduce ZeroFlow, the first benchmark designed to evaluate gradient-free optimization algorithms for overcoming forgetting. ZeroFlow examines a suite of forward pass-based methods across various algorithms, forgetting scenarios, and datasets. Our results show that forward passes alone can be sufficient to mitigate forgetting. We uncover novel optimization principles that highlight the potential of forward pass-based methods in mitigating forgetting, managing task conflicts, and reducing memory demands. Additionally, we propose new enhancements that further improve forgetting resistance using only forward passes. This work provides essential tools and insights to advance the development of forward-pass-based methods for continual learning.","Modern AI systems often struggle to retain what they've learned when they take on new tasks—a problem known as ""catastrophic forgetting."" Traditionally, researchers rely on a technique called backpropagation, which uses information about how a model's predictions go wrong (gradients) to update its knowledge and reduce forgetting. But in many real-world situations, getting access to these gradients isn’t possible, like when using black-box systems, dealing with hardware limits, or working with non-trainable components.To address this, we introduce ZeroFlow, the first benchmark for studying how gradient-free methods—using only the model’s outputs—can reduce forgetting. Our results show that these simple forward-pass methods can be surprisingly effective. Our experiments show that surprisingly, even without gradients, AI models can still hold onto old knowledge effectively. We also discover new techniques that help models avoid conflicts between tasks and save memory, all by using forward outputs only. This research opens up new possibilities for building smarter, more flexible AI systems that keep learning, even without access to gradients."
