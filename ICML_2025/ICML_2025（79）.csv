type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping,https://ICML.cc//virtual/2025/poster/45979,"Guangyi Liu, Suzan Iloglu, Michael Caldara, Joseph Durham, Michael Zavlanos","In Amazon robotic warehouses, the destination-to-chute mapping problem is crucial for efficient package sorting. Often, however, this problem is complicated by uncertain and dynamic package induction rates, which can lead to increased package recirculation. To tackle this challenge, we introduce a Distributionally Robust Multi-Agent Reinforcement Learning (DRMARL) framework that learns a destination-to-chute mapping policy that is resilient to adversarial variations in induction rates. Specifically, DRMARL relies on group distributionally robust optimization (DRO) to learn a policy that performs well not only on average but also on each individual subpopulation of induction rates within the group that capture, for example, different seasonality or operation modes of the system. This approach is then combined with a novel contextual bandit-based estimator of the worst-case induction distribution for each state-action pair, significantly reducing the cost of exploration and thereby increasing the learning efficiency and scalability of our framework. Extensive simulations demonstrate that DRMARL achieves robust chute mapping in the presence of varying induction distributions, reducing package recirculation by an average of 80% in the simulation scenario.","In robotic warehouses, packages arrive at unpredictable rates throughout the day, especially during seasonal peaks or special promotions. These fluctuating package induction rates make it difficult to assign destinations to eject chutes efficiently, often causing packages to be recirculated multiple times.We developed a new reinforcement learning framework that learns robust chute assignment policies capable of handling these changing conditions. Our method, called Distributionally Robust Multi-Agent Reinforcement Learning (DRMARL), prepares for the worst-case induction patterns using historical data. To make training efficient, we also created a contextual bandit-based tool that estimates which package arrival patterns are most problematic for each decision.This allows the system to adapt to future package arrival behaviors it hasn't seen before, improving throughput and reducing the number of times packages need to be recirculated. In simulations, our method reduced recirculation by up to 80% and remained stable across many different induction scenarios. This research improves the resilience and efficiency of warehouse automation, ensuring smoother operation even under unexpected load spikes."
Poster,Distributionally Robust Policy Learning under Concept Drifts,https://ICML.cc//virtual/2025/poster/45171,"Jingyuan Wang, Zhimei Ren, Ruohan Zhan, Zhengyuan Zhou","Distributionally robust policy learning aims to find a policy that performs well     under the worst-case distributional shift, and yet most existing methods for     robust policy learning consider the worst-case *joint* distribution of     the covariate and the outcome. The joint-modeling strategy can be unnecessarily conservative    when we have more information on the source of distributional shifts. This paper studies    a more nuanced problem --- robust policy learning under the *concept drift*,     when only the conditional relationship between the outcome and the covariate changes.     To this end, we first provide a doubly-robust estimator for evaluating    the worst-case average reward of a given policy under a set of perturbed conditional distributions.     We show that the policy value estimator enjoys asymptotic normality even if the nuisance parameters     are estimated with a slower-than-root-$n$ rate.    We then propose a learning algorithm that outputs the policy maximizing the     estimated policy value within a given policy class $\Pi$, and show    that the sub-optimality gap of the proposed algorithm is of the order     $\kappa(\Pi)n^{-1/2}$, where $\kappa(\Pi)$ is the entropy integral of $\Pi$ under the Hamming distance    and $n$ is the sample size. A matching lower bound is provided to show the optimality of the rate.    The proposed methods are implemented and evaluated in numerical studies,     demonstrating substantial improvement compared with existing benchmarks.","Most of the current robust offline policy learning literature adopts the joint-modeling strategy, which can be unnecessarily conservative when we have more information on the source of distributional shifts. We study the policy learning problem under concept drift, and develop a minimax optimal policy learning algorithm. Our methodology efficiently learns a policy with optimal worst-case average performance under concept drift, and can be extended to a more general setting where there is an additional identifiable covariate shift."
Poster,Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective,https://ICML.cc//virtual/2025/poster/46110,"Yujin Oh, Pengfei Jin, Sangjoon Park, Sekeun Kim, Siyeop yoon, Jin Kim, Kyungsang Kim, Xiang Li, Quanzheng Li","Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoE's role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code is available at https://github.com/tvseg/dMoE.","Medical images play a key role in helping doctors diagnose and treat patients. However, AI systems that analyze these images can sometimes show biased performance—especially when they are trained on imbalanced data, such as having more images from certain age groups, genders, or disease stages. This means they may work better for some patient groups than others. To address this problem, we developed a new approach called Distribution-aware Mixture of Experts (dMoE), inspired by concepts from control theory—a field that focuses on managing and guiding complex systems.This method helps AI systems adjust to differences in patient data, making the image analysis more fair and reliable. We tested dMoE with several types of AI models and found that it worked well across different tasks. It performed better than other methods on both publicly available 2D datasets and our own 3D medical image data. By including information like age, sex, and disease severity, our approach reduces bias and improves performance. This work shows how tools from other areas of science, like control theory, can help make AI in healthcare more fair and effective. We believe our distribution-aware approach is a meaningful step toward fairer AI systems in healthcare, especially in real-world situations where patient data can be unbalanced or varied."
Poster,DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation,https://ICML.cc//virtual/2025/poster/46258,"Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang","Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes.In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands.DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings, and the diffusion transformer subsequently generates the next patch based on the output of the language model.For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.","In this work, we propose DiTAR, a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive modeling for continuous tokens and reduces computational demands. For inference, we introduce temperature as the introduction time point for noise while solving the reverse diffusion ODE. Applied to zero-shot speech synthesis, DiTAR achieves SOTA robustness, speaker similarity, and naturalness with substantially lower computational requirements."
Poster,Diverging Preferences: When do Annotators Disagree and do Models Know?,https://ICML.cc//virtual/2025/poster/43935,"Michael Zhang, Zhilin Wang, Jena Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin","We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning ten categories across four high-level classes and find that the majority of disagreements are due to factors such as task underspecification or response style. Our findings challenge a standard assumption in reward modeling methods that annotator disagreements can be attributed to simple noise. We then explore how these findings impact two areas of LLM development: reward modeling training and evaluation. In our experiments, we demonstrate how standard reward modeling (e.g., Bradley-Terry) and LLM-as-Judge evaluation methods fail to account for divergence between annotators. These findings highlight challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence in evaluations and during LLM training.","We explore user disagreements in the preferred responses from LLMs. We analyze what factors lead to disagreement majority of disagreements are due to factors such as task underspecification or response style. We then examine how disagreements are handled in existing LLM training and evaluation methods, finding that standard methods incentivize LLMs to decisively prefer one response even when users disagree. We then propose methods for mitigating these behaviors in LLM training and evaluation."
Poster,Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift,https://ICML.cc//virtual/2025/poster/43937,"Nguyen Nhat Minh To, Paul Wilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi, Rahul G. Krishnan","Subpopulation shift, characterized by a disparity in subpopulation distribution between the training and target datasets, can significantly degrade the performance of machine learning models. Current solutions to subpopulation shift involve modifying empirical risk minimization with re-weighting strategies to improve generalization. This strategy relies on assumptions about the number and nature of subpopulations and annotations on group membership, which are unavailable for many real-world datasets. Instead, we propose using an ensemble of diverse classifiers to adaptively capture risk associated with subpopulations. Given a feature extractor network, we replace its standard linear classification layer with a mixture of prototypical classifiers, where each member is trained to classify the data while focusing on different features and samples from other members. In empirical evaluation on nine real-world datasets, covering diverse domains and kinds of subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often outperforms the prior state-of-the-art in worst-group accuracy. The code is available at https://github.com/minhto2802/dpe4subpop.","Machine learning models often struggle when they encounter situations that differ slightly from what they were trained on. This is a major issue when data includes hidden subgroups, such as different types of people, environments, or medical conditions, that are not equally represented. For example, a model trained mostly on healthy patients might not work well on those with rare diseases.Our research introduces a new technique called the Diversified Prototypical Ensemble (DPE) to tackle this problem. Instead of using just one model, we create a group of simple classifiers called prototypes. Each one learns to focus on different patterns or features in the data. We encourage these classifiers to be as different as possible, so together they can cover a broader variety of hidden subgroups.The key benefit of DPE is that it does not require prior knowledge of the subgroups. It can automatically discover and adapt to them using only the data itself. This makes it especially useful in real-world situations where such subgroup labels are missing or hard to define. Across nine challenging datasets, our method consistently outperforms existing solutions and helps make machine learning models more fair and reliable when used in diverse populations."
Poster,Diversified Flow Matching with Translation Identifiability,https://ICML.cc//virtual/2025/poster/45403,"Sagar Shrestha, Xiao Fu","Diversified distribution matching (DDM) finds a unified translation function mapping a diverse collection of conditional source distributions to their target counterparts. DDM was proposed to resolve content misalignment issues in unpaired domain translation, achieving translation identifiability. However, DDM has only been implemented using GANs due to its constraints on the translation function. GANs are often unstable to train and do not provide the transport trajectory information---yet such trajectories are useful in applications such as single-cell evolution analysis and robot route planning. This work introduces *diversified flow matching* (DFM), an ODE-based framework for DDM. Adapting flow matching (FM) to enforce a unified translation function as in DDM is challenging, as FM learns the translation function's velocity rather than the translation function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and a structural reformulation are proposed to address these challenges, offering a tangible implementation. To our knowledge, DFM is the first ODE-based approach guaranteeing translation identifiability. Experiments on synthetic and real-world datasets validate the proposed method.","We deal with the problem of translating data from one format or domain to another—like turning photos into cartoons or analyzing cellular evolution. Since it is hard to obtain paired data (e.g., photo and cartoon of the same person), existing methods, based on the recent generative model called flow matching (FM), did not allow explicit control over what gets translated to what, e.g., a photo of one person could be translated to a cartoon of a different person resulting in content misalignment. The challenge is that it is not clear how to ""control"" FM based generation. Our paper provides an in-depth analysis of the core issue, and proposes a novel approach to control the FM based generation and produce content aligned translation. Tests with synthetic and real-world examples show it reliably solves previous issues, making it practical and effective for applications ranging from image editing to swarm robot navigation."
Poster,Diversifying Robot Locomotion Behaviors with Extrinsic Behavioral Curiosity,https://ICML.cc//virtual/2025/poster/43824,"Zhenglin Wan, Xingrui Yu, David Bossens, Yueming LYU, Qing Guo, Flint Xiaofeng Fan, Yew Soon ONG, Ivor Tsang","Imitation learning (IL) has shown promise in robot locomotion but is often limited to learning a single expert policy, constraining behavior diversity and robustness in unpredictable real-world scenarios. To address this, we introduce Quality Diversity Inverse Reinforcement Learning (QD-IRL), a novel framework that integrates quality-diversity optimization with IRL methods, enabling agents to learn diverse behaviors from limited demonstrations. This work introduces Extrinsic Behavioral Curiosity (EBC), which allows agents to receive additional curiosity rewards from an external critic based on how novel the behaviors are with respect to a large behavioral archive. To validate the effectiveness of EBC in exploring diverse locomotion behaviors, we evaluate our method on multiple robot locomotion tasks. EBC improves the performance of QD-IRL instances with GAIL, VAIL, and DiffAIL across all included environments by up to 185\%, 42\%, and 150\%, even surpassing expert performance by 20\% in Humanoid. Furthermore, we demonstrate that EBC is applicable to Gradient-Arborescence-based Quality Diversity Reinforcement Learning  (QD-RL) algorithms, where it substantially improves performance and provides a generic technique for diverse robot locomotion. The source code of this work is provided at https://github.com/vanzll/EBC.","Robots often learn how to move by observing expert demonstrations, a process called imitation learning. While this works well in controlled settings, it usually teaches the robot only one way to move. This lack of flexibility makes robots less capable in unpredictable or changing environments.To improve the diversity of robot behaviors, we develop a new method called **Quality Diversity Inverse Reinforcement Learning (QD-IRL)**. This technique allows robots to learn many different ways to move—even from a small number of expert examples—making them more adaptable and robust.As a key part of the QD-IRL algorithm, we propose  **Extrinsic Behavioral Curiosity (EBC)**. EBC rewards the robot for trying new and different movement styles, not just following what it has already learned. It does this using an external system that tracks which behaviors are “novel” and encourages the robot to explore those.We test our approach in various simulated robot environments (like walking, jumping, or adapting to damage) and find that robots trained with EBC display a wider variety of effective movement styles. In fact, in some cases, they even performed better than the original expert demonstrations.While the approach is applied to a particular algorithm called Proximal Policy Gradient Arborescence, the approach can also potentially be used in a wide variety of quality diversity algorithms and in traditional reinforcement learning in addition to imitation learning. In these settings, EBC could be a powerful tool for teaching robots to handle complex, real-world challenges with diverse and creative behavior."
Poster,Diversity By Design: Leveraging Distribution Matching for Offline Model-Based Optimization,https://ICML.cc//virtual/2025/poster/44088,"Michael S Yao, James Gee, Osbert Bastani","The goal of offline model-based optimization (MBO) is to propose new designs that maximize a reward function given only an offline dataset. However, an important desiderata is to also propose a *diverse* set of final candidates that capture many optimal and near-optimal design configurations. We propose **D**iversit**y** I**n** **A**dversarial **M**odel-based **O**ptimization (**DynAMO**) as a novel method to introduce design diversity as an explicit objective into any MBO problem. Our key insight is to formulate diversity as a *distribution matching problem* where the distribution of generated designs captures the inherent diversity contained within the offline dataset. Extensive experiments spanning multiple scientific domains show that DynAMO can be used with common optimization methods to significantly improve the diversity of proposed designs while still discovering high-quality candidates.","When scientists try to design new things (e.g., medicines, materials, or robots), they often rely on computer models trained on previously collected data to suggest promising candidate designs. This is useful because testing each design in real life can be very expensive or slow. However, these computer models are not always accurate. Furthermore, existing methods often 'play it safe' and suggest very similar designs, potentially missing out on other great options.Our research introduces **DynAMO**, a new tool that helps computers suggest a greater *diversity* of potentially high-quality designs. Instead of just looking for the single 'best' solution, DynAMO is an algorithm that explores **many different kinds of good ideas** by mimicking the diversity seen in past successful designs. It also enforces constraints to ensure the proposed designs are realistic and make sense.We tested DynAMO on several real-world design problems, such as engineering DNA sequences and designing new robots, and found that DynAMO suggests more diverse designs without sacrificing their quality.Why does this matter? In real-world innovation, having a broader set of strong options increases the chances of discovering something truly groundbreaking. DynAMO makes that exploration faster and more trustworthy."
Poster,Divide and Conquer: Exploring Language-centric Tree Reasoning for Video Question-Answering,https://ICML.cc//virtual/2025/poster/43530,"Zhaohe Liao, Jiangtong Li, Siyu Sun, Qingyang Liu, Fengshun Xiao, Tianjiao Li, Qiang Zhang, Guang Chen, Li Niu, Changjun Jiang, Liqing Zhang","Video Question-Answering (VideoQA) remains challenging in achieving advanced cognitive reasoning due to the uncontrollable and opaque reasoning processes in existing Multimodal Large Language Models (MLLMs). To address this issue, we propose a novel Language-centric Tree Reasoning (LTR) framework that targets on enhancing the reasoning ability of models. In detail, it recursively divides the original question into logically manageable parts and conquers them piece by piece, enhancing the reasoning capabilities and interpretability of existing MLLMs. Specifically, in the first stage, the LTR focuses on language to recursively generate a language-centric logical tree, which gradually breaks down the complex cognitive question into simple perceptual ones and plans the reasoning path through a RAG-based few-shot approach. In the second stage, with the aid of video content, the LTR performs bottom-up logical reasoning within the tree to derive the final answer along with the traceable reasoning path. Experiments across 11 VideoQA benchmarks demonstrate that our LTR framework significantly improves both accuracy and interpretability compared to state-of-the-art MLLMs. To our knowledge, this is the first work to implement a language-centric logical tree to guide MLLM reasoning in VideoQA, paving the way for language-centric video understanding from perception to cognition.","For human, when asked a complex question related to a visual scene, instead of directly answering the complex question, we naturally breaks down the complex questions into simpler ones, search the result of these simple questions, and progressively reasons the answer of original complex questions based on them. However, the advanced visual understanding models can not proceeds such language-centric structured reasoning. Therefore, in this paper, we introduce such reasoning ability for large language model based visual understanding models by prompting them to emulate the former discribed human reasoning process, and significantly advances the accuracy and interpretability for the best video understanding models."
