type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,When and How Does CLIP Enable Domain and Compositional Generalization?,https://ICML.cc//virtual/2025/poster/45573,"Elias Kempf, Simon Schrodi, Max Argus, Thomas Brox","The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However,  key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric *and* mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits.","CLIP is a widely used foundation model that is often used to help large language models understand images. In this work, we studied how well CLIP generalizes to unseen visual styles (called *domains*) of images of objects and animals, such as photos, drawings, or sketches. We focused on two key questions: (1) Can CLIP generalize to sketches (or any other domain) it has not seen during training? (2) If CLIP has seen sketches of cats and photos, paintings, etc. of both cats and dogs, can it generalize to sketches of dogs?To address these questions, we created carefully curated datasets that control for seen and unseen domains and object/animal classes. Our experiments reaffirmed that including *diverse* domains in training improves generalization. Surprisingly, CLIP can sometimes perform worse when it has *partially* seen a domain than when it has *not seen it at all*. We also found that robust generalization requires learning *sufficiently shared internal representations and mechanisms* across domains.Our findings reveal previously overlooked limitations in CLIP’s generalization and advance our understanding of the factors that affect generalization. These insights can guide the development of models that generalize more reliably across visual domains."
Poster,When Bad Data Leads to Good Models,https://ICML.cc//virtual/2025/poster/45199,"Kenneth Li, Yida Chen, Fernanda Viégas, Martin Wattenberg","In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we re-examine the notion of ""quality"" from the perspective of pre- and post-training co-design. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.","This paper challenges the usual idea that cleaner training data always leads to better language models. It shows that training a model with some toxic data can actually make it easier to reduce harmful outputs later on. Through experiments, the authors find that toxic content helps the model learn clearer representations of toxicity, which in turn makes it easier to control. When post-training methods are used to reduce toxicity, models trained with some toxic data perform better overall---maintaining their abilities while being safer."
Poster,When can in-context learning generalize out of task distribution?,https://ICML.cc//virtual/2025/poster/44914,"Chase Goddard, Lindsay Smith, Wave Ngampruetikorn, David Schwab","In-context learning (ICL) is a remarkable capability of pretrained transformers that allows models to generalize to unseen tasks after seeing only a few examples. We investigate empirically the conditions necessary on the pretraining distribution for ICL to emerge and generalize \emph{out-of-distribution}. Previous work has focused on the number of distinct tasks necessary in the pretraining dataset. Here, we use a different notion of task diversity to study the emergence of ICL in transformers trained on linear functions. We find that as task diversity increases, transformers undergo a transition from a specialized solution, which exhibits ICL only within the pretraining task distribution, to a solution which generalizes out of distribution to the entire task space. We also investigate the nature of the solutions learned by the transformer on both sides of the transition, and observe similar transitions in nonlinear regression problems. We construct a phase diagram to characterize how our concept of task diversity interacts with the number of pretraining tasks. In addition, we explore how factors such as the depth of the model and the dimensionality of the regression problem influence the transition.","Modern machine learning methods, including transformers, often display a capability called in-context learning (ICL). ICL is when a machine learning model learns to perform a new task by simply seeing a few examples of that task within the instructions you give it, without needing to be completely retrained. This capability makes AI models much more flexible and efficient, because completely retraining an AI from scratch is often costly and time-consuming.One natural scientific question is to ask what the conditions are in order for ICL to appear. In order to answer this, it helps to simplify the tasks being considered by the model -- following earlier work, we use a mathematically simple task based on linear regression. In this simple setting, we investigate the conditions under which models are able to learn in-context based on what tasks they see during their initial training period. While earlier work demonstrated that the model must see a sufficient number of tasks during the initial training, we show that models must also see tasks that are sufficiently diverse: the tasks must be different enough from each other. Importantly, this diversity between tasks allows the model to perform well not only on tasks similar to the training tasks, but also on tasks that are ""far away"" from those tasks the model has already seen. Our work is an important step in understanding *how* artificial intelligence methods work — understanding how these systems operate can help build trust in AI and ensure AI safety."
Poster,When Can Proxies Improve the Sample Complexity of Preference Learning?,https://ICML.cc//virtual/2025/poster/46116,"Yuchen Zhu, Daniel Augusto de Souza, Zhengyan Shi, Mengyue Yang, Pasquale Minervini, Matt Kusner, Alexander D&#x27;Amour","We address the problem of reward hacking, where maximising a proxy reward does not necessarily increase the true reward. This is a key concern for Large Language Models (LLMs), as they are often fine-tuned on human preferences that may not accurately reflect a true objective. Existing work uses various tricks such as regularisation, tweaks to the reward model, and reward hacking detectors, to limit the influence that such proxy preferences have on a model. Luckily, in many contexts such as medicine, education, and law, a sparse amount of expert data is often available. In these cases, it is often unclear whether the addition of proxy data can improve policy learning. We outline a set of sufficient conditions on proxy feedback that, if satisfied, indicate that proxy data can provably improve the sample complexity of learning the ground truth policy. These conditions can inform the data collection process for specific tasks. The result implies a parameterisation for LLMs that achieves this improved sample complexity. We detail how one can adapt existing architectures to yield this improved sample complexity.","We address the challenge of *reward hacking*, where AI models, such as large language models (LLMs), optimize for proxy rewards—like human preferences—that don’t always align with the true objective. This is especially relevant when LLMs are fine-tuned using human feedback, which may be biased or incomplete. While existing methods try to reduce this issue using techniques like regularization or reward model adjustments, we focus on a different angle. In fields like medicine, education, or law, small amounts of expert data are often available alongside less reliable proxy data. It’s not always clear whether using this additional proxy feedback helps or hurts learning. We identify a set of conditions under which proxy data can *reliably* improve learning efficiency—reducing the amount of expert data needed. These findings can guide how feedback is collected and used. We also describe how to adapt current LLM architectures to benefit from these insights and achieve better learning outcomes."
Poster,When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need,https://ICML.cc//virtual/2025/poster/43988,"Ziming Hong, Runnan Chen, Zengmao Wang, Bo Han, Bo Du, Tongliang Liu","Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator's attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers.","We present the first study on distilling non-transferable learning (NTL) teachers using data-free knowledge distillation (DFKD). We show that NTL teachers can mislead DFKD by diverting its focus from meaningful in-distribution (ID) knowledge to misleading out-of-distribution (OOD) knowledge (i.e., OOD trap effect), raising broader concerns about the trustworthiness, safety, and robustness of existing DFKD methods. To tackle this issue, we propose Adversarial Trap Escaping (ATEsc)—a plug-and-play module that identifies and filters out OOD-like synthetic samples, thereby calibrating DFKD to focus on learning correct ID knowledge. Experimental results demonstrate that ATEsc significantly improves the performance of DFKD when applied to NTL teachers."
Poster,When Diffusion Models Memorize: Inductive Biases in Probability Flow of Minimum-Norm Shallow Neural Nets,https://ICML.cc//virtual/2025/poster/45032,"Chen Zeno, Hila Manor, Gregory Ongie, Nir Weinberger, Tomer Michaeli, Daniel Soudry","While diffusion models generate high-quality images via probability flow, the theoretical understanding of this process remains incomplete. A key question is when probability flow converges to training samples or more general points on the data manifold.We analyze this by studying the probability flow of shallow ReLU neural network denoisers trained with minimal $\ell^2$ norm. For intuition, we introduce a simpler score flow and show that for orthogonal datasets, both flows follow similar trajectories, converging to a training point or a sum of training points. However, early stopping by the diffusion time scheduler allows probability flow to reach more general manifold points.This reflects the tendency of diffusion models to both memorize training samples and generate novel points that combine aspects of multiple samples, motivating our study of such behavior in simplified settings. We extend these results to obtuse simplex data and, through simulations in the orthogonal case, confirm that probability flow converges to a training point, a sum of training points, or a manifold point. Moreover, memorization decreases when the number of training samples grows, as fewer samples accumulate near training points.","Diffusion models are a popular type of generative AI that create realistic images through a gradual process of refining random noise into structure. Although they perform remarkably well, researchers still don’t fully understand why these models are so effective. A central open question is whether diffusion models are simply memorizing training images or generating new ones by blending features from multiple examples.To explore this, we study a simplified version of a diffusion model using small neural networks. We examine how these models behave over time, observing whether they return to exact training examples or converge on new, intermediate points that mix features from several images. Our findings show that both memorization and creative generalization can occur, depending on how long the generation process is allowed to run.These insights help explain how diffusion models can produce both familiar-looking and entirely novel images, and offer a better understanding of the trade-offs in their behavior."
Poster,When Do LLMs Help With Node Classification? A Comprehensive Analysis,https://ICML.cc//virtual/2025/poster/45461,"Xixi Wu, Yifei Shen, Fangzhou Ge, Caihua Shan, Yizhu Jiao, Xiangguo Sun, Hong Cheng","Node classification is a fundamental task in graph analysis, with broad applications across various fields. Recent breakthroughs in Large Language Models (LLMs) have enabled LLM-based approaches for this task. Although many studies demonstrate the impressive performance of LLM-based methods, the lack of clear design guidelines may hinder their practical application.  In this work, we aim to establish such guidelines through a fair and systematic comparison of these algorithms. As a first step, we developed LLMNodeBed, a comprehensive codebase and testbed for node classification using LLMs. It includes 10 homophilic datasets, 4 heterophilic datasets, 8 LLM-based algorithms, 8 classic baselines, and 3 learning paradigms. Subsequently, we conducted extensive experiments, training and evaluating over 2,700 models, to determine the key settings (e.g., learning paradigms and homophily) and components (e.g., model size and prompt) that affect performance. Our findings uncover 8 insights, e.g., (1) LLM-based methods can significantly outperform traditional methods in a semi-supervised setting, while the advantage is marginal in a supervised setting; (2) Graph Foundation Models can beat open-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot setting. We hope that the release of LLMNodeBed, along with our insights, will facilitate reproducible research and inspire future studies in this field. Codes and datasets are released at https://llmnodebed.github.io/.","With the rapid advancements in large language models (LLMs), we wanted to explore their potential and benefits for the task of node classification, a key problem in machine learning on graphs. To do this, we developed LLMNodeBed, a comprehensive codebase and testbed designed to evaluate LLMs for node classification.LLMNodeBed includes 14 datasets, 8 LLM-based algorithms, 8 classic baselines, and 3 learning paradigms. Using this testbed, we trained and evaluated over 2,700 models to understand the impact of factors such as learning paradigms, graph homophily, language model type and size, and prompt design on performance.Our findings provide 8 novel insights, along with intuitive explanations and practical guidelines for applying LLM-based algorithms in real-world settings. This work offers a valuable resource for researchers and practitioners aiming to leverage LLMs for graph-related tasks in diverse applications."
Poster,When do neural networks learn world models?,https://ICML.cc//virtual/2025/poster/44058,"Tianren Zhang, Guanyu Chen, Feng Chen","Humans develop _world models_ that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a _multi-task_ setting, models with a _low-degree bias_ provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.","Will neural networks trained on trillions of data points ""understand"" the world behind the data like humans? We take an initial step toward formulating and answering this question by leveraging a universal learning tendency shared by both neural networks and humans: preferring ""simple"" solutions among all solutions that explain the data.Our paper’s main result shows that a neural network model can understand the data--in the sense of recovering the underlying data-generating process--if two conditions are met: the model prefers ""simple"" solutions, and it is trained on a sufficient number of tasks. Our second result shows that the model’s architecture also influences its understanding of the data.These findings have implications for assessing what modern neural networks actually learn from data and how they make predictions on previously unseen data. As a by-product, we also find a natural way to define ""simplicity"" that may apply to other scenarios."
Poster,When Dynamic Data Selection Meets Data Augmentation: Achieving Enhanced Training Acceleration,https://ICML.cc//virtual/2025/poster/44959,"Suorong Yang, Peng Ye, Furao Shen, Dongzhan Zhou","Dynamic data selection aims to accelerate training with lossless performances.However, reducing training data inherently limits data diversity, potentially hindering generalization.While data augmentation is widely used to enhance diversity, it is typically not optimized in conjunction with selection.As a result, directly combining these techniques fails to fully exploit their synergies.To tackle the challenge, we propose a novel online data training framework that, for the first time, unifies dynamic data selection and augmentation, achieving both training efficiency and enhanced performance.Our method estimates each sample's joint distribution of local density and multimodal semantic consistency, allowing for the targeted selection of augmentation-suitable samples while suppressing the inclusion of noisy or ambiguous data.This enables a more significant reduction in dataset size without sacrificing model generalization.Experimental results demonstrate that our method outperforms existing state-of-the-art approaches on various benchmark datasets and architectures, e.g., reducing 50% training costs on ImageNet-1k with lossless performance.Furthermore, our approach enhances noise resistance and improves model robustness, reinforcing its practical utility in real-world scenarios.","Training powerful AI models usually takes massive amounts of data and time. But not all data are equally helpful — some are noisy, repetitive, or hard to learn from. While data selection techniques aim to speed up training by removing less useful samples, this can hurt performance by reducing data diversity. On the other hand, data augmentation improves diversity, but is rarely coordinated with selection.In this work, we propose a new approach that combines both strategies in a smart, unified way. Our method evaluates how rare and semantically consistent each data point is — for example, whether the image content matches its label — and selects only those that are both informative and suitable for transformation. This ensures the model trains on diverse, high-quality data without being distracted by noise.Our method significantly reduces training time, especially on large-scale datasets, while maintaining or even improving model performance. This paves the way for more efficient, robust, and accessible AI training in real-world applications."
Poster,When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network,https://ICML.cc//virtual/2025/poster/44056,"Dong Xiao, Guangyao Chen, Peixi Peng, Yangru Huang, Yifan Zhao, Yongxing Dai, Yonghong Tian","Anomaly detection is essential for the safety and reliability of autonomous driving systems. Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios. In this paper, we introduce real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy. We propose a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images. This combination effectively captures both the temporal dynamics and spatial details of the driving environment, enabling swift and precise anomaly detection. Extensive experiments on benchmark datasets show that our approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance.","Ensuring the safety of self-driving cars relies on their ability to quickly notice and respond to unexpected events, like a pedestrian stepping into the road or another car making a sudden move. Our research introduces a new approach that allows autonomous vehicles to detect these anomalies both quickly and accurately. We achieve this by combining two types of data: standard video from regular cameras and rapid, detailed signals from special sensors called event cameras. Event cameras can capture tiny changes in the environment almost instantly, while regular cameras provide a broader view.By merging these two sources of information in a specially designed system, our method lets self-driving cars recognize dangers in real time, responding within just milliseconds. This advancement brings us closer to safer roads, as self-driving cars become better equipped to handle sudden risks and prevent accidents before they happen."
