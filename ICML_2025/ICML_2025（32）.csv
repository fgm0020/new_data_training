type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,BAME: Block-Aware Mask Evolution for Efficient N:M Sparse Training,https://ICML.cc//virtual/2025/poster/44977,"Chenyi yang, Wenjie Nie, Yuxin Zhang, Yuhang Wu, Xiawu Zheng, GUANNAN JIANG, Rongrong Ji","N:M sparsity stands as a progressively important tool for DNN compression, achieving practical speedups by stipulating at most N non-zero components within M sequential weights. Unfortunately, most existing works identify the N:M sparse mask through dense backward propagation to update all weights, which incurs exorbitant training costs. In this paper, we introduce BAME, a method that maintains consistent sparsity throughout the N:M sparse training process. BAME perpetually keeps both sparse forward and backward propagation, while iteratively performing weight pruning-and-regrowing within designated weight blocks to tailor the N:M mask. These blocks are selected through a joint assessment based on accumulated mask oscillation frequency and expected loss reduction of mask adaptation, thereby ensuring stable and efficient identification of the optimal N:M mask. Our empirical results substantiate the effectiveness of BAME, illustrating it performs comparably to or better than previous works that fully maintaining dense backward propagation during training. For instance, BAME attains a 72.0% top-1 accuracy while training a 1:16 sparse ResNet-50 on ImageNet, eclipsing SR-STE by 0.5%, despite achieving 2.37 training FLOPs reduction. Code is released at \url{https://github.com/BAME-xmu/BAME}","Deep neural networks often use a technique called N:M sparsity to reduce their size and speed up computations by keeping only a small number of important weights (N) in each group of M weights. However, current methods require expensive training processes that update all weights, even the unimportant ones. We introduce BAME, a new approach that keeps the network sparse from start to finish. Instead of updating all weights, BAME continuously prunes and regrows weights within small weight blocks, focusing only on the most useful ones. It selects these blocks based on how often they change and how much they improve performance, making training faster and more stable. Our experiments show that BAME works as well as—or even better than—previous methods while being much more efficient."
Poster,BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms,https://ICML.cc//virtual/2025/poster/44460,"Yunlong Hou, Fengzhuo Zhang, Cunxiao Du, Xuan Zhang, Jiachun Pan, Tianyu Pang, Chao Du, Vincent Tan, Zhuoran Yang","Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both  stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and  the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.","Large Language Models (LLMs), such as ChatGPT, generate text by predicting one word at a time, which can be slow. A method called speculative decoding speeds this up by using a lightweight “draft” model to guess several words ahead, then verifying them with the full model. However, current speculative decoding methods often use a fixed setup for all prompts, regardless of the task — whether it’s writing code or generating stories. This limits their effectiveness. Our work introduces BanditSpec, a smarter and more adaptive speculative decoding framework inspired by multi-armed bandit algorithms — a type of decision-making strategy that balances exploration and exploitation. BanditSpec dynamically learns which decoding configuration works best for each prompt as generation progresses, without requiring extra training. We design two algorithms, UCBSpec and EXP3Spec, that select the best setup in real time. Experiments with popular models like LLaMA3 and Qwen2 show that BanditSpec significantly improves text generation speed and closely matches the performance of the best possible fixed setup (the ""oracle""). In summary, BanditSpec makes LLMs faster by learning how to guess better -- on the fly."
Poster,BAnG: Bidirectional Anchored Generation for Conditional RNA Design,https://ICML.cc//virtual/2025/poster/44645,"Roman Klypa, Alberto Bietti, Sergei Grudinin","Designing RNA molecules that interact with specific proteins is a critical challenge in experimental and computational biology. Existing computational approaches require a substantial amount of experimentally determined RNA sequences for each specific protein or a detailed knowledge of RNA structure, restricting their utility in practice. To address this limitation, we develop RNA-BAnG, a deep learning-based model designed to generate RNA sequences for protein interactions without these requirements. Central to our approach is a novel generative method, Bidirectional Anchored Generation (BAnG), which leverages the observation that protein-binding RNA sequences often contain functional binding motifs embedded within broader sequence contexts. We first validate our method on generic synthetic tasks involving similar localized motifs to those appearing in RNAs, demonstrating its benefits over existing generative approaches. We then evaluate our model on biological sequences, showing its effectiveness for conditional RNA sequence design given a binding protein.","Designing RNA molecules that bind to specific proteins is important for both understanding biology and developing new therapies. Experimental approaches, however, are often time-consuming and costly. While AI tools offer a faster alternative, current models face significant limitations. Some of them require custom training for each protein, which requires data that is often unavailable. Others rely on having the precise 3D structure of the RNA, a detail that is rarely accessible.To overcome these barriers, we developed a new way to generate RNA sequences by starting at the point where the RNA binds to the protein and building outward. This approach outperforms existing methods on simplified tasks and enables us to build a general model, RNA-BAnG, that works on any protein without extra training. We computationally validated its performance on real biological data and showed it can successfully design binding RNA sequences for a wide range of proteins.By removing the need for hard-to-get data and making RNA design more flexible, RNA-BAnG offers a powerful new tool for biology and medicine. To support further research and applications, we released our model and code publicly. We hope our results will encourage experimental biologists to validate RNA-BAnG in the lab and explore new ways to use it in RNA research and medicine."
Poster,Banyan: Improved Representation Learning with Explicit Structure,https://ICML.cc//virtual/2025/poster/46480,"Mattia Opper, Siddharth N","We present Banyan, a model that efficiently learns semantic representations by leveraging explicit hierarchical structure. While transformers excel at scale, they struggle in low-resource settings. Conversely recent structured models have shown promise as efficient learners, but lack performance. Banyan bridges this gap with two key innovations: an entangled hierarchical tree structure and diagonalized message passing, enabling it to outperform larger transformer models with just 14 non-embedding parameters. It excels in low-resource settings, offering a viable alternative for under-represented languages and highlighting its potential for efficient, interpretable NLP in resource-constrained environments.","We present a model called Banyan, which utilises a new architecture to efficiently learn embeddings - useful for comparing how similar to pieces of text are to each other, for example in search or retrieval. For languages that are well resourced like English, existing models already do well, but in low resource settings they fail because they really on scale to succeed. It is precisely here that Banyan shines, because it can be optimised without much compute or data and still be highly effective."
Poster,BARK: A Fully Bayesian Tree Kernel for Black-box Optimization,https://ICML.cc//virtual/2025/poster/46003,"Toby Boyne, Jose Pablo Folch, Robert Lee, Behrang Shafei, Ruth Misener","We perform Bayesian optimization using a Gaussian process perspective on Bayesian Additive Regression Trees (BART). Our BART Kernel (BARK) uses tree agreement to define a posterior over piecewise-constant functions, and we explore the space of tree kernels using a Markov chain Monte Carlo approach. Where BART only samples functions, the resulting BARK model obtains samples of Gaussian processes defining distributions over functions, which allow us to build acquisition functions for Bayesian optimization. Our tree-based approach enables global optimization over the surrogate, even for mixed-feature spaces. Moreover, where many previous tree-based kernels provide uncertainty quantification over function values, our sampling scheme captures uncertainty over the tree structure itself. Our experiments show the strong performance of BARK on both synthetic and applied benchmarks, due to the combination of our fully Bayesian surrogate and the optimization procedure.","Bayesian optimization (BO) is a powerful tool for optimizing unknown functions, such as the maximizing the yield of a product in a chemistry experiment. Many real-world problems have a mixed structure to - for example, a chemical reaction may have temperature as a continuous input, and choice of catalyst as a categorical input. Modeling and optimizing over these mixed spaces is an emerging field in BO.We propose a new model, BARK, that combines the modeling power of tree-based methods with the strong uncertainty quantification of Gaussian processes (GP), which are the typical model of choice in BO. By taking inspiration from Bayesian tree approaches, we improve over past work by capturing the uncertainty in the tree structure itself, which allows us to better understand which new potential experiments are worth exploring. Moreover, we can formulate the optimization as a 'mixed integer program', which allows the optimization to be solved globally, without needing gradients. We show competitive performance in a selection of synthetic and applied BO problems, outperforming current state-of-the-art tree based methods."
Poster,BARNN: A Bayesian Autoregressive and Recurrent Neural Network,https://ICML.cc//virtual/2025/poster/44327,"Dario Coscia, Max Welling, Nicola Demo, Gianluigi Rozza","Autoregressive and recurrent networks have achieved remarkable progress across various fields, from weather forecasting to molecular generation and Large Language Models. Despite their strong predictive capabilities, these models lack a rigorous framework for addressing uncertainty, which is key in scientific applications such as PDE solving, molecular generation and machine l earning Force Fields.To address this shortcoming we present BARNN: a variational Bayesian Autoregressive and Recurrent Neural Network. BARNNs aim to provide a principled way to turn any autoregressive or recurrent model into its Bayesian version. BARNN is based on the variational dropout method, allowing to apply it to large recurrent neural networks as well. We also introduce a temporal version of the“Variational Mixtures of Posteriors” prior (tVAMP-prior) to make Bayesian inference efficient and well-calibrated. Extensive experiments on PDE modelling and molecular generation demonstrate that BARNN not only achieves comparable or superior accuracy compared to existing methods, but also excels in uncertainty quantification and modelling long-range dependencies.","Sequence models in deep learning drive advances in areas like weather forecasting and molecular design, but they lack reliable ways to quantify uncertainty, which is crucial for scientific applications. We introduce BARNN, a Bayesian approach that equips these models with well-calibrated uncertainty estimates, enabling more trustworthy predictions in scientific applications."
Poster,Batch List-Decodable Linear Regression via Higher Moments,https://ICML.cc//virtual/2025/poster/45045,"Ilias Diakonikolas, Daniel Kane, Sushrut Karmalkar, Sihan Liu, Thanasis Pittas","We study the task of list-decodable linear regression using batches, recently introduced by Das et al. 2023.. In this setting, we are given $m$ batches with each batch containing $n$ points in $\mathbb R^d$. A batch is called clean if the points it contains are i.i.d. samples from an unknown linear regression distribution. For a parameter $\alpha \in (0, 1/2)$, an unknown $\alpha$-fraction of the batches are clean and no assumptions are made on the remaining batches. The goal is to output a small list of vectors at least one of which is close to the true regressor vector in $\ell_2$-norm. Das et al. 2023 gave an efficient algorithm for this task, under natural distributional assumptions, with the following guarantee. Under the assumption that the batch size satisfies $n \geq \tilde{\Omega}(\alpha^{-1})$ and the total number of batches is $m = \text{poly}(d, n, 1/\alpha)$, their algorithm runs in polynomial time and outputs a list of $O(1/\alpha^2)$ vectors at least one of which is $\tilde{O}(\alpha^{-1/2}/\sqrt{n})$ close to the target regressor. Here we design a new polynomial-time algorithm for this task with significantly stronger guarantees under the assumption that the low-degree moments of the covariates distribution are Sum-of-Squares (SoS) certifiably bounded.Specifically, for any constant $\delta>0$, as long as the batch size is $n \geq \Omega_{\delta}(\alpha^{-\delta})$and the degree-$\Theta(1/\delta)$ moments of the covariates are SoS certifiably bounded, our algorithm uses $m = \text{poly}((dn)^{1/\delta}, 1/\alpha)$ batches,runs in polynomial-time, and outputs an $O(1/\alpha)$-sized list of vectors one of which is $O(\alpha^{-\delta/2}/\sqrt{n})$ close to the target. That is, our algorithm substantially improves both the minimum batch size and the final error guarantee, while achieving the optimal list size. Our approach leverages higher-order moment information by carefully combining the SoS paradigm interleaved with an iterative method and a novel list  pruning procedure for this setting. In the process, we give an SoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader applicability.",This paper extends the algorithm toolbox for statistical tasks in crowdsource settings.
Poster,BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation,https://ICML.cc//virtual/2025/poster/44892,"Lian Liu, Xiandong Zhao, Guanchen Li, Dong Li, Wang, Yinhe Han, Xiaowei Li, ying wang","One-shot post-training pruning enhances the deployment of billion-scale large language models (LLMs), with the pruning metric playing a pivotal role in determining which weights to remove. However, existing metrics underperform due to their reliance on a simple symbolic combination of weights and activations, overlooking imbalanced weight magnitudes and the disproportionate influence of activation outliers.To overcome these limitations, we introduce \textbf{BaWA}, a novel pruning metric that systematically \underline{Ba}lances \underline{W}eight and \underline{A}ctivation distributions for more effective pruning.BaWA introduces two key innovations: \textbf{magnitude normalization}, which mitigates weight imbalance across channels for fairer pruning decisions, and \textbf{outlier regularization}, which reduces the impact of activation outliers, ensuring more appropriate channel prioritization. To further enhance its effectiveness, BaWA incorporates an efficient and automatic framework for optimizing normalization and regularization hyperparameters. Extensive experiments validate BaWA as a state-of-the-art (SOTA) pruning metric. For instance, applying BaWA to induce 2:4 sparsity in Mistral-7B reduces perplexity in language comprehension by 2.49 and improves average downstream task accuracy by 3.08\%, outperforming the previous SOTA method Wanda.","This paper introduces a new method called BaWA to make large AI language models smaller and more efficient without losing performance. Large models like ChatGPT have billions of parameters, making them slow and resource-heavy. Traditional methods for simplifying these models often remove parts unevenly, either cutting too much from some areas or missing less obvious but important components. BaWA solves this by balancing two key factors: the size of the model’s internal parameters (weights) and the impact of unusual data points (outliers) during calculations. It adjusts how much each part of the model matters, ensuring fairer decisions about what to remove. Additionally, BaWA automatically fine-tunes its settings to find the best balance, taking just minutes to optimize even for huge models. Tests show BaWA outperforms existing methods. For example, when applied to the Mistral-7B model, it reduced errors in language understanding by 2.49 points and improved accuracy on tasks by 3.08% compared to the previous best method. It works well across different model sizes and can be combined with other techniques for even better results. This advancement helps deploy powerful AI models faster and cheaper, especially on devices with limited resources like phones or laptops."
Poster,BaxBench: Can LLMs Generate Correct and Secure Backends?,https://ICML.cc//virtual/2025/poster/44337,"Mark Vero, Niels Mündler, Viktor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanović, Jingxuan He, Martin Vechev","Automatic program generation has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could successfully execute security exploits on around half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.","A longstanding goal of computer science is the automation of software generation. On current coding benchmarks, often focused on short and algorithmic coding tasks, it seems that large language models (e.g., ChatGPT) are making decisive progress into this direction. In our paper, we examine this progress from a critical angle, constructing a benchmark for testing the models’ capabilities of generating correct and secure lone-standing software backend modules. This is a key task in modern modular software development—correctness ensures that users do not encounter issues during using the software, and security implies that the service and its users are protected from malicious actors. To this end, we define 28 coding scenarios asking for the implementation of modular tasks, such as programming a login, calculator, or an email unsubscription module. Then, we task the models to implement these 28 scenarios in 14 different backend development web-frameworks, such as Python Django, JavaScript Nest, or Go Fiber. We test the correctness and security of the models using concrete inputs to the programmed modules. Evaluating 11 LLMs on our benchmark, we find that none of them perform to a satisfactory level, with the models generating incorrect or insecure solutions more than 60% of the time. In our experiments, we observe the promise of increasing the time and resource spent at generating the solutions for solving these issues, and give concrete guidance for developers on enhancing the secure and correct coding capabilities of their models."
Poster,Bayesian Active Learning for Bivariate Causal Discovery,https://ICML.cc//virtual/2025/poster/43615,"Yuxuan Wang, Mingzhou Liu, Xinwei Sun, Wei Wang, Yizhou Wang","Determining the direction of relationships between variables is fundamental for understanding complex systems across scientific domains. While observational data can uncover relationships between variables, it cannot distinguish between cause and effect without experimental interventions. To effectively uncover causality, previous works have proposed intervention strategies that sequentially optimize the intervention values. However, most of these approaches primarily maximized information-theoretic gains that may not effectively measure the reliability of direction determination. In this paper, we formulate the causal direction identification as a hypothesis-testing problem, and propose a Bayes factor-based intervention strategy, which can quantify the evidence strength of one hypothesis (*e.g.*, causal) over the other (*e.g.*, non-causal). To balance the immediate and future gains of testing strength, we propose a sequential intervention objective over intervention values in multiple steps. By analyzing the objective function, we develop a dynamic programming algorithm that reduces the complexity from non-polynomial to polynomial. Experimental results on bivariate systems, tree-structured graphs, and an embodied AI environment demonstrate the effectiveness of our framework in direction determination and its extensibility to both multivariate settings and real-world applications.","Figuring out ""what causes what"" is a big challenge in science, like whether a gene causes a disease. Just looking at data often isn't enough, and doing experiments can be very expensive. Current smart methods for choosing experiments might focus on gathering the most information, rather than directly finding the correct cause-and-effect relationship most reliably.We've developed a smarter way to design these investigative experiments. Our method uses a mathematical tool called ""Bayes factors"" to weigh the evidence from each small experiment, much like a detective evaluating clues to solve a case. It intelligently plans the sequence of experiments to quickly make good judgments while ensuring we can reach a confident conclusion within a limited budget.This new approach helps scientists uncover causal links in complex systems – from gene networks to how robots understand their surroundings – more efficiently and cost-effectively. Our tests in simulations and robotic tasks show its effectiveness and potential for broad real-world applications."
