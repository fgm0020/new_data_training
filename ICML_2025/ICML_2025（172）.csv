type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Linear Bandits with Partially Observable Features,https://ICML.cc//virtual/2025/poster/45420,"Wonyoung Kim, Sungwoo PARK, Garud Iyengar, Assaf Zeevi, Min-hwan Oh","We study the linear bandit problem that accounts for partially observable features. Without proper handling, unobserved features can lead to linear regret in the decision horizon $T$, as their influence on rewards is unknown.To tackle this challenge, we propose a novel theoretical framework and an algorithm with sublinear regret guarantees.The core of our algorithm consists of: (i) feature augmentation, by appending basis vectors that are orthogonal to the row space of the observed features; and (ii) the introduction of a doubly robust estimator.Our approach achieves a regret bound of $\tilde{O}(\sqrt{(d + d_h)T})$, where $d$ denotes the dimension of the observed features, and $d_h$ represents the number of nonzero coefficients in the parameter associated with the reward component projected onto the subspace orthogonal to the row space spanned by the observed features.Notably, our algorithm requires no prior knowledge of the unobserved feature space, which may expand as more features become hidden.Numerical experiments confirm that our algorithm outperforms both non-contextual multi-armed bandits and linear bandit algorithms depending solely on observed features.","In many decision-making systems—like recommending advertisements—outcomes depend not only on observable information but also on hidden factors. For example, in advertising, unobserved factors such as emotional appeal or creative design can significantly influence users' click-through rates. Most existing methods assume decisions are based solely on visible data, but ignoring hidden factors can lead to poor outcomes.Our research aims to address this limitation. We develop a method that makes effective and efficient decision-making even when information is unobservable. Rather than attempting to recover the hidden factors directly, our approach leverages observed data in a way that indirectly accounts for the unobserved parts. Additionally, we incorporate a technique that reduces errors caused by missing information.This work matters because it aligns more closely with real-world decision-making, where not everything is observable. Our method does not rely on assumptions about the nature of hidden factors, thus broadly applicable. We demonstrate that our approach consistently outperforms conventional methods across various settings, both theoretically and empirically, enabling systems to make more accurate decisions even without access to the full picture."
Poster,Linear Contextual Bandits With Interference,https://ICML.cc//virtual/2025/poster/44517,"Yang Xu, Wenbin Lu, Rui Song","Interference, a key concept in causal inference, extends the reward modeling process by accounting for the impact of one unit's actions on the rewards of others. In contextual bandit (CB) settings where multiple units are present in the same round, interference can significantly affect the estimation of expected rewards for different arms, thereby influencing the decision-making process. Although some prior work has explored multi-agent and adversarial bandits in interference-aware settings, how to model interference in CB remains significantly underexplored. In this paper, we introduce a systematic framework to address interference in Linear CB (LinCB), bridging the gap between causal inference and online decision-making. We propose a series of algorithms that explicitly quantify the interference effect in the reward modeling process and provide comprehensive theoretical guarantees, including sublinear regret bounds, finite sample upper bounds, and asymptotic properties. The effectiveness of our approach is demonstrated through simulations and a synthetic data generated based on MovieLens data.","Ever noticed how watching a movie with someone can change how much you enjoy it? Or how one person’s choice to quarantine can affect the health of those around them? These kinds of ripple effects, known as ""interference"" in causal inference, are everywhere in real life.In many real-world scenarios like these, existing methods don’t make decisions with the goal of maximizing overall outcomes. Instead, they often handle individuals one at a time and ignore how people might influence each other. Even more, these methods often fall short in capturing the personalized differences that matter in decision-making.We set out to change that. Our research introduces a simple yet effective structure to capture interference effects, allowing long-term learning systems like bandits to adapt more intelligently as they collect experience. Through rigorous theoretical analysis, we show that our approach is not only intuitive but also statistically grounded, with clear uncertainty quantification and strong performance guarantees.Our findings show that this broader framework consistently outperforms traditional linear contextual bandit methods. It enables more coordinated, robust, and effective decision-making across systems where individuals interact and affect one another."
Poster,Linear convergence of Sinkhorn's algorithm for generalized static Schrödinger bridge,https://ICML.cc//virtual/2025/poster/46671,"Rahul Choudhary, Hanbaek Lyu","The classical static Schrödinger Bridge (SSB) problem, which seeks the most likely stochastic evolution between two marginal probability measures, has been studied extensively in the optimal transport and statistical physics communities, and more recently in machine learning communities in the surge of generative models. The standard approach to solve SSB is to first identify its Kantorovich dual and use Sinkhorn's algorithm to find the optimal potential functions. While the original SSB is only a strictly convex minimization problem, this approach is known to warrant linear convergence under mild assumptions. In this work, we consider a generalized SSB allowing any strictly increasing divergence functional, far generalizing the entropy functional $x\log x$ in the standard SSB. This problem naturally arises in a wide range of seemingly unrelated problems in entropic optimal transport, random graphs/matrices, and combinatorics. We establish Kantorovich duality and linear convergence of Sinkhorn's algorithm for the generalized SSB problem under mild conditions. Our results provide a new rigorous foundation for understanding Sinkhorn-type iterative methods in the context of large-scale generalized Schrödinger bridges.","How do you transform one set of possibilities into another — like moving a cloud of particles, reshaping an image, or generating new data? Problems like this lie at the heart of physics, statistics, and machine learning. A popular method called Sinkhorn’s algorithm solves such tasks by computing the most efficient transition between probability distributions - but traditionally it relies on a single mathematical notion of difference, called KL divergence.Our research circumvents this limitation. We generalize the Schrödinger Bridge framework, allowing a wide range of ways to measure how distributions differ — better matching the needs of complex real-world systems. Crucially, we prove that our generalized Sinkhorn algorithm still converges quickly, ensuring speed and scalability.This flexibility opens new possibilities for building faster, more accurate generative models, simulating physical systems, and solving modern AI problems."
Poster,Linearization Turns Neural Operators into Function-Valued Gaussian Processes,https://ICML.cc//virtual/2025/poster/46474,"Emilia Magnani, Marvin Pförtner, Tobias Weber, Philipp Hennig","Neural operators generalize neural networks to learn mappings between function spaces from data. They are commonly used to learn solution operators of parametric partial differential equations (PDEs) or propagators of time-dependent PDEs. However, to make them useful in high-stakes simulation scenarios, their inherent predictive error must be quantified reliably. We introduce LUNO, a novel framework for approximate Bayesian uncertainty quantification in trained neural operators. Our approach leverages model linearization to push (Gaussian) weight-space uncertainty forward to the neural operator's predictions.We show that this can be interpreted as a probabilistic version of the concept of currying from functional programming, yielding a function-valued (Gaussian) random process belief. Our framework provides a practical yet theoretically sound way to apply existing Bayesian deep learning methods such as the linearized Laplace approximation to neural operators. Just as the underlying neural operator, our approach is resolution-agnostic by design.The method adds minimal prediction overhead, can be applied post-hoc without retraining the network, and scales to large models and datasets.We evaluate these aspects in a case study on Fourier neural operators.","Neural operators are a powerful class of AI models that can learn to predict the solutions of complex physical systems, like how fluids move or how temperatures evolve over time. They are especially useful for scientific applications because they can handle entire families of equations rather than just individual problems. However, these models currently offer no indication of how confident they are in their predictions. This is a serious issue when used in safety-critical areas like climate modeling or engineering design. To address this, we introduce LUNO, a method that equips neural operators with uncertainty estimates.  The key idea is to mathematically linearize the model around its learned parameters, allowing us to translate uncertainty in the model's internal settings into a structured, probabilistic description of its predictions. This turns the neural operator into a kind of function-valued Gaussian process — a well-established tool for modeling uncertainty — but extended to the setting of operators. Our experiments show that LUNO delivers reliable uncertainty estimates with minimal computational overhead. This enables safer decision-making, more efficient data collection, and builds trust in AI tools used for scientific discovery."
Poster,Linear Mode Connectivity between Multiple Models modulo Permutation Symmetries,https://ICML.cc//virtual/2025/poster/43933,"Akira Ito, Masanori Yamada, Atsutoshi Kumagai","Ainsworth et al. empirically demonstrated that linear mode connectivity (LMC) can be achieved between two independently trained neural networks (NNs) by applying an appropriate parameter permutation. LMC is satisfied if a linear path with non-increasing test loss exists between the models, suggesting that NNs trained with stochastic gradient descent (SGD) converge to a single approximately convex low-loss basin under permutation symmetries. However, Ainsworth et al. verified LMC for two models and provided only limited discussion on its extension to multiple models. In this paper, we conduct a more detailed empirical analysis. First, we show that existing permutation search methods designed for two models can fail to transfer multiple models into the same convex low-loss basin. Next, we propose a permutation search method using a straight-through estimator for multiple models (STE-MM). We then experimentally demonstrate that even when multiple models are given, the test loss of the merged model remains nearly the same as the losses of the original models when using STE-MM, and the loss barriers between all permuted model pairs are also small. Additionally, from the perspective of the trace of the Hessian matrix, we show that the loss sharpness around the merged model decreases as the number of models increases with STE-MM, indicating that LMC for multiple models is more likely to hold. The source code implementing our method is available at https://github.com/e5-a/STE-MM.","Neural networks are the backbone of modern AI, and they’re usually trained from scratch to solve specific tasks. But what happens when we train several of them separately—can we somehow combine their knowledge?Recent research showed that two separately trained neural networks can often be “aligned” by rearranging their internal parts, allowing them to be blended without degrading performance. This led us to wonder: can the same approach be applied to more than two networks?Our study shows that current methods don’t scale well to multiple networks. So we developed a new technique called STE-MM, which intelligently aligns and merges multiple models simultaneously. As a result, the merged model performs just as well as the original ones—and in some cases, it’s even more stable.This is an important step toward more flexible AI systems, where different trained models can be combined rather than starting from scratch each time."
Poster,Linear Transformers as VAR Models:  Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting,https://ICML.cc//virtual/2025/poster/45192,"Jiecheng Lu, Shihao Yang","Autoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention often outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMoVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models.","Forecasting future data, such as weather or stock prices, is often done using powerful but complex machine learning models like Transformers. However, deeper Transformers usually lose interpretability because they stray from clear, understandable methods like Vector Autoregression (VAR). Our research reveals that a simpler Transformer variant (""linear attention"") aligns well with VAR. Building on this insight, we propose SAMoVAR, a Transformer designed specifically to maintain VAR’s clear structure. SAMoVAR enhances forecasting accuracy, interpretability, and speed, clearly showing how past data affects future outcomes. This helps users better understand predictions made from time series data."
Poster,LineFlow: A Framework to Learn Active Control of Production Lines,https://ICML.cc//virtual/2025/poster/46304,"Kai Müller, Martin Wenzel, Tobias Windisch","Many production lines require active control mechanisms, such as adaptive routing, worker reallocation, and rescheduling, to maintain optimal performance. However, designing these control systems is challenging for various reasons, and while reinforcement learning (RL) has shown promise in addressing these challenges, a standardized and general framework is still lacking. In this work, we introduce LineFlow, an extensible, open-source Python framework for simulating production lines of arbitrary complexity and training RL agents to control them. To demonstrate the capabilities and to validate the underlying theoretical assumptions of LineFlow, we formulate core subproblems of active line control in ways that facilitate mathematical analysis. For each problem, we provide optimal solutions for comparison. We benchmark state-of-the-art RL algorithms and show that the learned policies approach optimal performance in well-understood scenarios. However, for more complex, industrial-scale production lines, RL still faces significant challenges, highlighting the need for further research in areas such as reward shaping, curriculum learning, and hierarchical control.","Modern production lines often rely on complex control strategies—like adaptive routing,rescheduling, or reallocating workers—to stay efficient. Designing these control systems isdifficult, especially as production lines grow more automated and interconnected. Reinforcementlearning (RL) has shown promise in learning such control policies, but researchers lack astandardized, flexible framework to test and develop solutions. To address this, we developed LineFlow, an open-source Python toolkit that simulates realistic production lines and allows RL agents to interact with them. LineFlow supports highly customizablesetups and we study several core subproblems designed to be mathematically tractable. For each, weprovide optimal solutions so RL agents can be benchmarked reliably. We show that current RL methods perform well in controlled scenarios but struggle in more realistic, industrial-scale settings. This gap reveals important open challenges in applying RL to real-worldmanufacturing. Our work offers the community a testbed to explore these challenges and build more capable RLsystems for industrial control—bringing us one step closer to smarter, self-optimizing factories."
Poster,LipsNet++: Unifying Filter and Controller into a Policy Network,https://ICML.cc//virtual/2025/poster/45632,"Xujie Song, Liangfa Chen, Tong Liu, Wenxuan Wang, Yinuo Wang, Shentao Qin, Yinsong Ma, Jingliang Duan, Shengbo Li","Deep reinforcement learning (RL) is effective for decision-making and control tasks like autonomous driving and embodied AI. However, RL policies often suffer from the action fluctuation problem in real-world applications, resulting in severe actuator wear, safety risk, and performance degradation. This paper identifies the two fundamental causes of action fluctuation: observation noise and policy non-smoothness. We propose LipsNet++, a novel policy network with Fourier filter layer and Lipschitz controller layer to separately address both causes. The filter layer incorporates a trainable filter matrix that automatically extracts important frequencies while suppressing noise frequencies in the observations. The controller layer introduces a Jacobian regularization technique to achieve a low Lipschitz constant, ensuring smooth fitting of a policy function. These two layers function analogously to the filter and controller in classical control theory, suggesting that filtering and control capabilities can be seamlessly integrated into a single policy network. Both simulated and real-world experiments demonstrate that LipsNet++ achieves the state-of-the-art noise robustness and action smoothness. The code and videos are publicly available at https://xjsong99.github.io/LipsNet_v2.","Deep reinforcement learning (RL) systems used in autonomous vehicles and robots often produce unstable, jittery actions due to noisy sensor data and overly sensitive decision-making algorithms, causing accelerated hardware wear and safety risks. In response, we introduce LipsNet++, a unified policy architecture inspired by classical control theory that embeds an adaptive filtering stage—analogous to noise-canceling headphones removing spurious signals—and a Lipschitz smoothing stage—akin to shock absorbers damping abrupt motions. Experimental validation across simulated and physical platforms shows LipsNet++ substantially reduces action fluctuations compared to standard deep RL policies. By reducing action fluctuation, LipsNet++ enhances the robustness, reliability and lifespan of AI systems operating in complex, unpredictable environments."
Poster,LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces,https://ICML.cc//virtual/2025/poster/45204,"Rashid Mushkani, Perampalli Shravan Nayak, Hugo Berard, Allison Cohen, Shin Koseki, Hadrien Bertrand","We introduce the *Local Intersectional Visual Spaces* (LIVS) dataset, a benchmark for multi-criteria alignment, developed through a two-year participatory process with 30 community organizations to support the pluralistic alignment of text-to-image (T2I) models in inclusive urban planning. The dataset encodes 37,710 pairwise comparisons across 13,462 images, structured along six criteria—Accessibility, Safety, Comfort, Invitingness, Inclusivity, and Diversity—derived from 634 community-defined concepts. Using Direct Preference Optimization (DPO), we fine-tune Stable Diffusion XL to reflect multi-criteria spatial preferences and evaluate the LIVS dataset and the fine-tuned model through four case studies: (1) DPO increases alignment with annotated preferences, particularly when annotation volume is high; (2) preference patterns vary across participant identities, underscoring the need for intersectional data; (3) human-authored prompts generate more distinctive visual outputs than LLM-generated ones, influencing annotation decisiveness; and (4) intersectional groups assign systematically different ratings across criteria, revealing the limitations of single-objective alignment. While DPO improves alignment under specific conditions, the prevalence of neutral ratings indicates that community values are heterogeneous and often ambiguous. LIVS provides a benchmark for developing T2I models that incorporate local, stakeholder-driven preferences, offering a foundation for context-aware alignment in spatial design.","Public spaces such as parks and streets shape daily life in cities, but the way these places are designed is often driven by professional or financial interests. This means that people who use these spaces, especially those from marginalized groups, may not have much influence over how they look or function. Community members who want to participate in design processes face barriers like inaccessible venues, scheduling issues, and visual tools that do not reflect social or sensory needs.To address this, we worked with 30 community organizations in Montréal over two years to collect local perspectives on public space. Together, we identified six important criteria for inclusive design: accessibility, safety, comfort, invitingness, inclusivity, and diversity. Community members compared images of parks, streets, and plazas, producing over 37,000 annotations that reflect a wide range of experiences.Using these annotations, we adapted an open-source text-to-image AI model. We trained the model to better represent local preferences by fine-tuning it with the community-generated feedback. When tested, the new model sometimes reflected community priorities more closely, especially for criteria that had more training data. However, many participants found it difficult to choose between options, especially for concepts like inclusivity or diversity. This suggests that people’s needs are varied and not always easy to represent in one system. Our approach provides a way for residents and planners to explore design choices together, but it does not remove the challenges that come from competing priorities within a community."
Poster,LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models,https://ICML.cc//virtual/2025/poster/44918,"Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, Patrick Schramowski","This paper introduces Llavaguard, a suite of VLM-based vision safeguards that address the critical need for reliable tools in the era of large-scale data and models. To this end, we establish a novel open framework, describing a customizable safety taxonomy, data preprocessing, augmentation, and training setup. For teaching a VLM safeguard on safety, we further create a multimodal safety dataset with high-quality human expert annotations, where each image is labeled with a safety rating, category, and rationale. We also employ advanced augmentations to support context-specific assessments. The resulting Llavaguard models, ranging from 0.5B to 7B, serve as a versatile tool for evaluating the safety compliance of visual content against flexible policies. In comprehensive experiments, Llavaguard outperforms both state-of-the-art safeguards and VLMs in accuracy and in flexibly handling different policies. Additionally, we demonstrate Llavaguard's performance in two real-world applications: large-scale dataset annotation and moderation of text-to-image models. We make our entire framework, including the dataset, model weights, and training code, publicly available at https://ml-research.github.io/human-centered-genai/projects/llavaguard.","We introduce Llavaguard, a new family of vision-language safety checkers built for the challenges of today’s massive image collections and AI models. Llavaguard can evaluate images according to any safety policy you provide, offering detailed assessments for each image, including safety rating, safety category, as well as a rationale. Our open framework and end-to-end pipeline make it easy for anyone to build and customize their own safety models. To train these models, we created a multimodal dataset with high-quality annotations from human experts—every image includes a safety rating, category, and explanation. The resulting Llavaguard models, ranging from 0.5 to 7 billion parameters, can flexibly determine whether visual content meets your chosen guidelines. Through extensive experiments, we show that Llavaguard outperforms existing safety filters and vision-language models, both in accuracy and in its ability to adapt to different safety policies. We demonstrate its effectiveness in real-world scenarios, such as labeling large image datasets and moderating content produced by text-to-image generators. All of our code, data, and model weights are freely available at https://ml-research.github.io/human-centered-genai/projects/llavaguard."
