type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,End-to-End Learning Framework for Solving Non-Markovian Optimal Control,https://ICML.cc//virtual/2025/poster/46617,"Xiaole Zhang, Peiyu Zhang, Xiongye Xiao, Shixuan Li, Vasileios Tzoumas, Vijay Gupta, Paul Bogdan","Integer-order calculus fails to capture the long-range dependence (LRD) and memory effects found in many complex systems. Fractional calculus addresses these gaps through fractional-order integrals and derivatives, but fractional-order dynamical systems pose substantial challenges in system identification and optimal control tasks. In this paper, we theoretically derive the optimal control via linear quadratic regulator (LQR) for fractional-order linear time-invariant (FOLTI) systems and develop an end-to-end deep learning framework based on this theoretical foundation. Our approach establishes a rigorous mathematical model, derives analytical solutions, and incorporates deep learning to achieve data-driven optimal control of FOLTI systems. Our key contributions include: (i) proposing a novel method for system identification and optimal control strategy in FOLTI systems, (ii) developing the first end-to-end data-driven learning framework, Fractional-Order Learning for Optimal Control (FOLOC), that learns control policies from observed trajectories, and (iii) deriving theoretical bounds on the sample complexity for learning accurate control policies under fractional-order dynamics. Experimental results indicate that our method accurately approximates fractional-order system behaviors without relying on Gaussian noise assumptions, pointing to promising avenues for advanced optimal control.","Markovian dynamics often fail to capture the long-range dependencies and memory effects present in many real-world processes. In contrast, fractional non-Markovian dynamics have shown promise as a powerful modeling tool for complex systems with memory. However, identifying such systems and designing optimal controllers remain challenging due to the analytical complexity of fractional calculus. To address this, we aim to provide both a theoretical convergence-guaranteed control algorithm and an end-to-end machine learning framework to simplify the control of fractional-order dynamical systems.We develop a two-step algorithm that first performs system identification (i.e., parameter estimation) and then synthesizes an optimal controller. Alongside this, we provide sample complexity guarantees, quantifying the number of samples required to achieve accurate control in complex real-world settings. We also introduce Fractional-Order Learning for Optimal Control (FOLOC), the first end-to-end data-driven framework that learns control policies directly from observed trajectories.Our contributions include a theoretically grounded learning algorithm and a practical machine learning framework for solving optimal control problems in fractional-order dynamical systems. This work offers a novel tool for learning and controlling non-Markovian systems, enabling more accurate modeling and control of real-world processes."
Poster,Energy-Based Flow Matching for Generating 3D Molecular Structure,https://ICML.cc//virtual/2025/poster/44407,"Wenyin Zhou, Christopher I Sprague, Vsevolod Viliuga, Matteo Tadiello, Arne Elofsson, Hossein Azizpour","Molecular structure generation is a fundamental problem that involves determining the 3D positions of molecules' constituents. It has crucial biological applications, such as molecular docking, protein folding, and molecular design.Recent advances in generative modeling, such as diffusion models and flow matching, have made great progress on these tasks by modeling molecular conformations as a distribution.In this work, we focus on flow matching and adopt an energy-based perspective to improve training and inference of structure generation models. Our view results in a mapping function, represented by a deep network, that is directly learned to \textit{iteratively} map random configurations, i.e. samples from the source distribution, to target structures, i.e. points in the data manifold. This yields a conceptually simple and empirically effective flow matching setup that is theoretically justified and has interesting connections to fundamental properties such as idempotency and stability, as well as the empirically useful techniques such as structure refinement in AlphaFold. Experiments on protein docking as well as protein backbone generation consistently demonstrate the method's effectiveness, where it outperforms recent baselines of task-associated flow matching and diffusion models, using a similar computational budget.",An instance of energy-based flow matching that trains the flow matching model to iteratively predict and refine the sample towards the target molecular structure.
Poster,Energy-Based Preference Model Offers Better Offline Alignment than the Bradley-Terry Preference Model,https://ICML.cc//virtual/2025/poster/43792,"Yuzhong Hong, Hanshan Zhang, Junwei Bao, Hongfei Jiang, yang song","Since the debut of DPO, it has been shown that aligning a target LLM with human preferences via the KL-constrained RLHF loss is mathematically equivalent to a special kind of reward modeling task. Concretely, the task requires: 1) using the target LLM to parameterize the reward model, and 2) tuning the reward model so that it has a 1:1 linear relationship with the true reward. However, we identify a significant issue: the DPO loss might have multiple minimizers, of which only one satisfies the required linearity condition. The problem arises from a well-known issue of the underlying Bradley-Terry preference model: it does not always have a unique maximum likelihood estimator (MLE). Consequently, the minimizer of the RLHF loss might be unattainable because it is merely one among many minimizers of the DPO loss. As a better alternative, we propose an energy-based preference model (EBM) that always has a unique MLE, inherently satisfying the linearity requirement. To showcase the practical utility of replacing BTM with our EBM in the context of offline alignment, we adapt a simple yet scalable objective function from the recent literature on fitting EBM and name it as Energy Preference Alignment (EPA). Empirically, we demonstrate that EPA consistently delivers better performance on open benchmarks compared to DPO, thereby validating the theoretical superiority of our EBM.","The Bradley-Terry model (BTM) has served as the default preference model for RLHF and DPO applications. While this model performs adequately for the general purpose of training reward models, we identify a theoretical flaw when applying it to train implicit reward models in DPO: the DPO loss function, formulated as the maximum likelihood estimation of the BTM, admits multiple minimizers.Building on this theoretical finding, we question the established equivalence between the maximum likelihood estimation of BTM (when parameterized with the target policy) and the optimization of the RLHF objective. To address this limitation, we propose an alternative approach that circumvents this issue: the Infinite Preference Model, an energy-based model that can be trained using either the same pairwise datasets employed in DPO or datasets containing more than two responses per prompt."
Poster,Enforcing Idempotency in Neural Networks,https://ICML.cc//virtual/2025/poster/44290,"Nikolaj Jensen, Jamie Vicary","In this work, we propose a new architecture-agnostic method for training idempotent neural networks. An idempotent operator satisfies $f(x) = f(f(x))$, meaning it can be applied iteratively with no effect beyond the first application. Some neural networks used in data transformation tasks, such as image generation and augmentation, can represent non-linear idempotent projections. Using methods from perturbation theory we derive the recurrence relation ${\mathbf{K}' \leftarrow 3\mathbf{K}^2 - 2\mathbf{K}^3}$ for iteratively projecting a real-valued matrix $\mathbf{K}$ onto the manifold of idempotent matrices. Our analysis shows that for linear, single-layer MLP networks this projection 1) has idempotent fixed points, and 2) is attracting only around idempotent points. We give an extension to non-linear networks by considering our approach as a substitution of the gradient for the canonical loss function, achieving an architecture-agnostic training scheme. We provide experimental results for MLP- and CNN-based architectures with significant improvement in idempotent error over the canonical gradient-based approach. Finally, we demonstrate practical applications of the method as we train a generative network successfully using only a simple reconstruction loss paired with our method.","Applying an idempotent operation multiple times has the same effect as applying it once. Idempotency is a feature of many data transformation tasks we commonly tackle with machine learning, and recently it has been shown to also promote generative behaviour in neural networks. Gradient descent-based approaches to optimising for idempotency, however, are in many cases inefficient. We propose an alternative way to optimise for idempotency, using ideas from perturbation theory to derive a training scheme that is significantly more effective and without computational overhead. Our work suggests that alternative methods to gradient-based optimisation in neural networks are practically viable, opening the door to new approaches in neural network training generally."
Poster,Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation,https://ICML.cc//virtual/2025/poster/45983,"Alessandro Palma, Sergei Rybakov, Leon Hetzel, Stephan Günnemann, Fabian Theis","Latent space interpolations are a powerful tool for navigating deep generative models in applied settings. An example is single-cell RNA sequencing, where existing methods model cellular state transitions as latent space interpolations with variational autoencoders, often assuming linear shifts and Euclidean geometry. However, unless explicitly enforced, linear interpolations in the latent space may not correspond to geodesic paths on the data manifold, limiting methods that assume Euclidean geometry in the data representations. We introduce FlatVI, a novel training framework that regularises the latent manifold of discrete-likelihood variational autoencoders towards Euclidean geometry, specifically tailored for modelling single-cell count data. By encouraging straight lines in the latent space to approximate geodesic interpolations on the decoded single-cell manifold, FlatVI enhances compatibility with downstream approaches that assume Euclidean latent geometry. Experiments on synthetic data support the theoretical soundness of our approach, while applications to time-resolved single-cell RNA sequencing data demonstrate improved trajectory reconstruction and manifold interpolation.","Understanding how the state of a cell, the basic unit of life, changes during disease or development is a cornerstone of modern biomedical research. To a molecular biologist, a cell’s state is defined by the expression patterns of hundreds of genes at once, shaping the intricate molecular landscape of living organisms. To a data scientist, on the other hand, cells are massive matrices of numbers, each entry capturing how many copies of a particular gene were detected in a single cell.Because profiling cells involves thousands of noisy, sparse measurements, it’s common to compress this complexity into a dense, interpretable representation, what we call a latent space. To study how cells evolve into one another, techniques use linear interpolations of latent representations to approximate shifts within biological processes. But this raises a key question: how can we be sure that straight lines in this abstract space actually correspond to real biological progressions?In our work, we propose a principled yet simple answer. We introduce a penalty in the latent space that encourages Euclidean behavior, ensuring that the straight line between any two points is also the shortest path. In essence, we shape the latent space so that straight lines correspond to plausible gene expression transitions, offering an intuitive and mathematically grounded way to explore how cells change. Our method can be easily plugged into existing single-cell analysis tools, enriching their insights with representations that align with their core assumptions."
Poster,Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability,https://ICML.cc//virtual/2025/poster/45410,"Jie Bao, Chuangyin Dang, Rui Luo, Hanwei Zhang, Zhixin Zhou","As deep learning models are increasingly deployed in high-risk applications, robust defenses against adversarial attacks and reliable performance guarantees become paramount. Moreover, accuracy alone does not provide sufficient assurance or reliable uncertainty estimates for these models. This study advances adversarial training by leveraging principles from Conformal Prediction. Specifically, we develop an adversarial attack method, termed OPSA (OPtimal Size Attack), designed to reduce the efficiency of conformal prediction at any significance level by maximizing model uncertainty without requiring coverage guarantees. Correspondingly, we introduce OPSA-AT (Adversarial Training), a defense strategy that integrates OPSA within a novel conformal training paradigm. Experimental evaluations demonstrate that our OPSA attack method induces greater uncertainty compared to baseline approaches for various defenses. Conversely, our OPSA-AT defensive model significantly enhances robustness not only against OPSA but also other adversarial attacks, and maintains reliable prediction. Our findings highlight the effectiveness of this integrated approach for developing trustworthy and resilient deep learning models for safety-critical domains. Our code is available at https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.","This research exploits an important but underexplored topic in deep learning adversarial attacks: targeting conformal prediction set size rather than classification accuracy. We develop a novel conformal training framework that uses hard quantile thresholds instead of smoothed approximations, which can be inaccurate, serving dual purposes of identifying the strongest attacks and training the most robust defensive models. Our approach includes OPSA (Optimal Size Attack), which maximizes uncertainty by enlarging prediction sets without knowing the defender's confidence requirements, and OPSA-AT (Adversarial Training), a defense strategy that leverages our hard quantile framework to train models resistant to uncertainty-maximizing attacks while maintaining reliable prediction coverage and compact prediction sets. Experiments on standard image datasets demonstrate that our attack method generates significantly larger prediction sets than existing approaches, while our defense method produces more compact, reliable prediction sets compared to baselines. This work is particularly valuable for safety-critical applications like autonomous driving and medical diagnosis, where both robustness against attacks and reliable uncertainty quantification are essential."
Poster,Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss,https://ICML.cc//virtual/2025/poster/45247,"Bo-Han Lai, Pin-Han Huang, Bo-Han Kung, Shang-Tse Chen","Lipschitz neural networks are well-known for providing certified robustness in deep learning. In this paper, we present a novel, efficient Block Reflector Orthogonal (BRO) layer that enhances the capability of orthogonal layers on constructing more expressive Lipschitz neural architectures. In addition, by theoretically analyzing the nature of Lipschitz neural networks, we introduce a new loss function that employs an annealing mechanism to increase margin for most data points. This enables Lipschitz models to provide better certified robustness. By employing our BRO layer and loss function, we design BRONet — a simple yet effective Lipschitz neural network that achieves state-of-the-art certified robustness. Extensive experiments and empirical analysis on CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms existing baselines. The implementation is available at [GitHub Link](https://github.com/ntuaislab/BRONet).","Today, artificial intelligence (AI) plays a role in many aspects of our lives, from image recognition to medical diagnostics. However, these systems can still be easily fooled by small changes to their input—such as a slightly modified photo—that mislead the AI but not a human. This vulnerability makes it difficult to fully trust AI in critical applications like self-driving cars or healthcare. To address this, we developed a new building block for AI models called the **Block Reflector Orthogonal (BRO) layer**. This component helps construct stronger **Lipschitz models**—a class of neural networks that respond more consistently to slight changes in input. Since these models can be harder to train effectively, we also introduced a new training objective called the **Logit Annealing Loss**, which encourages the model to learn better decision boundaries across a wider range of data, not just a subset. Our work moves us closer to creating AI systems that are not only intelligent but also robust, trustworthy, and safer to use in real-world settings."
Poster,Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration,https://ICML.cc//virtual/2025/poster/45186,"Andreas Kontogiannis, Konstantinos Papathanasiou, Yi Shen, Giorgos Stamou, Michael Zavlanos, George Vouros","Learning to cooperate in distributed partially observable environments with no communication abilities poses significant challenges for multi-agent deep reinforcement learning (MARL). This paper addresses key concerns in this domain, focusing on inferring state representations from individual agent observations and leveraging these representations to enhance agents' exploration and collaborative task execution policies. To this end, we propose a novel state modelling framework for cooperative MARL, where agents infer meaningful belief representations of the non-observable state, with respect to optimizing their own policies, while filtering redundant and less informative joint state information. Building upon this framework, we propose the MARL SMPE$^2$ algorithm. In SMPE$^2$, agents enhance their own policy's discriminative abilities under partial observability, explicitly by incorporating their beliefs into the policy network, and implicitly by adopting an adversarial type of exploration policies which encourages agents to discover novel, high-value states while improving the discriminative abilities of others. Experimentally, we show that SMPE$^2$ outperforms a plethora of state-of-the-art MARL algorithms in complex fully cooperative tasks from the MPE, LBF, and RWARE benchmarks.","Cooperating in complex environments is tough for AI agents, especially when they can’t see the whole picture or talk to each other. This paper tackles that challenge by helping agents better understand their surroundings using only what they individually observe. The key idea is to create a smarter way for each agent to guess what’s going on in the environment and to use that guess to make better decisions—both for exploring and working together with others. We introduce a new approach called SMPE², which gives agents two big advantages. First, it helps them build better internal representations (or ""beliefs"") about the world. Second, it trains them to explore in a way that helps both themselves and their teammates discover useful parts of the environment. Our method makes agents not just smarter individually, but better at teamwork. Tests on standard AI cooperation tasks show that SMPE² beats existing top-performing methods, especially in challenging, fully cooperative scenarios."
Poster,Enhancing Decision-Making of Large Language Models via Actor-Critic,https://ICML.cc//virtual/2025/poster/45712,"Heng Dong, Kefei Duan, Chongjie Zhang","Large Language Models (LLMs) have achieved remarkable advancements in natural language processing tasks, yet they encounter challenges in complex decision-making scenarios that require long-term reasoning and alignment with high-level objectives. Existing methods either rely on short-term auto-regressive action generation or face limitations in accurately simulating rollouts and assessing outcomes, leading to sub-optimal decisions. This paper introduces a novel LLM-based Actor-Critic framework, termed LAC, that effectively improves LLM policies with long-term action evaluations in a principled and scalable way. Our approach addresses two key challenges: (1) extracting robust action evaluations by computing Q-values via token logits associated with positive/negative outcomes, enhanced by future trajectory rollouts and reasoning; and (2) enabling efficient policy improvement through a gradient-free mechanism. Experiments across diverse environments -- including high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text), and large action spaces (WebShop) -- demonstrate the framework’s generality and superiority over state-of-the-art methods. Notably, our approach achieves competitive performance using 7B/8B parameter LLMs, even outperforming baseline methods employing GPT-4 in complex tasks. These results underscore the potential of integrating structured policy optimization with LLMs’ intrinsic knowledge to advance decision-making capabilities in multi-step environments.","Large Language Models (LLMs) like ChatGPT have shown impressive abilities in generating text and answering questions. But when it comes to making complex decisions — especially those that require long-term planning — they often fall short. Current approaches either rely too much on the LLM’s initial instincts, or on simulated trial-and-error strategies that can be inaccurate or misleading.We developed a new method, called an “LLM-based Actor-Critic,” that helps LLMs make better decisions by combining their built-in knowledge with smarter planning. First, our method evaluates possible actions by analyzing the LLM's internal confidence signals — essentially asking, “Does the model believe this action will succeed?” We then use this information to refine the model’s decision-making, but in a way that avoids the expensive and slow process of traditional learning.This approach works across many different types of tasks, from high-level planning to detailed step-by-step actions, and it outperforms existing state-of-the-art methods — even when using smaller models."
Poster,Enhancing Diversity In Parallel Agents: A Maximum State Entropy Exploration Story,https://ICML.cc//virtual/2025/poster/46450,"Vincenzo De Paola, Riccardo Zamboni, Mirco Mutti, Marcello Restelli","Parallel data collection has redefined Reinforcement Learning (RL), unlocking unprecedented efficiency and powering breakthroughs in large-scale real-world applications. In this paradigm, $N$ identical agents operate in $N$ replicas of an environment simulator, accelerating data collection by a factor of $N$. A critical question arises: *Does specializing the policies of the parallel agents hold the key to surpass the $N$ factor acceleration?*In this paper, we introduce a novel learning framework that maximizes the entropy of collected data in a parallel setting. Our approach carefully balances the entropy of individual agents with inter-agent diversity, effectively minimizing redundancies. The latter idea is implemented with a centralized policy gradient method, which shows promise when evaluated empirically against systems of identical agents, as well as synergy with batch RL techniques that can exploit data diversity.Finally, we provide an original concentration analysis that shows faster rates for specialized parallel sampling distributions, which supports our methodology and may be of independent interest.","Modern reinforcement learning systems often speed up learning by running many identical agents in parallel environments, collecting data much faster than a single agent could. But this raises an important question: could we go even further by letting these agents specialize, instead of making them all act the same?In our work, we propose a new approach where each agent explores the environment in its own unique way. We designed a method that encourages agents to behave differently, increasing the variety of the data they collect. This reduces redundancy and makes the overall dataset more informative. We also use a centralized learning technique to manage this coordination efficiently.Our results show that this method improves learning speed and quality compared to traditional identical-agent systems. It also works well with other RL strategies that benefit from diverse data. We back our findings with theoretical analysis, suggesting that smarter specialization in parallel RL could significantly advance real-world AI applications."
