type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Controlling Large Language Model with Latent Action,https://ICML.cc//virtual/2025/poster/44697,"Chengxing Jia, Ziniu Li, Pengyuan Wang, Yi-Chen Li, Zhenyu Hou, Yuxiao Dong, Yang Yu","Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of specifying the action space. This paper studies learning a compact latent action space to enhance the controllability and exploration of RL for LLMs. Inspired by reinforcement learning from observations, we propose **Co**ntrolling Large Language Models with **L**atent **A**ctions **CoLA**, a framework that integrates a latent action space into pre-trained LLMs. **CoLA** employs an \emph{inverse dynamics model} to extract latent actions conditioned on future tokens, ensuring that the next token prediction is partially influenced by these actions. Simultaneously, **CoLA** fine-tunes the pre-trained LLM to function as a \emph{language world model}, capable of incorporating latent actions as inputs. Additionally, **CoLA** trains a \emph{policy model} to generate actions within this language world model. The policy model can be trained via behavior cloning to mimic a standard language model or through RL to maximize task-specific rewards. In this work, we apply **CoLA** to the Llama-3.1-8B model. Our experiments demonstrate that, compared to RL with token-level actions, **CoLA**'s latent actions enable greater semantic diversity. For enhancing downstream tasks, we show that **CoLA** with RL achieves a score of 42.4 on the \emph{math500} benchmark, surpassing the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo Tree Search variant. Furthermore, **CoLA** with RL consistently improves performance on agent-based tasks without degrading the pre-trained LLM's capabilities, unlike the baseline. Finally, **CoLA** reduces computation time by half in tasks involving enhanced thinking prompts for LLMs via RL. These results highlight **CoLA**'s potential to advance RL-based adaptation of LLMs for downstream applications. The CoLA model is available at  \url{https://huggingface.co/LAMDA-RL/Llama-3.1-CoLA-10B}.","This paper introduces a new method called **CoLA** to improve  how large language models (like chatbots) can be fine-tuned for specific tasks using reinforcement learning (RL)—a technique that helps AI learn by trial and error. Normally, adjusting these models with RL can be slow and inefficient because the possible actions (like choosing words) are too broad and hard to control.  **CoLA** simplifies this process by learning a smaller, more meaningful set of “latent actions"" that guide the model’s responses instead of dealing with every possible word choice. This makes the model easier to control and helps it explore better solutions faster.  In tests, **CoLA** improved performance on reasoning and agentic tasks, achieving higher scores than standard methods while also being twice as fast in some cases. Importantly, it fine-tunes the model without harming its original abilities, unlike some existing approaches. This makes **CoLA** a promising tool for adapting large language models to real-world applications more efficiently.  The model is publicly available for researchers and developers to use."
Poster,Controlling Neural Collapse Enhances Out-of-Distribution Detection and Transfer Learning,https://ICML.cc//virtual/2025/poster/46287,"Md Yousuf Harun, Jhair Gallardo, Christopher Kanan","Out-of-distribution (OOD) detection and OOD generalization are widely studied in Deep Neural Networks (DNNs), yet their relationship remains poorly understood. We empirically show that the degree of Neural Collapse (NC) in a network layer is inversely related with these objectives: stronger NC improves OOD detection but degrades generalization, while weaker NC enhances generalization at the cost of detection. This trade-off suggests that a single feature space cannot simultaneously achieve both tasks. To address this, we develop a theoretical framework linking NC to OOD detection and generalization. We show that entropy regularization mitigates NC to improve generalization, while a fixed Simplex ETF projector enforces NC for better detection. Based on these insights, we propose a method to control NC at different DNN layers. In experiments, our method excels at both tasks across OOD datasets and DNN architectures.","Modern AI systems often struggle in unfamiliar situations — like a self-driving car encountering something it has never seen before. To make AI safer and more adaptable, it needs two key abilities: recognizing unfamiliar inputs (out-of-distribution detection) and learning from them (out-of-distribution generalization).These two abilities are usually studied separately. But our research reveals they are linked by a hidden pattern in how neural networks organize information internally — a phenomenon known as Neural Collapse. We found that when this pattern is strong, AI becomes better at spotting the unfamiliar but worse at learning from it. When the pattern weakens, the opposite is true.To address this trade-off, we designed an AI system that manages this internal pattern differently across its components. This allows one part to specialize in detection and another in learning, enabling the system to do both tasks effectively. Our approach moves us closer to building AI that can safely and reliably adapt to open-ended, ever-changing environments."
Poster,Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration,https://ICML.cc//virtual/2025/poster/44096,"Shiqing Gao, Jiaxin Ding, Luoyi Fu, Xinbing Wang","Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.","Many AI systems learn by trial and error, but in safety-critical applications like robotics or autonomous driving, this can lead to costly or dangerous mistakes. We noticed that existing algorithms often underestimate risks, causing them to violate safety constraints during training.To tackle this, we developed a method inspired by how people remember and avoid dangerous experiences. Our approach, called MICE, lets AI “remember” risky situations it has seen before. By tracking and learning from these past dangers, our method helps the AI become more cautious and reduces the chance of making unsafe decisions.With this new approach, we found that AI systems could train much more safely without losing their ability to perform well. This makes our work a step forward in deploying AI in real-world scenarios where safety can’t be compromised, such as large language model, robotics, or autonomous driving."
Poster,Convergence Analysis of Policy Gradient Methods with Dynamic Stochasticity,https://ICML.cc//virtual/2025/poster/44972,"Alessandro Montenegro, Marco Mussi, Matteo Papini, Alberto Maria Metelli","*Policy gradient* (PG) methods are effective *reinforcement learning* (RL) approaches, particularly for continuous problems. While they optimize stochastic (hyper)policies via action- or parameter-space exploration, real-world applications often require deterministic policies. Existing PG convergence guarantees to deterministic policies assume a fixed stochasticity in the (hyper)policy, tuned according to the desired final suboptimality, whereas practitioners commonly use a dynamic stochasticity level.This work provides the theoretical foundations for this practice. We introduce PES, a phase-based method that reduces stochasticity via a deterministic schedule while running PG subroutines with fixed stochasticity in each phase. Under gradient domination assumptions, PES achieves last-iterate convergence to the optimal deterministic policy with a sample complexity of order $\widetilde{\mathcal{O}}(\epsilon^{-5})$.Additionally, we analyze the common practice, termed SL-PG, of jointly learning stochasticity (via an appropriate parameterization) and (hyper)policy parameters. We show that SL-PG also ensures last-iterate convergence with a rate $\widetilde{\mathcal{O}}(\epsilon^{-3})$, but to the optimal stochastic (hyper)policy only, requiring stronger assumptions compared to PES.","Reinforcement Learning (RL) is a subfield of machine learning in which agents learn through interaction with an environment to determine the optimal behavior in sequential decision-making problems. Among the various families of RL methods, policy gradient (PG) approaches have demonstrated notable success in tackling continuous control tasks. These methods directly learn the parameters of stochastic (hyper)policies by exploring either at the action level or the parameter level, depending on a certain level of exploration.While theoretical convergence guarantees for PG methods typically assume a fixed level of exploration, practitioners often adjust it dynamically during training. In this work, we bridge this gap between theory and practice by providing convergence guarantees for PG methods under a dynamically changing level of exploration, thus offering a theoretical foundation for a common empirical practice."
Poster,Convergence of Consistency Model with Multistep Sampling under General Data Assumptions,https://ICML.cc//virtual/2025/poster/43651,"Yiding Chen, Yiyi Zhang, Owen Oertell, Wen Sun","Diffusion models accomplish remarkable success in data generation tasks across various domains. However, the iterative sampling process is computationally expensive. Consistency models are proposed to learn consistency functions to map from noise to data directly, which allows one-step fast data generation and multistep sampling to improve sample quality. In this paper, we study the convergence of consistency models when the self-consistency property holds approximately under the training distribution. Our analysis requires only mild data assumption and applies to a family of forward processes. When the target data distribution has bounded support or has tails that decay sufficiently fast, we show that the samples generated by the consistency model are close to the target distribution in Wasserstein distance; when the target distribution satisfies some smoothness assumption, we show that with an additional perturbation step for smoothing, the generated samples are close to the target distribution in total variation distance. We provide two case studies with commonly chosen forward processes to demonstrate the benefit of multistep sampling.","Consistency models can generate realistic images or data very quickly, sometimes in just one step. This speed makes them appealing for real-world use, but it also raises two important questions: Why do these fast methods work? And how can we improve their results even further?In this paper, we study consistency models from a theoretical perspective. We build a rigorous mathematical framework to explain their behavior. Our analysis shows that when these models are trained well, the data they generate closely matches the true underlying data distribution.We also study a technique called multistep sampling, which improves the model’s output without retraining it. Instead of generating a sample in one step, the model takes several carefully designed steps. This adds some computation but can lead to much higher-quality results. However, more steps aren’t always better — adding too many can actually hurt performance. We explain when and why this happens and show how to choose the right balance.Together, our findings help explain the success of consistency models and offer practical guidance for making them even more effective in fast data generation tasks."
Poster,Convergence of Mean-Field Langevin Stochastic Descent-Ascent for Distributional Minimax Optimization,https://ICML.cc//virtual/2025/poster/43696,"Zhangyi Liu, Feng Liu, Rui Gao, Shuang Li","We study convergence properties of the discrete-time Mean-Field Langevin Stochastic Descent-Ascent (MFL-SDA) algorithm for solving distributional minimax optimization. These problems arise in various applications, such as zero-sum games, generative adversarial networks and distributionally robust learning. Despite the significance of MFL-SDA in these contexts, the discrete-time convergence rate remains underexplored.To address this gap, we establish a last-iterate convergence rate of $O(\frac{1}{\epsilon}\log\frac{1}{\epsilon})$ for MFL-SDA. This rate is nearly optimal when compared to the complexity lower bound of its Euclidean counterpart. This rate also matches the complexity of mean-field Langevin stochastic gradient descent for distributional minimization and the outer-loop iteration complexity of an existing double-loop algorithm for distributional minimax problems.By leveraging an elementary analysis framework that avoids PDE-based techniques, we overcome previous limitations and achieve a faster convergence rate.","The identification of mixed Nash equilibrium points in zero-sum games has long been a subject of significant research interest, primarily due to the inherent challenges associated with optimization in distributional spaces. A prevalent approach for analyzing optimization convergence in such spaces involves mean-field Langevin dynamics. Conventional methodologies typically first establish convergence analysis in continuous time via gradient flow techniques, followed by time discretization. In contrast, our approach directly conducts stepsize analysis. This methodological advancement enables our convergence analysis to achieve asymptotic optimality for the problem at hand. We also demonstrate that our framework enables convergence analysis for a wide range of prominent problems, including Generative Adversarial Networks (GANs) and zero-sum games, thereby illustrating its broad applicability."
Poster,Convergence of Policy Mirror Descent Beyond Compatible Function Approximation,https://ICML.cc//virtual/2025/poster/46241,"Uri Sherman, Tomer Koren, Yishay Mansour","Modern policy optimization methods roughly follow the policy mirror descent (PMD) algorithmic template, for which there are by now numerous theoretical convergence results.    However, most of these either target tabular environments, or can be applied effectively only when the class of policies being optimized over satisfies strong closure conditions, which is typically not the case when working with parametric policy classes in large-scale environments.     In this work, we develop a theoretical framework for PMD for general policy classes where we replace the closure conditions with a generally weaker variational gradient dominance assumption, and obtain upper bounds on the rate of convergence to the best-in-class policy. Our main result leverages a novel notion of smoothness with respect to a local norm induced by the occupancy measure of the current policy, and casts PMD as a particular instance of smooth non-convex optimization in non-Euclidean space.","Modern policy optimization methods for reinforcement learning (such as the popular proximal policy optimization algorithm) roughly follow an algorithmic template called policy mirror descent (PMD). There are an abundance of theoretical research works that establish convergence of PMD in either (i) the tabular setting where the state space is small, or (ii) in the function approximation setting (where the policies are represented by e.g., neural networks) but only subject to strong assumptions on the policy class called closure conditions.Unfortunately, closure conditions are generally deemed too strong to hold in practice, since roughly speaking, they require the policy class to be ""essentially complete"" --- to contain all possible policies. Modern large scale environements are many orders of magnitude too large for realistically sized neural networks to represent all possible policies.Our work establishes convergence of PMD in the function approximation setup, while relaxing closure conditions and assuming instead a variational gradient dominance assumption, which is generally weaker. In particular, our assumption accommodates agnostic, non-realizable settings, while closure conditions do not, and seems more plausible of an assumption to adopt in the context of real world problems."
Poster,Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning,https://ICML.cc//virtual/2025/poster/43543,"Ian Gemp, Andreas Haupt, Luke Marris, Siqi Liu, Georgios Piliouras","Behavioral diversity, expert imitation, fairness, safety goals and others give rise to preferences in sequential decision making domains that do not decompose additively across time. We introduce the class of convex Markov games that allow general convex preferences over occupancy measures. Despite infinite time horizon and strictly higher generality than Markov games, pure strategy Nash equilibria exist. Furthermore, equilibria can be approximated empirically by performing gradient descent on an upper bound of exploitability. Our experiments reveal novel solutions to classic repeated normal-form games, find fair solutions in a repeated asymmetric coordination game, and prioritize safe long-term behavior in a robot warehouse environment. In the prisoner's dilemma, our algorithm leverages transient imitation to find a policy profile that deviates from observed human play only slightly, yet achieves higher per-player utility while also being three orders of magnitude less exploitable.","In a traditional reinforcement learning problem or Markov decision process (MDP), a single agent aims to find a policy to maximize their discounted sum of rewards (also called return). Prior work has already considered extending this traditional problem to the multi-agent setting (called a Markov game) where every agent is *simultaneously* maximizing their return (i.e., they are at a Nash equilibrium). Research in single agent MDPs has since moved beyond utilities that encode the traditional discounted sum of rewards, an example utility being the entropy of an agent's visitation of states in the environment used to encourage learning a good exploration policy. This more general MDP is sometimes called a convex MDP. What can we say about a similar multi-agent extension of MDPs with generalized utilities? Do Nash equilibria even exist? What kinds of domains can we model with such a framework and can it help us uncover interesting multi-agent behavior?To that end, we formulate the convex Markov game. Certain properties of a convex Markov game break previous assumptions leveraged to prove existence of Nash equilibria. We appeal to more general techniques to prove Nash equilibria indeed exist. In addition, measuring how ""close"" a multi-agent system is to a Nash equilibrium is more expensive than in a vanilla Markov game. Therefore, we propose a cheap alternative that upper bounds how close the multi-agent system is to equilibrium. To solve for Nash equilibria, we simply employ this upper bound as a loss function and descend it with gradient descent.The convex Markov game framework has many applications including helping multi-agent systems better explore their environment, more closely imitate desired target behavior, avoid ""unsafe"" regions of the environment, and more."
Poster,Cooperation of Experts: Fusing Heterogeneous Information with Large Margin,https://ICML.cc//virtual/2025/poster/44195,"Shuo Wang, Shunyang Huang, Jinghui Yuan, Zhixiang Shen, zhao kang","Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the **Cooperation of Experts (CoE)** framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By transcending modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel **large margin** mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework’s feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability.","In today's world, data comes in many forms—images, text, social interactions—and understanding how to combine them is a major challenge. Our research introduces **CoE**, which helps the model to better make sense of this diverse information. Instead of using one model to learn from all data types, we train multiple experts, each specialized in a different type of information. These experts work together rather than compete, learning from both individual data types and their combinations.To make the system more reliable, we introduce a way for the experts to adjust their influence depending on how confident they are in their predictions. We also design an optimization method that ensures experts don't just agree—they make accurate and meaningful decisions together. Our experiments show that CoE outperforms existing models on a wide range of tasks and remains robust even when data is noisy or incomplete. This opens up new possibilities for building smarter, more adaptive systems that can handle the messy, complex information of the real world."
Poster,Copilot Arena: A Platform for Code LLM Evaluation in the Wild,https://ICML.cc//virtual/2025/poster/46219,"Wayne Chi, Valerie Chen, Anastasios Angelopoulos, Wei-Lin Chiang, Aditya Mittal, Naman Jain, Tianjun Zhang, Ion Stoica, Chris Donahue, Ameet Talwalkar","Evaluating in-the-wild coding capabilities of large language models (LLMs) is a challenging endeavor with no existing solution. We introduce Copilot Arena, a platform to collect user preferences through native integration into a developer's working environment. Copilot Arena comprises a novel interface for comparing pairs of model outputs, a sampling strategy to reduce experienced latency, and a prompting scheme to enable code completion functionality. Copilot Arena has served over 4.5 million suggestions from 10 models and collected over 11k pairwise judgements. Our results highlight the importance of model evaluations in integrated settings. We find that model rankings from Copilot Arena differ from those of existing evaluations, which we attribute to the unique distribution of data and tasks contained in Copilot Arena. We also identify novel insights into human preferences on code such as an observed consistency in user preference across programming languages yet significant variation in preference due to task category. We open-source Copilot Arena and release data to enable human-centric evaluations and improve understanding of coding assistants.","We developed a tool called Copilot Arena to better understand how people use code suggestions from large language models. Traditional ways of testing these models do not fully reflect how developers work in real environments, so we created a system that integrates directly into coding tools, making it easier to collect real user feedback.Copilot Arena shows users two different code suggestions and asks which one they prefer. It also uses smart techniques to show code suggestions in a timely manner with proper formatting. So far, it has delivered over 4.5 million suggestions from 10 different AI models and gathered more than 11,000 user votes.Our findings show that user preferences for model suggestions can vary depending on the task but are often consistent across programming languages. Interestingly, some top-performing models in prior tests do not always perform best in real-world coding. We’ve made Copilot Arena and its data public to help others build better AI coding tools."
