type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition,https://ICML.cc//virtual/2025/poster/44295,"Zichen Wang, Chuanhao Li, Huazheng Wang","We investigate the problem of identifying the optimal scoring rule within the principal-agent framework for online information acquisition problem. We focus on the principal's perspective, seeking to determine the desired scoring rule through interactions with the agent. To address this challenge, we propose two algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget settings, respectively. Our theoretical analysis demonstrates that OIAFC can extract the desired $(\epsilon, \delta)$-scoring rule with a efficient instance-dependent sample complexity or an instance-independent sample complexity. Our analysis also shows that OIAFB matches the instance-independent performance bound of OIAFC, while both algorithms share the same complexity across fixed confidence and fixed budget settings.","We study how a manager (the principal) can figure out the best way to reward a worker (the agent) for collecting useful information over time. This is important in situations where the manager can't directly see the worker's effort but still wants to encourage honest and high-quality work. To solve this, we design two algorithms—OIAFC and OIAFB—that help the manager learn the best reward strategy. One works when the manager wants to be very confident in the result (fixed confidence), and the other works when there is a limited budget (fixed budget). Our analysis shows that both algorithms are efficient, and even though they work in different settings, they perform equally well in terms of how much data they need to make good decisions."
Poster,Provably Efficient Exploration in Inverse Constrained Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44588,"Bo Yue, Jian Li, Guiliang Liu","Optimizing objective functions subject to constraints is fundamental in many real-world applications. However, these constraints are often not readily defined and must be inferred from expert agent behaviors, a problem known as Inverse Constraint Inference. Inverse Constrained Reinforcement Learning (ICRL) is a common solver for recovering feasible constraints in complex environments, relying on training samples collected from interactive environments. However, the efficacy and efficiency of current sampling strategies remain unclear. We propose a strategic exploration framework for sampling with guaranteed efficiency to bridge this gap. By defining the feasible cost set for ICRL problems, we analyze how estimation errors in transition dynamics and the expert policy influence the feasibility of inferred constraints. Based on this analysis, we introduce two exploratory algorithms to achieve efficient constraint inference via 1) dynamically reducing the bounded aggregate error of cost estimations or 2) strategically constraining the exploration policy around plausibly optimal ones. Both algorithms are theoretically grounded with tractable sample complexity, and their performance is validated empirically across various environments.","Imagine teaching a robot to follow rules in a factory, but without ever telling it the rules directly. Instead, we show it how experts behave and ask it to ""guess"" what rules they’re following. This is a common challenge in AI called inverse constraint inference: figuring out the hidden rules behind smart behavior.To do this, AI systems often explore their environment and learn from what happens. However, exploration takes time and resources, and not all is equally useful. Our research develops smarter ways for AI to explore while learning these hidden rules, making the process faster and more reliable.We created two new strategies that help AI focus its attention on the most useful parts of the environment or adjust its learning to reduce errors. Both strategies come with theoretical guarantees and work well in real-world-like test scenarios. This makes them promising tools for building more efficient and trustworthy AI systems that need to learn from expert behavior without explicit instructions."
Poster,Provably Efficient RL for Linear MDPs under Instantaneous Safety Constraints in Non-Convex Feature Spaces,https://ICML.cc//virtual/2025/poster/43846,"Amirhossein Roknilamouki, Arnob Ghosh, Ming Shi, Fatemeh Nourzad, Eylem Ekici, Ness Shroff","In Reinforcement Learning (RL), tasks with instantaneous hard constraints present significant challenges, particularly when the decision space is non-convex or non-star-convex. This issue is especially relevant in domains like autonomous vehicles and robotics, where constraints such as collision avoidance often take a non-convex form. In this paper, we establish a regret bound of $\tilde{\mathcal{O}}((1 + \tfrac{1}{\tau})  \sqrt{\log(\frac{1}{\tau})  d^3 H^4 K})$, applicable to both star-convex and non-star-convex cases, where $d$ is the feature dimension, $H$ the episode length, $K$ the number of episodes, and $\tau$ the safety threshold.  Moreover, the violation of safety constraints is zero with high probability throughout the learning process. A key technical challenge in these settings is bounding the covering number of the value-function class, which is essential for achieving value-aware uniform concentration in model-free function approximation. For the star-convex setting, we develop a novel technique called *Objective–Constraint Decomposition* (OCD) to properly bound the covering number. This result also resolves an error in a previous work on constrained RL. In non-star-convex scenarios, where the covering number can become infinitely large, we propose a two-phase algorithm, Non-Convex Safe Least Squares Value Iteration (NCS-LSVI), which first reduces uncertainty about the safe set by playing a known safe policy. After that, it carefully balances exploration and exploitation to achieve the regret bound. Finally, numerical simulations on an autonomous driving scenario demonstrate the effectiveness of NCS-LSVI.","Self-driving cars and robots must learn from experience, yet even a single crash during training is unacceptable. We introduce a two-stage strategy that behaves like a student driver: it begins by cruising cautiously on quiet streets to map out what is safe, then, once confident, moves on to explore actions while still being safe. Despite this cautious start, it learns to explore nearly as efficiently as a risk-taking learner. We prove mathematically that the risk of an accident remains near zero throughout training while achieving (nearly) optimal regret. Our analysis introduces a new mathematical tool to handle hard safety scenarios and also corrects a flaw in earlier research. In our simulated driving tests, the system completed every route without a single collision, while nearly matching the performance of unsafe safe approaches, validating our theoretical insights."
Poster,Provably Improving Generalization of Few-shot models with Synthetic Data,https://ICML.cc//virtual/2025/poster/45612,"Lan-Cuong Nguyen, Quan Nguyen-Tri, Bang Khanh, Dung D. Le, Long Tran-Thanh, Khoat Than","Few-shot image classification remains challenging due to the scarcity of labeled training examples. Augmenting them with synthetic data has emerged as a promising way to alleviate this issue, but models trained on synthetic samples often face performance degradation due to the inherent gap between real and synthetic distributions. To address this limitation, we develop a theoretical framework that quantifies the impact of such distribution discrepancies on supervised learning, specifically in the context of image classification. More importantly, *our framework suggests practical ways to generate good synthetic samples and to train a predictor with high generalization ability*. Building upon this framework, we propose a novel theoretical-based algorithm that integrates prototype learning to optimize both data partitioning and model training, effectively bridging the gap between real few-shot data and synthetic data. Extensive experiments results show that our approach demonstrates superior performance compared to state-of-the-art methods, outperforming them across multiple datasets.","Training artificial intelligence (AI) systems often requires huge amounts of labeled data—for example, hundred thousands of images labeled by humans. But collecting this data can be slow, expensive, and sometimes even impossible. One solution is to use *synthetic data*, which is computer-generated to mimic real data. However, AI models trained on synthetic data often do not work well in the real world because the fake data may not be quite the same as the real thing.In our research, we explore how to use a mix of synthetic and just a few real examples to train AI models more effectively. We found that what matters most is not just how “real” the synthetic data looks, but how *useful* it is for teaching the AI. Using mathematical analysis, we identified what makes synthetic data *helpful*, and designed a new way to train AI that takes full advantage of it.This research provides both practical tools and theoretical insights for improving the reliability of synthetic data in AI. It could lead to more cost-effective, scalable, and robust AI systems in domains where labeled data is limited, such as healthcare, agriculture, and education."
Poster,Provably Near-Optimal Federated Ensemble Distillation with Negligible Overhead,https://ICML.cc//virtual/2025/poster/46350,"Won-Jun Jang, Hyeon-Seo Park, Si-Hyeon Lee","Federated ensemble distillation addresses client heterogeneity by generating pseudo-labels for an unlabeled server dataset based on client predictions and training the server model using the pseudo-labeled dataset. The unlabeled server dataset can either be pre-existing or generated through a data-free approach. The effectiveness of this approach critically depends on the method of assigning weights to client predictions when creating pseudo-labels, especially in highly heterogeneous settings. Inspired by theoretical results from GANs, we propose a provably near-optimal weighting method that leverages client discriminators trained with a server-distributed generator and local datasets. Our experiments on various image classification tasks demonstrate that the proposed method significantly outperforms baselines.  Furthermore, we show that the additional communication cost, client-side privacy leakage, and client-side computational overhead introduced by our method are negligible, both in scenarios with and without a pre-existing server dataset.","In federated learning, multiple devices (clients) collaboratively train a machine learning model without sharing their private data. However, differences in data across devices—called heterogeneity—can hurt the overall model performance. One promising solution is federated ensemble distillation, which allows a central server to learn from client predictions by generating “pseudo-labels” for an unlabeled dataset.This study introduces a smarter way to combine the predictions from different clients by assigning more reliable weights to each prediction. Inspired by insights from generative adversarial networks (GANs), the proposed approach finds nearly the best possible weighting strategy, even when the clients’ data are very different.Experiments on image classification tasks show that this technique significantly improves performance over existing methods. Importantly, it does so without adding much communication cost, privacy risk, or extra work for each client—even when the server starts without any dataset of its own."
Poster,PROXSPARSE: REGULARIZED LEARNING OF SEMI-STRUCTURED SPARSITY MASKS FOR PRETRAINED LLMS,https://ICML.cc//virtual/2025/poster/43457,"Hongyi Liu, Rajarshi Saha, Zhen Jia, Youngsuk Park, Jiaji Huang, Shoham Sabach, Yu-Xiang Wang, George Karypis","Large Language Models (LLMs) have demonstrated exceptional performance in natural language processing tasks, yet their massive size makes serving them inefficient and costly. Semi-structured pruning has emerged as an effective method for model acceleration, but existing approaches are suboptimal because they focus on local, layer-wise optimizations using heuristic rules, failing to leverage global feedback. We present ProxSparse, a learning-based framework for mask selection enabled by regularized optimization. ProxSparse transforms the rigid, non-differentiable mask selection process into a smoother optimization procedure, allowing gradual mask exploration with flexibility. ProxSparse does not involve additional weight updates once the mask is determined. Our extensive evaluations on 7 widely used models show that ProxSparse consistently outperforms previously proposed semi-structured mask selection methods with significant improvement, demonstrating the effectiveness of our learned approach towards semi-structured pruning.",We developed a new method that makes LLMs run faster and use less memory by learning how to efficiently remove unnecessary parts of the model without hurting its performance.
Poster,Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting,https://ICML.cc//virtual/2025/poster/45163,"Chen Huang, Skyler Seto, Hadi Pouransari, Mehrdad Farajtabar, Raviteja Vemulapalli, Fartash Faghri, Oncel Tuzel, Barry-John Theobald, Joshua M Susskind","Vision foundation models pre-trained on massive data encode rich representations of real-world concepts, which can be adapted to downstream tasks by fine-tuning. However, fine-tuning foundation models on one task often leads to the issue of *concept forgetting* on other tasks. Recent methods of robust fine-tuning aim to mitigate forgetting of prior knowledge without affecting the fine-tuning performance. Knowledge is often preserved by matching the original and fine-tuned model weights or feature pairs. However, such point-wise matching can be too strong, without explicit awareness of the feature neighborhood structures that encode rich knowledge as well. We propose a novel regularization method **Proxy-FDA** that explicitly preserves the structural knowledge in feature space. Proxy-FDA performs Feature Distribution Alignment (using nearest neighbor graphs) between the pre-trained and fine-tuned feature spaces, and the alignment is further improved by informative proxies that are generated dynamically to increase data diversity. Experiments show that Proxy-FDA significantly reduces concept forgetting during fine-tuning, and we find a strong correlation between forgetting and a distributional distance metric (in comparison to L2 distance). We further demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end, few-shot and continual tuning) and across different tasks like image classification, captioning and VQA.","Large AI models for vision tasks learn general knowledge about real-world concepts by training on massive image datasets. When these models are later ""fine-tuned"" for a specific task (like classifying birds), they often ""forget"" knowledge they previously learned, which makes them worse at handling other tasks — a problem known as concept forgetting. Existing approaches try to prevent this problem by forcing the model’s internal feature representations to stay close to the original ones on individual images, which can be overly restrictive. This paper instead preserves the overall structure of image features — how different pieces of knowledge relate to each other in the feature space. Our new method does so by aligning the overall feature distributions before and after fine-tuning, and it dynamically synthesize ""virtual"" features to add variety during this alignment process. Our approach is shown to significantly reduce concept forgetting in many settings, including when fine-tuning with a lot of data, just a few examples, or over time with multiple tasks. It also performs well across various tasks, like recognizing images, generating image captions, or answering visual questions."
Poster,Prune 'n Predict: Optimizing LLM Decision-making with Conformal Prediction,https://ICML.cc//virtual/2025/poster/46415,"Harit Vishwakarma, Alan Mishler, Thomas Cook, Niccolo Dalmasso, Natraj Raman, Sumitra Ganesh","Large language models (LLMs) are empowering decision-making in several applications, including tool or API usage and answering multiple-choice questions (MCQs). However, incorrect outputs pose significant risks in high-stakes domains like healthcare and finance. To quantify LLM uncertainty and thereby mitigate these risks, recent works employ conformal prediction (CP), a model- and distribution-agnostic framework that uses LLM outputs to generate a \emph{prediction set} containing the true answer with high probability. Leveraging CP, we propose \emph{conformal revision of questions} (CROQ), which revises the question by narrowing down the available choices to those in the prediction set and asking the LLM the revised question. We expect LLMs to be more accurate on revised questions with fewer choices. Furthermore, we expect CROQ to be effective when the prediction sets from CP are small. Commonly used logit scores often lead to large sets, diminishing CROQ's effectiveness. To overcome this, we propose CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Our extensive experiments on MMLU,  ToolAlpaca, and TruthfulQA datasets with multiple LLMs show that CROQ improves accuracy over the standard inference, with more pronounced gains when paired with CP-OPT.","Large language models (LLMs) are increasingly used to make decisions in tasks like answering multiple-choice questions or selecting tools in software systems. But they may make mistakes, often with high confidence — a risky trait in areas like healthcare or finance. Our paper introduces a method called CROQ (Conformal Revision of Questions) that helps LLMs make better decisions by narrowing down their options before they answer. Inspired by the human test-taking strategy of eliminating obviously wrong choices, CROQ uses a statistical technique called conformal prediction to remove unlikely answers, then re-asks the question with just the remaining options. With fewer choices, the model is more likely to choose correctly. However, how many options are pruned depends on the quality of the scoring used in conformal prediction — and standard scores aren't very efficient. To fix this, we propose CP-OPT, an optimized way to score options that removes more irrelevant answers while still ensuring the correct one is likely to stay. Across a variety of benchmarks and models, our approach consistently improves accuracy, offering a practical way to make LLMs more trustworthy in high-stakes settings."
Poster,Pruning for GNNs: Lower Complexity with Comparable Expressiveness,https://ICML.cc//virtual/2025/poster/46397,"Dun Ma, Jianguo Chen, Wenguo Yang, Suixiang Gao, Shengminjie Chen","In recent years, the pursuit of higher expressive power in graph neural networks (GNNs) has often led to more complex aggregation mechanisms and deeper architectures. To address these issues, we have identified redundant structures in GNNs, and by pruning them, we propose Pruned MP-GNNs, K-Path GNNs, and K-Hop GNNs based on their original architectures. We show that 1) Although some structures are pruned in Pruned MP-GNNs and Pruned K-Path GNNs, their expressive power has not been compromised. 2) K-Hop MP-GNNs and their pruned architecture exhibit equivalent expressiveness on regular and strongly regular graphs. 3) The complexity of pruned K-Path GNNs and pruned K-Hop GNNs is lower than that of MP-GNNs, yet their expressive power is higher. Experimental results validate our refinements, demonstrating competitive performance across benchmark datasets with improved efficiency.","Graph neural networks (GNNs) are powerful tools that help computers understand complex connections, like social networks or molecules. But to make them more accurate, researchers often add more layers and features — which also makes them slower and harder to train.In our work, we asked: can we make GNNs simpler without losing their ability to understand complex structures?  We discovered that many parts of GNNs are redundant, and by carefully removing them, the expressive power of pruned GNN remains unchangedThis makes GNNs more practical for real-world applications, especially where computing power is limited — like mobile devices, or analyzing very large graphs."
Poster,PTTA: Purifying Malicious Samples for Test-Time Model Adaptation,https://ICML.cc//virtual/2025/poster/45190,"Jing Ma, Hanlin Li, Xiang Xiang","Test-Time Adaptation (TTA) enables deep neural networks to adapt to arbitrary distributions during inference. Existing TTA algorithms generally tend to select benign samples that help achieve robust online prediction and stable self-training. Although malicious samples that would undermine the model's optimization should be filtered out, it also leads to a waste of test data. To alleviate this issue, we focus on how to make full use of the malicious test samples for TTA by transforming them into benign ones, and propose a plug-and-play method, PTTA. The core of our solution lies in the purification strategy, which retrieves benign samples having opposite effects on the objective function to perform Mixup with malicious samples, based on a saliency indicator for encoding benign and malicious data. This strategy results in effective utilization of the information in malicious samples and an improvement of the models' online test accuracy. In this way, we can directly apply the purification loss to existing TTA algorithms without the need to carefully adjust the sample selection threshold. Extensive experiments on four types of TTA tasks as well as classification, segmentation, and adversarial defense demonstrate the effectiveness of our method. Code is available at https://github.com/HAIV-Lab/PTTA.","How can deep neural networks evolve through self-supervision without human intervention? This is a recent research trend, but difficult to solve well due to real-world complexity. Our paper identifies the ""malicious sample hazards"" as an obstacle to model self-evolution. Prior solutions typically select and filter out malicious samples that negatively impact model optimization, which also reduces utilization of already limited data. Rather than discarding them, why not purify malicious samples into benign ones? Surprisingly, we found that superimpose benign samples—which exert the most opposite influence on the objective function—onto malicious samples effectively mitigates these hazards. Our findings reveal a new direction: using purification strategies to boost sample utilization during autonomous machine learning. This enables stable and efficient self-supervised evolution of deep neural networks."
