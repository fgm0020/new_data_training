type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models,https://ICML.cc//virtual/2025/poster/43594,"Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, Zhou Zhao","Orientation is a fundamental attribute of objects, essential for understanding their spatial pose and arrangement. However, practical solutions for estimating the orientation of open-world objects in monocular images remain underexplored. In this work, we introduce Orient Anything, the first foundation model for zero-shot object orientation estimation. A key challenge in this task is the scarcity of orientation annotations for open-world objects. To address this, we propose leveraging the vast resources of 3D models. By developing a pipeline to annotate the front face of 3D objects and render them from random viewpoints, we curate 2 million images with precise orientation annotations across a wide variety of object categories. To fully leverage the dataset, we design a robust training objective that models the 3D orientation as probability distributions over three angles and predicts the object orientation by fitting these distributions. Besides, we propose several strategies to further enhance the synthetic-to-real transfer. Our model achieves state-of-the-art orientation estimation accuracy on both rendered and real images, demonstrating impressive zero-shot capabilities across various scenarios. Furthermore, it shows great potential in enhancing high-level applications, such as understanding complex spatial concepts in images and adjusting 3D object pose.","While object orientation is fundamental to discerning spatial relationships within images, its estimation remains an under-researched domain. Our work introduces a visual foundation model engineered to infer the orientation of arbitrary objects within a single image. This innovation is poised to bolster advanced applications, including the comprehension of sophisticated spatial concepts and the refinement of 3D object pose adjustments."
Poster,Origin Identification for Text-Guided Image-to-Image Diffusion Models,https://ICML.cc//virtual/2025/poster/46505,"Wenhao Wang, Yifan Sun, Zongxin Yang, Zhentao Tan, Zhengdong Hu, Yi Yang","Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such a powerful technique can be misused for *spreading misinformation*, *infringing on copyrights*, and *evading content tracing*. This motivates us to introduce the task of origin **ID**entification for text-guided **I**mage-to-image **D**iffusion models (**ID$\mathbf{^2}$**), aiming to retrieve the original image of a given translated query. A straightforward solution to ID$^2$ involves training a specialized deep embedding model to extract and compare features from both query and reference images. However, due to *visual discrepancy* across generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID$^2$ task, we contribute the first dataset and a theoretically guaranteed method, both emphasizing generalizability. The curated dataset, **OriPID**, contains abundant **Ori**gins and guided **P**rompts, which can be used to train and test potential **ID**entification models across various diffusion models. In the method section, we first prove the *existence* of a linear transformation that minimizes the distance between the pre-trained Variational Autoencoder embeddings of generated samples and their origins. Subsequently, it is demonstrated that such a simple linear transformation can be *generalized* across different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods (+31.6% mAP), even those with generalization designs. The project is available at https://id2icml.github.io.","Modern AI tools can now rewrite any picture just by following a short text instruction—turning a daytime street into a rainy night scene, or adding new objects that never existed. While fun and useful, this power makes it easy to spread fake images, dodge copyright rules, and hide the true source of a picture. We tackle this problem by asking a simple but crucial question: given an edited image, can we reliably find the original photo it came from?Our new task, called ID² (Origin IDentification for text-guiding Image-to-image Diffusion), shows why earlier “look-for-similar-parts” tricks break down: different AI editors leave very different fingerprints, so a system trained on one often fails on another. To fix this, we built OriPID, the first large benchmark that pairs thousands of originals with their AI-altered versions from many diffusion models. We then prove that a single linear tweak to the images’ hidden VAE features can pull each edited picture back toward its source—and that this tweak works across editors. In tests, our lightweight method beats previous similarity-based approaches by over 31 percentage points in mean average precision, bringing practical image provenance a big step closer. Code and data: id2icml.github.io."
Poster,Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection,https://ICML.cc//virtual/2025/poster/45843,"Zhiyuan Yan, Jiangming Wang, Peng Jin, Ke-Yue Zhang, Chengchun Liu, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, Li Yuan","Detecting AI-generated images (AIGIs), such as natural images or face images, has become increasingly important yet challenging. In this paper, we start from a new perspective to excavate the reason behind the failure generalization in AIGI detection, named the asymmetry phenomenon, where a naively trained detector tends to favor overfitting to the limited and monotonous fake patterns, causing the feature space to become highly constrained and low-ranked, which is proved seriously limiting the expressivity and generalization. One potential remedy is incorporating the pre-trained knowledge within the vision foundation models (higher-ranked) to expand the feature space, alleviating the model's overfitting to fake. To this end, we employ Singular Value Decomposition (SVD) to decompose the original feature space into two orthogonal subspaces. By freezing the principal components and adapting only the remained components, we preserve the pre-trained knowledge while learning fake patterns. Compared to existing full-parameters and LoRA-based tuning methods, we explicitly ensure orthogonality, enabling the higher rank of the whole feature space, effectively minimizing overfitting and enhancing generalization. We finally identify a crucial insight: our method implicitly learns a vital prior that fakes are actually derived from the real, indicating a hierarchical relationship rather than independence. Modeling this prior, we believe, is essential for achieving superior generalization. Our codes are publicly available at https://github.com/YZY-stack/Effort-AIGI-Detection.","In recent years, the rise of AI-generated images (AIGIs) has created both excitement and challenges. One major issue we noticed is that many detection methods struggle to identify the fake images effectively in the field of security (e.g. face recognition system). This happens because they often focus too much on a limited set of fake patterns neglecting the importance of real information, which makes it hard for them to recognize new or different types of fakes. To address this problem, we start from the perspective of how to distinguish fakes while learning good real information. Specifically, we break down the detection model into two parts: the important ones keeping real images's information from existing advanced models and the others that can adapt to identify fakes. With this fresh approach, we improved the model's ability to recognize a wider variety of AI-generated images.Our research not only improves the detection capacity but also highlights that the fakes often come from real images, suggesting a significant connection between them. Understanding this relationship is key to creating more stable detection systems to ensure the responsible use of AI-generated content."
Poster,OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference,https://ICML.cc//virtual/2025/poster/46443,"Seungjun Shin, Jaehoon Oh, Dokwan Oh","Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receives disproportionately high attention despite their limited semantic role. In this paper, we first expand the relationship between the sink token and other tokens, moving beyond attention to explore their similarity in hidden states, considering the layer depth. We observe that as the layers get deeper, the cosine similarity between the normalized hidden states of the sink token and those of other tokens increases, and that the normalized hidden states of the sink token exhibit negligible changes. These imply that other tokens consistently are directed toward the sink token throughout the layers. Next, we propose a dynamic token selection method, called OrthoRank, using these findings to select important tokens. Specifically, in a certain layer, we define token importance by the speed at which the token moves toward the sink token. This is converted into orthogonality with the sink token, meaning that tokens that are more orthogonal to the sink token are assigned greater importance. Finally, through extensive experiments, we demonstrated that our method results in lower perplexity and higher zero-shot accuracy compared to layer pruning methods at the same sparsity ratio with comparable throughput, while also achieving superior performance on LongBench.","Large language models (LLMs) are powerful but slow, partly because they process every word equally, even when some no longer need it.We observed that the first word in a sentence, though often meaningless, receives heavy attention. As the model goes deeper, other words start to behave like this first word, which stays mostly unchanged.Inspired by this, we developed OrthoRank. It identifies which words are still actively changing during processing and updates only those. The rest are temporarily skipped to save time.This simple idea speeds up AI models and reduces computation, while maintaining or even improving performance. OrthoRank works with many existing models and does not require retraining."
Poster,Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads,https://ICML.cc//virtual/2025/poster/44637,"Siqi Kou, Jiachun Jin, Zhihong Liu, Chang Liu, Ye Ma, jian jia, Quan Chen, Peng Jiang, Zhijie Deng","We introduce Orthus, a unified multimodal model that excels in generating interleaved images and text from mixed-modality inputs by simultaneously handling discrete text tokens and continuous image features under the \textbf{AR} modeling principle. The continuous treatment of visual signals minimizes the information loss while the fully AR formulation renders the characterization of the correlation between modalities straightforward. Orthus leverages these advantages through its modality-specific heads---one regular language modeling (LM) head predicts discrete text tokens and one diffusion head generates continuous image features. We devise an efficient strategy for building Orthus---by substituting the Vector Quantization (VQ) operation in the existing unified AR model with a soft alternative, introducing a diffusion head, and tuning the added modules to reconstruct images, we can create an Orthus-base model effortlessly (e.g., within 72 A100 GPU hours). Orthus-base can further embrace post-training to craft lengthy interleaved image-text, reflecting the potential for handling intricate real-world tasks. For visual understanding and generation, Orthus achieves a GenEval score of 0.58 and an MME-P score of 1265.8 using 7B parameters, outperforming competing baselines including Show-o and Chameleon.","Today’s AI models often need to understand both pictures and words — like describing an image, generating a picture from a sentence, or even combining the two in a creative way. But making one model that handles both smoothly is hard because images and text are different. Existing solutions either lose information when processing images or struggle to connect the two types of data well.We introduce a unified multimodal model called Orthus that learns to handle both text and images together, without forcing images into lossy representations. It uses one regular language modeling head to deal with discrete text and another diffusion head to generate continuous images, but both parts work together within a single transformer.Our model can understand and create mixed content like storybooks with interleaved images and text. We hope Orthus will make future multimodal models more creative, flexible, and useful for real-world tasks."
Poster,Oscillation-Reduced MXFP4 Training for Vision Transformers,https://ICML.cc//virtual/2025/poster/45588,"Yuxiang Chen, Haocheng Xi, Jun Zhu, Jianfei Chen","Pre-training Transformers in FP4 precision is becoming a promising approach to gain substantial speedup, but it comes with a considerable loss of accuracy. Microscaling (MX) data format provides a fine-grained per-group quantization method to improve the representation ability of the FP4 format and is supported by the next-generation Blackwell GPU architecture. However, training with MXFP4 data format still results in significant degradation and there is a lack of systematic research on the reason.In this work, we propose a novel training method TetraJet for a more accurate FP4 training. We comprehensively evaluate all of the quantizers involved in the training, and identify the weight oscillation problem in the forward pass as the main source of the degradation in MXFP4 training. Therefore, we introduce two novel methods, EMA Quantizer (Q-EMA) and Adaptive Ramping Optimizer (Q-Ramping), to resolve the oscillation problem. Extensive experiments on Vision Transformers demonstrate that TetraJet consistently outperforms the existing 4-bit training methods, and Q-EMA \& Q-Ramping can provide additional enhancement by effectively reducing oscillation. We decreased the accuracy degradation by more than 50% compared to the baseline, and can even achieve competitive performance compared to full precision training.","Pre-training large AI models like Transformers usually requires a lot of computing power. One way to make training faster and more efficient is to use lower-precision numbers like 4-bit floating point (FP4). While promising, FP4 often leads to a noticeable drop in model accuracy. A newer 4-bit format called ""Microscaling FP4"" shows promise, but it still struggles with performance issues, and the reasons behind this are not well understood.In our work, we introduce **TetraJet**, a new training method specifically designed to improve the accuracy of FP4 training with unbiased gradient calculation. Unlike previous approaches that may only use low-precision in parts of the model, TetraJet uses FP4 for **activations, weights, and gradients**, making it a truly low-precision training solution. Further detailed analysis shows that a key issue is ""weight oscillation"": unstable changes in model parameters during training. To fix this, we introduce a new training framework that includes two simple but powerful techniques: one that smooths out the quantization process (**Q-EMA**) and another that stabilizes optimization (**Q-Ramping**).Together, these methods make low-precision training much more reliable. We show that our approach significantly reduces accuracy loss by more than half, and in some cases, performs almost as well as training with full precision, but with much lower computational cost."
Poster,OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction,https://ICML.cc//virtual/2025/poster/45131,"Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel","Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained vision-language models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zero-shot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.","Teaching robots to follow instructions like “pick up the red cup” is hard, especially with new objects or settings. Most methods retrain the robot’s vision and language models, which can weaken their vision and language understanding.We introduce OTTER, a new approach that helps robots follow instructions by working with perception models that already understand how images relate to language, without retraining them. OTTER selectively focuses only on the parts of an image that are relevant to the instruction—like just the “red cup”—and passes that focused information to the robot’s decision-making system.Experiments show OTTER outperforms current methods, bringing us closer to robots that understand and act on instructions in many situations."
Poster,Otter: Generating Tests from Issues to Validate SWE Patches,https://ICML.cc//virtual/2025/poster/44767,"Toufique Ahmed, Jatin Ganhotra, Rangeet Pan, Avraham Shinnar, Saurabh Sinha, Martin Hirzel","While there has been plenty of work on generating tests from existing code, there has been limited work on generating tests from issues. A correct test must validate the code patch that resolves the issue. This paper focuses on the scenario where that code patch does not yet exist. Doing so supports two major use-cases. First, it supports TDD (test-driven development), the discipline of ""test first, write code later"" that has well-documented benefits for human software engineers. Second, it also validates SWE (software engineering) agents, which generate code patches for resolving issues. This paper introduces TDD-Bench-Verified, a benchmark for generating tests from issues, and Otter, an LLM-based solution for this task. Otter augments LLMs with rule-based analysis to check and repair their outputs, and introduces a novel self-reflective action planner. Experiments show Otter outperforming state-of-the-art systems for generating tests from issues, in addition to enhancing systems that generate patches from issues. We hope that Otter helps make developers more productive at resolving issues and leads to more robust, well-tested code.","While there has been extensive work on generating tests from existing code, there is a gap in creating tests directly from issues, especially before a code patch is developed. This is crucial for practices like Test-Driven Development (TDD) and validating software engineering agents that generate code patches. We developed Otter, an LLM-based solution that generates tests from issues. Otter enhances large language models with rule-based analysis to check and repair their outputs and introduces a novel self-reflective action planning stage. This ensures that the tests are accurate and effective.  Otter outperforms state-of-the-art systems in generating tests from issues and assists systems that generate patches from issues. By improving test generation, Otter helps developers be more productive in resolving issues and leads to more robust, well-tested code, thereby improving software quality and reliability."
Poster,Outlier-Aware Post-Training Quantization for Discrete Graph Diffusion Models,https://ICML.cc//virtual/2025/poster/43639,"Zheng Gong, Ying Sun","Discrete Graph Diffusion Models (DGDMs) mark a pivotal advancement in graph generation, effectively preserving sparsity and structural integrity, thereby enhancing the learning of graph data distributions for diverse generative applications. Despite their potential, DGDMs are computationally intensive due to the numerous low-parameter yet high-computation operations, thereby increasing the need of inference acceleration. A promising solution to mitigate this issue is model quantization. However, existing quantization techniques for Image Diffusion Models (IDMs) face limitations in DGDMs due to differing diffusion processes, while Large Language Model (LLM) quantization focuses on reducing memory access latency of loading large parameters, unlike DGDMs, where inference bottlenecks are computations due to smaller model sizes. To fill this gap, we introduce Bit-DGDM, a post-training quantization framework for DGDMs which incorporates two novel ideas: (i) sparse-dense activation quantization sparsely modeling the activation outliers through adaptively selected, data-free thresholds in full-precision and quantizing the remaining to low-bit, and (ii) ill-conditioned low-rank decomposition decomposing the weights into low-rank component enable faster inference and an $\alpha$-sparsity matrix that models outliers. Extensive experiments demonstrate that Bit-DGDM not only reducing the memory usage from the FP32 baseline by up to $2.8\times$ and achieve up to $2.5\times$ speedup, but also achieve comparable performance to ultra-low precision of up to 4-bit.","Discrete Graph Diffusion Models (DGDMs) significantly advance graph generation by preserving sparsity and structural integrity, improving learning of graph data distributions, but their many low-parameter yet high-computation operations make them computationally intensive, increasing the need for inference acceleration. We introduce Bit-DGDM, an outlier-aware post-training quantization framework for DGDMs, which enables faster inference and reduces memory usages."
Poster,Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models,https://ICML.cc//virtual/2025/poster/43693,"Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu","A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models.","When training AI models, not all examples are helpful — some can confuse the model or even make it worse. These “bad” training examples can be challenging to identify, especially in massive datasets used for modern deep learning models. Existing tools for finding them are either not very accurate or too slow to use with today’s large models.In this work, we introduce a faster, simpler method called Outlier Gradient Analysis. Instead of relying on heavy computations, we look at how each training example influences the model’s learning process through its gradient — a kind of learning signal. By identifying outliers in these gradients, we can spot training examples that are likely to hurt performance.We test our method on a range of tasks, like identifying mislabeled images and finding beneficial data for large language models. Outlier Gradient Analysis is highly effective across different domains, and offers a practical way of identifying errors in training datasets and building more reliable AI systems."
