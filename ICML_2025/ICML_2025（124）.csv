type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Geometry-Informed Neural Networks,https://ICML.cc//virtual/2025/poster/44083,"Arturs Berzins, Andreas Radler, Eric Volkmann, Sebastian Sanokowski, Sepp Hochreiter, Johannes Brandstetter","Geometry is a ubiquitous tool in computer graphics, design, and engineering. However, the lack of large shape datasets limits the application of state-of-the-art supervised learning methods and motivates the exploration of alternative learning strategies. To this end, we introduce geometry-informed neural networks (GINNs) -- a framework for training shape-generative neural fields without data by leveraging user-specified design requirements in the form of objectives and constraints. By adding diversity as an explicit constraint, GINNs avoid mode-collapse and can generate multiple diverse solutions, often required in geometry tasks. Experimentally, we apply GINNs to several problems spanning physics, geometry, and engineering design, showing control over geometrical and topological properties, such as surface smoothness or the number of holes. These results demonstrate the potential of training shape-generative models without data, paving the way for new generative design approaches without large datasets.","Creating machine-learning systems that can generate new shapes, like airplane parts or structural components, usually requires vast amounts of high-quality data. But such data is often unavailable in real-world engineering workflows. What if, instead, we could teach computers to generate useful, varied shapes without needing any examples at all?We explore Geometry-Informed Neural Networks (GINNs) -- models that learn purely from user-defined design requirements, such as manufacturability, weight, or attachment points. Rather than learning from data, GINNs are trained to satisfy these constraints directly. To reflect the need for design exploration, we also encourage the model to generate multiple distinct solutions. Surprisingly, this leads the model to discover meaningful and structured variations in shape, without being told what those variations should look like.This points to a promising new direction for generative design in data-scarce environments, which are common in engineering. More broadly, it offers an alternative paradigm for training generative models -- one that minimizes reliance on data, whether due to scarcity, exclusivity, or copyright concerns."
Poster,Geometry Informed Tokenization of Molecules for Language Model Generation,https://ICML.cc//virtual/2025/poster/46459,"Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Heng Ji, Shuiwang Ji","We consider molecule generation in 3D space using language models (LMs), which requires discrete tokenization of 3D molecular geometries. Although tokenization of molecular graphs exists, that for 3D geometries is largely unexplored. Here, we attempt to bridge this gap by proposing a novel method which converts molecular geometries into SE(3)-invariant 1D discrete sequences. Our method consists of canonical labeling and invariant spherical representation steps, which together maintain geometric and atomic fidelity in a format conducive to LMs. Our experiments show that, when coupled with our proposed method, various LMs excel in molecular geometry generation, especially in controlled generation tasks. Our code has been released as part of the AIRS library (https://github.com/divelab/AIRS/).","Designing new molecules with desired properties is a central challenge in drug discovery and material science. While recent AI models have made strides in generating molecules as strings (like text), they often ignore the molecule’s 3D shape—crucial for how it behaves in the real world. Our research tackles this problem by introducing a method called Geo2Seq, which teaches language models to understand and generate molecules using both their atomic structures and their 3D geometry.We designed a novel way to convert 3D molecules into sequences that retain spatial information, allowing language models to “read” and “write” molecules more effectively. We also built a geometric-aware decoder that reconstructs full 3D molecules from the generated sequences. This two-part approach makes it possible to generate realistic, valid, and diverse 3D molecular structures.Our method improves how well AI can design new molecules with specific geometric properties, which could accelerate the discovery of new drugs and materials. By combining powerful language models with 3D chemical knowledge, Geo2Seq bridges a major gap between structure-based science and generative AI."
Poster,GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing,https://ICML.cc//virtual/2025/poster/44111,"Akashah Shabbir, Ilmuz Zaman Mohammed Zumri, Mohammed Bennamoun, Fahad Khan, Salman Khan","Recent advances in large multimodal models (LMMs) have recognized fine-grained grounding as an imperative factor of visual understanding and dialogue. However, the benefits of such representation in LMMs are limited to the natural image domain, and these models perform poorly for remote sensing (RS). The distinct overhead viewpoint, scale variation, and presence of small objects in high-resolution RS imagery present a unique challenge in region-level comprehension. Moreover, the development of the grounding conversation capability of LMMs within RS is hindered by the lack of granular, RS domain-specific grounded data. Addressing these limitations, we propose GeoPixel - the first end-to-end high-resolution RS-LMM that supports pixel-level grounding. This capability allows fine-grained visual perception by generating interleaved masks in conversation. GeoPixel supports up to 4K HD resolution in any aspect ratio, ideal for high-precision RS image analysis. To support the grounded conversation generation (GCG) in RS imagery, we curate a visually grounded dataset GeoPixelD through a semi-automated pipeline that utilizes set-of-marks prompting and spatial priors tailored for RS data to methodically control the data generation process. GeoPixel demonstrates superior performance in pixel-level comprehension, surpassing existing LMMs in both single-target and multi-target segmentation tasks. Our methodological ablation studies validate the effectiveness of each component in the overall architecture. Our code and data will be publicly released.","Modern artificial intelligence (AI) systems are getting better at understanding both pictures and language at the same time. These models are called large multimodal models (LMMs). But most of these models are trained on everyday photos, and they don’t work well with satellite or aerial images used in remote sensing (RS) that show the Earth from above and help with tasks like mapping, urban planning, and environmental monitoring.We propose GeoPixel, an LMM designed especially for remote sensing and can perform “pixel grounding”, which means it can look at a satellite image, understand what’s in it, and then link specific words to the exact parts of the image they describe, down to the pixel level. This allows for a detailed and accurate understanding of what’s in the image.Here’s how GeoPixel stands out:- It works with high-resolution images (up to 4K quality), which is important for spotting small objects like cars from above.- It can generate detailed descriptions of remote sensing images while also providing segmentation masks that highlight exactly where objects are in the image.- We also created an RS dataset called GeoPixelD. This includes over 600,000 objects and associated descriptions, helping the model learn to understand and describe remote sensing images accurately.- Compared to existing models, GeoPixel performs better at identifying and describing multiple objects in a single image and is especially good at linking language with specific visual features.GeoPixel outperformed other models in describing scenes and identifying objects in complex, high-resolution remote sensing images. It can help with practical applications such as disaster response, environmental monitoring, and infrastructure planning by offering more precise, grounded, and interpretable information."
Poster,GHOST: Generalizable One-Shot Federated Graph Learning with Proxy-Based Topology Knowledge Retention,https://ICML.cc//virtual/2025/poster/44117,"Jiaru Qian, Guancheng Wan, Wenke Huang, Guibin Zhang, Yuxin Wu, Bo Du, Mang Ye","Federated Graph Learning (FGL) proposes an effective approach to collaboratively training Graph Neural Networks (GNNs) while maintaining privacy. Nevertheless, communication efficiency becomes a critical bottleneck in environments with limited resources. In this context, one-shot FGL emerges as a promising solution by restricting communication to a single round. However, prevailing FGL methods face two key challenges in the one-shot setting: 1) They heavily rely on gradual personalized optimization over multiple rounds, undermining the capability of the global model to efficiently generalize across diverse graph structures. 2) They are prone to overfitting to local data distributions due to extreme structural bias, leading to catastrophic forgetting. To address these issues, we introduce **GHOST**, an innovative one-shot FGL framework. In GHOST, we establish a proxy model for each client to leverage diverse local knowledge and integrate it to train the global model. During training, we identify and consolidate parameters essential for capturing topological knowledge, thereby mitigating catastrophic forgetting. Extensive experiments on real-world tasks demonstrate the superiority and generalization capability of GHOST. The code is available at https://github.com/JiaruQian/GHOST.","Federated Graph Learning (FGL) enables collaborative training of graph-based models without sharing private data, but often suffers from high communication costs. To address this, we propose **GHOST**, a one-shot FGL framework that requires only a single round of communication. GHOST introduces proxy models to capture diverse local knowledge and uses a Topology-Consistency Criterion to retain critical structural information, preventing overfitting and forgetting. Experiments on real-world tasks show that GHOST achieves strong generalization and outperforms existing methods."
Poster,GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation,https://ICML.cc//virtual/2025/poster/46217,"Jiashu HE, Mingyu Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro","Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT). However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass. Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains. We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input. GIVE guides the LLM agent to select the most pertinent expert data ($\textbf{observe}$), engage in query-specific associative thinking ($\textbf{reflect}$), and then synthesize this information to produce the final output ($\textbf{speak}$). Extensive experiments demonstrated the following benefits of our framework: (1) GIVE increases the performance of LLMs across various sizes. (2) In some scenarios, GIVE allows smaller LLMs to surpass larger, more sophisticated ones in scientific tasks ($\textbf{GPT3.5T + GIVE > GPT4}$). (3) GIVE is effective on scientific and open-domain assessments. (4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to $\textbf{43.5}$\% $\rightarrow$ $\textbf{88.2}$\% accuracy improvement). (5) GIVE allows LLM agents to reason using both restricted (very small) and noisy (very large) knowledge sources, accommodating knowledge graphs (KG) ranging from $\textbf{135}$ to more than $\textbf{840k}$ nodes. (6) The reasoning process involved in GIVE is fully interpretable. Our code is available at https://github.com/Jason-Tree/GIVE","Large language models are powerful text generators but often stumble on complex questions due to lack of domain-specific knowledge. Existing fixes either lean entirely on the model’s internal “memory” or demand huge external databases—both of which can be impractical for specialized scientific topics. We introduce Graph Inspired Veracity Extrapolation (GIVE), a three-step method that lets the model tap into the right expert facts, think through them step by step, and then craft a clear answer. GIVE first observes by selecting pertinent data, then reflects through query-specific associative thinking, and finally speaks by synthesizing a coherent response. In experiments, GIVE boosts reasoning accuracy across model sizes—enabling smaller models to outperform much larger ones on scientific tasks (for example, GPT-3.5 + GIVE beats GPT-4). It works without any additional training, handles knowledge graphs from a few dozen to hundreds of thousands of nodes, and shines in both open-domain and niche scientific benchmarks. Because its simple observe-reflect-speak process is fully interpretable, GIVE offers a transparent, training-free way to give LLMs real-world reasoning power."
Poster,GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras,https://ICML.cc//virtual/2025/poster/45802,"Ekaterina Filimoshina, Dmitry Shirokov","We propose, implement, and compare with competitors a new architecture of equivariant neural networks based on geometric (Clifford) algebras: Generalized Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are equivariant to all pseudo-orthogonal transformations, including rotations and reflections, of a vector space with any non-degenerate or degenerate symmetric bilinear form. We propose a weight-sharing parametrization technique that takes into account the fundamental structures and operations of geometric algebras. Due to this technique, GLGENN architecture is parameter-light and has less tendency to overfitting than baseline equivariant models. GLGENN outperforms or matches competitors on several benchmarking equivariant tasks, including estimation of an equivariant function and a convex hull experiment, while using significantly fewer optimizable parameters.","Many physical systems, such as robots, molecules, or charged particles, behave the same way even if you rotate or mirror them. Equivariant neural networks are a special class of machine learning models designed to recognize and preserve these kinds of symmetries in data. This makes equivariant models particularly powerful for applications in physics, engineering, biology, computer vision, and other scientific fields, for example, for modeling physical systems, molecules, and other scientific data. However, current equivariant models often require large numbers of trainable parameters, leading to inefficient training times and a tendency to overfit, especially when training data is limited. In our research, we introduce a new equivariant architecture called GLGENN (Generalized Lipschitz Group Equivariant Neural Networks), which achieves symmetry-aware learning with far fewer parameters. We base our design on a well-known mathematical framework called Clifford (geometric) algebras and introduce a novel technique for weight sharing that respects the underlying algebraic structures. We validate our model on multiple benchmark tasks, including physics-inspired simulations and geometric regression, and show that it matches or outperforms existing methods with significantly fewer trainable parameters. GLGENN open the door to more efficient and broadly applicable equivariant models in machine learning."
Poster,Global Context-aware Representation Learning for Spatially Resolved Transcriptomics,https://ICML.cc//virtual/2025/poster/44294,"Yunhak Oh, Junseok Lee, Yeongmin Kim, Sangwoo Seo, Namkyeong Lee, Chanyoung Park","Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that captures the spatial context of cells within tissues, enabling the study of complex biological networks. Recent graph-based methods leverage both gene expression and spatial information to identify relevant spatial domains. However, these approaches fall short in obtaining meaningful spot representations, especially for spots near spatial domain boundaries, as they heavily emphasize adjacent spots that have minimal feature differences from an anchor node. To address this, we propose Spotscape, a novel framework that introduces the Similarity Telescope module to capture global relationships between multiple spots. Additionally, we propose a similarity scaling strategy to regulate the distances between intra- and inter-slice spots, facilitating effective multi-slice integration. Extensive experiments demonstrate the superiority of Spotscape in various downstream tasks, including single-slice and multi-slice scenarios.","Understanding how cells are organized and function within tissues is important for biology and medicine. Spatially Resolved Transcriptomics (SRT) is a new technology that measures gene activity in cells along with their exact locations inside tissues microenvironment. However, analyzing this complex data is challenging because cells near boundaries between tissue regions look very similar locally, making it hard to distinguish meaningful patterns.To solve this, we developed Spotscape, a new machine learning method that looks beyond immediate neighbors and captures global relationships between many cells at once. This helps to better identify distinct tissue regions and important gene expression patterns. Spotscape also integrates data from multiple tissue slices, overcoming technical differences that often confuse analysis.Our method outperforms existing approaches on various SRT datasets and reveals biologically meaningful insights. Spotscape is fast and scalable, making it useful for large and complex spatial transcriptomics studies. This work can advance biomedical research by providing more accurate tools to understand tissue organization and disease mechanisms."
Poster,Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $\mu$ Parametrization,https://ICML.cc//virtual/2025/poster/44572,"Zixiang Chen, Greg Yang, Qingyue Zhao, Quanquan Gu","Despite deep neural networks' powerful representation learning capabilities, theoretical understanding of how networks can simultaneously achieve meaningful feature learning and global convergence remains elusive. Existing approaches like the neural tangent kernel (NTK) are limited because features stay close to their initialization in this parametrization, leaving open questions about feature properties during substantial evolution. In this paper, we investigate the training dynamics of infinitely wide, $L$-layer neural networks using the tensor program (TP) framework. Specifically, we show that, when trained with stochastic gradient descent (SGD) under the Maximal Update parametrization ($\mu$P) and mild conditions on the activation function, SGD enables these networks to learn linearly independent features that substantially deviate from their initial values. This rich feature space captures relevant data information and ensures that any convergent point of the training process is a global minimum. Our analysis leverages both the interactions among features across layers and the properties of Gaussian random variables, providing new insights into deep representation learning. We further validate our theoretical findings through experiments on real-world datasets.","Artificial intelligence systems called neural networks have achieved remarkable success in tasks like image recognition and language processing. However, scientists still don't fully understand why these systems work so well. A key puzzle is whether neural networks can simultaneously do two important things: learn useful patterns from data (called ""feature learning"") and find the best possible solution to a problem (called ""global optimization"").Previous approaches for deep L-layer networks either allow networks to learn patterns but have little understanding about global optimization, or guarantee finding good solutions but prevent meaningful learning. Our research resolves this puzzle by studying a specific way of setting up neural networks called ""Maximal Update Parametrization"". We prove mathematically that when networks are made very wide and trained using this approach, they can indeed do both things at once: they learn rich, meaningful patterns from data while also finding globally optimal solutions.We validate our theory through experiments showing how networks maintain diverse, independent features throughout training. This work provides new theoretical foundations for understanding why certain AI training methods work better than others, potentially informing the design of more effective AI systems."
Poster,Global curvature for second-order optimization of neural networks,https://ICML.cc//virtual/2025/poster/44556,Alberto Bernacchia,"Second-order optimization methods, which leverage the local curvature of the loss function, have the potential to dramatically accelerate the training of machine learning models. However, these methods are often hindered by the computational burden of constructing and inverting large curvature matrices with $\mathcal{O}(p^2)$ elements, where $p$ is the number of parameters. In this work, we present a theory that predicts the \emph{exact} structure of the global curvature by leveraging the intrinsic symmetries of neural networks, such as invariance under parameter permutations. For Multi-Layer Perceptrons (MLPs), our approach reveals that the global curvature can be expressed in terms of $\mathcal{O}(d^2 + L^2)$ independent factors, where $d$ is the number of input/output dimensions and $L$ is the number of layers, significantly reducing the computational burden compared to the $\mathcal{O}(p^2)$ elements of the full matrix. These factors can be estimated efficiently, enabling precise curvature computations.To evaluate the practical implications of our framework, we apply second-order optimization to synthetic data, achieving markedly faster convergence compared to traditional optimization methods.Our findings pave the way for a better understanding of the loss landscape of neural networks, and for designing more efficient training methodologies in deep learning.Code: \href{https://github.com/mtkresearch/symo_notebooks}{github.com/mtkresearch/symo\_notebooks}","Training machine learning models can be slow, but advanced optimization techniques that use the ""curvature"" (or shape) of the loss function could speed things up. However, these methods usually require a lot of computation. In this work, we discovered that the built-in symmetries of neural networks (like how rearranging some neurons doesn’t change the output) simplify these calculations. The curvature can be described using far fewer values than expected, making computations much faster. This research helps us better understand how neural networks learn and could lead to faster and more efficient training methods in the future."
Poster,Global-Local Dirichlet Processes for Clustering Grouped Data in the Presence of Group-Specific Idiosyncratic Variables,https://ICML.cc//virtual/2025/poster/43711,"Arhit Chakrabarti, Yang Ni, Debdeep Pati, Bani Mallick","We consider the problem of clustering grouped data for which the observations may include group-specific variables in addition to the variables that are shared across groups. This type of data is quite common; for example, in cancer genomic studies, molecular information is available for all cancers whereas cancer-specific clinical information may only be available for certain cancers. Existing grouped clustering methods only consider the shared variables but ignore valuable information from the group-specific variables. To allow for these group-specific variables to aid in the clustering, we propose a novel Bayesian nonparametric approach, termed global-local (GLocal) Dirichlet process, that models the ""global-local"" structure of the observations across groups. We characterize the GLocal Dirichlet process using the stick-breaking representation and the representation as a limit of a finite mixture model. We theoretically quantify the approximation errors of the truncated prior, the corresponding finite mixture model, and the associated posterior distribution. We develop a fast variational Bayes algorithm for scalable posterior inference, which we illustrate with extensive simulations and a TCGA pan-gastrointestinal cancer dataset.","In many real-world studies, data is collected from different groups that share some common information across the groups, but each group may also be accompanied by its own unique information. For example, in cancer research, we might have genetic data shared across all cancer types, while certain clinical details might only be available for specific types of cancer. Current methods that group or ""cluster"" such data usually only look at the shared or global information and ignore the group-specific or local features, which can lead to less accurate results. To address this, we introduce a new statistical method called the global-local (GLocal) Dirichlet process, which can handle both shared and group-specific information. This method is based on a flexible Bayesian approach that does not require fixing the number of clusters in advance. It captures the structure of the data at both the global (shared) and local (group-specific) levels. In our paper, we explain how this method works under the hood, show how it can be approximated for practical purpose, and provide mathematical guarantees for the accuracy of approximation. Finally, we develop a fast algorithm to apply our proposed method to large datasets. We show that our proposed method and algorithm works well using simulations and a real cancer data from a large public database."
