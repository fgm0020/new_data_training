type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,VIP: Vision Instructed Pre-training for Robotic Manipulation,https://ICML.cc//virtual/2025/poster/44680,"Zhuoling Li, LiangLiang Ren, Jinrong Yang, Yong Zhao, Xiaoyang Wu, Zhenhua Xu, Xiang Bai, Hengshuang Zhao","The effectiveness of scaling up training data in robotic manipulation is still limited. A primary challenge in manipulation is the tasks are diverse, and the trained policy would be confused if the task targets are not specified clearly. Existing works primarily rely on text instruction to describe targets. However, we reveal that current robotic data cannot train policies to understand text instruction effectively, and vision is much more comprehensible. Therefore, we introduce utilizing vision instruction to specify targets. A straightforward implementation is training a policy to predict the intermediate actions linking the current observation and a future image. Nevertheless, a single future image does not describe the task target in insufficient detail. To handle this problem, we propose to use sparse point flows to provide more detailed information. Extensive tasks are designed based on real and simulated environments to evaluate the effectiveness of our vision instructed pre-training (VIP) method. The results indicate VIP improves the performance on diverse tasks significantly, and the derived policy can complete competitive tasks like ``opening the lid of a tightly sealed bottle''.",This work points out that vision instruction is more comprehensible than text instruction for current embodied policies and develops a novel manipulation pre-train paradigm based on sparse point flow.
Poster,Vision Graph Prompting via Semantic Low-Rank Decomposition,https://ICML.cc//virtual/2025/poster/43863,"Zixiang Ai, Zichen Liu, Jiahuan Zhou","Vision GNN (ViG) demonstrates superior performance by representing images as graph structures, providing a more natural way to capture irregular semantic patterns beyond traditional grid or sequence-based representations. To efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning techniques like visual prompting become increasingly essential. However, existing prompting methods are primarily designed for Transformer-based models, neglecting the rich topological relationships among nodes and edges in graph-based representations, limiting their capacity to model complex semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel framework tailored for vision graph structures. Our core insight reveals that semantically connected components in the graph exhibit low-rank properties. Building on this observation, we introduce a semantic low-rank prompting method that decomposes low-rank semantic features and integrates them with prompts on vision graph topologies, capturing both global structural patterns and fine-grained semantic dependencies. Extensive experiments demonstrate our method significantly improves ViG’s transfer performance on diverse downstream tasks, achieving results comparable to full fine-tuning while maintaining parameter efficiency.","In this paper, we propose Vision Graph Prompting (VGP), a novel framework tailored for vision graph structures. Our core insight reveals that semantically connected components in the graph exhibit low-rank properties. Building on this observation, we introduce a semantic low-rank prompting method that decomposes low-rank semantic features and integrates them with prompts on vision graph topologies, capturing both global structural patterns and fine-grained semantic dependencies. Extensive experiments demonstrate our method significantly improves ViG’s transfer performance on diverse downstream tasks, achieving results comparable to full fine-tuning while maintaining parameter efficiency."
Poster,Vision-Language Models Create Cross-Modal Task Representations,https://ICML.cc//virtual/2025/poster/46340,"Grace Luo, Trevor Darrell, Amir Bar","Autoregressive vision-language models (VLMs) can handle many tasks within a single model, yet the representations that enable this capability remain opaque. We find that VLMs align conceptually equivalent inputs into a shared task vector, which is invariant to modality (text, image) and format (examples, instruction), and may simplify VLM processing. We measure this alignment via cross-modal transfer--the ability of a task vector derived in one modality to trigger the correct generation in another--on a range of tasks and model architectures. Although the task vector is highly compressed, we find that this single vector outperforms prompting the model with the full task information, unique to this cross-modal case. Furthermore, we show that task vectors can be transferred from a base language model to its fine-tuned vision-language counterpart, and that they can be derived solely from instructions without the need for examples. Taken together, our findings shed light on how VLMs internally process task information, and how they map different modalities into common semantic representations.","Vision-language models, or VLMs, are a class of models that process images based on user prompts. This means that VLMs can handle many computer vision tasks simply by adjusting the prompt, but we don't understand how they internally route what task to do based on the inputs.We find that VLMs encode tasks in a representation space that is shared across text and images. For example, we show it is possible to define a task with text examples and use that same representation to trigger the model to perform the same task on an image query. Even more surprisingly, we show that representations from the base language model (from which the VLM was initialized) can trigger the same behavior in the vision-language model. Taken together, our findings shed light on how VLMs internally process task information, and how they map different modalities into common semantic representations."
Poster,Vision-Language Model Selection and Reuse for Downstream Adaptation,https://ICML.cc//virtual/2025/poster/44712,"Hao-Zhe Tan, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo","Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called **M**odel **L**abel **L**earning (**MLL**). The proposal contains three key modules: *model labeling*, which assigns labels to each VLM to describe their specialty and utility; *model selection*, which matches the requirements of the target task with model labels; and *model reuse*, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs.","Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called **M**odel **L**abel **L**earning (**MLL**). The proposal contains three key modules: *model labeling*, which assigns labels to each VLM to describe their specialty and utility; *model selection*, which matches the requirements of the target task with model labels; and *model reuse*, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs."
Poster,VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters,https://ICML.cc//virtual/2025/poster/46441,"Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Wang, Jianling Sun, Chenghao Liu","Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either repurpose large language models (LLMs) or build large-scale time series datasets to develop TSF foundation models for universal forecasting. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. This paper explores a new road to building a TSF foundation model from rich, high-quality natural images. Our key insight is that a visual masked autoencoder, pre-trained on the ImageNet dataset, can naturally be a numeric series forecaster. By reformulating TSF as an image reconstruction task, we bridge the gap between image pre-training and TSF downstream tasks. Surprisingly, without further adaptation in the time series domain, the proposed VisionTS could achieve better zero-shot forecast performance than existing TSF foundation models. With fine-tuning for one epoch, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases. Extensive experiments reveal intrinsic similarities between images and real-world time series, suggesting that visual models may offer a ""free lunch"" for TSF and highlight the potential for future cross-modality research. Our code is available in the https://github.com/Keytoyze/VisionTS.","These days, powerful AI models called ""foundation models"" have revolutionized fields like language processing and computer vision. People now want to use a *universal* foundation model for various fields of time series forecasting, like predicting energy use, weather, or traffic. But current approaches struggle because time series data from different areas are very different, making it hard to build one universal model.We had a creative idea: **Why not use a model already trained on *images* for forecasting?** We took an AI model pre-trained on ImageNet and treated time series data like images. By turning numbers into ""images"" and having the model reconstruct them, we bridged the gap between vision and forecasting. Amazingly, this worked without any extra training on actual time series data.Our model, VisionTS, outperformed existing specialized time series models **even without any time series training**. This suggests images and time series share surprising hidden similarities. Using visual models for forecasting could be a powerful ""free lunch,"" opening exciting new paths for AI research across different data types."
Poster,Visual Abstraction: A Plug-and-Play Approach for Text-Visual Retrieval,https://ICML.cc//virtual/2025/poster/46021,"Guofeng Ding, Yiding Lu, Peng Hu, Mouxing Yang, Yijie Lin, Xi Peng","Text-to-visual retrieval often struggles with semantic redundancy and granularity mismatches between textual queries and visual content. Unlike existing methods that address these challenges during training, we propose VISual Abstraction (VISA), a test-time approach that enhances retrieval by transforming visual content into textual descriptions using off-the-shelf large models. The generated text descriptions, with their dense semantics, naturally filter out low-level redundant visual information. To further address granularity issues, VISA incorporates a question-answering process, enhancing the text description with the specific granularity information requested by the user. Extensive experiments demonstrate that VISA brings substantial improvements in text-to-image and text-to-video retrieval for both short- and long-context queries, offering a plug-and-play enhancement to existing retrieval systems.","Finding the right image or video by typing a short description, like “a woman skiing with a child”, is harder for computers than it seems. Visuals often include details that don’t neatly match the words we use.Our method, called VISA, improves this by turning images into clear text summaries using AI tools, and then refining them with smart follow-up questions. It doesn’t require retraining, and can easily plug into existing systems to make search results more accurate and meaningful."
Poster,Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning,https://ICML.cc//virtual/2025/poster/43761,"Rina Bao, Shilong Dong, Zhenfang Chen, Sheng He, Patricia Ellen Grant, Yangming Ou","Medical Visual Question Answering (MVQA) requires AI models to answer questions related to medical images, offering significant potential to assist medical professionals in evaluating and diagnosing diseases, thereby improving early interventions. However, existing MVQA datasets primarily focus on basic questions regarding visual perception and pattern recognition, without addressing the more complex questions that are critical in clinical diagnosis and decision-making. This paper introduces a new benchmark designed for professional-level medical reasoning, simulating the decision-making process. We achieve this by collecting MRI and clinical data related to Hypoxic-Ischemic Encephalopathy, enriched with expert annotations and insights. Building on this data, we generate clinical question-answer pairs and MRI interpretations to enable comprehensive diagnosis, interpretation, and prediction of neurocognitive outcomes. Our evaluation of current large vision-language models (LVLMs) shows limited performance on this benchmark, highlighting both the challenges of the task and the importance of this benchmark for advancing medical AI. Furthermore, we propose a novel ``Clinical Graph of Thoughts"" model, which integrates domain-specific medical knowledge and clinical reasoning processes with the interpretive abilities of LVLMs. The model demonstrates promising results, achieving around 15\% absolute gain on the most important neurocognitive outcome task, while the benchmark still reveals substantial opportunities for further research innovation.","This study focuses on making artificial intelligence (AI) better at helping doctors understand and diagnose brain injuries in newborns using MRI scans. Right now, most AI systems that answer questions about medical images are good at spotting simple patterns, but they struggle with the complex thinking doctors do when making real medical decisions.To tackle this, we created a new benchmark, using real MRI scans and expert knowledge about a neonatal brain condition called Hypoxic-Ischemic Encephalopathy (HIE). They built a set of medical questions and answers based on expert interpretations, aiming to mimic how doctors analyze MRI images and predict future brain development in affected infants.When we tested current advanced AI models on this challenge, the models didn’t perform well, showing that there’s still a long way to go. We also built a new AI model called the ""Clinical Graph of Thoughts,"" which combines medical knowledge and clinical reasoning. This model did much better—improving prediction accuracy by about 15%—and shows promise for future tools that could support doctors in diagnosing and treating brain injuries more effectively.In short, this work takes a step toward smarter, more helpful AI tools in medicine to actually think like clinicians."
Poster,Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models,https://ICML.cc//virtual/2025/poster/43450,"Mingi Jung, Saehyung Lee, Eunji Kim, Sungroh Yoon","Detailed image captioning is essential for tasks like data generation and aiding visually impaired individuals. High-quality captions require a balance between precision and recall, which remains challenging for current multimodal large language models (MLLMs). In this work, we hypothesize that this limitation stems from weakening and increasingly noisy visual attention as responses lengthen. To address this issue, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method that enhances the contribution of visual tokens during decoding. SPARC is founded on three key observations: (1) increasing the influence of all visual tokens reduces recall; thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen, visual attention becomes noisier, so SPARC identifies critical visual tokens by leveraging attention differences across time steps; (3) as visual attention gradually weakens, SPARC reinforces it to preserve its influence. Our experiments, incorporating both automated and human evaluations, demonstrate that existing methods improve the precision of MLLMs at the cost of recall. In contrast, our proposed method enhances both precision and recall with minimal computational overhead.","When computers describe images in detail — a task important for things like creating new data and helping people who are visually impaired — they need to be both accurate (precision) and thorough (recall). However, today’s advanced models often struggle to balance these two goals.We found that as these models generate longer descriptions, their focus on the image starts to blur, and they rely less on the actual visual content, leading to errors. To fix this, we created a simple technique called SPARC. It works by carefully boosting the most important visual details while the model writes the caption, helping it stay focused on the image even as the description grows longer.Unlike other methods that improve precision but harm recall, SPARC improves both, and it does so without needing extra training or heavy computation. This can make automatic image descriptions more useful and reliable."
Poster,Visual Autoregressive Modeling for Image Super-Resolution,https://ICML.cc//virtual/2025/poster/45697,"Yunpeng Qu, Kun Yuan, Jinhua Hao, Kai Zhao, Qizhi Xie, Ming Sun, Chao Zhou","Image Super-Resolution (ISR) has seen significant progress with the introduction of remarkable generative models.However, challenges such as the trade-off issues between fidelity and realism, as well as computational complexity, have also posed limitations on their application.Building upon the tremendous success of autoregressive models in the language domain, we propose \textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with the form of next-scale prediction.To effectively integrate and preserve semantic information in low-resolution images, we propose using prefix tokens to incorporate the condition.Scale-aligned Rotary Positional Encodings are introduced to capture spatial structures and the diffusion refiner is utilized for modeling quantization residual loss to achieve pixel-level fidelity.Image-based Classifier-free Guidance is proposed to guide the generation of more realistic images.Furthermore, we collect large-scale data and design a training process to obtain robust generative priors.Quantitative and qualitative results show that VARSR is capable of generating high-fidelity and high-realism images with more efficiency than diffusion-based methods.Our codes are released at \url{https://github.com/quyp2000/VARSR}.","In reality, images often undergo various degradations and damages during processes such as capturing and transmission. The image super-resolution problem aims to generate high-quality images that are faithful to the original from these degraded low-resolution images. Past methods have faced challenges in balancing fidelity and quality, often requiring lengthy computational times.Our work explores a novel progressive image restoration paradigm, VARSR, where we first generate an image at a coarse low-resolution scale and then progressively refine it from coarse to fine based on the content generated in the previous step. We have introduced information and spatial structures from degraded images in a more effective and efficient manner to ensure that the generated images have fully faithful content. We have also implemented re-guidance during the image generation process to ensure the production of higher-quality images.Our method has achieved surprising results, enabling the generation of high-fidelity and realistic images at a faster pace. It demonstrates excellent performance across different scenarios and content types."
Poster,Visual Generation Without Guidance,https://ICML.cc//virtual/2025/poster/44479,"Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang Su, Jun Zhu","Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free.","Generative AI models can create realistic images, but they often rely on a technique called ""Classifier-Free Guidance"" (CFG), which uses two versions of the model during image generation. This makes the process more expensive and slower. In our work, we introduce a new method called Guidance-Free Training (GFT) that removes the need for this extra guidance. It’s easy to use, works with a variety of generative techniques, and cuts the computing cost in half. Our experiments show that GFT performs just as well, or even better, than CFG on many tasks, making it a more efficient and practical choice for building powerful AI image generators."
