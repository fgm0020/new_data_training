type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing,https://ICML.cc//virtual/2025/poster/45758,"Zijie Qiu, Jiaqi Wei, Xiang Zhang, Sheng Xu, Kai Zou, Zhi Jin, ZhiQiang Gao, Nanqing Dong, Siqi Sun","De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (**P**eptide **M**ass **D**eviation) and RMD (**R**esidual**M**ass **D**eviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models—those whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.","Identifying the exact structure of proteins is crucial for understanding how our bodies work and for developing new drugs, but current AI methods struggle with the noisy data from lab instruments used to analyze proteins.We created RankNovo, a new AI system that combines the strengths of multiple protein analysis models instead of relying on just one. RankNovo acts like a panel of experts that reviews multiple possible protein structures and selects the most accurate one by considering how the candidates relate to each other.Our approach significantly improves protein identification accuracy compared to existing methods. Remarkably, RankNovo can even enhance the performance of protein analysis tools it wasn't specifically trained on, making it a versatile solution for researchers across different labs and experiments. This advancement will help scientists better understand diseases and develop more effective treatments by providing more reliable protein analysis."
Poster,Universal Length Generalization with Turing Programs,https://ICML.cc//virtual/2025/poster/45283,"Kaiying Hou, David Brandfonbrener, Sham Kakade, Samy Jelassi, Eran Malach","Length generalization refers to the ability to extrapolate from short training sequences to long test sequences and is a challenge for current large language models. While prior work has proposed some architecture or data format changes to achieve length generalization, these proposals typically apply to a limited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT) techniques, we propose *Turing Programs*, a novel CoT strategy that decomposes an algorithmic task into steps mimicking the computation of a Turing Machine. This framework is both universal, as it can accommodate any algorithmic task, and simple, requiring only copying text from the context with small modifications. We show that by using Turing Programs, we obtain robust length generalization on a range of algorithmic tasks: addition, multiplication and in-context SGD. We then demonstrate that transformers achieve length generalization on random Turing Programs, suggesting that length generalization is possible for any algorithmic task. Finally, we theoretically prove that transformers can implement Turing Programs, constructing a simple RASP (Weiss et al.) program that simulates an arbitrary Turing machine.","If language models are trained only on 10-digit addition problems, can they solve 20-digit additions? This example illustrates length generalization—a model's ability to extrapolate to longer tasks than it encountered during training. While previous research has demonstrated length generalization for specific problems, we sought to develop a technique that would work across many different tasks. Our solution was Turing Programs, a Chain-of-Thought technique that breaks complex problems into simple steps modeled after how Turing machines perform computations. In each step, the model would copy data from the previous step while performing a single “unit” of computation.  Using this approach, we achieved length generalization across various tasks, including addition and multiplication."
Poster,Universal Neural Optimal Transport,https://ICML.cc//virtual/2025/poster/43796,"Jonathan Geuter, Gregor Kornhardt, Ingimar Tomasson, Vaios Laschos","Optimal Transport (OT) problems are a cornerstone of many applications, but solving them is computationally expensive. To address this problem, we propose UNOT (Universal Neural Optimal Transport), a novel framework capable of accurately predicting (entropic) OT distances and plans between discrete measures of variable resolution for a given cost function. UNOT builds on Fourier Neural Operators, a universal class of neural networks that map between function spaces and that are discretization-invariant, which enables our network to process measures of varying sizes. The network is trained adversarially using a second, generating network and a self-supervised bootstrapping loss. We theoretically justify the use of FNOs, prove that our generator is universal, and that minimizing the bootstrapping loss provably minimizes the ground truth loss. Through extensive experiments, we show that our network not only accurately predicts optimal transport distances and plans across a wide range of datasets, but also captures the geometry of the Wasserstein space correctly. Furthermore, we show that our network can be used as a state-of-the-art initialization for the Sinkhorn algorithm, significantly outperforming existing approaches.","Optimal Transport is an area of mathematics that plays an important role in many applications, spanning fields such as artificial intelligence (AI), logistics, biology, physics, seismology, predicting climate and weather patterns, or economics. Many of these applications involve computing so-called Optimal Transport distances. These distances can be thought of as how expensive it is to move something from one arrangement or location to another. They can be used to compute the similarity between objects such as delivery trucks and houses, comparing two images or paintings, comparing audio recordings, cloud or smoke patterns, weather patterns, biological cell processes, predictions made by AI algorithms, and much more. However, computing these distances is oftentimes very expensive, even for computers. We propose a new machine learning method which can be used to predict these distances more quickly. Importantly, we construct our method in such a way that, unlike previous methods, it can be applied more broadly to different tasks and also to objects of different sizes."
Poster,Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment,https://ICML.cc//virtual/2025/poster/45097,"Harrish Thasarathan, Julian Forsyth, Thomas Fel, Matthew Kowal, Konstantinos Derpanis","We present Universal Sparse Autoencoders (USAEs), a framework for uncovering and aligning interpretable concepts spanning multiple pretrained deep neural networks. Unlike existing concept-based interpretability methods, which focus on a single model, USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models at once. Our core insight is to train a single, overcomplete sparse autoencoder (SAE) that ingests activations from any model and decodes them to approximate the activations of any other model under consideration. By optimizing a shared objective, the learned dictionary captures common factors of variation—concepts—across different tasks, architectures, and datasets. We show that USAEs discover semantically coherent and important universal concepts across vision models; ranging from low-level features (e.g., colors and textures) to higher-level structures (e.g., parts and objects). Overall, USAEs provide a powerful new method for interpretable cross-model analysis and offers novel applications—such as coordinated activation maximization—that open avenues for deeper insights in multi-model AI systems.","Modern computer vision models are increasingly diverse, trained using various datasets and architectures to accomplish specific visual tasks such as depth estimation or object recognition. These design choices shape what visual ""concepts"" or features each model learns—from recognizing edges and textures to understanding objects and scenes. This raises a core scientific question: do these models, despite their differences, converge on learning the same fundamental visual concepts? Answering this question is challenging because the internal representations these models learn are encoded in ways that humans cannot directly interpret. Our work introduces Universal Sparse Autoencoders (USAE), to create a universal, interpretable concept space that reveals what multiple vision models learn in common about the visual world. Our approach enables us to identify the most important universal concepts shared across models, while also discovering features that are unique to specific models. This analysis provides insight into which architectural and training choices lead to better visual representations, and which concepts appear to be fundamental building blocks for visual understanding. This work advances our ability to understand and compare how different AI systems perceive and process visual information."
Poster,Unlocking Post-hoc Dataset Inference with Synthetic Data,https://ICML.cc//virtual/2025/poster/44819,"Bihe Zhao, Pratyush Maini, Franziska Boenisch, Adam Dziedzic","The remarkable capabilities of Large Language Models (LLMs) can be mainly attributed to their massive training datasets, which are often scraped from the internet without respecting data owners’ intellectual property rights. Dataset Inference (DI) offers a potential remedy by identifying whether a suspect dataset was used in training, thereby enabling data owners to verify unauthorized use. However, existing DI methods require a private set—known to be absent from training—that closely matches the compromised dataset’s distribution. Such in-distribution, held-out data is rarely available in practice, severely limiting the applicability of DI. In this work, we address this challenge by synthetically generating the required held-out set. Our approach tackles two key obstacles: (1) creating high-quality, diverse synthetic data that accurately reflects the original distribution, which we achieve via a data generator trained on a carefully designed suffix-based completion task, and (2) bridging likelihood gaps between real and synthetic data, which is realized through post-hoc calibration. Extensive experiments on diverse text datasets show that using our generated data as a held-out set enables DI to detect the original training sets with high confidence, while maintaining a low false positive rate. This result empowers copyright owners to make legitimate claims on data usage and demonstrates our method’s reliability for real-world litigations. Our code is available at https://github.com/sprintml/PostHocDatasetInference.","**Problem:** Large language models (LLMs) are trained on massive amounts of text scraped from the internet, often without permission from authors and data curators. While existing dataset inference methods can detect if someone's writing was used to train these models, they require having similar unpublished text for comparison, which most data curators do not have saved up for legal purposes.**Solution:** We developed a way to synthetically generate the comparison text. Our method works in two steps: first, we train a small AI system on published data to create synthetic text that matches the original style and topics. Then, we use a calibration technique that can distinguish between changes caused by our text generation versus actual signals that the original data was used in training.**Impact:** Our approach successfully identified when specific datasets were used to train LLMs across diverse text types while avoiding false accusations against model providers. This gives writers, journalists, and other content creators a practical tool to prove their work was used without permission, potentially supporting copyright lawsuits and helping establish data ownership rights in the age of AI."
Poster,Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection,https://ICML.cc//virtual/2025/poster/43687,"Peipeng Yu, Jianwei Fei, Hui Gao, Xuan Feng, Zhihua Xia, Chip Hong Chang","Current Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding multimodal data, but their potential remains underexplored for deepfake detection due to the misalignment of their knowledge and forensics patterns. To this end, we present a novel framework that unlocks LVLMs' potential capabilities for deepfake detection. Our framework includes a Knowledge-guided Forgery Detector (KFD), a Forgery Prompt Learner (FPL), and a Large Language Model (LLM). The KFD is used to calculate correlations between image features and pristine/deepfake image description embeddings, enabling forgery classification and localization. The outputs of the KFD are subsequently processed by the Forgery Prompt Learner to construct fine-grained forgery prompt embeddings. These embeddings, along with visual and question prompt embeddings, are fed into the LLM to generate textual detection responses. Extensive experiments on multiple benchmarks, including FF++, CDF2, DFD, DFDCP, DFDC, and DF40, demonstrate that our scheme surpasses state-of-the-art methods in generalization performance, while also supporting multi-turn dialogue capabilities.","We explore the following question: large visual-language large models, having been trained on massive datasets and thus understanding a wide variety of objects, might they be harnessed both to detect deepfakes accurately and to leverage their question-answering capabilities for improved interpretability? To this end, we first align image and text representations via a visual–language model to enable precise forgery classification and localization. We then feed the resulting forgery embeddings into a large language model and fine-tune it to perform deepfake detection within a question-answering framework. Our approach not only improves detection accuracy on previously unseen manipulated images but also, through its interactive Q&A interface, significantly enhances usability and explainability in real-world scenarios."
Poster,Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective,https://ICML.cc//virtual/2025/poster/44025,"Junze Deng, Qinhang Wu, Peizhong Ju, Sen Lin, Yingbin LIANG, Ness Shroff","Rehearsal-based methods have shown superior performance in addressing catastrophic forgetting in continual learning (CL) by storing and training on a subset of past data alongside new data in current task. While such a concurrent rehearsal strategy is widely used, it remains unclear if this approach is always optimal. Inspired by human learning, where sequentially revisiting tasks helps mitigate forgetting, we explore whether sequential rehearsal can offer greater benefits for CL compared to standard concurrent rehearsal. To address this question, we conduct a theoretical analysis of rehearsal-based CL in overparameterized linear models, comparing two strategies: 1) Concurrent Rehearsal, where past and new data are trained together, and 2) Sequential Rehearsal, where new data is trained first, followed by revisiting past data sequentially. By explicitly characterizing forgetting and generalization error, we show that sequential rehearsal performs better when tasks are less similar. These insights further motivate a novel Hybrid Rehearsal method, which trains similar tasks concurrently and revisits dissimilar tasks sequentially. We characterize its forgetting and generalization performance, and our experiments with deep neural networks further confirm that the hybrid approach outperforms standard concurrent rehearsal. This work provides the first comprehensive theoretical analysis of rehearsal-based CL.","In continual learning (CL), models often forget old knowledge when learning new tasks. We propose a novel sequential rehearsal strategy and analyze its performance alongside the widely used concurrent rehearsal method in overparameterized models.  Our theory shows that sequential rehearsal performs better when tasks are less similar. We further propose a novel hybrid rehearsal framework based on our theoretical observation. Our experiments on real-world data validates our theory, offering a more effective solution to forgetting in CL."
Poster,Unlocking the Power of SAM 2 for Few-Shot Segmentation,https://ICML.cc//virtual/2025/poster/45174,"Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao","Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few classes to segment arbitrary classes, but at the risk of overfitting. To address this, some methods use the well-learned knowledge of foundation models (e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM by supporting video segmentation, whose class-agnostic matching ability is useful to FSS. A simple idea is to encode support foreground (FG) features as memory, with which query FG features are matched and fused. Unfortunately, the FG objects in different frames of SAM 2's video data are always the same identity, while those in FSS are different identities, i.e., the matching step is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo query memory, matching with query features in a compatible way. However, the memories can never be as accurate as the real ones, i.e., they are likely to contain incomplete query FG, and some unexpected query background (BG) features, leading to wrong segmentation. Hence, we further design Iterative Memory Refinement to fuse more query FG features into the memory, and devise a Support-Calibrated Memory Attention to suppress the unexpected query BG features in memory. Extensive experiments have been conducted on PASCAL-5$^i$ and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot mIoU can be 4.2\% better than the best baseline.","Teaching computers to segment objects in images with minimal examples—a task called Few-Shot Segmentation (FSS)—is challenging because models often overfit to limited training data. While advanced tools like SAM 2 (Segment Anything Model 2) excel at identifying objects in videos, they struggle when applied to FSS, where each example might show a different object (e.g., segmenting rare animals after seeing just one example). To bridge this gap, we developed two innovations: (1) a Pseudo Prompt Generator that mimics how diverse objects might appear, avoiding mismatches between training and real-world examples, and (2) an Iterative Memory Refinement system that continuously improves the model’s “memory” of key object features while filtering out distracting background details. Testing on standard benchmarks showed our method achieves 4.2\% higher accuracy than existing techniques in single-example scenarios, paving the way for more reliable AI tools in medical imaging (e.g., detecting rare tumors) or robotics (e.g., adapting to unseen objects)."
Poster,unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning,https://ICML.cc//virtual/2025/poster/43684,"Yafei YANG, Zihui Zhang, Bo Yang","We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse. Our code and data are available at https://github.com/vLAR-group/unMORE.","Identifying multiple objects in a single image without prior labels is a tough challenge in computer vision. However, humans can effortlessly recognize multiple objects in new situations, like identifying various animals after reading a book. Inspired by this ability, we investigate two critical issues: 1) how to define objects (i.e. objectness), 2) how to discover objects in unseen scenes.We firstly learn objectness from monolithic object images. Then, the learned objectness priors are used to discover multiple objects in unseen scenes, without needing extra training or human labels. Specifically, we find that object centers and boundaries are informative characteristics that contribute to effective object reasoning. Our work demonstrates state-of-the-art performance on challenging crowded images and paves a different way for unsupervised object discovery. By moving beyond traditional similarity-based methods, we highlight the potential of exploring richer object characteristics to improve object perception."
Poster,Unnatural Languages Are Not Bugs but Features for LLMs,https://ICML.cc//virtual/2025/poster/44282,"Keyu Duan, Yiran Zhao, Zhili Feng, Jinjie Ni, Tianyu Pang, Qian Liu, Tianle Cai, Longxu Dou, Kenji Kawaguchi, Anirudh Goyal, Zico Kolter, Michael Shieh","Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving \(49.71\) win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis,  we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words. Our code is publicly available at https://github.com/John-AI-Lab/Unnatural_Language.","Large language models (AI systems like ChatGPT) are usually trained on human-readable text, but they can also understand and process text that looks like gibberish to us. This paper explores how these models handle such ""unnatural"" language—strings of words or symbols that don’t make sense to humans but still carry meaning for the AI. Surprisingly, the models can extract useful information from these unnatural inputs, even performing as well as when trained on normal language in some tasks. The study also shows that AI processes these strange inputs by filtering out noise and piecing together meaning from the remaining clues. This discovery challenges the assumption that AI only works well with human-like text and opens up new ways to think about how these models understand language.[Code available here: https://github.com/John-AI-Lab/Unnatural_Language]"
