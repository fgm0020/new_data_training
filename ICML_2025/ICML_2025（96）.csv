type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A Randomization Inference Approach with Conformal Selective Borrowing,https://ICML.cc//virtual/2025/poster/44990,"Ke Zhu, Shu Yang, Xiaofei Wang","External controls from historical trials or observational data can augment randomized controlled trials when large-scale randomization is impractical or unethical, such as in drug evaluation for rare diseases. However, non-randomized external controls can introduce biases, and existing Bayesian and frequentist methods may inflate the type I error rate, particularly in small-sample trials where external data borrowing is most critical. To address these challenges, we propose a randomization inference framework that ensures finite-sample exact and model-free type I error rate control, adhering to the “analyze as you randomize” principle to safeguard against hidden biases. Recognizing that biased external controls reduce the power of randomization tests, we leverage conformal inference to develop an individualized test-then-pool procedure that selectively borrows comparable external controls to improve power. Our approach incorporates selection uncertainty into randomization tests, providing valid post-selection inference. Additionally, we propose an adaptive procedure to optimize the selection threshold by minimizing the mean squared error across a class of estimators encompassing both no-borrowing and full-borrowing approaches. The proposed methods are supported by non-asymptotic theoretical analysis, validated through simulations, and applied to a randomized lung cancer trial that integrates external controls from the National Cancer Database.","Clinical trials are the gold standard for testing new treatments, but they can be difficult to conduct when the disease is rare or patient enrollment is limited. In such cases, researchers often look to external data, such as historical trials or medical records, to strengthen the analysis. However, because this external data is not randomized, using it without caution can introduce bias and lead to misleading conclusions.We developed a new statistical method that carefully evaluates whether and how external data should be incorporated into a trial. It combines two ideas: randomization inference, which provides valid conclusions even in small trials, and conformal prediction, which identifies external data points that are comparable to the enrolled participants. By selectively borrowing comparable data, our method improves accuracy while protecting against hidden biases. It also includes an automatic tuning step that determines how much external information to borrow in each case.This method enhances underpowered trials by effectively utilizing existing data while maintaining validity. In a real-world lung cancer study, it boosted the robustness and precision of treatment evaluations, showcasing its ability to progress clinical research in scenarios where conventional trials encounter practical limitations."
Poster,Enhancing Target-unspecific Tasks through a Features Matrix,https://ICML.cc//virtual/2025/poster/45705,"Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu","Recent developments in prompt learning of large Vision-Language Models (VLMs) have significantly improved performance in target-specific tasks. However, these prompting methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge. The general knowledge has a strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2)  the FM significantly showcases its effectiveness in enhancing target-unspecific tasks (base-to-novel generalization, domain generalization, and cross-dataset generalization), achieving state-of-the-art performance.","Recent advancements in training large Vision-Language Models (VLMs) using prompts have greatly boosted their performance in specific tasks. However, these methods often struggle with more general tasks that require broader understanding. One reason behind this challenge is that during training, the model can become too focused on specific details, forgetting its broader knowledge. This general knowledge is crucial for handling tasks that are not narrowly defined. To address this issue, we introduce a new approach called the Features Matrix (FM) method, aimed at improving VLMs on general tasks. Our method works by extracting and utilizing this general knowledge to create a Features Matrix (FM). The FM captures the deep semantics of various inputs, preserving essential knowledge and reducing the risk of becoming too specialized. Through thorough evaluations, we have found that the FM can seamlessly integrate with existing frameworks as a versatile module. Moreover, it significantly enhances performance on general tasks such as base-to-novel generalization, domain generalization, and cross-dataset generalization, achieving top-tier results."
Poster,Enhancing the Influence of Labels on Unlabeled Nodes in Graph Convolutional Networks,https://ICML.cc//virtual/2025/poster/44784,"Jincheng Huang, Yujie Mo, Xiaoshuang Shi, Lei Feng, Xiaofeng Zhu","The message-passing mechanism of graph convolutional networks (i.e., GCNs) enables label information to reach more unlabeled neighbors, thereby increasing the utilization of labels. However, the additional label information does not always contribute positively to the GCN. To address this issue, we propose a new two-step framework called ELU-GCN. In the first stage,  ELU-GCN conducts graph learning to learn a new graph structure (i.e., ELU-graph), which allows the additional label information to positively influence the predictions of GCN. In the second stage, we design a new graph contrastive learning on the GCN framework for representation learning by exploring the consistency and mutually exclusive information between the learned ELU graph and the original graph.  Moreover, we theoretically demonstrate that the proposed method can ensure the generalization ability of GCNs. Extensive experiments validate the superiority of our method.","1. The message-passing mechanism of Graph Convolutional Networks (GCNs) allows labels to influence neighboring unlabeled nodes, but it does not guarantee that such influence is always positive.2. We propose a new objective function to learn a modified graph structure (ELU graph) that ensures label information positively contributes to GCN, and we develop an efficient optimization algorithm to achieve this.3. We introduce a point-wise contrastive constraint to fully explore the complementary and conflicting information between the original graph and the enhanced (ELU) graph.4. This will inspire new directions in graph structure learning and promote the development of better graph learning objectives."
Poster,Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective,https://ICML.cc//virtual/2025/poster/45589,"Hechuan Wen, Tong Chen, Mingming Gong, Li Kheng Chai, Shazia Sadiq, Hongzhi Yin","Although numerous complex algorithms for treatment effect estimation have been developed in recent years, their effectiveness remains limited when handling insufficiently labeled training sets due to the high cost of labeling the post-treatment effect, e.g., the expensive tumor imaging or biopsy procedures needed to evaluate treatment effects. Therefore, it becomes essential to actively incorporate more high-quality labeled data, all while adhering to a constrained labeling budget. To enable data-efficient treatment effect estimation, we formalize the problem through rigorous theoretical analysis within the active learning context, where the derived key measures -- factual and counterfactual covering radii determine the risk upper bound. To reduce the bound, we propose a greedy radius reduction algorithm, which excels under an idealized, balanced data distribution. To generalize to more realistic data distributions, we further propose FCCM, which transforms the optimization objective into the Factual and Counterfactual Coverage Maximization to ensure effective radius reduction during data acquisition. Furthermore, benchmarking FCCM against other baselines demonstrates its superiority across both fully synthetic and semi-synthetic datasets. Code: https://github.com/uqhwen2/FCCM.","Existing literature bears the assumption of enough training data for the treatment effect estimation, which could be breached under the low-data regime. Thus, we are inspired to extend the dataset while adhering to a constrained budget to cater for numerous downstream complex algorithms for effect estimation.To use the limited budget most efficiently for extending the training dataset, we build a theoretical framework and identify the most related quantities to value each of the potential data point to be acquired. Then, given the vast unlabeled pool set, we rank the unlabeled data by their value based on our theory and acquire them in the order from the most-valued to the least-valued within the monetary budget.Our work entails many potential societal benefits, including enhanced accuracy in treatment estimation for new drugs and improved robustness in causal inference models under data sparsity."
Poster,Enhancing Visual Localization with Cross-Domain Image Generation,https://ICML.cc//virtual/2025/poster/43861,"Yuanze Wang, Yichao Yan, Shiming Song, Jin, Yilan Huang, Xingdong Sheng, Dianxi Shi","Visual localization aims to predict the absolute camera pose for a single query image. However, predominant methods focus on single-camera images and scenes with limited appearance variations, limiting their applicability to cross-domain scenes commonly encountered in real-world applications. Furthermore, the long-tail distribution of cross-domain datasets poses additional challenges for visual localization. In this work, we propose a novel cross-domain data generation method to enhance visual localization methods. To achieve this, we first construct a cross-domain 3DGS to accurately model photometric variations and mitigate the interference of dynamic objects in large-scale scenes. We introduce a text-guided image editing model to enhance data diversity for addressing the long-tail distribution problem and design an effective fine-tuning strategy for it. Then, we develop an anchor-based method to generate high-quality datasets for visual localization. Finally, we introduce positional attention to address data ambiguities in cross-camera images. Extensive experiments show that our method achieves state-of-the-art accuracy, outperforming existing cross-domain visual localization methods by an average of 59\% across all domains. Project page: https://yzwang-sjtu.github.io/CDG-Loc.","Teaching agents to understand where a photo was taken — known as visual localization — is crucial for technologies like augmented reality and robot navigation. However, most current methods only work well with images from a single type of camera and in controlled environments. In real-world scenarios, images often vary due to differences in lighting, camera devices, or moving objects, making the task much more challenging.To address this, we developed a new approach that generates more diverse and realistic training data, helping agents better cope with such variations. First, we build a 3D scene model that captures changes in lighting and filters out distractions like people or vehicles. Next, we use a text-guided image editing tool to synthesize rare or uncommon images that are often missing from existing datasets. Finally, we design an anchor-based strategy to produce high-quality and diverse datasets, and introduce a position-aware attention mechanism to resolve ambiguities in images captured across different devices.With these improvements, our method achieves state-of-the-art performance in cross-domain visual localization, paving the way for its practical application in complex real-world environments."
Poster,EnIGMA: Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities,https://ICML.cc//virtual/2025/poster/45428,"Talor Abramovich, Meet Udeshi, Minghao Shao, Kilian Lieret, Haoran Xi, Kimberly Milner, Sofija Jancheska, John Yang, Carlos Jimenez, Farshad Khorrami, Prashanth Krishnamurthy, Brendan Dolan-Gavitt, Muhammad Shafique, Karthik Narasimhan, Ramesh Karri, Ofir Press","Although language model (LM) agents have demonstrated increased performance in multiple domains, including coding and web-browsing, their success in cybersecurity has been limited. We present *EnIGMA*, an LM agent for autonomously solving  Capture The Flag (CTF) challenges. We introduce new tools and interfaces to improve the agent's ability to find and exploit security vulnerabilities, focusing on interactive terminal programs.  These novel *Interactive Agent Tools* enable LM agents, for the first time,  to run interactive utilities, such as a debugger and a server connection tool, which are essential for solving these challenges.Empirical analysis on 390 CTF challenges across four benchmarks demonstrate that these new tools and interfaces substantially improve our agent's performance, achieving state-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we analyze data leakage, developing new methods to quantify it and identifying a new phenomenon we term *soliloquizing*, where the model self-generates hallucinated observations without interacting with the environment.","Modern computer systems are complex, and even small mistakes in their code can open the door to hackers. To find and fix these vulnerabilities before they can be exploited, security researchers use simulated hacking challenges called ""Capture The Flag"" (CTF) competitions. These puzzles mimic real-world attacks and require deep technical knowledge to solve.We created **EnIGMA**, an AI agent powered by language models, that can *solve these complex cybersecurity challenges on its own*. EnIGMA uses special tools that let it interact with computer systems like a cyber expert would—connecting to servers, analyzing and debugging code. These tools help it understand and exploit vulnerabilities in ways previous AI systems couldn’t.Our experiments show that *EnIGMA outperforms earlier AI agents on hundreds of security tasks*. This opens new possibilities for using AI not just to assist experts, but to independently detect vulnerabilities in software—potentially speeding up security audits and reducing the risk of cyberattacks before they happen."
Poster,Ensemble Distribution Distillation via Flow Matching,https://ICML.cc//virtual/2025/poster/43616,"Jonggeon Park, Giung Nam, Hyunsu Kim, Jongmin Yoon, Juho Lee","Neural network ensembles have proven effective in improving performance across a range of tasks; however, their high computational cost limits their applicability in resource-constrained environments or for large models. Ensemble distillation, the process of transferring knowledge from an ensemble teacher to a smaller student model, offers a promising solution to this challenge. The key is to ensure that the student model is both cost-efficient and achieves performance comparable to the ensemble teacher. With this in mind, we propose a novel ensemble distribution distillation method, which leverages flow matching to effectively transfer the diversity from the ensemble teacher to the student model. Our extensive experiments demonstrate the effectiveness of our proposed method compared to existing ensemble distillation approaches.","Ensembles, which combine multiple neural networks, improve the accuracy and reliability of predictions but are too costly for many real-world applications. Ensemble distillation addresses this by training a smaller student model to imitate a larger teacher ensemble, effectively distilling the knowledge from teacher to student. A key challenge is preserving the diversity of predictions, one of the ensemble’s main strengths. We propose a method that helps the student learn not just the average output but also the variation in the ensemble’s answers. This is achieved using a technique called flow matching, which captures richer information. Our method outperforms existing ensemble distillation approaches and enables efficient, high-quality models for use in resource-limited environments."
Poster,Ensemble Learned Bloom Filters: Two Oracles are Better than One,https://ICML.cc//virtual/2025/poster/44338,"Ming Lin, Lin CHEN","Bloom filters (BF) are space-efficient probabilistic data structures for approximate membership testing. Boosted by the proliferation of machine learning, learned Bloom filters (LBF) were recently proposed by augmenting the canonical BFs with a learned oracle as a pre-filter, the size of which is crucial to the compactness of the overall system. In this paper, inspired by ensemble learning, we depart from the state-of-the-art single-oracle LBF structure by demonstrating that, by leveraging multiple learning oracles of smaller size and carefully optimizing the accompanied backup  filters, we can significantly boost the performance of LBF under the same space budget. We then design and optimize ensemble learned Bloom filters for mutually independent and correlated learning oracles respectively. We also empirically demonstrate the performance improvement of our propositions under three practical data analysis tasks.","Bloom filters are efficient tools for checking if an item is part of a set, but they can sometimes give false positives. Recently, learned Bloom filters were introduced, using a machine learning model to improve accuracy. However, the model can become large and inefficient, limiting their practical use.Our research proposes to use multiple smaller machine learning models instead of one large one. By combining these smaller models, we can significantly reduce the false positive rate while keeping the system compact and efficient. We develope algorithms to optimize the selection and combination of these models, ensuring the best performance under a given memory budget.Our experiments demonstrate that our proposition outperforms existing methods in terms of accuracy and efficiency. Our approach can be applied to various real-world tasks, such as detecting malicious URLs or scanning virus signatures, making these processes faster and more reliable. By improving the accuracy of membership checks, we can enhance the security and efficiency of many applications that rely on Bloom filters."
Poster,EnsLoss: Stochastic Calibrated Loss Ensembles for Preventing Overfitting in Classification,https://ICML.cc//virtual/2025/poster/43623,Ben Dai,"Empirical risk minimization (ERM) with a computationally feasible surrogate loss is a widely accepted approach for classification. Notably, the convexity and calibration (CC) properties of a loss function ensure consistency of ERM in maximizing accuracy, thereby offering a wide range of options for surrogate losses. In this article, we propose a novel ensemble method, namely *EnsLoss*, which extends the ensemble learning concept to combine loss functions within the ERM framework. A key feature of our method is the consideration on preserving the ""legitimacy"" of the combined losses, i.e., ensuring the CC properties. Specifically, we first transform the CC conditions of losses into loss-derivatives, thereby bypassing the need for explicit loss functions and directly generating calibrated loss-derivatives. Therefore, inspired by Dropout, *EnsLoss* enables loss ensembles through one training process with doubly stochastic gradient descent (i.e., random batch samples and random calibrated loss-derivatives). We theoretically establish the statistical consistency of our approach and provide insights into its benefits.  The numerical effectiveness of *EnsLoss* compared to fixed loss methods is demonstrated through experiments on a broad range of 45 pairs of CIFAR10 datasets, the PCam image dataset, and 14 OpenML tabular datasets and  with various deep learning architectures. Python repository and source code are available on our Github (https://github.com/statmlben/ensLoss).","When teaching computers to classify data—like identifying animals in photos—researchers use mathematical tools called ""loss functions"" to measure and guide how well the computer is learning. These functions act like scorecards, penalizing the model when it makes mistakes. Traditionally, researchers select just one loss function and use it throughout the training process.Our research introduces a new approach called *EnsLoss* that combines multiple loss functions during training, similar to how consulting several experts often leads to better decisions than relying on just one. The challenge was ensuring these combinations maintain the mathematical properties needed for reliable learning. We solved this by transforming the problem to work with the derivatives of loss functions. This allows us to randomly switch between different valid loss functions during training.Our mathematical analysis proves this approach is theoretically sound, and our experiments on 45 image classification tasks, medical images, and 14 tabular datasets show that *EnsLoss* consistently outperforms traditional single-loss methods across various neural network architectures.We've made our code freely available on GitHub for other researchers to use and build upon."
Poster,ENSUR: Equitable and Statistically Unbiased Recommendation,https://ICML.cc//virtual/2025/poster/44363,"Nitin Bisht, Xiuwen Gong, Guandong Xu","Although Recommender Systems (RS) have been well-developed for various fields of applications, they often suffer from a crisis of platform credibility with respect to RS confidence and fairness, which may drive users away, threatening the platform's long-term success. In recent years, some works have tried to solve these issues; however, they lack strong statistical guarantees. Therefore, there is an urgent need to solve both issues with a unifying framework with robust statistical guarantees. In this paper, we propose a novel and reliable framework called Equitable and Statistically Unbiased Recommendation (ENSUR)) to dynamically generate prediction sets for users across various groups, which are guaranteed 1) to include ground-truth items with user-predefined high confidence/probability (e.g., 90\%); 2) to ensure user fairness across different groups; 3) to have minimum efficient average prediction set sizes.We further design an efficient algorithm named Guaranteed User Fairness Algorithm (GUFA) to optimize the proposed method and derive upper bounds of risk and fairness metrics to speed up the optimization process.Moreover, we provide rigorous theoretical analysis concerning risk and fairness control and minimum set size. Extensive experiments validate the effectiveness of the proposed framework, which aligns with our theoretical analysis.","Recommender systems suggest products, movies, or songs by learning from what we click on. Yet there is always a lingering question  $\textit{``Whether these suggestions can be trusted or are they fair to everyone with certainty?''}$ Despite recent advancements, the systems rarely answer either question with mathematical guarantees.Our work introduces $\textbf{ENSUR}$, a simple add-on that wraps around any recommender model to deliver three simultaneous guarantees: a) for each user, the recommended set is almost certain (e.g., ≥ 90\%) to contain at least one item they will truly like. b) the system meets the same accuracy target for both advantaged and disadvantaged user groups (for instance, across genders or regions). and finally, c) the recommended list is kept short and focused, so users are not overwhelmed by incorrect predictions.The framework does this by treating recommendations as a game of ""set prediction"". A new greedy search procedure, $\textbf{GUFA}$, quickly tunes the size of each user’s list until it meets the desired confidence and fairness thresholds, all backed by statistical theory. In large-scale tests on e-commerce, movie, and music data, ENSUR consistently produced more focused, fairer, and reliable recommendation lists than existing approaches, without retraining the underlying model with negligible runtime overhead."
