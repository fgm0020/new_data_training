type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning,https://ICML.cc//virtual/2025/poster/44168,"Wanyun Xie, Francesco Tonin, Volkan Cevher","Training data mixtures greatly impact the generalization performance of large language models. Existing domain reweighting methods often rely on costly weight computations and require retraining when new data is introduced. To this end, we introduce a flexible and efficient data mixing framework, Chameleon, that employs leverage scores to quantify domain importance within a learned embedding space. We first construct a domain affinity matrix over domain embeddings. The induced leverage scores determine a mixture that upweights domains sharing common representations in embedding space. This formulation allows direct transfer to new data by computing the new domain embeddings. In experiments, we demonstrate improvements over three key scenarios: (i) our computed weights improve performance on pretraining domains with a fraction of the compute of existing methods; (ii) Chameleon can adapt to data changes without proxy retraining, boosting few-shot reasoning accuracies when transferred to new data; (iii) our method enables efficient domain reweighting in finetuning, consistently improving test perplexity on all finetuning domains over uniform mixture. Our code is available at https://github.com/LIONS-EPFL/Chameleon.","Training large language models (LLMs) is heavily impacted by the composition of their training data. Existing methods for mixing data domains are often computationally expensive and impractical. We introduce Chameleon, a flexible and efficient data-mixing framework that quantifies domain importance using leverage scores within a learned embedding space. This approach ($i$) improves universal generalization, the fundamental goal of domain reweighting;  ($ii$) adapts to domain modifications -- data naturally evolves between preparation and LLM training, making frequent recalibration impractical; ($iii$) handles different training stages including both pertaining and fine-tuning."
Poster,Channel Normalization for Time Series Channel Identification,https://ICML.cc//virtual/2025/poster/45365,"Seunghan Lee, Taeyoung Park, Kibok Lee","Channel identifiability (CID) refers to the ability to distinguish among individual channels in time series (TS) modeling. The absence of CID often results in producing identical outputs for identical inputs, disregarding channel-specific characteristics. In this paper, we highlight the importance of CID and propose Channel Normalization (CN), a simple yet effective normalization strategy that enhances CID by assigning distinct affine transformation parameters to each channel. We further extend CN in two ways: 1) Adaptive CN (ACN) dynamically adjusts parameters based on the input TS, improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a set of learnable prototypes instead of per-channel parameters, enabling applicability to datasets with unknown or varying number of channels and facilitating use in TS foundation models. We demonstrate the effectiveness of CN and its variants by applying them to various TS models, achieving significant performance gains for both non-CID and CID models. In addition, we analyze the success of our approach from an information theory perspective. Code is available at [https://github.com/seunghan96/CN](https://github.com/seunghan96/CN).","When analyzing time series (TS) data — like sensor readings or financial trends — many TS models fail to tell individual input channels apart. This can lead to treating very different signals as if they were the same, hurting the model’s performance. Our research tackles this problem by helping AI models better distinguish between input channels, a property we call channel identifiability.We introduce a simple method called Channel Normalization (CN) that enables TS models to distinguish between different channels using channel-specific parameters. We also develop two extensions: one that adapts to different inputs, and another that can handle situations where the number of input channels changes. These techniques make models more flexible and accurate across a wide range of real-world TS tasks. Our findings highlight how improving a model’s awareness of its inputs can lead to better decisions."
Poster,Chaos Meets Attention: Transformers for Large-Scale Dynamical Prediction,https://ICML.cc//virtual/2025/poster/45254,"Yi He, Yiming Yang, Xiaoyuan Cheng, Hai Wang, Xiao Xue, Boli Chen, Yukun Hu","Generating long-term trajectories of dissipative chaotic systems autoregressively is a highly challenging task. The inherent positive Lyapunov exponents amplify prediction errors over time.Many chaotic systems possess a crucial property — ergodicity on their attractors, which makes long-term prediction possible. State-of-the-art methods address ergodicity by preserving statistical properties using optimal transport techniques. However, these methods face scalability challenges due to the curse of dimensionality when matching distributions. To overcome this bottleneck, we propose a scalable transformer-based framework capable of stably generating long-term high-dimensional and high-resolution chaotic dynamics while preserving ergodicity. Our method is grounded in a physical perspective, revisiting the Von Neumann mean ergodic theorem to ensure the preservation of long-term statistics in the $\mathcal{L}^2$ space. We introduce novel modifications to the attention mechanism, making the transformer architecture well-suited for learning large-scale chaotic systems. Compared to operator-based and transformer-based methods, our model achieves better performances across five metrics, from short-term prediction accuracy to long-term statistics. In addition to our methodological contributions, we introduce new chaotic system benchmarks: a machine learning dataset of 140$k$ snapshots of turbulent channel flow and a processed high-dimensional Kolmogorov Flow dataset, along with various evaluation metrics for both short- and long-term performances. Both are well-suited for machine learning research on chaotic systems.","Chaotic systems are widely known for the “butterfly effect,” where tiny changes can lead to vastly different outcomes. Predicting their long-term behavior is extremely hard, especially when the system has many moving parts (high dimensions) and fine details (high resolution). While chaos seems unpredictable, these systems often follow hidden patterns over time.Our research tackles the challenge of simulating these systems by combining physics insights with multi-stage accelerations for scaling. We designed a new transformer-based model—similar to those powering language and vision models— we innovate the attention mechanism and training methods, creating a faster, more robust way to generate long-term chaotic dynamics.We also introduce new benchmarks designed specifically for early-stage machine learning research on chaotic systems to the community. Our goal is to make large-scale chaos a little more predictable and accelerate the related machine learning research and the applications."
Poster,CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation,https://ICML.cc//virtual/2025/poster/46027,"Minghao Fu, Guo-Hua Wang, Liangfu Cao, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang","Diffusion models have emerged as a dominant approach for text-to-image generation. Key components such as the human preference alignment and classifier-free guidance play a crucial role in ensuring generation quality. However, their independent application in current text-to-image models continues to face significant challenges in achieving strong text-image alignment, high generation quality, and consistency with human aesthetic standards. In this work, we for the first time, explore facilitating the collaboration of human performance alignment and test-time sampling to unlock the potential of text-to-image models. Consequently, we introduce CHATS (Combining Human-Aligned optimization and Test-time Sampling), a novel generative framework that separately models the preferred and dispreferred distributions and employs a proxy-prompt-based sampling strategy to utilize the useful information contained in both distributions. We observe that CHATS exhibits exceptional data efficiency, achieving strong performance with only a small, high-quality funetuning dataset. Extensive experiments demonstrate that CHATS surpasses traditional preference alignment methods, setting new state-of-the-art across various standard benchmarks. The code is publicly available at github.com/AIDC-AI/CHATS.","Imagine having two friendly artists in your computer: one whose only job is to remember what beautiful, on-point images look like, and another who spots mistakes and odd quirks you definitely don’t want. You feed them around 7,500 pairs of “I love this” and “please don’t show this” examples, and they learn their crafts. When you type in a prompt, a clever mixer called the proxy-prompt sampler blends their advice, nudging the final picture toward the good stuff and steering clear of the bad. The result? Lifelike, clear, and creative images that match your words and your taste—without complicated tuning or massive data. We tested this on popular challenges for beauty, object counts, and scene details, and it beats all the usual methods. With CHATS, turning simple text into stunning visuals feels as natural as sketching a quick doodle, bringing your ideas to life in just a few seconds."
Poster,Chip Placement with Diffusion Models,https://ICML.cc//virtual/2025/poster/44664,"Vint Lee, Minh Nguyen, Leena Elzeiny, Chun Deng, Pieter Abbeel, Wawrzynek","Macro placement is a vital step in digital circuit design that defines the physical location of large collections of components, known as macros, on a 2D chip. Because key performance metrics of the chip are determined by the placement, optimizing it is crucial. Existing learning-based methods typically fall short because of their reliance on reinforcement learning (RL), which is slow and struggles to generalize, requiring online training on each new circuit. Instead, we train a diffusion model capable of placing new circuits zero-shot, using guided sampling in lieu of RL to optimize placement quality. To enable such models to train at scale, we designed a capable yet efficient architecture for the denoising model, and propose a novel algorithm to generate large synthetic datasets for pre-training. To allow zero-shot transfer to real circuits, we empirically study the design decisions of our dataset generation algorithm, and identify several key factors enabling generalization. When trained on our synthetic data, our models generate high-quality placements on unseen, realistic circuits, achieving competitive performance on placement benchmarks compared to state-of-the-art methods.","When designing chips, the positioning (or placement) of circuit components plays a pivotal role in determining the performance and power consumption of the resulting chip. We trained a generative model, similar to those used for image generation, to create near-optimal placements for any given circuit. However, very few training examples are publicly available, since many chip designs are closely-guarded secrets. To overcome this, we devised a method to generate large amounts of synthetic data from scratch, allowing us to train large models without the use of any real chip designs. Our models produce better placements using less time than existing techniques, potentially helping designers produce cheaper, faster chips."
Poster,Circumventing Backdoor Space via Weight Symmetry,https://ICML.cc//virtual/2025/poster/44614,"Jie Peng, Hongwei Yang, Jing Zhao, Hengji Dong, Hui He, Weizhe Zhang, Haoyu He","Deep neural networks are vulnerable to backdoor attacks, where malicious behaviors are implanted during training. While existing defenses can effectively purify compromised models, they typically require labeled data or specific training procedures, making them difficult to apply beyond supervised learning settings. Notably, recent studies have shown successful backdoor attacks across various learning paradigms, highlighting a critical security concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC), a novel backdoor purification defense that operates independently of data format and requires only a small fraction of clean samples. Through theoretical analysis, we prove that by leveraging permutation invariance in neural networks and quadratic mode connectivity, TSC amplifies the loss on poisoned samples while maintaining bounded clean accuracy. Experiments demonstrate that TSC achieves robust performance comparable to state-of-the-art methods in supervised learning scenarios. Furthermore, TSC generalizes to self-supervised learning frameworks, such as SimCLR and CLIP, maintaining its strong defense capabilities. Our code is available at https://github.com/JiePeng104/TSC.","Imagine an AI system, like one that helps doctors diagnose diseases or powers your favorite app. What if this AI could be secretly tricked during its learning phase to make specific, harmful mistakes later on, without anyone noticing the manipulation until it's too late? This is called a ""backdoor attack,"" and it makes AI systems vulnerable and untrustworthy. Fixing these secretly compromised AIs is a big challenge. Current fixes for these compromised AIs often require specific data or complex setups, limiting their use, especially as these attacks are now seen in various AI learning styles. Our research introduces a new and more flexible way to purify these AI systems, called Two-stage Symmetry Connectivity (TSC). TSC stands out because it works regardless of the data type (images, sound, etc.) and needs only a tiny amount of clean data. We discovered that by using certain inherent mathematical properties of AI networks, TSC can make the ""poisoned"" parts of the AI stand out, allowing the malicious behavior to be neutralized while ensuring the AI remains good on initial tasks."
Poster,CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries,https://ICML.cc//virtual/2025/poster/43680,"Ni Mu, Hao Hu, Xiao Hu, Yiqin Yang, Bo XU, Qing-Shan Jia","Preference-based reinforcement learning (PbRL) bypasses explicit reward engineering by inferring reward functions from human preference comparisons, enabling better alignment with human intentions. However, humans often struggle to label a clear preference between similar segments, reducing label efficiency and limiting PbRL’s real-world applicability. To address this, we propose an offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback (CLARIFY), which learns a trajectory embedding space that incorporates preference information, ensuring clearly distinguished segments are spaced apart, thus facilitating the selection of more unambiguous queries. Extensive experiments demonstrate that CLARIFY outperforms baselines in both non-ideal teachers and real human feedback settings. Our approach not only selects more distinguished queries but also learns meaningful trajectory embeddings.","When training AI systems with human feedback, a common challenge is that people often find it difficult to choose between similar options. For instance, selecting a favorite painting from two nearly identical artworks can be frustrating and unproductive. Our method, CLARIFY, addresses this by focusing on two key concepts. First, we train the AI to identify meaningful differences between options, like distinguishing whether a robot arm successfully turns a dial or merely waves near it. Second, CLARIFY eliminates ambiguous pairings, akin to a teacher avoiding tricky questions in favor of clear comparisons, thereby enhancing the efficiency and effectiveness of feedback. In our tests, AI systems trained using this approach performed tasks such as opening drawers or walking more effectively, while reducing unclear feedback by 20-40%. This method acts as a ""common sense guide"" for AI, improving its understanding of human preferences without unnecessary confusion."
Poster,Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off,https://ICML.cc//virtual/2025/poster/46080,"Yuecheng Li, Lele Fu, Tong Wang, Jian Lou, Bin Chen, Lei Yang, Jian Shen, Zibin Zheng, Chuan Chen","To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named **FedCEO**, designed to strike a trade-off between model utility and user privacy by letting clients ""***C**ollaborate with **E**ach **O**ther*"". Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to ***flexibly*** truncate high-frequency components in spectral space. This capability implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative datasets and observe significant performance improvements and strict privacy guarantees under different privacy settings. The **code** is available at https://github.com/6lyc/FedCEO_Collaborate-with-Each-Other.","Protecting user privacy in collaborative AI training (federated learning) requires adding carefully designed noise. However, this noise can unevenly disrupt different parts of each device's learned knowledge over time – like obscuring facial features in one device's animal recognition model while blurring limb details in another's.We introduce FedCEO, a new approach where devices ""Collaborate with Each Other"" under server coordination. FedCEO intelligently combines the complementary knowledge from all devices. When one device's understanding of a concept is disrupted by privacy protection, others help fill those gaps.This CEO-like coordination gradually enhances semantic smoothness across devices as training progresses. The server blends the partial understandings into a coherent whole, allowing the global model to recover disrupted patterns while maintaining privacy. The result is significantly improved AI performance across diverse privacy settings and extended training periods."
Poster,CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models,https://ICML.cc//virtual/2025/poster/45167,"David Dai, Peilin Chen, Malinda Lu, Daniel A. Li, Haowen Wei, Hejie Cui, Paul Pu Liang","Recent advances in clinical AI have enabled remarkable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well-being. To bridge this gap, we introduce Clinical Large-scale Integrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities. CLIMB comprises 4.51 million patient samples totaling 19.01 terabytes distributed across 2D imaging, 3D video, time series, graphs, and multimodal data. Through extensive empirical evaluation, we demonstrate that multitask pretraining significantly improves performance on understudied domains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis over single-task learning. Pretraining on CLIMB also effectively improves models' generalization capability to new tasks, and strong unimodal encoder performance translates well to multimodal performance when paired with task-appropriate fusion strategies. Our findings provide a foundation for new architecture designs and pretraining strategies to adavance clinical AI research. Code is released at https://github.com/DDVD233/climb.","Current AI systems designed to help doctors analyze medical data typically focus on just one type of information at a time—like reading X-rays or processing patient notes—but doctors naturally combine many different types of medical data to make comprehensive diagnoses. This narrow focus limits AI's ability to provide the holistic patient assessments that clinicians need for effective healthcare decisions.We created CLIMB, a massive medical dataset that brings together 4.51 million patient samples across diverse data types: medical images (X-rays, CT scans, ultrasounds), patient records, EEG/ECG signals, genetic information, and molecular data from 33 medical institutions. We then trained AI models on this comprehensive dataset to learn patterns across all these different medical data types simultaneously, rather than learning each type in isolation.Our approach dramatically improved AI performance on challenging medical tasks, achieving up to 29% better accuracy in ultrasound analysis and 23% improvement in ECG diagnosis compared to traditional single-task methods. This research demonstrates that AI systems trained on diverse medical data can better generalize to new clinical challenges and provides a foundation for developing more comprehensive AI tools that could assist doctors in making more informed, holistic patient care decisions across multiple medical specialties."
Poster,Clipped SGD Algorithms for Performative Prediction: Tight Bounds for Stochastic Bias and Remedies,https://ICML.cc//virtual/2025/poster/43628,"Qiang Li, Michal Yemini, Hoi To Wai","This paper studies the convergence of clipped stochastic gradient descent (SGD) algorithms with decision-dependent data distribution. Our setting is motivated by privacy preserving optimization algorithms that interact with performative data where the prediction models can influence future outcomes. This challenging setting involves the non-smooth clipping operator and non-gradient dynamics due to distribution shifts. We make two contributions in pursuit for a performative stable solution with these algorithms. First, we characterize the stochastic bias with projected clipped SGD (PCSGD) algorithm which is caused by the clipping operator that prevents PCSGD from reaching a stable solution. When the loss function is strongly convex, we quantify the lower and upper bounds for this stochastic bias and demonstrate a bias amplification phenomenon with the sensitivity of data distribution. When the loss function is non-convex, we bound the magnitude of stationarity bias. Second, we propose remedies to mitigate the bias either by utilizing an optimal step size design for PCSGD, or to apply the recent DiceSGD algorithm [Zhang et al., 2024]. Our analysis is also extended to show that the latter algorithm is free from stochastic bias in the performative setting. Numerical experiments verify our findings.","Modern machine learning models often learn from data that is influenced by their own predictions — like a recommendation system that shapes what users click on next. In such cases, training becomes more complex, especially when using techniques that protect user privacy by clipping overly large gradients — that is, limiting the magnitude of each update to avoid revealing sensitive information.Our study reveals that while clipping helps with privacy, it also introduces a problem: clipping bias. This bias can prevent the model from fully learning, especially when the data changes in response to the model’s outputs. We analyze how this bias behaves under different learning conditions — showing that it can grow significantly depending on how sensitive the data is.To address this, we propose two effective fixes: (1) carefully tuning the model’s learning rate, and (2) using a newer training method called DiceSGD, which, in our setting, provably eliminates clipping bias. Our work highlights the trade-off between privacy and learning accuracy, and offers practical guidance to balance both in real-world systems."
