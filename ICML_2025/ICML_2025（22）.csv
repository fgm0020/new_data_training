type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,A Parameter-Free and Near-Optimal Zeroth-Order Algorithm for Stochastic Convex Optimization,https://ICML.cc//virtual/2025/poster/45690,"Kunjie Ren, Luo Luo","This paper studies zeroth-order optimization for stochastic convex minimization problems. We propose a parameter-free stochastic zeroth-order method (POEM), which introduces a step-size scheme based on the distance over finite difference and an adaptive smoothingparameter. Our theoretical analysis shows that POEM achieves near-optimal stochastic zeroth-order oracle complexity. Furthermore, numerical experiments demonstrate that POEM outperforms existing zeroth-order methods in practice.","In many real-world problems, we need to find the best setting of some inputs even when we can’t directly measure how steeply the outcome is changing (no “gradient” information). This paper introduces a new black-box search strategy called POEM that learns to pick how big its steps should be—and other internal settings—automatically, without the need for manual tuning. We prove that POEM finds high-quality solutions almost as quickly as theoretically possible, and our computer tests show it consistently beats popular existing methods. In short, POEM makes efficient, hands-off optimization practical for a wide range of noisy, complex problems."
Poster,A Parametric Contextual Online Learning Theory of Brokerage,https://ICML.cc//virtual/2025/poster/45850,"François Bachoc, Tommaso Cesari, Roberto Colomboni","We study the role of contextual information in the online learning problem of brokerage between traders.In this sequential problem, at each time step, two traders arrive with secret valuations about an asset they wish to trade.The learner (a broker) suggests a trading (or brokerage) price based on contextual data about the asset and the market conditions.Then, the traders reveal their willingness to buy or sell based on whether their valuations are higher or lower than the brokerage price.A trade occurs if one of the two traders decides to buy and the other to sell, i.e., if the broker's proposed price falls between the smallest and the largest of their two valuations.We design algorithms for this problem and prove optimal theoretical regret guarantees under various standard assumptions.","We study the following sequential machine learning problem. During each of a sequence of interactions, the learner (a broker) suggests a trading (or brokerage) price to two traders based on contextual data about the asset and the market conditions. Then, the traders reveal their willingness to buy or sell based on whether their valuations are higher or lower than the brokerage price. A trade occurs if one of the two traders decides to buy and the other to sell, i.e., if the broker's proposed price falls between the smallest and the largest of their two valuations. We design algorithms for this problem and prove optimal performance guarantees under various standard assumptions."
Poster,A Peer-review Look on Multi-modal Clustering: An Information Bottleneck Realization Method,https://ICML.cc//virtual/2025/poster/46541,"Zhengzheng Lou, Hang Xue, Chaoyang Zhang, Shizhe Hu","Despite the superior capability in complementary information exploration and consistent clustering structure learning, most current weight-based multi-modal clustering methods still contain three limitations: 1) lack of trustworthiness in learned weights; 2) isolated view weight learning; 3) extra weight parameters. Motivated by the peer-review mechanism in the academia, we in this paper give a new peer-review look on the multi-modal clustering problem and propose to iteratively treat one modality as ""author"" and the remaining modalities as ""reviewers"" so as to reach a peer-review score for each modality. It essentially explores the underlying relationships among modalities. To improve the trustworthiness, we further design a new trustworthy score with a self-supervision working mechanism. Following that, we propose a novel Peer-review Trustworthy Information Bottleneck (PTIB) method for weighted multi-modal clustering, where both the above scores are simultaneously taken into account for accurate and parameter-free modality weight learning. Extensive experiments on eight multi-modal datasets suggest that PTIB can outperform the state-of-the-art multi-modal clustering methods.","We want to know how much help the multiple modalities (e.g., images, text, etc.) of samples can provide for classifying them without true labels. Most existing methods often let a modality independently claim how much useful information it has to answer this question. But it is usually better for multiple people to discuss with each other than to ponder alone. Analogous to the peer review mechanism in academia, we regard one modality as the ""author"" and the other modalities as the ""reviewers"" to score the ""author"". And to ensure the fairness and reliability of this process, we regard the joint classification results of all modalities as the editor-in-chief or associate editor for judging whether the reviewer's score is reasonable. Finally, by selectively considering the scores of all ""reviewers"",  the contribution of an ""author"" modality will be reasonably determined. We obtained better results than many existing methods. Our research provides a new possible idea for integrating multi-source information."
Poster,A Physics-Augmented Deep Learning Framework for Classifying Single Molecule Force Spectroscopy Data,https://ICML.cc//virtual/2025/poster/45169,"Cailong Hua, Sivaraman Rajaganapathy, Rebecca Slick, Joseph Vavra, Joseph Muretta, James Ervasti, Murti Salapaka","Deciphering protein folding and unfolding pathways under tension is essential for deepening our understanding of fundamental biological mechanisms. Such insights hold the promise of developing treatments for a range of debilitating and fatal conditions, including muscular disorders like Duchenne Muscular Dystrophy and neurodegenerative diseases such as Parkinson's disease. Single molecule force spectroscopy (SMFS) is a powerful technique for investigating forces involved in protein domains folding and unfolding. However, SMFS trials often involve multiple protein molecules, necessitating filtering to isolate measurements from single-molecule trials. Currently, manual visual inspection is the primary method for classifying single-molecule data; a process that is both time-consuming and requires significant expertise. Here, we both apply state-of-the-art machine learning models and present a novel deep learning model tailored to SMFS data. The proposed model employs a dual-branch fusion strategy; one branch integrates the physics of protein molecules, and the other operates independently of physical constraints. This model automates the isolation of single-molecule measurements, significantly enhancing data processing efficiency. To train and validate our approach, we developed a physics-based Monte Carlo engine to simulate force spectroscopy datasets, including trials involving single molecules, multiple molecules, and no molecules. Our model achieves state-of-the-art performance, outperforming five baseline methods on both simulated and experimental datasets. It attains nearly 100\% accuracy across all simulated datasets and an average accuracy of $79.6 \pm 5.2$\% on experimental datasets, using only $\sim$30 training samples, surpassing baseline methods by 11.4\%. Notably, even without expert annotations on experimental data, the model achieves an average accuracy of $72.0 \pm 5.9$\% when pre-trained on corresponding simulated datasets. With our deep learning approach, the time required to extract meaningful statistics from single-molecule SMFS trials is reduced from a day to under an hour. This work results in SMFS experimental datasets from four important protein molecules crucial to many biological pathways. To support further research, we have made our datasets publicly available and provided a Python-based toolbox (https://github.com/SalapakaLab-SIMBioSys/SMFS-Identification).","Understanding protein folding and unfolding under mechanical forces is important for understanding biological processes. Such insights hold the promise of developing treatments for serious conditions, including muscular disorders like Duchenne Muscular Dystrophy and neurodegenerative diseases such as Parkinson’s disease. Single molecule force spectroscopy (SMFS) is a powerful technique for investigating forces involved in protein folding and unfolding. However, real-world SMFS data often includes trials from multiple proteins, which can confound meaningful results. To interpret these data correctly, researchers need to isolate clean, single-molecule trials—a process that traditionally required manual inspection by experts and could take an entire day for just one experiment.In this work, we automated this filtering process by applying several state-of-the-art machine learning models and introducing a novel deep learning model that incorporates physical knowledge of proteins. We also provide a simulation engine to generate realistic datasets alongside extensive experimental data from a variety of proteins. Our model outperforms existing methods and reduces the time needed to extract meaningful results from a full day to under an hour. To support future research, we have made our datasets and Python-based toolbox publicly available."
Poster,A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems,https://ICML.cc//virtual/2025/poster/45200,"Manan Tayal, Aditya Singh, Shishir Nadubettu Yadukumar, Somil Bansal","As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems.","Autonomous systems—like self-driving cars and drones—are becoming more common in our daily lives. For these systems to be trustworthy, they must not only perform well but also operate safely at all times. However, making a system both safe and high-performing is challenging because these goals often conflict. Some learning-based approaches allow these systems to perform very well, but they don’t always guarantee safety. On the other hand, mathematical techniques that ensure safety often make the system too conservative. Our research combines the best of both worlds: it uses physics informed machine learning to learn how to control these systems safely while still allowing them to perform well. We also include a way to measure how confident we are in the safety of our system, even when the controller is learned from data. This helps make sure the system won't fail in real-world situations. We test our approach in several examples and show that it works effectively, even for complex systems."
Poster,Approximate Differential Privacy of the $\ell_2$ Mechanism,https://ICML.cc//virtual/2025/poster/43510,"Matthew Joseph, Alex Kulesza, Alexander Yu","We study the $\ell_2$ mechanism for computing a $d$-dimensional statistic with bounded $\ell_2$ sensitivity under approximate differential privacy. Across a range of privacy parameters, we find that the $\ell_2$ mechanism obtains error approaching that of the Laplace mechanism as $d \to 1$ and approaching that of the Gaussian mechanism as $d \to \infty$; however, it dominates both in between.","For a certain kind of statistic, we provide a new algorithm that computes the statistic with a quantitative privacy guarantee, with lower error than previous such algorithms."
Poster,Approximate Forest Completion and Learning-Augmented Algorithms for Metric Minimum Spanning Trees,https://ICML.cc//virtual/2025/poster/43905,"Nate Veldt, Thomas Stanley, Benjamin Priest, Trevor Steil, Keita Iwabuchi, T.S. Jayram, Geoffrey Sanders","Finding a minimum spanning tree (MST) for $n$ points in an arbitrary metric space is a fundamental primitive for hierarchical clustering and many other ML tasks, but this takes $\Omega(n^2)$ time to even approximate. We introduce a framework for metric MSTs that first (1) finds a forest of trees using practical heuristics, and then (2) finds a small weight set of edges to connect disjoint components in the forest into a spanning tree. We prove that optimally solving step (2) still takes $\Omega(n^2)$ time, but we provide a subquadratic 2.62-approximation algorithm. In the spirit of learning-augmented algorithms, we then show that if the heuristic forest found in step (1) overlaps with an optimal MST, we can approximate the original MST problem in subquadratic time, where the approximation factor depends on a measure of overlap. In practice, we find nearly optimal spanning trees for a wide range of metrics, while being orders of magnitude faster than exact algorithms.","Finding a minimum spanning tree for a set of data points is a fundamental computational task that can be used, among other applications, for clustering data into similar groups of points. There are already exact algorithms for solving this problem that rely on computing distances between all pairs of points as a first step. However, this approach is too slow for applications that involve massive datasets, especially when relationships between data points are given by complicated distance functions. In our work, we design a fast new approach for finding a spanning tree that does not require computing distances between all pairs of data points. The method starts by connecting some subsets of points with a fast machine learning heuristic. It then finds a near optimal way to connect these pieces together to form a full spanning tree. A key contribution of our work is our theoretical analysis: we show that if the starting point computed by the fast machine learning heuristic overlaps well with an optimal minimum spanning tree, then the spanning tree our algorithm produces is provably close to optimal."
Poster,Approximately Correct Label Distribution Learning,https://ICML.cc//virtual/2025/poster/46395,"Weiwei Li, Haitao Wu, Yunan Lu, Xiuyi Jia","Label distribution learning (LDL) is a powerful learning paradigm that emulates label polysemy by assigning label distributions over the label space. However, existing LDL evaluation metrics struggle to capture meaningful performance differences due to their insensitivity to subtle distributional changes, and existing LDL learning objectives often exhibit biases by disproportionately emphasizing a small subset of samples with extreme predictions. As a result, the LDL metrics lose their discriminability, and the LDL objectives are also at risk of overfitting. In this paper, we propose DeltaLDL, a percentage of predictions that are approximately correct within the context of LDL, as a solution to the above problems. DeltaLDL can serve as a novel evaluation metric, which is parameter-free and reflects more on real performance improvements. DeltaLDL can also serve as a novel learning objective, which is differentiable and encourages most samples to be predicted as approximately correct, thereby mitigating overfitting. Our theoretical analysis and empirical results demonstrate the effectiveness of the proposed solution.","Teaching ML models to handle ambiguous labels, like recognizing someone is ""70% happiness and 30% surprise"", is tricky. Current evaluation metrics often miss small but important differences in these percentages, while current learning objectives focus too much on extreme cases (like 99% happiness). This makes the system less reliable and prone to overfitting.We introduce DeltaLDL, a new way to measure and train these systems. Instead of demanding perfect predictions, it rewards being ""close enough"", like accepting that 65-35 is reasonably close to 70-30. This makes evaluations more meaningful and reduces overfitting by encouraging balanced learning across all examples, not just the easiest or hardest ones.Tests show our method reflects real-world performance and improves generalization better. Think of it like grading a student on overall understanding rather than penalizing every small mistake. It’s a fairer and more practical approach for messy, real-life labeling tasks."
Poster,Approximating Latent Manifolds in Neural Networks via Vanishing Ideals,https://ICML.cc//virtual/2025/poster/45012,"Nico Pelleriti, Max Zimmer, Elias Wirth, Sebastian Pokutta","Deep neural networks have reshaped modern machine learning by learning powerful latent representations that often align with the manifold hypothesis: high-dimensional data lie on lower-dimensional manifolds. In this paper, we establish a connection between manifold learning and computational algebra by demonstrating how vanishing ideals can characterize the latent manifolds of deep networks. To that end, we propose a new neural architecture that (i) truncates a pretrained network at an intermediate layer, (ii) approximates each class manifold via polynomial generators of the vanishing ideal, and (iii) transforms the resulting latent space into linearly separable features through a single polynomial layer. The resulting models have significantly fewer layers than their pretrained baselines, while maintaining comparable accuracy, achieving higher throughput, and utilizing fewer parameters. Furthermore, drawing on spectral complexity analysis, we derive sharper theoretical guarantees for generalization, showing that our approach can in principle offer tighter bounds than standard deep networks. Numerical experiments confirm the effectiveness and efficiency of the proposed approach.","Modern AI systems are incredibly powerful, but they often require very large, deep networks with millions of parameters. This makes them expensive to run, hard to interpret, and slower to use in practice.We explored whether we could keep the smart part of an AI system — its early ability to understand patterns — and replace the later, heavier parts with a simpler mathematical model. Specifically, we used a type of math called polynomial equations to describe the structure of the data the AI sees. By doing this, we built a much smaller, faster system that still performs just as well.Our method cuts down the size of the network significantly, reduces the amount of computation, and offers better theoretical understanding of how and why the system makes decisions. This could make future AI models more efficient, easier to explain, and more accessible to use in real-world applications."
Poster,Approximation to Smooth Functions by Low-Rank Swish Networks,https://ICML.cc//virtual/2025/poster/44850,"Zimeng Li, Hongjun LI, Jingyuan Wang, Ke Tang","While deep learning has witnessed remarkable achievements in a wide range of applications, its substantial computational cost imposes limitations on application scenarios of neural networks. To alleviate this problem, low-rank compression is proposed as a class of efficient and hardware-friendly network compression methods, which reduce computation by replacing large matrices in neural networks with products of two small ones. In this paper, we implement low-rank networks by inserting a sufficiently narrow linear layer without bias between each of two adjacent nonlinear layers. We prove that low-rank Swish networks with a fixed depth are capable of approximating any function from the Hölder ball $\mathcal{C}^{\beta, R}([0,1]^d)$ within an arbitrarily small error where $\beta$ is the smooth parameter and $R$ is the radius. Our proposed constructive approximation ensures that the width of linear hidden layers required for approximation is no more than one-third of the width of nonlinear layers, which implies that the computational cost can be decreased by at least one-third compared with a network with the same depth and width of nonlinear layers but without narrow linear hidden layers. Our theoretical finding can offer a theoretical basis for low-rank compression from the perspective of universal approximation theory.","Deep learning drives breakthroughs in AI, but its massive computational demands hinder real-world use. Cutting network size via low-rank compression shows promise, yet lacks theoritical guarantees, limiting reliability. We offer a theoretical basis for low-rank compression from the perspective of universal approximation theory by proving any function from a board class can be approximated by a Swish network with low-rank weight matrices. Our findings partially guarantee that low-rank compression often serves as a viable approach for network compression, as it generally maintains performance while reducing model size."
