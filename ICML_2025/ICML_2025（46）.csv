type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Causal Abstraction Inference under Lossy Representations,https://ICML.cc//virtual/2025/poster/45031,"Kevin Xia, Elias Bareinboim","The study of causal abstractions bridges two integral components of human intelligence: the ability to determine cause and effect, and the ability to interpret complex patterns into abstract concepts. Formally, causal abstraction frameworks define connections between complicated low-level causal models and simple high-level ones. One major limitation of most existing definitions is that they are not well-defined when considering lossy abstraction functions in which multiple low-level interventions can have different effects while mapping to the same high-level intervention (an assumption called the abstract invariance condition). In this paper, we introduce a new type of abstractions called projected abstractions that generalize existing definitions to accommodate lossy representations. We show how to construct a projected abstraction from the low-level model and how it translates equivalent observational, interventional, and counterfactual causal queries from low to high-level. Given that the true model is rarely available in practice we prove a new graphical criteria for identifying and estimating high-level causal queries from limited low-level data. Finally, we experimentally show the effectiveness of projected abstraction models in high-dimensional image settings.","Determining cause and effect is important since it helps us understand the difference between whether eating vegetables prevents cancer or whether individuals who happened to eat vegetables were less likely to smoke, therefore preventing cancer. Determining this from data is difficult, so it may be helpful to work more abstractly: for example, choosing to think about calories rather than individual nutrients like carbohydrates, fat, and protein. The problem is that sometimes important details may be abstracted away, like perhaps the amount of fat is relevant for determining cancer risk.Our paper develops a generalized approach to abstractions that accommodates this loss of information by treating the abstract quantities as distributions over the low-level details they are abstracting. For example, in an image of a dog, one can see many details of the dog, like the breed, the texture and color of its fur, and its posture. If the pixels were simply abstracted away as a single word label ""dog"", we lose all of this information. There may be many dog images in the dataset, each with a different breed. Still, if breed is important information, our paper allows one to solve two problems: (1) any causal calculations performed on this dataset can still be done by simply randomly sampling over different dog breeds with probabilities based on which breeds are more common, and (2) despite the lack of details, the image can still be reconstructed by simply sampling whatever is lost in the abstraction, allowing for a full high-quality dog image from simply the label ""dog""."
Poster,Causal Abstraction Learning based on the Semantic Embedding Principle,https://ICML.cc//virtual/2025/poster/45717,"Gabriele DAcunto, Fabio Massimo Zennaro, Yorgos Felekis, Paolo Di Lorenzo","Structural causal models (SCMs) allow us to investigate complex systems at multiple levels of resolution.The causal abstraction (CA) framework formalizes the mapping between high- and low-level SCMs. We address CA learning in a challenging and realistic setting, where SCMs are inaccessible, interventional data is unavailable, and sample data is misaligned.A key principle of our framework is *semantic embedding*, formalized as the high-level distribution lying on a subspace of the low-level one. This principle naturally links linear CA to the geometry of the *Stiefel manifold*.We present a category-theoretic approach to SCMs that enables the learning of a CA by finding a morphism between the low- and high-level probability measures, adhering to the semantic embedding principle.Consequently, we formulate a general CA learning problem.As an application, we solve the latter problem for linear CA; considering Gaussian measures and the Kullback-Leibler divergence as an objective.Given the nonconvexity of the learning task, we develop three algorithms building upon existing paradigms for Riemannian optimization.We demonstrate that the proposed methods succeed on both synthetic and real-world brain data with different degrees of prior information about the structure of CA.","Consider two scientists using mathematical tools called structural causal models to investigate a city’s dynamics from different vantage points. One works from a bird’s-eye, coarse-grained perspective; the other maps detailed, fine-grained street-level interactions. They aim to link their causal models — without fully sharing either — despite messy, incomplete, or misaligned data.Borrowing tools from pure mathematics (category theory), we formulate a general learning problem for causal abstractions: mapping a fine-grained (e.g., street-level) causal model to a coarse-grained (e.g., bird’s-eye) one while preserving key causal properties. At the core is a novel principle — semantic embedding — which states that coarse-grained causal knowledge resides within the fine-grained one and is preserved when moving from coarse to fine and back again. Interestingly, this principle naturally connects causal abstractions with Riemannian geometry.We exploit this connection to devise three methods for learning a causal abstraction based on semantic embedding. They reliably recover the intended abstractions  on both simulated data and real brain recordings, offering promising tools for causal analysis at different levels of granularity in realistic scenarios."
Poster,Causal Attribution Analysis for Continuous Outcomes,https://ICML.cc//virtual/2025/poster/45575,"Shanshan Luo, Yu yixuan, Chunchen LIU, Feng Xie, zhi geng","Previous studies have extensively addressed the attribution problem for binary outcome variables. However, in many practical scenarios, the outcome variable is continuous, and simply binarizing it may result in information loss or biased conclusions. To address this issue, we propose a series of posterior causal estimands for retrospectively evaluating multiple correlated causes from a continuous outcome. These estimands include posterior intervention effects, posterior total causal effects, and posterior natural direct effects. Under assumptions of sequential ignorability, monotonicity, and perfect positive rank, we show that the posterior causal estimands of interest are identifiable and present the corresponding identification equations. We also provide a simple but effective estimation procedure and establish asymptotic properties of the proposed estimators. An artificial hypertension example and a real developmental toxicity dataset are employed to illustrate our method.","In many research fields, scholars aim to understand what causes certain outcomes. Previous studies have mostly focused on binary outcomes, for example, whether lung cancer is caused by smoking. However, in real-world settings, many important outcomes, such as blood pressure or income, are measured on a continuous scale. Simplifying these outcomes into binary categories can obscure meaningful patterns and potentially lead to misleading conclusions.To better address such situations, our study introduces a new way to identify which factors are likely responsible for continuous outcomes. Our approach allows researchers to evaluate how different causes, especially those that may be related to each other, jointly affect a result. It also helps determine whether the effect came directly from a cause or through more complex paths.We show that, under reasonable assumptions, this method can yield reliable retrospective causal inferences. To illustrate the proposed method, we apply it to a simulated example of hypertension."
Poster,Causal Discovery from Conditionally Stationary Time Series,https://ICML.cc//virtual/2025/poster/44317,"Carles Balsells-Rodas, Xavier Sumba, Tanmayee Narendra, Ruibo Tu, Gabriele Schweikert, Hedvig Kjellström, Yingzhen Li","Causal discovery, i.e., inferring underlying causal relationships from observational data, is highly challenging for AI systems. In a time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. We develop a causal discovery approach to handle a wide class of nonstationary time series that are _conditionally stationary_, where the nonstationary behaviour is modeled as stationarity conditioned on a set of latent state variables. Named State-Dependent Causal Inference (SDCI), our approach is able to recover the underlying causal dependencies, with provable identifiablity for the state-dependent causal structures. Empirical experiments on nonlinear particle interaction data and gene regulatory networks demonstrate SDCI's superior performance over baseline causal discovery methods. Improved results over non-causal RNNs on modeling NBA player movements demonstrate the potential of our method and motivate the use of causality-driven methods for forecasting.","Understanding cause and effect from data that changes over time, e.g. weather or player movements in sports, is a big challenge for AI systems. Most current methods assume these relationships stay the same over time, but real-world systems often change in complex ways. In this work, we develop a new approach called State-Dependent Causal Inference (SDCI). It allows us to find changing cause-and-effect relationships by assuming that the system is stable when we take into account hidden ""states"" behind the scenes. These states help explain why and how relationships shift over time. We show that our method can correctly uncover these changing patterns, even in real-world systems. This includes predicting how genes interact in cell organisms or how basketball players move during a game. We also prove a key mathematical property: identifiability. This means our method can uniquely determine the right explanation from the data. This is very important for making scientific discoveries you can trust."
Poster,Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants,https://ICML.cc//virtual/2025/poster/46549,"Daniele Tramontano, Yaroslav Kivva, Saber Salehkaleybar, Negar Kiyavash, Mathias Drton","This paper investigates causal effect identification in latent variable Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants, addressing two prominent setups that are challenging in the presence of latent confounding: (1) a single proxy variable that may causally influence the treatment and (2) underspecified instrumental variable cases where fewer instruments exist than treatments. We prove that causal effects are identifiable with a single proxy or instrument and provide corresponding estimation methods. Experimental results demonstrate the accuracy and robustness of our approaches compared to existing methods, advancing the theoretical and practical understanding of causal inference in linear systems with latent confounders.","Understanding cause and effect is crucial in science and decision-making, but it's extremely difficult when some important variables are unmeasured. This is especially true in complex systems, like healthcare or economics, where data often misses key factors. Our research focuses on a class of models called Linear Non-Gaussian Acyclic Models (LiNGAM), which are commonly used to uncover causal relationships from data. A major challenge arises when only a limited number of ""proxy"" or ""instrumental"" variables are available to help identify these hidden causes. We show that even in these tough situations, it's still possible to reliably estimate causal effects. Our approach uses higher sample moments of the observed data to uncover these effects. We back our theory with experiments that show our method outperforms existing techniques in both accuracy and robustness. By improving how we handle hidden factors, our work helps move causal inference from idealized settings closer to real-world applications."
Poster,Causal Invariance-aware Augmentation for Brain Graph Contrastive Learning,https://ICML.cc//virtual/2025/poster/45908,"Minqi Yu, Jinduo Liu, Junzhong Ji","Deep models are increasingly used to analyze brain graphs for the diagnosis and understanding of brain diseases. However, due to the multi-site data aggregation and individual differences, brain graph datasets exhibit widespread distribution shifts, which impair the model’s generalization ability to the test set, thereby limiting the performance of existing methods. To address these issues, we propose a Causally Invariance-aware Augmentation for brain Graph Contrastive Learning, called CIA-GCL. This method first generates a brain graph by extracting node features based on the topological structure. Then, a learnable brain invariant subgraph is identified based on a causal decoupling approach to capture the maximum label-related invariant information with invariant learning. Around this invariant subgraph, we design a novel invariance-aware augmentation strategy to generate meaningful augmented samples for graph contrast learning. Finally, the extracted invariant subgraph is utilized for brain disease classification, effectively mitigating distribution shifts while also identifying critical local graph structures, enhancing the model’s interpretability. Experiments on three real-world brain disease datasets demonstrate that our method achieves state-of-the-art performance, effectively generalizes to multi-site brain datasets, and provides certain interpretability.","Brain graph analysis is a key tool for  computer-aided diagnosis and understanding brain diseases, but identifying the commonalities of patients across multi-site data remains difficult. To tackle this challenge, we aim to extract local structures that remain highly similar across complex environments to better distinguish patients. Building on this, we propose CIA-GCL，a Causally Invariance-aware Augmentation framework for brain graph contrastive learning.Specifically, we recognize the importance of key brain areas and connections. The brain is a highly intricate system, like a finely tuned instrument, where even the disruption of certain crucial areas or pathways can throw off the entire system. We design an invariant subgraph extractor to capture critical brain regions and use a brain-specific augmentation strategy to simulate diverse environments.We extract key distinguishing biomarkers by mining subgraphs inward and externally enforcing their consistency across different environments.These biomarkers not only help in differentiating patients but also provide valuable insights into the abnormal regions associated with brain diseases."
Poster,Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection,https://ICML.cc//virtual/2025/poster/45960,"HyunGi Kim, Jisoo Mok, Dong Jun Lee, Jaihyun Lew, Sungjae Sungjae, Sungroh Yoon","Utilizing the complex inter-variable causal relationships within multivariate time-series provides a promising avenue toward more robust and reliable multivariate time-series anomaly detection (MTSAD) but remains an underexplored area of research. This paper proposes Causality-Aware contrastive learning for RObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that incorporates the notion of causality into contrastive learning. CAROTS employs two data augmentors to obtain causality-preserving and -disturbing samples that serve as a wide range of normal variations and synthetic anomalies, respectively. With causality-preserving and -disturbing samples as positives and negatives, CAROTS performs contrastive learning to train an encoder whose latent space separates normal and abnormal samples based on causality. Moreover, CAROTS introduces a similarity-filtered one-class contrastive loss that encourages the contrastive learning process to gradually incorporate more semantically diverse samples with common causal relationships. Extensive experiments on five real-world and two synthetic datasets validate that the integration of causal relationships endows CAROTS with improved MTSAD capabilities. The code is available at https://github.com/kimanki/CAROTS.","Detecting anomalies in complex multivariate time series is crucial but challenging—especially when multiple factors influence each other in subtle ways. Traditional methods often overlook these interdependencies, leading to missed or incorrect detections. Our research tackles this issue by focusing on causal relationships—understanding how one variable can influence another over time. We developed a new approach called CAROTS, which teaches a machine learning model to tell apart normal and abnormal patterns by recognizing when these causal links are preserved or disrupted. To do this, we simulate both realistic and intentionally flawed versions of data, helping the model learn the difference. We also introduce a special learning method that gradually includes more diverse examples, as long as they share meaningful causal patterns. Our approach was tested on various real and synthetic datasets and showed significant improvements over existing techniques. This research opens the door to more reliable anomaly detection in critical applications where understanding cause and effect is essential."
Poster,Causality Inspired Federated Learning for OOD Generalization,https://ICML.cc//virtual/2025/poster/44000,"Jiayuan Zhang, Xuefeng Liu, Jianwei Niu, Shaojie Tang, Haotian Yang, Xinghao Wu","The out-of-distribution (OOD) generalization problem in federated learning (FL) has recently attracted significant research interest. A common approach, derived from centralized learning, is to extract causal features which exhibit causal relationships with the label.  However, in FL, the global feature extractor typically captures only invariant causal features shared across clients and thus discards many other causal features that are potentially useful for OOD generalization. To address this problem, we propose FedUni, a simple yet effective architecture trained to extract all possible causal features from any input. FedUni consists of a comprehensive feature extractor, designed to identify a union of all causal feature types in the input, followed by a feature compressor, which discards potential \textit{inactive} causal features.  With this architecture, FedUni can benefit from collaborative training in FL while avoiding the cost of model aggregation (i.e., extracting only invariant features). In addition, to further enhance the feature extractor's ability to capture causal features, FedUni add a causal intervention module on the client side, which employs a counterfactual generator to generate counterfactual examples that simulate distributions shifts. Extensive experiments and theoretical analysis demonstrate that our method significantly improves OOD generalization performance.","Deep learning models often struggle when facing unfamiliar data, especially in real-world settings where information is collected from many different sources. This is a major challenge for federated learning, a method where many devices train a shared model without sharing their raw data. One popular approach is to teach the model to focus on ""causal features"" , which are parts of the input that truly affect the outcome and are more stable across different environments. However, existing methods usually extract only the features shared by all devices, which ignores many useful ones. To solve this, we propose a new method called FedUni. It teaches the model to collect all possible causal features across devices, and then selectively filter out the ones that are irrelevant in new situations. We also introduce a way for each device to simulate ""what-if"" versions of its data, helping the model better understand cause and effect. Our experiments show that this leads to much more reliable performance when the model sees unfamiliar data. By making deep learning models more flexible and robust, this research can help ensure safer and more trustworthy AI systems in real-world applications."
Poster,Causal Logistic Bandits with Counterfactual Fairness Constraints,https://ICML.cc//virtual/2025/poster/46644,"Jiajun Chen, Jin Tian, Chris Quinn","Artificial intelligence will play a significant role in decision making in numerous aspects of society. Numerous fairness criteria have been proposed in the machine learning community, but there remains limited investigation into fairness as defined through specified attributes  in a sequential decision-making framework.  In this paper, we focus on causal logistic bandit problems where the learner seeks to make fair decisions, under a notion of fairness that accounts for counterfactual reasoning.  We propose and analyze an algorithm by leveraging primal-dual optimization for constrained causal logistic bandits where the non-linear constraints are a priori unknown and must be learned in time.  We obtain sub-linear regret guarantees with leading term similar to that for  unconstrained logistic bandits (Lee et al., 2024)  while guaranteeing sub-linear constraint violations.   We show how to achieve zero cumulative constraint violations with a small increase in the regret bound.","- We study fairness in an online learning framework, where fairness is defined using counterfactual reasoning — that is, reasoning about what would have happened if an individual had belonged to a different demographic group.- Our framework allows the learning algorithm to make potentially unfair decisions, but penalizes the expected reward for actions that exhibit large counterfactual fairness violations.- These results contribute to the broader research on causal decision-making systems,"
Poster,Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel,https://ICML.cc//virtual/2025/poster/45328,"Carlota Parés Morlans, Michelle Yi, Claire Chen, Sarah A Wu, Rika Antonova, Tobias Gerstenberg, Jeannette Bohg","Tasks that involve complex interactions between objects with unknown dynamics make planning before execution difficult. These tasks require agents to iteratively improve their actions after actively exploring causes and effects in the environment. For these type of tasks, we propose Causal-PIK, a method that leverages Bayesian optimization to reason about causal interactions via a Physics-Informed Kernel to help guide efficient search for the best next action. Experimental results on Virtual Tools and PHYRE physical reasoning benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring fewer actions to reach the goal. We also compare Causal-PIK to human studies, including results from a new user study we conducted on the PHYRE benchmark. We find that Causal-PIK remains competitive on tasks that are very challenging, even for human problem-solvers.","Humans are skilled at solving tasks that involve complex object interactions—such as dropping an object onto one end of a lever to create a catapult effect, launching another object from the opposite end. These tasks pose a major challenge for artificial intelligence systems, as the outcomes of actions depend on complex and often unknown physical dynamics. Humans approach these problems with physical intuition, learning quickly from previous attempts to refine their strategies.Inspired by this ability, we developed Causal-PIK, a method that helps artificial agents to solve physical reasoning tasks more efficiently by reasoning about object interactions. Just like how humans efficiently learn from their mistakes when solving physical problems, our method allows agents to learn from their mistakes and make increasingly informed decisions with fewer trials. The key innovation lies in the algorithm's ability to understand direct cause-and-effect relationships and perform counterfactual reasoning — allowing it to imagine alternate scenarios and evaluate what could have happened under different conditions. Our findings show that the ability to predict which actions will cause similar effects significantly impacts an agent’s efficiency in exploring complex environments."
