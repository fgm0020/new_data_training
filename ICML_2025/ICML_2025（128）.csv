type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Graph Inverse Style Transfer for Counterfactual Explainability,https://ICML.cc//virtual/2025/poster/46421,"Bardh Prenkaj, Efstratios Zaradoukas, Gjergji Kasneci","Counterfactual explainability seeks to uncover model decisions by identifying minimal changes to the input that alter the predicted outcome. This task becomes particularly challenging for graph data due to preserving structural integrity and semantic meaning. Unlike prior approaches that rely on forward perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the first framework to re-imagine graph counterfactual generation as a backtracking process, leveraging spectral style transfer. By aligning the global structure with the original input spectrum and preserving local content faithfulness, GIST produces valid counterfactuals as interpolations between the input style and counterfactual content. Tested on 8 binary and multi-class graph classification benchmarks, GIST achieves a remarkable +7.6% improvement in the validity of produced counterfactuals and significant gains (+45.5%) in faithfully explaining the true class distribution. Additionally, GIST's backtracking mechanism effectively mitigates overshooting the underlying predictor's decision boundary, minimizing the spectral differences between the input and the counterfactuals. These results challenge traditional forward perturbation methods, offering a novel perspective that advances graph explainability.","Modern AI systems, like those used in medical diagnoses or fraud detection, often rely on complex graph structures to make predictions. However, these systems can feel like black boxes - it’s hard to understand why they make one decision over another. To make AI more transparent, researchers use counterfactual explanations: they answer questions like, *“What small change would have led the model to make a different decision?”*Explaining decisions on graphs is uniquely challenging because graphs are more than just data points - they have structure and relationships that need to be preserved. Traditional approaches try to “nudge” the graph forward into a new decision outcome, but this can break its structure or lose important information.Our research introduces GIST (Graph Inverse Style Transfer), a new way to explain AI decisions on graphs by thinking in *reverse*. Instead of pushing a graph forward until the AI flips its decision, we first overshoot and then carefully step backward to find a meaningful alternative. Inspired by how style transfer works in image editing, GIST uses the ""style"" of a graph -  its overall structure - and combines it with local details to produce realistic and faithful explanations.GIST outperforms previous methods by a large margin, generating explanations that are not only valid (they change the AI’s decision) but also make sense structurally and contextually. This backtracking strategy is especially useful when the AI model is hard to access or can’t be queried frequently - a common real-world constraint.By making graph-based AI more understandable, GIST helps bring trustworthy and interpretable machine learning closer to sensitive applications like healthcare, finance, and law."
Poster,Graph Minimum Factor Distance and Its Application to Large-Scale Graph Data Clustering,https://ICML.cc//virtual/2025/poster/44384,Jicong Fan,"Measuring the distance or similarity between graphs is the foundation of many graph analysis tasks, such as graph classification and clustering, but remains a challenge on large datasets. In this work, we treat the adjacency matrices of two graphs as two kernel matrices given by some unknown indefinite kernel function performed on two discrete distributions and define the distance between the two distributions as a measure, called MMFD, of the dissimilarity between two graphs. We show that MMFD is a pseudo-metric. Although the initial definition of MMFD seems complex, we show that it has a closed-form solution with extremely simple computation. To further improve the efficiency of large-scale clustering, we propose an MMFD-KM with linear space and time complexity with respect to the number of graphs. We also provide a generalization of MMFD, called MFD, which is more effective in exploiting the information of factors of adjacency matrices. The experiments on simulated graphs intuitively show that our methods are effective in comparing graphs. The experiments on real-world datasets demonstrate that, compared to the competitors, our methods have much better clustering performance in terms of three evaluation metrics and time cost.","Calculating the distance or similarity between graphs is key to many graph analysis tasks, like graph classification and clustering. However, it's still tough when dealing with large datasets. In our work, we represent two graphs as two discrete distributions and calculate the distance between the two distributions, which leads to a distance measure between graphs, called MMFD. MMFD is a pseudo-metric and has a very simple closed-form solution. To make large-scale clustering more efficient, we introduce MMFD-KM, which has linear space and time complexity relative to the number of graphs. We also expand on MMFD with MFD, which better uses the information in the adjacency matrices. Experiments on simulated graphs show our methods work well for comparing graphs. Real-world dataset tests prove our methods outperform competitors in clustering, based on three metrics and time cost."
Poster,Graph Neural Network Generalization With Gaussian Mixture Model Based Augmentation,https://ICML.cc//virtual/2025/poster/45707,"Yassine Abbahaddou, Fragkiskos Malliaros, Johannes Lutzeyer, Amine Aboussalah, Michalis Vazirgiannis","Graph Neural Networks (GNNs) have shown great promise in tasks like node and graph classification, but they often struggle to generalize, particularly to unseen or out-of-distribution (OOD) data. These challenges are exacerbated when training data is limited in size or diversity. To address these issues, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GRATIN, an efficient graph data augmentation algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques in terms of generalization but also offers improved time complexity, making it highly suitable for real-world applications.","Imagine you're teaching an AI to recognize different types of social or biological networks. These networks are complex. A powerful AI tool called Graph Neural Networks (GNNs) is used to learn from this kind of structured data.However, GNNs often struggle to understand networks they haven't seen before, especially if they only learned from a small or limited set of network examples. It's like teaching a child only about cats and then asking them to identify a tiger, they might struggle because it's a new animal.Our research tackles this problem. We've developed a new way to understand why GNNs struggle with new data and, more importantly, how to fix it. We use a concept called Rademacher complexity to measure how well a GNN will perform on unseen networks.This understanding helped us create a new technique called GRATIN. Think of GRATIN as a smart data generator. It creates many new, realistic-looking example networks. These new networks are subtly different from the original ones but still representative, giving the GNN much more diverse training data.GRATIN helps GNNs learn more effectively from limited data, making them much better at recognizing and classifying new, unseen networks. It also offers improved time complexity, making it highly suitable for real-world applications."
Poster,Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization,https://ICML.cc//virtual/2025/poster/45652,"Robbert Reijnen, Yaoxin Wu, Zaharah Bukhsh, Yingqian Zhang","Deep reinforcement learning (DRL) has been widely used for dynamic algorithm configuration, particularly in evolutionary computation, which benefits from the adaptive update of parameters during the algorithmic execution. However, applying DRL to algorithm configuration for multi-objective combinatorial optimization (MOCO) problems remains relatively unexplored. This paper presents a novel graph neural network (GNN) based DRL to configure multi-objective evolutionary algorithms. We model the dynamic algorithm configuration as a Markov decision process, representing the convergence of solutions in the objective space by a graph, with their embeddings learned by a GNN to enhance the state representation. Experiments on diverse MOCO challenges indicate that our method outperforms traditional and DRL-based algorithm configuration methods in terms of efficacy and adaptability. It also exhibits advantageous generalizability across objective types and problem sizes, and applicability to different evolutionary computation methods.","Algorithms need to be set up with the right parameters to perform well. However, the best settings often change as the algorithm runs, based on the progress it makes. Dynamic algorithm configuration addresses this challenge by allowing the algorithm to adjust its parameters on the fly. In this paper, we explore how to make these adaptive decisions smarter and more effective, especially for problems with multiple conflicting optimization goals (such as cost vs. quality). We use a deep reinforcement learning (DRL) based to train an agent that learns from past experience how to adjust the algorithm’s settings dynamically. To help the agent see how solutions are improving, we represent the solutions as a graph and use a graph neural network (GNN) to learn the patterns in this progress. This helps the system understand when and how to adjust its settings. Our results show that this method works better than both traditional fixed settings and other learning-based methods. It not only finds better solutions but also generalizes to different problem variants and sizes."
Poster,Graph World Model,https://ICML.cc//virtual/2025/poster/43569,"Tao Feng, Yexin Wu, Guanyu Lin, Jiaxuan You","World models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks.Existing WMs primarily focus on unstructured data while cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on 6 tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselines' performance, benefits from multi-hop structures, and demonstrate strong zero-shot/few-shot capabilities on unseen new tasks. Our codes for GWM is released at https://github.com/ulab-uiuc/GWM.","Structured data like graphs are everywhere—in social networks, molecules, and recommender systems—but most world models only work with unstructured inputs like images or text. We wanted to explore whether a world model could understand and act in graph-based environments just as well as it does in pixel-based ones. Our paper introduces the Graph World Model (GWM), which treats each state as a graph and each task as an “action node” connected to relevant information. We designed two versions: one based on language tokens, and another on compact embeddings, both using message-passing to integrate context. Surprisingly, GWM performs competitively with domain-specific models across six diverse tasks, despite using a unified architecture. This challenges the assumption that structured and unstructured data need separate models. Our findings suggest that bridging graphs and world models unlocks new paths for general AI systems that can plan, reason, and generate across data types and domains."
Poster,Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for Agents,https://ICML.cc//virtual/2025/poster/45046,"Nolan Koblischke, Hyunseok Jang, Kristen Menou, Mohamad Ali-Dib","Modern science emerged from reasoning over repeatedly-observed planetary motions. We present Gravity-Bench-v1, an environment-based benchmark that challenges AI agents on tasks that parallel this historical development. Gravity-Bench-v1 evaluates agents on the discovery of physics concealed within a dynamic environment, using rigorous gravitational dynamics simulations. Gravity-Bench includes out-of-distribution cases, i.e. with physics that deviates from the real world, to evaluate true scientific generalization capabilities. Agents must plan to collect data within an experimental budget and must perform a dynamic form of data analysis and reasoning to solve tasks efficiently. Our benchmark admits an open-ended space of solutions. Reference solutions for each task are provided to calibrate AI performance against human expertise. Technically at an upper-undergraduate level, our benchmark proves challenging to baseline AI agents. Gravity-Bench-v1 and planned extensions should help map out AI progress towards scientific discovery capabilities.","For AI to support scientific discoveries, it must not only analyze data, but actively explore and figure things out, just like human scientists do. To test and measure these skills, we created Gravity-Bench, an interactive test inspired by the historical scientific breakthroughs involving gravity and planetary motion.In Gravity-Bench, AI agents act as astronomers exploring binary star systems, where two stars orbit each other. Instead of just analyzing ready-collected data, the AI agent has to plan and gather its own observations cautiously, within a budget. The tasks go beyond textbook examples: sometimes the simulated environment introduces new laws of physics that differ from the ones we know, compelling AI agents to adapt to solve novel problems.Our tests show that current AI models, like those that power ChatGPT, still struggle with these challenges, particularly with planning observations and drawing correct conclusions. Gravity-Bench provides researchers a scientifically meaningful framework to track progress towards AI capable of original scientific discovery."
Poster,Great Models Think Alike and this Undermines AI Oversight,https://ICML.cc//virtual/2025/poster/46528,"Shashwat Goel, Joschka Strüber, Ilze Amanda Auzina, Karuna Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Pandurang Prabhu, Matthias Bethge, Jonas Geiping","As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as *AI Oversight*. We study how model similarity affects both aspects of AI oversight by proposing *Chance Adjusted Probabilistic Agreement (CAPA)*--a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that *LLM-as-a-judge* scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from *weak-to-strong generalization*. As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend--model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.","Currently, there are hundreds of different language models (LM) available, as each tech company creates and releases their own chatbots. How different are these models really? Do all of them fail (or succeed) in the same ways? In this work, we measure model similarity based on how often they make the same mistakes. As LM capabilities advance, we find that model mistakes are becoming more similar, that is, Great Models Think Alike. At the same time, finding these mistakes and fixing them now needs more effort and expertise, making it expensive and time-consuming for humans. Recent research is trying to automate this process using another LM as a judge, or as a teacher, which we refer to as ""AI Oversight"". But could models thinking alike adversely affect AI oversight? Indeed, we find LM judgements show a bias, favouring more similar models. Moreover, when one LM is used as a 'teacher’ for another ‘student’ LM, we find lower performance improvements when models are more similar, perhaps because there is less complementary knowledge for the student to learn from. Overall, we show the importance of measuring model similarity, as it reveals insights beyond accuracy comparisons. To promote reporting of model similarity, we release a Python package lm-sim with many model similarity metrics, including ours."
Poster,Gridded Transformer Neural Processes for Spatio-Temporal Data,https://ICML.cc//virtual/2025/poster/45467,"Matthew Ashman, Cristiana Diaconu, Eric Langezaal, Adrian Weller, Richard E Turner","Effective modelling of large-scale spatio-temporal datasets is essential for many domains, yet existing approaches often impose rigid constraints on the input data, such as requiring them to lie on fixed-resolution grids. With the rise of foundation models, the ability to process diverse, heterogeneous data structures is becoming increasingly important. Neural processes (NPs), particularly transformer neural processes (TNPs), offer a promising framework for such tasks, but struggle to scale to large spatio-temporal datasets due to the lack of an efficient attention mechanism. To address this, we introduce gridded pseudo-token TNPs which employ specialised encoders and decoders to handle unstructured data and utilise a processor comprising gridded pseudo-tokens with efficient attention mechanisms. Furthermore, we develop equivariant gridded TNPs for applications where exact or approximate translation equivariance is a useful inductive bias, improving accuracy and training efficiency. Our method consistently outperforms a range of strong baselines in various synthetic and real-world regression tasks involving large-scale data, while maintaining competitive computational efficiency. Experiments with weather data highlight the potential of gridded TNPs and serve as just one example of a domain where they can have a significant impact.","Many real-world systems—such as weather, climate, scientific computing simulators—generate complex spatio-temporal data that are difficult to model. These data often come from heterogeneous sources (e.g., sensors, simulations) and are recorded at irregular times and locations, making them challenging to process with standard machine learning methods. Existing models that are able to deal with such large-scale data tend to require data to be structured on fixed grids, limiting their flexibility and generality.In this work, we address this limitation by developing **gridded Transformer Neural Processes** (gridded TNPs)—a modelling framework that can flexibly handle unstructured spatio-temporal data. Our approach uses attention-based mechanisms to first encode irregular data onto a grid and then apply efficient transformer architectures for learning. We also introduce a variant that incorporates spatial symmetries, which improves both training efficiency and generalisation capabilities in settings where the data are (roughly) stationary.We evaluate our method on synthetic and real-world datasets, including weather data, and show that it outperforms existing strong baselines while remaining computationally efficient. Our framework helps advance the development of data-driven, flexible, and scalable models for real-world spatio-temporal problems."
Poster,Griffin: Towards a Graph-Centric Relational Database Foundation Model,https://ICML.cc//virtual/2025/poster/45150,"Yanbo Wang, Xiyuan Wang, Quan Gan, Minjie Wang, Qibin Yang, David Wipf, Muhan Zhang","We introduce Griffin, the first foundation model attemptation designed specifically for Relational Databases (RDBs). Unlike previous smaller models focused on single RDB tasks, Griffin unifies the data encoder and task decoder to handle diverse tasks. Additionally, we enhance the architecture by incorporating a cross-attention module and a novel aggregator. Griffin utilizes pretraining on both single-table and RDB datasets, employing advanced encoders for categorical, numerical, and metadata features, along with innovative components such as cross-attention modules and enhanced message-passing neural networks (MPNNs) to capture the complexities of relational data. Evaluated on large-scale, heterogeneous, and temporal graphs extracted from RDBs across various domains (spanning over 150 million nodes), Griffin demonstrates superior or comparable performance to individually trained models, excels in low-data scenarios, and shows strong transferability with similarity and diversity in pretraining across new datasets and tasks, highlighting its potential as a universally applicable foundation model for RDBs. Code available at https://github.com/yanxwb/Griffin.","While computers are great with text and images, they often struggle with the complex databases that businesses and scientists use every day. These ""relational databases"" are tricky because they store all sorts of different information across many interconnected tables, making it hard for a single AI to understand it all.We've built an AI called ""Griffin"" that learns to see these databases like interconnected maps. Griffin is specially trained in steps to understand the varied data types it finds and, crucially, how all the different tables and pieces of information relate to each other.This new approach helps Griffin make accurate predictions using the database information. More importantly, Griffin can use what it has learned on new, unfamiliar databases, even if there's very little data available for a new problem. Our work aims to make AI much better at finding valuable insights hidden inside the everyday databases that power our world."
Poster,GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers,https://ICML.cc//virtual/2025/poster/43630,"GUOGUO AI, Guansong Pang, Hezhe Qiao, YuanGao, Hui Yan","Graph Transformers (GTs) have demonstrated remarkable performance in graph representation learning over popular graph neural networks (GNNs). However, self-attention, the core module of GTs, preserves only low-frequency signals in graph features, leading to ineffectiveness in capturing other important signals like high-frequency ones. Some recent GT models help alleviate this issue, but their flexibility and expressiveness are still limited since the filters they learn are fixed on predefined graph spectrum or spectral order. To tackle this challenge, we propose a Graph Fourier Kolmogorov-Arnold Transformer (GrokFormer), a novel GT model that learns highly expressive spectral filters with adaptive graph spectrum and spectral order through a Fourier series modeling over learnable activation functions. We demonstrate theoretically and empirically that the proposed GrokFormer filter offers better expressiveness than other spectral methods. Comprehensive experiments on 10 real-world node classification datasets across various domains, scales, and graph properties, as well as 5 graph classification datasets, show that GrokFormer outperforms state-of-the-art GTs and GNNs. Our code is available at https://github.com/GGA23/GrokFormer.","Graph Transformers (GTs) have shown remarkable performance in graph representation learning. However, their core component, self-attention, primarily retains low-frequency signals, making it difficult to model graphs with diverse properties. This paper presents GrokFormer, a novel GT model that learns expressive and adaptive spectral filters through order‑K Fourier series modeling, overcoming the limitations of self-attention by effectively capturing rich frequency signals across a broad range of graph spectra and orders. Comprehensive experiments on both synthetic and real-world datasets demonstrate the superiority of GrokFormer over state-of-the-art GNNs and GTs."
