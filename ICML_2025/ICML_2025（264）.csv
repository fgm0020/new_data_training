type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior",https://ICML.cc//virtual/2025/poster/45015,"Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine","The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured ""harm-benefit tree,"" which enumerates harmful and beneficial *actions* and *effects* the AI behavior may lead to, along with *likelihood*, *severity*, and *immediacy* labels that describe potential impacts on *stakeholders*. SafetyAnalyst then aggregates all effects into a harmfulness score using 28 fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this framework to develop an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On comprehensive benchmarks, we show that SafetyAnalyst (average F1=0.81) outperforms existing moderation systems (average F1$<$0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.","**Making AI Safety More Human-Understandable and Flexible****The Problem:** Current AI safety systems are often like ""black boxes""—it's hard to understand their decisions, and they aren't easily adjusted for the different safety needs of different applications and user populations.**Our Solution:** We created SafetyAnalyst, a system that transparently evaluates potential AI actions. It builds a ""harm-benefit tree"" detailing who might be affected by some given AI action, any harmful and beneficial consequences to them, and how severe the impacts could be. SafetyAnalyst then uses adjustable weights to calculate a ""harmfulness score.""**Why It Matters:** This makes AI safety decisions human-understandable and allows them to be tailored to specific rules or community values in a transparent way. Our tests show that SafetyAnalyst is more effective at identifying unsafe AI prompts than existing systems, making it an outstanding tool for enabling safer, more trustworthy AI that better aligns with human values."
Poster,Safety Certificate against Latent Variables with Partially Unidentifiable Dynamics,https://ICML.cc//virtual/2025/poster/46176,"Haoming Jing, Yorie Nakahira","Many systems contain latent variables that make their dynamics partially unidentifiable or cause distribution shifts in the observed statistics between offline and online data. However, existing control techniques often assume access to complete dynamics or perfect simulators with fully observable states, which are necessary to verify whether the system remains within a safe set (forward invariance) or safe actions are consistently feasible at all times. To address this limitation, we propose a technique for designing probabilistic safety certificates for systems with latent variables. A key technical enabler is the formulation of invariance conditions in probability space, which can be constructed using observed statistics in the presence of distribution shifts due to latent variables. We use this invariance condition to construct a safety certificate that can be implemented efficiently in real-time control. The proposed safety certificate can continuously find feasible actions that control long-term risk to stay within tolerance. Stochastic safe control and (causal) reinforcement learning have been studied in isolation until now. To the best of our knowledge, the proposed work is the first to use causal reinforcement learning to quantify long-term risk for the design of safety certificates. This integration  enables safety certificates to efficiently ensure long-term safety in the presence of latent variables. The effectiveness of the proposed safety certificate is demonstrated in numerical simulations.","Many real-world systems, like robots or self-driving cars, operate in environments where not everything can be seen or measured. This hidden information can make it hard to predict how the system will behave or to ensure it stays safe. Most existing safety methods assume we have complete knowledge or perfect simulations of these systems—which isn’t realistic. This paper introduces a new method to help systems stay safe even when some information is missing. This method can quickly compute safe actions while the system is running, helping it avoid risky situations in the long run. This is the first approach that combines safety techniques with a class of machine learning techniques—called causal reinforcement learning—to better understand and manage long-term risks when certain information in the environment is hidden."
Poster,Safety-Polarized and Prioritized Reinforcement Learning,https://ICML.cc//virtual/2025/poster/43599,"Ke Fan, Jinpeng Zhang, Xuefeng Zhang, Yunze Wu, Jingyu Cao, Yuan Zhou, Jianzhu Ma","Motivated by the first priority of safety in many real-world applications, we propose \textsc{MaxSafe}, a chance-constrained bi-level optimization framework for safe reinforcement learning. \textsc{MaxSafe} first minimizes the unsafe probability and then maximizes the return among the safest policies. We provide a tailored Q-learning algorithm for the \textsc{MaxSafe} objective, featuring a novel learning process for \emph{optimal action masks} with theoretical convergence guarantees. To enable the application of our algorithm to large-scale experiments, we introduce two key techniques: \emph{safety polarization} and \emph{safety prioritized experience replay}. Safety polarization generalizes the optimal action masking by polarizing the Q-function, which assigns low values to unsafe state-action pairs, effectively discouraging their selection. In parallel, safety prioritized experience replay enhances the learning of optimal action masks by prioritizing samples based on temporal-difference (TD) errors derived from our proposed state-action reachability estimation functions. This approach efficiently addresses the challenges posed by sparse cost signals.  Experiments on diverse autonomous driving and safe control tasks show that our methods achieve near-maximal safety and an optimal reward-safety trade-off.","We present \textsc{MaxSafe}, a framework for training AI agents that prioritize safety before performance. It first avoids risky actions, then selects the best among the safest options. To support effective learning, we introduce two techniques: masking unsafe actions and prioritizing experiences related to safety. Our approach achieves strong safety and performance in tasks like autonomous driving and classic control."
Poster,Safety Reasoning with Guidelines,https://ICML.cc//virtual/2025/poster/46121,"Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Dacheng Tao, Minhao Cheng","Training safe LLMs remains a critical challenge. The most widely used method, Refusal Training (RT), struggles to generalize against various Out-of-Distribution (OOD) jailbreaking attacks. Although various advanced methods have been proposed to address this issue, we instead question whether OOD attacks inherently surpass the capability of vanilla RT. Evaluations using Best-of-N (BoN) reveal significant safety improvements as N increases, indicating models possess adequate latent safety knowledge but RT fails to consistently elicit it under OOD scenarios. Further domain adaptation analysis reveals that direct RT causes reliance on superficial shortcuts, resulting in non-generalizable representation mappings. Inspired by our findings, we propose training model to perform safety reasoning for each query. Specifically, we synthesize reasoning supervision aligned with specified guidelines that reflect diverse perspectives on safety knowledge. This encourages model to engage in deeper reasoning, explicitly eliciting and utilizing latent safety knowledge for each query. Extensive experiments show that our method significantly improves model generalization against OOD attacks.","In this work, we analyze the reasons why the commonly used Refusal Training fails to generalize against OOD attacks and provide explanations for these failure modes. Based on our findings, we propose to train models to perform safety reasoning with specified guidelines, explicitly eliciting and utilizing latent knowledge from diverse perspective to learn generalizable representation mapping and improve OOD generalization. Extensive experiments and ablation studies verify the effectiveness of our method."
Poster,SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization,https://ICML.cc//virtual/2025/poster/44114,"Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia wei, Jun Zhu, Jianfei Chen","Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrixes $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about **3x** and **4.5x**, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation.","Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about **3x** and **4.5x**, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation."
Poster,SAH-Drive: A Scenario-Aware Hybrid Planner for Closed-Loop Vehicle Trajectory Generation,https://ICML.cc//virtual/2025/poster/45437,"Yuqi Fan, Zhiyong Cui, Zhenning Li, Yilong Ren, Haiyang Yu","Reliable planning is crucial for achieving autonomous driving. Rule-based planners are efficient but lack generalization, while learning-based planners excel in generalization yet have limitations in real-time performance and interpretability. In long-tail scenarios, these challenges make planning particularly difficult. To leverage the strengths of both rule-based and learning-based planners, we proposed the **Scenario-Aware Hybrid Planner** (SAH-Drive) for closed-loop vehicle trajectory planning. Inspired by human driving behavior, SAH-Drive combines a lightweight rule-based planner and a comprehensive learning-based planner, utilizing a dual-timescale decision neuron to determine the final trajectory. To enhance the computational efficiency and robustness of the hybrid planner, we also employed a diffusion proposal number regulator and a trajectory fusion module. The experimental results show that the proposed method significantly improves the generalization capability of the planning system, achieving state-of-the-art performance in interPlan, while maintaining computational efficiency without incurring substantial additional runtime.","Autonomous vehicles need to plan safe and efficient routes in real-time. Traditionally, engineers have used rule-based systems for this, like giving the car a detailed list of ""if-then"" instructions. These are fast and predictable but don’t handle unfamiliar or complex situations well. On the other hand, AI-based planners can adapt to new scenarios by learning from data, but they can be slow or hard to understand.To get the best of both worlds, we created a new system called **SAH-Drive**. It works a bit like how human drivers switch between habit and conscious thinking: using simple rules for routine driving and a more powerful AI when the situation gets tricky. The system includes a smart decision-making unit that decides when to rely on which part. Our approach outperforms existing methods in tough driving scenarios while still running efficiently — a step closer to making self-driving cars safer and more dependable in the real world."
Poster,SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation,https://ICML.cc//virtual/2025/poster/44779,"Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, Jiafei Duan","Robotic manipulation systems operating in diverse, dynamic environments must exhibit three critical abilities: multitask interaction, generalization to unseen scenarios, and spatial memory. While significant progress has been made in robotic manipulation, existing approaches often fall short in generalization to complex environmental variations and addressing memory-dependent tasks. To bridge this gap, we introduce **SAM2Act**, a multi-view robotic transformer-based policy that leverages multi-resolution upsampling with visual representations from large-scale foundation model. SAM2Act achieves a state-of-the-art average success rate of **86.8% across 18 tasks** in the RLBench benchmark, and demonstrates robust generalization on *The Colosseum* benchmark, with only a **4.3% performance gap** under diverse environmental perturbations. Building on this foundation, we propose **SAM2Act+**, a memory-based architecture inspired by SAM2, which incorporates a memory bank, an encoder, and an attention mechanism to enhance spatial memory. To address the need for evaluating memory-dependent tasks, we introduce ***MemoryBench***, a novel benchmark designed to assess spatial memory and action recall in robotic manipulation. SAM2Act+ achieves an average success rate of **94.3% on memory-based tasks** in *MemoryBench*, significantly outperforming existing approaches and pushing the boundaries of memory-based robotic systems.Project page: [sam2act.github.io](https://sam2act.github.io/).","Robots that work in real-world settings need to do three things well:1. **Handle new situations** they haven’t seen before.2. **Do precise tasks well**.3. **Remember where things are** while they work.Most current robot systems still struggle, especially with memory. Our team built a new system called **SAM2Act** that helps robots see their surroundings from several camera angles, understand what they’re looking at, and act accordingly. In tests covering 18 household-style tasks (like stacking blocks or opening a drawer), SAM2Act completed nearly nine out of ten attempts successfully. Even when we changed the lighting, object colors, and other conditions, its performance dropped by only about four percent. We then added a “memory bank” so the robot could store and recall visual snapshots while it moves. The upgraded version, **SAM2Act+**, lets the robot remember where objects were a few moments ago, crucial for tasks such as picking up an item it set aside earlier. Because no standard test existed for this kind of memory, we created *MemoryBench*, a new set of challenges that measure how well a robot can remember and act on past observations. SAM2Act+ topped this benchmark, showing that giving robots a working memory can make them far more reliable.To learn more about our project and view demonstrations, please visit our project page: [sam2act.github.io](https://sam2act.github.io/)."
Poster,Sample Complexity of Branch-length Estimation by Maximum Likelihood,https://ICML.cc//virtual/2025/poster/46053,"David Clancy, Hanbaek Lyu, Sebastien Roch","We consider the branch-length estimation problem on a bifurcating tree: a character evolves along the edges of a binary tree according to a two-state symmetric Markov process, and we seek to recover the edge transition probabilities from repeated observations at the leaves. This problem arises in phylogenetics, and is related to latent tree graphical model inference. In general, the log-likelihood function is non-concave and may admit many critical points. Nevertheless, simple coordinate maximization has been known to perform well in practice, defying the complexity of the likelihood landscape. In this work, we provide the first theoretical guarantee as to why this might be the case. We show that deep inside the Kesten-Stigum reconstruction regime, provided with polynomially many $m$ samples (assuming the tree is balanced), there exists a universal parameter regime (independent of the size of the tree) where the log-likelihood function is strongly concave and smooth with high probability. On this high-probability likelihood landscape event, we show that the standard coordinate maximization algorithm converges exponentially fast to the maximum likelihood estimator, which is within $O(1/\sqrt{m})$ from the true parameter, provided a sufficiently close initial point.","In evolutionary biology, researchers typically use trees to describe how species evolve from common ancestors. Each branching in the tree represents a point where one species splits into two. Here we are interested in estimating how much genetic change happens along each branch of the tree. One challenge is that we can usually only observe the “leaves” — the current species — and not what happened inside the tree. Simple algorithms based on making one small improvement at a time often work surprisingly well in practice, although establishing this mathematically remains difficult because the landscape of possible choices is complex and full of local traps.Our research makes progress towards this goal. We show that if you collect enough data from the leaves, there’s a universal condition — no matter how big or complex the tree is — where the landscape becomes smooth and well-behaved. In this case, the simple algorithm converges quickly and reliably to the correct answer. This helps provide support for widely used tools in evolutionary analysis and latent tree models in machine learning."
Poster,Sample Complexity of Correlation Detection in the Gaussian Wigner Model,https://ICML.cc//virtual/2025/poster/44089,"Dong Huang, Pengkun Yang","Correlation analysis is a fundamental step in uncovering meaningful insights from complex datasets. In this paper, we study the problem of detecting correlations between two random graphs following the Gaussian Wigner model with unlabeled vertices. Specifically, the task is formulated as a hypothesis testing problem: under the null hypothesis, the two graphs are independent, while under the alternative hypothesis, they are edge-correlated through a latent vertex permutation, yet maintain the same marginal distributions as under the null. We focus on the scenario where two induced subgraphs, each with a fixed number of vertices, are sampled. We determine the optimal rate for the sample size required for correlation detection, derived through an analysis of the conditional second moment. Additionally, we propose an efficient approximate algorithm that significantly reduces running time.","We show how to reliably detect whether two random networks are correlated, even when the node correspondence are hidden. Our work identifies the smallest data size needed for this task and introduces an efficient algorithm to perform the test quickly."
Poster,Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,https://ICML.cc//virtual/2025/poster/44017,"Yiting He, Zhishuai Liu, Weixin Wang, Pan Xu","Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.","When training an agent to make decisions by interacting with a simulated environment, it is crucial that the agent continues to perform well even if the real environment differs slightly. But how can we determine whether such robust learning is possible, and under what conditions?To address this question, we introduce a simple metric that compares how easily the agent can reach certain states in the training environment versus in the perturbed environment. This measure captures the difficulty of using exploration in the nominal environment to gather enough information for estimating the perturbed environment. When this value remains bounded, we design a learning algorithm and prove that robust learning is achievable.Our findings offer a quantitative framework to assess the impact of environmental changes on learning performance and help guide the development of algorithms that remain effective under uncertainty. We also provide sample complexity estimates for learning such a robust policy."
