type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation,https://ICML.cc//virtual/2025/poster/43876,"Jan Pauls, Max Zimmer, Berkant Turan, Sassan Saatchi, Philippe CIAIS, Sebastian Pokutta, Fabian Gieseke","With the rise in global greenhouse gas emissions, accurate large-scale tree canopy height maps are essential for understanding forest structure, estimating above-ground biomass, and monitoring ecological disruptions. To this end, we present a novel approach to generate large-scale, high-resolution canopy height maps over time. Our model accurately predicts canopy height over multiple years given Sentinel 2 time series satellite data. Using GEDI LiDAR data as the ground truth for training the model, we present the first 10 m resolution temporal canopy height map of the European continent for the period 2019–2022. As part of this product, we also offer a detailed canopy height map for 2020, providing more precise estimates than previous studies. Our pipeline and the resulting temporal height map are publicly available, enabling comprehensive large-scale monitoring of forests and, hence, facilitating future research and ecological analyses. For an interactive viewer, see https://europetreemap.projects.earthengine.app/view/europeheight.","Understanding forests is key to tackling climate change, but past tree height maps often lack detail or up-to-date information.This study produced the first high-resolution (10-meter) maps of tree height across Europe that show changes from 2019 to 2022. Using satellite data from Sentinel-1 and Sentinel-2, we analyzed a full year of monthly images with a custom AI model — capturing fine forest details better than ever before.The result: more accurate maps, especially for tall trees that store the most carbon. These time-based maps also help track forest changes like deforestation. Both the method and the maps are freely available to support climate research and forest monitoring."
Poster,CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models,https://ICML.cc//virtual/2025/poster/45903,"Guangzhi Sun, Xiao Zhan, Shutong Feng, Phil Woodland, Jose Such","Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the importance of the context where the query occurs and may cause undesired refusal of queries under safe contexts that diminish user experience. Addressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark that integrates context into safety assessments of LLMs. CASE-Bench assigns distinct, formally described contexts to categorized queries based on Contextual Integrity theory. Additionally, in contrast to previous studies which mainly rely on majority voting from just a few annotators, we recruited a sufficient number of annotators necessary to ensure the detection of statistically significant differences among the experimental conditions based on power analysis. Our extensive analysis using CASE-Bench on various open-source and commercial LLMs reveals a substantial and significant influence of context on human judgments ($p<$0.0001 from a z-test), underscoring the necessity of context in safety evaluations. We also identify notable mismatches between human judgments and LLM responses, particularly in commercial models within safe contexts. Code and data used in the paper are available at https://anonymous.4open.science/r/CASEBench-D5DB.","As powerful AI language models (like ChatGPT) become more common, it's crucial to make sure they act in ways that align with human values and safety. Right now, many safety tests often ignore the context in which a question is asked — which can lead to the model wrongly refusing to answer even when it's safe and appropriate to do so.To fix this, we created a new test called CASE-Bench, which adds context to the safety checks. This means it doesn’t just look at what the question is, but also the situation around it — like who is asking and why. We also used sufficiently large number of human annotators to make sure the results were accurate and meaningful.Our findings showed that context really matters — people judge the safety of a response very differently depending on the situation. We also found that some commercial AI models (like those from big tech companies) don’t always match what people expect, especially when it's actually safe to give an answer.In short, this study shows that AI safety testing needs to take context into account, or else we risk making these tools less helpful — and possibly less trustworthy."
Poster,Catching Two Birds with One Stone: Reward Shaping with Dual Random Networks for Balancing Exploration and Exploitation,https://ICML.cc//virtual/2025/poster/44893,"Haozhe Ma, Fangling Li, Jing Lim, Zhengding Luo, Thanh Vinh Vo, Tze-Yun Leong","Existing reward shaping techniques for sparse-reward reinforcement learning generally fall into two categories: novelty-based exploration bonuses and significance-based hidden state values. The former promotes exploration but can lead to distraction from task objectives, while the latter facilitates stable convergence but often lacks sufficient early exploration. To address these limitations, we propose Dual Random Networks Distillation (DuRND), a novel reward shaping framework that efficiently balances exploration and exploitation in a unified mechanism. DuRND leverages two lightweight random network modules to simultaneously compute two complementary rewards: a novelty reward to encourage directed exploration and a contribution reward to assess progress toward task completion. With low computational overhead, DuRND excels in high-dimensional environments with challenging sparse rewards, such as Atari, VizDoom, and MiniWorld, outperforming several benchmarks.","Training artificial intelligence (AI) agents to make good decisions is especially hard when they receive very little feedback—sometimes only finding out if they did well at the end of a task. To solve this problem, researchers often give the agent extra hints or “bonus points” to encourage learning. Some bonuses push the agent to explore new things, while others help it focus on achieving its goal. However, using only one type can either confuse the agent or slow down progress.In this work, we introduce a new method called Dual Random Network Distillation (DuRND, pronounced “Durian”) that gives the agent the best of both worlds. DuRND uses two simple modules to measure two helpful signals: one that rewards novelty to promote discovery, and another that rewards progress toward the goal. Together, these signals help the agent learn faster and more reliably, even in difficult situations where useful feedback is rare. Our method is efficient and works well in complex video game environments, showing clear improvements over existing approaches."
Poster,Catch Your Emotion: Sharpening Emotion Perception in Multimodal Large Language Models,https://ICML.cc//virtual/2025/poster/45730,"Yiyang Fang, Jian Liang, Wenke Huang, He Li, Kehua Su, Mang Ye","Multimodal large language models (MLLMs) have achieved impressive progress in tasks such as visual question answering and visual understanding, but they still face significant challenges in emotional reasoning. Current methods to enhance emotional understanding typically rely on fine-tuning or manual annotations, which are resource-intensive and limit scalability. In this work, we focus on improving the ability of MLLMs to capture emotions during the inference phase. Specifically, MLLMs encounter two main issues: they struggle to distinguish between semantically similar emotions, leading to misclassification, and they are overwhelmed by redundant or irrelevant visual information, which distracts from key emotional cues. To address these, we propose Sharpening Emotion Perception in MLLMs (SEPM), which incorporates a Confidence-Guided Coarse-to-Fine Inference framework to refine emotion classification by guiding the model through simpler tasks. Additionally, SEPM employs Focus-on-Emotion Visual Augmentation to reduce visual redundancy by directing the attention of models to relevant emotional cues in images. Experimental results demonstrate that SEPM significantly improves MLLM performance on emotion-related tasks, providing a resource-efficient and scalable solution for emotion recognition.","As AI models become more powerful in understanding images and language together, they still struggle to understand human emotions accurately. For example, these models often confuse similar emotions like ""joy"" and ""excitement,"" or get distracted by irrelevant parts of an image.To solve this, we propose a new method called SEPM that helps AI focus better on emotional content—without needing more training or labeled data. First, our method breaks down the emotion detection task into simpler steps, asking the model to classify whether the emotion is positive or negative before identifying the exact feeling. Then, we refine the image the AI sees by removing distracting details and guiding its attention to the parts that matter most for understanding emotion.Our approach makes these AI models better at detecting emotions, faster to use, and easier to apply in real-world situations like mental health monitoring or content moderation."
Poster,CAT: Contrastive Adversarial Training for Evaluating the Robustness of Protective Perturbations in Latent Diffusion Models,https://ICML.cc//virtual/2025/poster/46408,"Sen Peng, Mingyue Wang, Jianfei He, Jijia Yang, Xiaohua Jia","Latent diffusion models have recently demonstrated superior capabilities in many downstream image synthesis tasks. However, customization of latent diffusion models using unauthorized data can severely compromise the privacy and intellectual property rights of data owners.Adversarial examples as protective perturbations have been developed to defend against unauthorized data usage by introducing imperceptible noise to customization samples, preventing diffusion models from effectively learning them.In this paper, we first reveal that the primary reason adversarial examples are effective as protective perturbations in latent diffusion models is the distortion of their latent representations, as demonstrated through qualitative and quantitative experiments.We then propose the Contrastive Adversarial Training (CAT) utilizing lightweight adapters as an adaptive attack against these protection methods, highlighting their lack of robustness. Extensive experiments demonstrate that our CAT method significantly reduces the effectiveness of protective perturbations in customization, urging the community to reconsider and improve the robustness of existing protective perturbations. The code is available at \url{https://github.com/senp98/CAT}.","To prevent generative AI models from customizing on unauthorized personal images, researchers add imperceptible noise to images that weakens the model’s ability to learn from them.However, we show that these “protective perturbations” are not robust enough.Our proposed CAT can still extract meaningful information from the protected images using contrastive adversarial training.This reveals serious weaknesses in current protection strategies and highlights the need for stronger safeguards to truly safeguard data in AI training."
Poster,Categorical Distributional Reinforcement Learning with Kullback-Leibler Divergence: Convergence and Asymptotics,https://ICML.cc//virtual/2025/poster/44550,"Tyler Kastner, Mark Rowland, Yunhao Tang, Murat Erdogdu, Amir-massoud Farahmand","We study the problem of distributional reinforcement learning using categorical parametrisations and a KL divergence loss. Previous work analyzing categorical distributional RL has done so using a Cramér distance-based loss, simplifying the analysis but creating a theory-practice gap. We introduce a preconditioned version of the algorithm, and prove that it is guaranteed to converge. We further derive the asymptotic variance of the categorical estimates under different learning rate regimes, and compare to that of classical reinforcement learning. We finally empirically validate our theoretical results and perform an empirical investigation into the relative strengths of using KL losses, and derive a number of actionable insights for practitioners.","A popular approach to deep reinforcement learning is to use classification losses to learn the range of possible future outcomes. Previous theoretical works studying this algorithm change the loss used in order to simplify the analysis, but this creates a theory-practice gap. In this work, we directly study these learning algorithms with the classification loss used in practice, the KL divergence. We show that with some modifications to the dynamics (the use of a preconditioner matrix), the updates provably converge. We also study the efficiency of these methods compared to standard reinforcement learning, and we prove results on the exact variance of these algorithms as they approach convergence.Throughout our analysis, we obtain a number of insights that are valuable to anyone using these methods in practice, such as how to modify the learning rate used as one changes the number of atoms (a separate hyperparameter), and how the number and locations of these atoms affect the error incurred."
Poster,Categorical Schrödinger Bridge Matching,https://ICML.cc//virtual/2025/poster/45290,"Grigoriy Ksenofontov, Aleksandr Korotin","The Schrödinger Bridge (SB) is a powerful framework for solving generative modeling tasks such as unpaired domain translation. Most SB-related research focuses on continuous data space $\mathbb{R}^{D}$ and leaves open theoretical and algorithmic questions about applying SB methods to discrete data, e.g, on finite spaces $\mathbb{S}^{D}$. Notable examples of such sets $\mathbb{S}$ are codebooks of vector-quantized (VQ) representations of modern autoencoders, tokens in texts, categories of atoms in molecules, etc. In this paper, we provide a theoretical and algorithmic foundation for solving SB in discrete spaces using the recently introduced Iterative Markovian Fitting (IMF) procedure. Specifically, we theoretically justify the convergence of discrete-time IMF (D-IMF) to SB in discrete spaces. This enables us to develop a practical computational algorithm for SB, which we call Categorical Schrödinger Bridge Matching (CSBM). We show the performance of CSBM via a series of experiments with synthetic data and VQ representations of images. The code of CSBM is available at [this repository](https://github.com/gregkseno/csbm).","The task of bridging two domains without paired data is a promising direction in generative modeling. One of the approaches we focus on in this work is the Schrödinger Bridge (SB) problem. The SB problem ""optimally"" couples two data domains by connecting them through a stochastic process, with the notion of optimality varying depending on the choice of reference process. Unfortunately, existing methods for solving the SB problem are primarily designed for continuous data, such as images. This leaves a significant gap in applying SB methods to discrete domains, including text, discrete latent spaces of certain autoencoders, and molecular graphs with discrete nodes and edges. Moreover, the only current discrete SB method, DDSBM, is focused on a continuous-time formulation, which seems to lack some of the desirable properties, such as the theoretical ability to perform generation in a few steps. To address both challenges, we propose Categorical Schrödinger Bridge Matching (CSBM), a method designed to extend the SB framework to discrete space and time settings.The main goal of our work is to prove the existence of the solution in selected settings. We tackle this by leveraging a specific characterization of the SB, which guarantees the uniqueness of the solution. This result allows us to directly apply an established iterative procedure, discrete-time Iterative Markovian Fitting, to find the solution. Moreover, we introduce a couple of reference processes that define optimality under both Euclidean distance and Hamming distance. Through experiments, we demonstrate how different choices of reference processes affect the solution; highlight the theoretically grounded ability of CSBM to operate with a small number of steps; and showcase its applicability to tasks such as high-resolution image translation and text style transfer.The contribution of our work lies not only in addressing the gap in solving the Schrödinger Bridge problem in discrete space and time, but also in introducing key aspects relevant to discrete domains, such as the choice of reference processes (noting that commonly used Brownian motion is unsuitable for discrete spaces) and the theoretically grounded ability to accelerate inference by reducing the number of diffusion steps."
Poster,CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration,https://ICML.cc//virtual/2025/poster/43744,"Haoyun Jiang, Haolin li, jianwei zhang, Fei Huang, Qiang Hu, Minmin Sun, Shuai Xiao, Yong Li, Junyang Lin, Jiangchao Yao","Large language models (LLMs) have demonstrated strong capabilities in handling long-context tasks, but processing such long contexts remains challenging due to the substantial memory requirements and inference latency. In this work, we discover that certain attention heads exhibit sequential consistency in their attention patterns, which can be persistently identified using a coefficient-of-variation-based algorithm. Inspired by this observation, we propose CateKV, a hybrid KV cache method that retains only critical token information for consistent heads, thereby reducing KV cache size and computational overhead, while preserving the majority of KV pairs in adaptive heads to ensure high accuracy. We show the unique characteristics of our algorithm and its extension with existing acceleration methods. Comprehensive evaluations on long-context benchmarks show that, while maintaining accuracy comparable to full attention, CateKV reduces memory usage by up to $2.72\times$ and accelerates decoding by $2.18\times$ in single-sample inputs, and boosts throughput by $3.96\times$ in batch scenarios.","Modern language models excel at understanding long texts but often require significant memory and time to process them. We discovered that certain parts of these models consistently focus on specific information, which inspired us to develop CateKV—a method that retains only the most important information from these parts while preserving more detail where necessary. This approach reduces memory usage and speeds up processing without sacrificing accuracy. Our experiments show that CateKV can reduce memory consumption by nearly three times and double the speed for single-sample inputs, while boosting throughput for batch inputs by almost four times. This makes handling long documents more efficient and practical."
Poster,CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging,https://ICML.cc//virtual/2025/poster/43445,"Wenju Sun, Qingyong Li, Yangliao Geng, Boyang Li","Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified system without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors—defined as the parameter differences between pre-trained and fine-tuned models. However, task vector accumulation is often hindered by knowledge conflicts, where conflicting components across different task vectors can lead to performance degradation during the merging process. To address this challenge, we propose **Conflict-Aware Task Merging (CAT Merging)**, a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 4.7% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.","Modern machine learning systems are often customized for specific tasks, resulting in many separate models that are costly to store and maintain. Merging these task-specific models into a single, unified model would be ideal—but this often leads to interference between tasks, reducing overall performance.To address this, we developed CAT Merging, a new technique that combines multiple models without any additional training. Unlike existing methods that simply average model parameters, CAT Merging identifies and removes the components most likely to cause conflicts between tasks.Our method tailors the merging process to different types of model layers and uses lightweight operations guided by a few example inputs, making it both efficient and easy to apply in practice."
Poster,Catoni Contextual Bandits are Robust to Heavy-tailed Rewards,https://ICML.cc//virtual/2025/poster/46438,"Chenlu Ye, Yujia Jin, Alekh Agarwal, Tong Zhang","Typical contextual bandit algorithms assume that the rewards at each round lie in some fixed range $[0, R]$, and their regret scales polynomially with this reward range $R$. However, many practical scenarios naturally involve heavy-tailed rewards or rewards where the worst-case range can be substantially larger than the variance. In this paper, we develop an algorithmic approach building on Catoni's estimator from robust statistics, and apply it to contextual bandits with general function approximation. When the variance of the reward at each round is known, we use a variance-weighted regression approach and establish a regret bound that depends only on the cumulative reward variance and logarithmically on the reward range $R$ as well as the number of rounds $T$. For the unknown-variance case, we further propose a careful peeling-based algorithm and remove the need for cumbersome variance estimation. With additional dependence on the fourth moment, our algorithm also enjoys a variance-based bound with logarithmic reward-range dependence. Moreover, we demonstrate the optimality of the leading-order term in our regret bound through a matching lower bound.","Many real-world decision-making systems—like those used in online advertising or wireless networks—face unpredictable rewards that can be very large or ""heavy-tailed."" This makes it difficult for standard learning algorithms to make good decisions, since they are designed assuming rewards are relatively well-behaved and bounded.We tackle this problem by designing new algorithms for contextual bandits, a type of learning model that helps an agent choose actions based on observed data. Our algorithms use a statistical tool called the Catoni estimator to achieve robustness even when the observed rewards have a large range and normal variance. We provide two versions: one for when the reward variance is known in advance, and one that works even when it is not.Our methods achieve strong performance variance-dependent guarantees that enjoy logarithmic order on the worst-case range, meaning they are much more accurate and efficient in realistic scenarios. These results push the boundary of robust reinforcement learning and make it more practical for applications involving unreliable or extreme feedback, such as recommendation systems, finance, and networking."
