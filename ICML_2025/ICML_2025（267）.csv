type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Scalable Meta-Learning via Mixed-Mode Differentiation,https://ICML.cc//virtual/2025/poster/45495,"Iurii Kemaev, Dan Andrei Calian, Luisa Zintgraf, Gregory Farquhar, Hado van Hasselt","Gradient-based bilevel optimisation is a powerful technique with applications in hyperparameter optimisation, task adaptation, algorithm discovery, meta-learning more broadly, and beyond. It often requires differentiating through the gradient-based optimisation process itself, leading to ""gradient-of-a-gradient"" calculations with computationally expensive second-order and mixed derivatives. While modern automatic differentiation libraries provide a convenient way to write programs for calculating these derivatives, they oftentimes cannot fully exploit the specific structure of these problems out-of-the-box, leading to suboptimal performance. In this paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to construct more efficient and scalable computational graphs yielding over 10x memory and up to 25\% wall-clock time improvements over standard implementations in modern meta-learning setups.","Gradient-based bilevel optimisation is a powerful technique in Deep Learning with many important applications in meta-learning, hyperparameter tuning, and beyond. It is known to be computationally expensive, as it requires differentiating through the optimisation process itself. We introduce a novel practical algorithm called Mixed-Flow Meta-Gradients, which uses mixed-mode differentiation to drastically reduce the computational cost of this process by saving 90% of memory and 25% of compute, whilst fully preserving the original numerical results."
Poster,Scalable Model Merging with Progressive Layer-wise Distillation,https://ICML.cc//virtual/2025/poster/43576,"Jing Xu, Jiazheng Li, Jingzhao Zhang","Model merging offers an effective way to integrate the capabilities of multiple fine-tuned models. However, the performance degradation of the merged model remains a challenge, particularly when none or few data are available. This paper first highlights the necessity of domain-specific data for model merging by proving that data-agnostic algorithms can have arbitrarily bad worst-case performances. Building on this theoretical insight, we explore the relationship between model merging and distillation, introducing a novel few-shot merging algorithm, ProDistill (Progressive Layer-wise Distillation). Unlike common belief that layer-wise training hurts performance, we show that layer-wise teacher-student distillation not only enhances the scalability but also improves model merging performance. We conduct extensive experiments to show that compared to existing few-shot merging methods,ProDistill achieves state-of-the-art performance, with up to 6.14\% and 6.61\% improvements in vision and NLU tasks. Furthermore, we extend the experiments to models with over 10B parameters, showcasing the exceptional scalability of ProDistill.","Combining the strengths of different AI models is a promising way to build more capable systems, but doing so often leads to worse performance — especially when we don’t have much new data to guide the process.Our research shows that merging models without data can actually have bad performance, and that using at least a small amount of task-specific data is important to get good results. Based on this, we introduce a new method called ProDistill, which carefully combines models by having one model teach another, layer by layer — similar to how a teacher might guide a student through a complex topic step by step.While many researchers thought this approach might hurt performance, we found the opposite: ProDistill not only makes merging more accurate, but it also works well even for very large AI models. In tests across image and language tasks, it consistently outperformed other methods. This makes it a powerful tool for building stronger AI systems with minimal data."
Poster,Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment,https://ICML.cc//virtual/2025/poster/44221,"Yuhui Ding, Thomas Hofmann","Equivariant diffusion models have achieved impressive performance in 3D molecule generation. These models incorporate Euclidean symmetries of 3D molecules by utilizing an SE(3)-equivariant denoising network. However, specialized equivariant architectures limit the scalability and efficiency of diffusion models. In this paper, we propose an approach that relaxes such equivariance constraints. Specifically, our approach learns a sample-dependent SO(3) transformation for each molecule to construct an aligned latent space. A non-equivariant diffusion model is then trained over the aligned representations. Experimental results demonstrate that our approach performs significantly better than previously reported non-equivariant models. It yields sample quality comparable to state-of-the-art equivariant diffusion models and offers improved training and sampling efficiency. Our code is available at: https://github.com/skeletondyh/RADM","In this work, we investigate whether strict symmetry constraints on neural networks are necessary for generative models to deal with 3D molecules. We find that such constraints are not necessary. We propose a method that learns to rotate molecules and arrange their shared structures in similar orientations, enabling generative models to recognize them more easily."
Poster,Scalable Private Partition Selection via Adaptive Weighting,https://ICML.cc//virtual/2025/poster/45510,"Justin Chen, Vincent Cohen-Addad, Alessandro Epasto, Morteza Zadimoghaddam","In the differentially private partition selection problem (a.k.a. set union, key discovery), users hold subsets of items from an unbounded universe. The goal is to output as many items as possible from the union of the users' sets while maintaining user-level differential privacy. Solutions to this problem are a core building block for many privacy-preserving ML applications including vocabulary extraction in a private corpus, computing statistics over categorical data and learning embeddings over user-provided items. We propose an algorithm for this problem, MaxAdaptiveDegree (MAD), which adaptively reroutes weight from items with weight far above the threshold needed for privacy to items with smaller weight, thereby increasing the probability that less frequent items are output.  Our algorithm can be efficiently implemented in massively parallel computation systems allowing scalability to very large datasets. We prove that our algorithm stochastically dominates the standard parallel algorithm for this problem. We also develop a two-round version of our algorithm, MAD2R, where results of the computation in the first round are used to bias the weighting in the second round to maximize the number of items output. In experiments, our algorithms provide the best results among parallel algorithms and scale to datasets with hundreds of billions of items, up to three orders of magnitude larger than those analyzed in prior works.","When dealing with sensitive data, protecting individual privacy is of paramount importance. A fundamental data processing task is to output the set of items in a database, for example, the words that people use in a corpus of messages. However, this vocabulary may leak sensitive information, especially if some words are only ever used by one or a few individuals. There is an inherent tradeoff between privacy and the number of items one can output. We develop new, highly scalable algorithms which improve this tradeoff, allowing for more items to be released while maintaining the same level of privacy. Our algorithms can be run in parallel across many computers, allowing us to process datasets with hundreds of billions of entries."
Poster,Scalable Sobolev IPM for Probability Measures on a Graph,https://ICML.cc//virtual/2025/poster/45057,"Tam Le, Truyen Nguyen, Hideitsu Hino, Kenji Fukumizu","We investigate the Sobolev IPM problem for probability measures supported on a graph metric space. Sobolev IPM is an important instance  of integral probability metrics (IPM), and is obtained by constraining a critic function within a unit ball defined by the Sobolev norm. In particular, it has been used to compare probability measures and is crucial for several theoretical works in machine learning. However, to our knowledge, there are no efficient algorithmic approaches to compute Sobolev IPM effectively, which hinders its practical applications. In this work, we establish a relation between Sobolev norm and weighted $L^p$-norm, and leverage it to propose a *novel regularization* for Sobolev IPM. By exploiting the graph structure, we demonstrate that the regularized Sobolev IPM provides a *closed-form* expression for fast computation. This advancement addresses long-standing computational challenges, and paves the way to apply Sobolev IPM for practical applications, even in large-scale settings. Additionally, the regularized Sobolev IPM is negative definite. Utilizing this property, we design positive-definite kernels upon the regularized Sobolev IPM, and provide preliminary evidences of their advantages for comparing probability measures on a given graph for document classification and topological data analysis.","We study a mathematical method called Sobolev IPM, which helps compare two sets of data points in the form of probabilities. This is useful in machine learning, where we often need to compare data distributions, e.g., to tell whether they are similar or not. The Sobolev IPM problem is hard to calculate. Therefore, it has not been used much in real-world applications. We develop a way to make it much easier and faster to compute, especially when the data points in the sets are connected in a graph. More concretely, we connect Sobolev IPM to a simpler type of math, leading to a new version that gives us a fast answer with the new formula. Moreover, it is then possible to use Sobolev IPM even on large datasets. Furthermore, the new version can be used to build even more powerful tools, called kernels for comparing sets of data points. We then test the proposed approach on tasks like classifying documents and analyzing complex shapes, and obtain promising results."
Poster,Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks,https://ICML.cc//virtual/2025/poster/45865,"Shikai Qiu, Lechao Xiao, Andrew Wilson, Jeffrey Pennington, Atish Agarwala","Understanding neural network training dynamics at scale is an important open problem. Although realistic model architectures, optimizers, and data interact in complex ways that make predictive theory challenging, we show that compute-optimally trained models exhibit remarkably precise collective regularities. Specifically, loss curves from models of varying sizes collapse onto a single universal curve when training compute and loss are normalized to unity at the end of training. With learning rate decay, discrepancies between normalized curves fall below the noise floor of individual models' loss curves across random seeds, yielding an exceptionally tight collapse we term ""supercollapse."" We observe supercollapse across learning rate schedules, datasets, and architectures, including transformers trained on next-token prediction. This collapse breaks down when hyperparameters are scaled suboptimally, providing a practical indicator of proper scaling. We explain these phenomena by connecting collapse to the power-law structure in typical neural scaling laws, and analyzing a simple but effective model of SGD noise dynamics that accurately captures how learning rate schedules deform loss curves away from power laws while preserving universality, and why learning rate decay suppresses variance to enable supercollapse.","We find the loss curves of neural networks follow nearly identical shapes as they scale up in model size and training duration. We find evidence that this surprising phenomenon reveals valuable diagnostic information of neural network training dynamics at scale, and we provide some theoretical explanation of the mechanisms behind it."
Poster,Scaling Inference-Efficient Language Models,https://ICML.cc//virtual/2025/poster/43602,"Song Bian, Minghao Yan, Shivaram Venkataraman","Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to $3.5\times$ difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by $1.8\times$ while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff.Notably, our experiments reveal that wider and shallower models can yield efficiency gains while preserving accuracy.","Computers need large AI models to solve complex tasks, but these models are often slow and expensive to use. We asked: can we build models that are both fast to run and accurate? To answer this, we studied how different design choices—such as the depth or width of a model—affect its efficiency. We trained 63 models of varying sizes and amounts of training data to discover patterns that link size, training, and speed. These insights enable us to build a new model, Morph-1B, which is up to 1.8 times faster at making predictions while still performing well on real-world benchmarks. This shows that smarter model designs can help us get the best of both worlds: accuracy and speed. Our work helps AI developers create more sustainable and accessible systems without sacrificing performance."
Poster,Scaling Large Motion Models with Million-Level Human Motions,https://ICML.cc//virtual/2025/poster/45177,"Ye Wang, Sipeng Zheng, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu","Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named Being-M0, demonstrating robust performance across a wide range of human activities, including unseen ones.Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://beingbeyond.github.io/Being-M0/.","Inspired by advanced large language models, we aim to create large models that can truly understand and generate diverse human motions. However, progress is hindered by a lack of massive, high-quality motion data. To address this, we introduce MotionLib—a dataset with over a million human motions, much larger than existing ones, where each motion has detailed descriptions. We also develop MotionBook, a new method that compactly and accurately represents motions, preserving fine-grained details, to efficiently help models learn these motions. Using these, we train Being-M0, which shows strong performance across many activities, including those it has not seen before. Our work highlights that larger datasets and models are key to improving motion generation, paving the way for developing more versatile large models in fields like gaming and robotics."
Poster,Scaling Laws for Differentially Private Language Models,https://ICML.cc//virtual/2025/poster/46020,"Ryan McKenna, Yangsibo Huang, Amer Sinha, Borja de Balle Pigem, Zachary Charles, Christopher A. Choquette Choo, Badih Ghazi, Georgios Kaissis, Ravi Kumar, Ruibo Liu, Da Yu, Chiyuan Zhang","Scaling laws have emerged as important components of large language model (LLM) training as they can predict performance gains through scale, and provide guidance on important hyper-parameter choices that would otherwise be expensive. LLMs also rely on large, high-quality training datasets, like those sourced from (sometimes sensitive) user data. Training models on this sensitive user data requires careful privacy protections like differential privacy (DP). However, the dynamics of DP training are significantly different, and consequently their scaling laws are not yet fully understood. In this work, we establish scaling laws that accurately model the intricacies of DP LLM training, providing a complete picture of the compute-privacy-utility and the optimal training configurations in many settings.","This paper systematically studies the privacy / utility / compute trade-off for training language models with DP-SGD.  Our experiments and analysis allow training compute-optimal language models by efficiently allocating the compute budget among the batch size, model size, and number of iterations.  The results cover an exhaustive range of reasonable privacy budgets and dataset sizes, both of which significantly influence the optimal compute allocation and achievable loss."
Poster,Scaling Laws for Floating–Point Quantization Training,https://ICML.cc//virtual/2025/poster/46293,"Xingwu Sun, Shuaipeng Li, Ruobing Xie, Weidong Han, Kan Wu, Zhen Yang, Yixing Li, An Wang, SHUAI LI, Jinbao Xue, Yu Cheng, Yangyu Tao, Zhanhui Kang, Cheng-Zhong Xu, Di Wang, Jie Jiang","Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point (FP) quantization, and thus cannot well fit the LLM losses in this scenario. In contrast, while FP quantization training is more commonly implemented in production, it's research has been relatively superficial. In this paper, we thoroughly explore the effects of FP quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in FP quantization training performance of LLM models.In addition to an accurate FP quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal FP quantization precision is directly proportional to the computational power, but within a wide computational power range. We estimate that the best cost-performance precision should lie between 4-8 bits.","Low-precision training (using fewer bits to represent numbers) can reduce the cost of training and running AI models like large language models (LLMs). Most prior work focused on integer-based low-precision methods, but real-world systems often use floating-point (FP) quantization, where numbers are split into “exponent” and “mantissa” parts. This paper explores how different FP settings—like how many bits to assign to the exponent versus mantissa, or how to scale numbers—affect LLM performance.We propose the **Capybara Scaling Law** for float quantized training that could precisely predict the model loss related tothe data size, model size, exponent, mantissa, and block size of scaling factors. Key insights include: (1) Exponent bits slightly matter more than mantissa bits, and the optimal balance between them depends on total bits used—a guide for hardware designers. (2) Training with too much low-precision data can harm performance once a “critical data size” is exceeded. (3) The best precision (4-8 bits) balances cost and performance across most hardware setups.These findings help engineers design efficient systems for training AI models and suggest that pushing for ultra-low precision (e.g., 1-3 bits) might not be worth the tradeoffs in accuracy. Instead, moderate precision (4-8 bits) offers the best value for computational resources."
