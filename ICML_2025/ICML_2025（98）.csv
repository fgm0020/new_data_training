type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,ERICT: Enhancing Robustness by Identifying Concept Tokens in Zero-Shot Vision Language Models,https://ICML.cc//virtual/2025/poster/44869,"Xinpeng Dong, Min Zhang, Didi Zhu, Ye Jian, zhang keli, Aimin Zhou, Fei Wu, Kun Kuang","Pre-trained vision-language models (VLMs) have revolutionized the field of machine learning, demonstrating exceptional performance across a wide range of tasks. However, their robustness remains vulnerable to the spurious-correlation problem. Existing works often involve fine-tuning the model with labeled data or relying on large language models (LLMs) to generate more complex prompts. Although effective to some extent, these methods introduce new challenges, including additional computational costs and dependence on the quality of prompts without fully utilizing the vision modality. To address these limitations, we propose a novel method named ERICT to Enhance model Robustness by Identifying Concept Tokens. ERICT mitigates spurious correlation directly in the inference stage and comprises two key steps: (1) Identify concept tokens capturing invariant features through auxiliary prompts to generate a token-level mask. (2) Apply the mask to the attention weights of the CLS token in the vision encoder to help the model focus on the relevant image region. Extensive experiments show that ERICT significantly improves the overall performance including that of the worst group, and achieves new state-of-the-art results.","Vision language models (VLMs) often build spurious correlations—for example, if a cow is always on grass during pre-training, the model will identify grass as part of the cow.We developed ERICT technology to enable VLMs to automatically ""block out noise information"" when analyzing images. Just like humans selectively listening in a noisy environment, our method has two key steps: (1) helping VLMs identify the truly important features in the image; (2) dynamically adjusting the focus of VLMs' attention as they process the image to prevent them from being distracted by irrelevant details.This method is like installing a smart filter on VLMs, which takes effect immediately at the final decision stage without additional training. Experiments have shown that ERICT significantly improves robustness of VLMs in many scenarios."
Poster,Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems,https://ICML.cc//virtual/2025/poster/45529,"Maksim Zhdanov, Max Welling, Jan-Willem van de Meent","Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.","**Problem**: Scientists use computer simulations to study complex physical systems like weather patterns or molecular interactions. These simulations often involve thousands or millions of data points scattered irregularly in space, making them extremely slow to compute. Current AI methods struggle because they try to analyze every possible connection between data points, which becomes impossibly slow as systems grow larger.**Solution**: We developed Erwin, a new AI system inspired by techniques from physics that efficiently handle large-scale calculations. Instead of analyzing all connections at once, Erwin organizes data points into a hierarchy of ""neighborhoods."" It focuses on nearby interactions within each neighborhood while capturing broader patterns through this hierarchical structure. This approach reduces computation time significantly, making it practical for real-world applications.**Impact**: Erwin successfully handles massive simulations in cosmology, molecular dynamics, and fluid mechanics—achieving better accuracy while running much faster than existing methods. This breakthrough enables scientists to simulate larger, more realistic systems, potentially accelerating discoveries in climate science, drug development, and space research by making previously impossible calculations feasible."
Poster,ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport Plans,https://ICML.cc//virtual/2025/poster/45096,"Ashkan Shahbazi, Elaheh Akbari, Darian Salehi, XINRAN LIU, Navid NaderiAlizadeh, Soheil Kolouri","While self-attention has been instrumental in the success of Transformers, it can lead to over-concentration on a few tokens during training, resulting in suboptimal information flow. Enforcing doubly-stochastic constraints in attention matrices has been shown to improve structure and balance in attention distributions. However, existing methods rely on iterative Sinkhorn normalization, which is computationally costly. In this paper, we introduce a novel, fully parallelizable doubly-stochastic attention mechanism based on sliced optimal transport, leveraging Expected Sliced Transport Plans (ESP). Unlike prior approaches, our method enforces doubly stochasticity without iterative Sinkhorn normalization, significantly enhancing efficiency. To ensure differentiability, we incorporate a temperature-based soft sorting technique, enabling seamless integration into deep learning models. Experiments across multiple benchmark datasets, including image classification, point cloud classification, sentiment analysis, and neural machine translation, demonstrate that our enhanced attention regularization consistently improves performance across diverse applications. Our implementation code can be found at \url{https://github.com/dariansal/ESPFormer}.","We’ve found that modern “attention” in AI models—how they decide which pieces of data to focus on—can become overly concentrated on just a few inputs, causing the models to miss important context. Previous fixes forced a strict balance by running a slow, back-and-forth normalization routine. Our ESPFormer method instead uses a clever mathematical shortcut (expected sliced transport) to balance attention in one fully parallel step, and applies a gentle “soft sorting” trick so it fits seamlessly into regular training. The result is exactly balanced attention maps without the heavy iterative cost. When we tested ESPFormer on image recognition, 3D point-cloud classification, text sentiment analysis, and machine translation, it consistently improved accuracy and ran faster than earlier approaches. Our open-source code makes it easy for anyone to try and build on this more efficient, balanced attention mechanism."
Poster,ETTA: Elucidating the Design Space of Text-to-Audio Models,https://ICML.cc//virtual/2025/poster/44876,"Sang-gil Lee, Zhifeng Kong, ARUSHI GOEL, Sungwon Kim, Rafael Valle, Bryan Catanzaro","Recent years have seen significant progress in Text-To-Audio (TTA) synthesis, enabling users to enrich their creative workflows with synthetic audio generated from natural language prompts. Despite this progress, the effects of data, model architecture, training objective functions, and sampling strategies on target benchmarks are not well understood. With the purpose of providing a holistic understanding of the design space of TTA models, we set up a large-scale empirical experiment focused on diffusion and flow matching models. Our contributions include: 1) AF-Synthetic, a large dataset of high quality synthetic captions obtained from an audio understanding model; 2) a systematic comparison of different architectural, training, and inference design choices for TTA models; 3) an analysis of sampling methods and their Pareto curves with respect to generation quality and inference speed. We leverage the knowledge obtained from this extensive analysis to propose our best model dubbed Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps, ETTA provides improvements over the baselines trained on publicly available data, while being competitive with models trained on proprietary data. Finally, we show ETTA's improved ability to generate creative audio following complex and imaginative captions -- a task that is more challenging than current benchmarks.","Text-to-audio models convert text descriptions into sound. While recent models can produce realistic audio from simple descriptions, they often struggle with complex and imaginative ones — such as generating music made from fictional or abstract sources. Understanding what makes these models work well, and what limits their capabilities, remain an open question.In this work, we conduct a large-scale study to analyze how different factors — including data quality, model architecture, training methods, and sampling strategies — affect the result of text-to-audio models. We introduce AF-Synthetic, a new dataset containing over one million high-quality text–audio pairs, and use it to train our model called ETTA.ETTA demonstrates strong results on standard benchmarks and shows a significantly improved ability to follow complex and creative descriptions, including generating audio that does not have real-world counterparts. This suggests that models can go beyond mimicking the real world to synthesizing entirely novel sounds, powered by model and data scaling."
Poster,Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators,https://ICML.cc//virtual/2025/poster/46046,"Yilun Zhou, Austin Xu, PeiFeng Wang, Caiming Xiong, Shafiq Joty","Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses.","Large language models (LLMs) are commonly used in various NLP tasks. One particular use case is to judge, or evaluate, the responses generated by other LLMs. This capability is often used to establish the relative quality of different LLMs in which they are asked to respond to a common set of queries. Orthogonally, an emerging use of judge models is to improve model responses. For example, for a single query (e.g., a math problem), the stochastic nature of an LLM allows it to generate multiple responses, with some correct and others incorrect. If LLM-judges can identify the quality of each response, then they should be able to select a correct one. In this paper, we systematically evaluate the helpfulness of LLM-judges in these settings, also known as test-time scaling. We propose the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge helpfulness in three different test-time scaling strategies across eight datasets in three domains (math reasoning, code generation and instruction following). We use JETTS to comprehensively assess the strengths and weaknesses of judge models, which both gives recommendations for machine learning practitioners and informs directions of future research."
Poster,Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving,https://ICML.cc//virtual/2025/poster/43822,"Yuxuan Zhou, Xien Liu, Chenwei Yan, Chen Ning, Xiao Zhang, Boxun Li, Xiangling Fu, Shijin Wang, Guoping Hu, Yu Wang, Ji Wu","Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.","Large language models (LLMs), like ChatGPT, have shown impressive results on medical tests, but it’s still unclear how well they understand and reason through medical problems of different complexity. In this study, we designed a new way to evaluate LLMs’ medical abilities by testing them at three levels of thinking: basic knowledge grasp, complex knowledge application, and scenario-based problem-solving. We used this method to assess a wide range of popular LLMs, including those developed by Meta, OpenAI, and others. We found that while most models handle simple questions well, they struggle more as the problems become more complex. Larger models tend to perform better on these harder tasks. Our results suggest that future improvements should focus on helping LLMs think more like doctors when faced with complex clinical situations."
Poster,Evaluating Neuron Explanations: A Unified Framework with Sanity Checks,https://ICML.cc//virtual/2025/poster/46256,"Tuomas Oikarinen, Ge Yan, Lily Weng","Understanding the function of individual units in a neural network is an important building block for mechanistic interpretability. This is often done by generating a simple text explanation of the behavior of individual neurons or units. For these explanations to be useful, we must understand how reliable and truthful they are. In this work we unify many existing explanation evaluation methods under one mathematical framework. This allows us to compare and contrast existing evaluation metrics, understand the evaluation pipeline with increased clarity and apply existing statistical concepts on the evaluation. In addition, we propose two simple sanity checks on the evaluation metrics and show that many commonly used metrics fail these tests and do not change their score after massive changes to the concept labels. Based on our experimental and theoretical results, we propose guidelines that future evaluations should follow and identify a set of reliable evaluationmetrics.","Current neural networks can achieve impressive results, but we don't really understand what happens inside them. Understanding the function of individual units in a neural network can help bring light to this and bring us towards safer and more trustworthy models. Individual neurons are often explained by generating a simple text explanation of the behavior of individual neurons or units. For these explanations to be useful, we must understand how reliable and truthful they are. In this work we unify many existing explanation evaluation methods under one mathematical framework. This allows us to compare and contrast existing evaluation metrics, understand the evaluation pipeline with increased clarity and apply existing statistical concepts on the evaluation. In addition, we propose two simple sanity checks on the evaluation metrics and show that many commonly used metrics fail these tests and do not change their score after massive changes to the concept labels. Based on our results we propose new guidelines on how we should evaluate neuron explanations."
Poster,Event-Customized Image Generation,https://ICML.cc//virtual/2025/poster/46312,"Zhen Wang, Yilei JIANG, Dong Zheng, Jun Xiao, Long Chen","Customized Image Generation, generating customized images with user-specified concepts, has raised significant attention due to its creativity and novelty. With impressive progress achieved in subject customization, some pioneer works further explored the customization of action and interaction beyond entity (i.e., human, animal, and object) appearance. However, these approaches only focus on basic actions and interactions between two entities, and their effects are limited by insufficient ''exactly same'' reference images. To extend customized image generation to more complex scenes for general real-world applications, we propose a new task: event-customized image generation. Given a single reference image, we define the ''event'' as all specific actions, poses, relations, or interactions between different entities in the scene. This task aims at accurately capturing the complex event and generating customized images with various target entities. To solve this task, we proposed a novel training-free event customization method: FreeEvent. Specifically, FreeEvent introduces two extra paths alongside the general diffusion denoising process: 1) Entity switching path: it applies cross-attention guidance and regulation for target entity generation. 2) Event transferring path: it injects the spatial feature and self-attention maps from the reference image to the target image for event generation. To further facilitate this new task, we collected two evaluation benchmarks: SWiG-Event and Real-Event. Extensive experiments and ablations have demonstrated the effectiveness of FreeEvent.","Imagine being able to change the people or objects in a photo, but still keep everything else — like their poses, actions, and how they interact — exactly the same. For example, replacing a person in a dance photo with someone else, while keeping the same dance move and background. This idea is called event-customized image generation, and it goes beyond just changing how someone looks — it focuses on keeping the “event” in the picture the same.In our research, we introduce a new way to do this. Instead of needing many matching photos for reference, our method only needs one image to learn the key details of an event, such as who is doing what, how they are posed, and how they relate to others in the scene. Then, it can generate new images where the same event happens, but with different people or objects.We call our method FreeEvent, and it doesn’t require extra training, which makes it more flexible and easier to use. It works in two main steps: one focuses on replacing people or objects while keeping their role in the scene, and the other copies the event details — like actions and positions — from the original photo to the new one.We also created two new test sets to measure how well our method works. The results show that FreeEvent can generate realistic and detailed images that faithfully keep the original event, even when using completely new characters."
Poster,Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition,https://ICML.cc//virtual/2025/poster/46414,"Zheyang Xiong, Jack Cai, John Cooper, Albert Ge, Vasilis Papageorgiou, Zack Sifakis, Angeliki Giannou, Ziqian Lin, Liu Yang, Saurabh Agarwal, Grigorios Chrysos, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos","Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term task superposition"". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of ""LLMs as superposition of simulators"", and raise questions about the mechanisms enabling simultaneous task execution.","Large Language Models (LLMs) are typically trained to perform one task at a time. However, our research uncovers a surprising phenomenon: LLMs can perform multiple, computationally distinct ICL tasks simultaneously when given a single prompt -- a phenomenon we term ""task superposition.""For instance, when given examples of both arithmetic problems and language translations in the same input, an LLM can correctly solve math equations and translate text at the same time. We tested this across various LLMs, including GPT-3.5 and Llama-3, and found that larger models are better at performing multiple tasks concurrently.Our findings suggest that LLMs internally combine representations of different tasks, enabling them to perform several functions at once. We hope that our findings will contribute to understanding in-context learning mechanisms and enhance our knowledge of LLMs overall."
Poster,EvFocus: Learning to Reconstruct Sharp Images from Out-of-Focus Event Streams,https://ICML.cc//virtual/2025/poster/44694,"Lin Zhu, Xiantao Ma, Xiao Wang, Lizhi Wang, Hua Huang","Event cameras are innovative sensors that capture brightness changes as asynchronous events rather than traditional intensity frames. These cameras offer substantial advantages over conventional cameras, including high temporal resolution, high dynamic range, and the elimination of motion blur. However, defocus blur, a common image quality degradation resulting from out-of-focus lenses, complicates the challenge of event-based imaging. Due to the unique imaging mechanism of event cameras, existing focusing algorithms struggle to operate efficiently on sparse event data. In this work, we propose EvFocus, a novel architecture designed to reconstruct sharp images from defocus event streams for the first time. Our work includes the development of an event-based out-of-focus camera model and a simulator to generate realistic defocus event streams for robust training and testing. EvDefous integrates a temporal information encoder, a blur-aware two-branch decoder, and a reconstruction and re-defocus module to effectively learn and correct defocus blur. Extensive experiments on both simulated and real-world datasets demonstrate that EvFocus outperforms existing methods across varying lighting conditions and blur sizes, proving its robustness and practical applicability in event-based defocus imaging.","Imagine trying to take a clear photo while running or in dim light—traditional cameras often struggle, producing blurry or dark images. Event cameras work differently: instead of recording full images at regular intervals, they detect tiny changes in light as they happen, making them ideal for fast-moving or low-light environments. They’re like having super-fast eyes that never blink. But even these advanced cameras have a blind spot: when the lens is out of focus, the data they collect becomes less useful, like trying to read a newspaper with foggy glasses.To solve this, we created EvFocus, a machine learning model that can take this fuzzy data and reconstruct sharp, clear images. It’s trained using a simulator we built to mimic how real-world blur affects event data. Our model uses both timing and blur cues—like a detective piecing together a scene from partial clues—to recover the original image.This technology could make event cameras more reliable in real-world tasks like autonomous driving, drone navigation, and security monitoring, where cameras can’t always be perfectly focused. EvFocus helps ensure these systems still ''see clearly'' when it matters most."
