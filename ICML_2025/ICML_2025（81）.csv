type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Does Generation Require Memorization? Creative Diffusion Models using Ambient Diffusion,https://ICML.cc//virtual/2025/poster/45841,"Kulin Shah, Alkis Kalavasis, Adam Klivans, Giannis Daras","There is strong empirical evidence that the stateof-the-art diffusion modeling paradigm leads to models that memorize the training set, especially when the training set is small. Prior methods to mitigate the memorization problem often lead to decrease in image quality. Is it possible to obtain strong and creative generative models, i.e., models that achieve high generation quality and low memorization? Despite the current pessimistic landscape of results, we make significant progress in pushing the trade-off between fidelity and memorization. We first provide theoretical evidence that memorization in diffusion models is only necessary for denoising problems at low noise scales (usually used in generating high-frequency details). Using this theoretical insight, we propose a simple, principled method to train the diffusion models using noisy data at large noise scales. We show that our method significantly reduces memorization without decreasing the image quality, for both text-conditional and unconditional models and for a variety of data availability settings.","Diffusion models have emerged as a prominent method for generating novel images. However,  these models can sometimes memorize their training data, leading to outputs that are mere variations of training images rather than genuinely generating new images by learning the underlying structure of natural images. This raises crucial privacy and ethical questions.Prior attempts to mitigate this memorization often resulted in a decline in the quality of the generated images. In this work, we introduce a simple approach based on learning diffusion models with noisy data that not only reduces model memorization but also concurrently improves the quality of the images generated."
Poster,Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis,https://ICML.cc//virtual/2025/poster/45209,"Qunzhong WANG, Xiangguo Sun, Hong Cheng","In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional ""pre-training and fine-tuning"" to ""pre-training and prompting,"" has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a ""dark cloud"" over the graph prompting area for deeper research. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: **First**, we provide a formal guarantee theorem, demonstrating graph prompts’ capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. **Second**, we derive upper bounds on the error of these data operations for a single graph and extend this discussion to batches of graphs, which are common in graph model training. **Third**, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees.","In recent years, ""Graph Prompting"" has gained attention as a way to adapt machine learning models to new tasks by making simple changes to graph data, without altering the model itself. However, its effectiveness has not been clearly understood or supported by theory. In our paper, we explore graph prompting from a ""data operation"" perspective and provide a theoretical explanation of why it works. We also validate our findings through extensive experiments, showing that graph prompting can be a powerful tool for real-world applications."
Poster,Does learning the right latent variables necessarily improve in-context learning?,https://ICML.cc//virtual/2025/poster/43755,"Sarthak Mittal, Eric Elmoznino, Léo Gagnon, Sangnie Bhardwaj, Guillaume Lajoie, Dhanya Sridhar","Large autoregressive models like Transformers can solve tasks through in-context learning (ICL) without learning new weights, suggesting avenues for efficiently solving new tasks. For many tasks, e.g., linear regression, the data factorizes: examples are independent given a task latent that generates the data, e.g., linear coefficients. While an optimal predictor leverages this factorization by inferring task latents, it is unclear if Transformers implicitly do so or instead exploit heuristics and statistical shortcuts through attention layers. In this paper, we systematically investigate the effect of explicitly inferring task latents by minimally modifying the Transformer architecture with a bottleneck to prevent shortcuts and incentivize structured solutions. We compare it against standard Transformers across various ICL tasks and find that contrary to intuition and recent works, there is little discernible difference between the two; biasing towards task-relevant latent variables does not lead to better out-of-distribution performance, in general. Curiously, we find that while the bottleneck effectively learns to extract latent task variables from context, downstream processing struggles to utilize them for robust prediction. Our study highlights the intrinsic limitations of Transformers in achieving structured ICL solutions that generalize, and shows that while inferring the right latents aids interpretability, it is not sufficient to alleviate this problem.","Large-scale machine learning models can learn to solve new problems just by looking at a few examples—this is called in-context learning. In this work, we explore whether it’s better to first clearly figure out the hidden rules behind a task and then solve it, or if it’s just as effective to let the model pick up those rules on its own, without making them explicit."
Poster,Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?,https://ICML.cc//virtual/2025/poster/44219,"Zi Liang, Haibo Hu, Qingqing Ye, Yaxin XIAO, RongHua Li","Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA’s training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.","As a widely used parameter-efficient fine-tuning (PEFT) method and almost a standard in large model training, LoRA (Low Rank Apdation)'s features and variants have been extensively discussed. It has even been adapted for use in almost all fields in LLM's training. However, one question remains unexplored: Is LoRA inherently less secure than full fine-tuning when it comes to training-time attacks?In this work, we formally model two representative attacks (backdoor and untargeted poisoning attack) and use NTK (Neural Tangent Kernel) and information geometry theory to quantitatively analyze the security of LoRA versus full fine-tuning. Our findings reveal that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning. Besides, it also demonstrates how two key factors in LoRA, initialization variance of matrix A and rank selection, affect its robustness.Beyond security, our analytical framework also provides a more intuitive and concise explanation for some of LoRA’s intriguing properties, such as: Why the two sub-matrices in LoRA are asymmetric? Why LoRA requires higher learning rates? Why LoRA’s initialization strategy matters? And other phenomena observed in prior research.If you’re interested in LoRA’s security analysis, theoretical foundations, or model architecture theory, please read this paper."
Poster,Does One-shot Give the Best Shot? Mitigating Model Inconsistency in One-shot Federated Learning,https://ICML.cc//virtual/2025/poster/46581,"Hui Zeng, Wenke Huang, Tongqing Zhou, Xinyi Wu, Guancheng Wan, Yingwen Chen, CAI ZHIPING","Turning the multi-round vanilla Federated Learning into one-shot FL (OFL) significantly reduces the communication burden and makes a big leap toward practical deployment. However, this work empirically and theoretically unravels that existing OFL falls into a garbage (inconsistent one-shot local models) in and garbage (degraded global model) out pitfall. The inconsistency manifests as divergent feature representations and sample predictions. This work presents a novel OFL framework FAFI that enhances the one-shot training on the client side to essentially overcome inferior local uploading. Specifically, unsupervised feature alignment and category-wise prototype learning are adopted for clients' local training to be consistent in representing local samples. On this basis, FAFI uses informativeness-aware feature fusion and prototype aggregation for global inference. Extensive experiments on three datasets demonstrate the effectiveness of FAFI, which facilitates superior performance compared with 11 OFL baselines (+10.86% accuracy). Code available at https://github.com/zenghui9977/FAFI_ICML25","One-shot federated learning (OFL) cuts communication costs by training models in just one round, but local models often present large inconsistencies due to heterogeneous data, harming global performance. This work proposes FAFI, a method that aligns local training to reduce inconsistencies and informative feature fusion during aggregation. Evaluations on three datasets show FAFI boosts accuracy by 10.86% over existing approaches, offering a practical solution for Federated Learning."
Poster,DOLPHIN: A Programmable Framework for Scalable Neurosymbolic Learning,https://ICML.cc//virtual/2025/poster/46682,"Aaditya Naik, Jason Liu, Claire Wang, Amish Sethi, Saikat Dutta, Mayur Naik, Eric Wong","Neurosymbolic learning enables the integration of symbolic reasoning with deep learning but faces significant challenges in scaling to complex symbolic programs, large datasets, or both. We introduce DOLPHIN, a framework that tackles these challenges by supporting neurosymbolic programs in Python, executing complex symbolic reasoning on the CPU while vectorizing probabilistic computations and gradient propagation on the GPU. Across 13 benchmarks spanning tasks over text, image, and video data, with symbolic reasoning features like recursion and blackbox functions, DOLPHIN converges to state-of-the-art accuracies on the more complex benchmarks while existing frameworks such as Scallop, ISED, and IndeCateR+ fail to converge within the time limit. On simpler benchmarks, DOLPHIN matches their performance, while achieving these results 1.71x to 62x faster than the baselines. Overall, DOLPHIN advances the scalability of neurosymbolic frameworks, achieving state-of-the-art efficiency and convergence on difficult benchmarks where existing frameworks struggle. The code is published at https://github.com/Dolphin-NeSy/Dolphin.","Combining deep learning with logical reasoning, also known as neurosymbolic learning, faces significant challenges, with most approaches either crashing when facing complex problems or taking forever to train. Our new framework, DOLPHIN, supports writing neurosymbolic programs in Python, and splits the work smartly: it handles logical reasoning on CPUs while running the probabilistic computations on GPUs. This division lets DOLPHIN scale to complex problems, running up to 62 times faster than the competition. DOLPHIN provides the ability to write and train complex neurosymbolic programs where existing frameworks struggle."
Poster,Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training,https://ICML.cc//virtual/2025/poster/44261,"Mozhi Zhang, Howe Tissue, Lu Wang, Xipeng Qiu","We introduce *Domain2Vec*, a novel approach that decomposes any dataset into a linear combination of several *meta-domains*, a new concept designed to capture the key underlying features of datasets.*Domain2Vec* maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary.These domain vectors enable the identification of optimal data mixture for language model (LM) pretraining in a training-free manner under the ***D**istribution **A**lignment **A**ssumption* (DA$^{2}$), which suggests that when the data distribution of the training set and the validation set is more aligned, a lower validation loss is achieved.Moreover, *Domain2Vec* can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods.Extensive experiments demonstrate that *Domain2Vec* helps find the data mixture that enhances downstream task performance with minimal computational overhead.Specifically, *Domain2Vec* achieves the same validation loss on Pile-CC using only $51.5$\% of the compute required when training on the original mixture of The Pile Dataset.Under equivalent compute budget, *Domain2Vec* improves downstream performance by an average of $2.83$\%.","Finding the optimal data mixture for LLM pretraining is crucial yet challenging due to the high computational costs involved.We propose *Domain2Vec*, a novel approach that employs domain vectors to represent each dataset in LLM pretraining. With *Domain2Vec*, the mixture of pretraining datasets can be elegantly transformed into a linear combination of the predefined meta-domains. Under our proposed ***D**istribution **A**lignment **A**ssumption* (DA$^{2}$), these computed domain vectors enable us to identify optimal data mixtures without training. Furthermore, we demonstrate how *Domain2Vec* can be seamlessly integrated into existing data mixture optimization frameworks to enhance both efficiency and scalability.Our experimental results show that our method achieves comparable performance while significantly reducing computational overhead compared to existing approaches. We believe this work offers valuable insights into optimizing data mixtures for LLM pretraining and paves the way for more efficient training strategies."
Poster,Domain-Adapted Diffusion Model for PROTAC Linker Design Through the Lens of Density Ratio in Chemical Space,https://ICML.cc//virtual/2025/poster/44288,"Zixing Song, Ziqiao Meng, Jose Miguel Hernandez-Lobato","Proteolysis-targeting chimeras (PROTACs) are a groundbreaking technology for targeted protein degradation, but designing effective linkers that connect two molecular fragments to form a drug-candidate PROTAC molecule remains a key challenge. While diffusion models show promise in molecular generation, current diffusion models for PROTAC linker design are typically trained on small molecule datasets, introducing distribution mismatches in the chemical space between small molecules and target PROTACs. Direct fine-tuning on limited PROTAC datasets often results in overfitting and poor generalization. In this work, we propose DAD-PROTAC, a domain-adapted diffusion model for PROTAC linker design, which addresses this distribution mismatch in chemical space through density ratio estimation to bridge the gap between small-molecule and PROTAC domains. By decomposing the target score estimator into a pre-trained score function and a lightweight score correction term, DAD-PROTAC achieves efficient fine-tuning without full retraining. Experimental results demonstrate its superior ability to generate high-quality PROTAC linkers.","PROTACs are a promising new type of drug that work by bringing a disease-causing protein together with the cell’s natural ""recycling"" system. This causes the unwanted protein to be destroyed. To make a PROTAC, chemists must connect two functional parts with a chemical chain, but finding the right chain (linker) is very challenging. Existing AI tools that generate molecules (called diffusion models) are usually trained on small molecules, not on complex PROTAC molecules. As a result, these models often struggle to design effective linkers for PROTACs when only limited examples are available. We introduce a new approach that bridges this gap. Instead of retraining a giant model from scratch, we adjust a pre-trained model so it learns what makes PROTAC-like molecules different."
Poster,Do Multiple Instance Learning Models Transfer?,https://ICML.cc//virtual/2025/poster/44403,"Daniel Shao, Richard Chen, Andrew Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood","Multiple Instance Learning (MIL) is a cornerstone approach in computational pathology for distilling embeddings from gigapixel tissue images into patient-level representations to predict clinical outcomes. However, MIL is frequently challenged by the constraints of working with small, weakly-supervised clinical datasets. Unlike fields such as natural language processing and computer vision, which effectively use transfer learning to improve model quality in data-scarce environments, the transferability of MIL models remains largely unexplored. We conduct the first comprehensive investigation into transfer learning capabilities of pretrained MIL models, evaluating 11 MIL models across 19 pretraining tasks spanning tissue subtyping, cancer grading, and molecular subtype prediction. We observe a substantial performance boost with finetuning pretrained models over training from randomly initialized weights, even with domain differences between pretraining and target tasks. Pretraining on pan-cancer datasets enables consistent generalization across organs and task types compared to single-disease pretraining. Remarkably, this pan-cancer pretraining leads to better transfer than that of a state-of-the-art slide-level foundation model, while using only 6.5\% of the training data. These findings indicate that MIL architectures exhibit robust adaptability, offering insights into the benefits of leveraging pretrained models to enhance performance in computational pathology.","Analyzing gigapixel images of human tissue is crucial for disease diagnosis. Multiple Instance learning (MIL) provides a means of consolidating the key insights from these massive images into a condensed embedding, but often exhibits poor generalizability due to insufficient training data. While transfer learning is a common remedy for data scarcity in other ML fields, MIL models are typically trained from scratch in our field, despite the challenges of data scarcity. Here, we investigate the feasability of whether transfer learning of MIL models can be leveraged to improve model generalization. We thoroughly investigate whether MIL models trained on one task can be transferred to new tasks, training 11 different MIL models on 21 total pretraining tasks spanning breast, brain, lung, and prostate cancer, as well as a challenging 108-class classification task across 19 cancer types.This allowed us to gain insights into how different attributes of the MIL model, pretrain task, and target task, affect transfer performance. Our research shows that pretrained MIL models perform much better than models trained from scratch, even if their initial training was on a different organ or disease. Importantly, models trained on the challenging pancancer data were able to generalize best across different organs and tasks, while using substantially less data than other self-supervised learning-based foundation models. These findings highlight MIL models as highly adaptable, and supervised pretraining as an effective and accessible means for addressing the challenges of learning clinically-meaningful representations from challenging tasks and data scarce regimes. We also provide a GitHub library to standardize MIL model initialization and loading model weights, available at https://github.com/mahmoodlab/MIL-Lab"
Poster,Do Not Mimic My Voice : Speaker Identity Unlearning for Zero-Shot Text-to-Speech,https://ICML.cc//virtual/2025/poster/46647,"Taesoo Kim, Jinju Kim, Dongchan Kim, Jong Hwan Ko, Gyeong-Moon Park","The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has enabled high-fidelity voice synthesis from minimal audio cues, raising significant privacy and ethical concerns. Despite the threats to voice privacy, research to selectively remove the knowledge to replicate unwanted individual voices from pre-trained model parameters has not been explored. In this paper, we address the new challenge of speaker identity unlearning for ZS-TTS systems. To meet this goal, we propose the first machine unlearning frameworks for ZS-TTS, especially Teacher-Guided Unlearning (TGU), designed to ensure the model forgets designated speaker identities while retaining its ability to generate accurate speech for other speakers. Our proposed methods incorporate randomness to prevent consistent replication of forget speakers' voices, assuring unlearned identities remain untraceable. Additionally, we propose a new evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF). This assesses the model's ability to disregard prompts associated with forgotten speakers, effectively neutralizing its knowledge of these voices. The experiments conducted on the state-of-the-art model demonstrate that TGU prevents the model from replicating forget speakers' voices while maintaining high quality for other speakers.The demo is available at https://speechunlearn.github.io/ .","Zero-Shot Text-to-Speech (ZS-TTS) models can now copy anyone’s voice from a 3-second clip, posing serious privacy and fraud risks.Once trained, today’s ZS-TTS systems cannot selectively prevent generating unwanted voice.We present the first “Speaker-Identity Unlearning” framework that lets developers erase unwanted voices while unharming all other abilities.Our Teacher-Guided Unlearning modifies the model so any request to clone a protected speaker is rendered in a random, untraceable voice.A new score, speaker-Zero Retrain Forgetting, confirms the attempt to clone an unwanted voice results in random voice for protected  (or ""forgotten"") speakers but remains identical for the other (""remain"") speakers.This method drives speaker similarity of forgotten voices down to the level of unrelated speakers while keeping word accuracy and naturalness unchanged. The performance is consistent when scaling to unlearn speakers of various sizes or unseen speakers the model has never been trained on.This means users can demand “do not mimic my voice” for voice privacy."
