type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Constrained Belief Updates Explain Geometric Structures in Transformer Representations,https://ICML.cc//virtual/2025/poster/44548,"Mateusz Piotrowski, Paul Riechers, Daniel Filan, Adam Shai","What computational structures emerge in transformers trained on next-token prediction? In this work, we provide evidence that transformers implement constrained Bayesian belief updating---a parallelized version of partial Bayesian inference shaped by architectural constraints. We integrate the model-agnostic theory of optimal prediction with mechanistic interpretability to analyze transformers trained on a tractable family of hidden Markov models that generate rich geometric patterns in neural activations. Our primary analysis focuses on single-layer transformers, revealing how the first attention layer implements these constrained updates, with extensions to multi-layer architectures demonstrating how subsequent layers refine these representations. We find that attention carries out an algorithm with a natural interpretation in the probability simplex, and create representations with distinctive geometric structure. We show how both the algorithmic behavior and the underlying geometry of these representations can be theoretically predicted in detail---including the attention pattern, OV-vectors, and embedding vectors---by modifying the equations for optimal future token predictions to account for the architectural constraints of attention. Our approach provides a principled lens on how architectural constraints shape the implementation of optimal prediction, revealing why transformers develop specific intermediate geometric structures.","How do AI systems like ChatGPT actually work inside? While these ""transformer"" models excel at predicting text, understanding their internal mechanisms has been like trying to reverse-engineer a black box. We faced a fundamental puzzle: optimal text prediction requires recurrent, step-by-step processing, but transformers process everything simultaneously in parallel. We studied transformers trained on simple, controlled data where we could mathematically predict what the optimal solution should look like. We discovered that transformers develop a clever workaround for this parallel-versus-sequential tension. They create intermediate internal representations that approximate optimal recurrent processing using parallel computation. Remarkably, we can predict exactly what these internal structures should look like using mathematical theory. This breakthrough provides a principled way to understand why AI systems learn specific internal representations. Rather than just observing what transformers do after the fact, we can now predict and explain their internal mechanisms based on the fundamental trade-off between optimal computation and architectural constraints. This theoretical framework could help us better understand, interpret, and potentially control large language models by revealing the mathematical principles governing how they balance computational efficiency with optimal prediction."
Poster,Constrained Exploitability Descent: An Offline Reinforcement Learning Method for Finding Mixed-Strategy Nash Equilibrium,https://ICML.cc//virtual/2025/poster/43717,"Runyu Lu, Yuanheng Zhu, Dongbin Zhao","This paper proposes Constrained Exploitability Descent (CED), a model-free offline reinforcement learning (RL) algorithm for solving adversarial Markov games (MGs). CED combines the game-theoretical approach of Exploitability Descent (ED) with policy constraint methods from offline RL. While policy constraints can perturb the optimal pure-strategy solutions in single-agent scenarios, we find the side effect less detrimental in adversarial games, where the optimal policy can be a mixed-strategy Nash equilibrium. We theoretically prove that, under the uniform coverage assumption on the dataset, CED converges to a stationary point in deterministic two-player zero-sum Markov games. We further prove that the min-player policy at the stationary point follows the property of mixed-strategy Nash equilibrium in MGs. Compared to the model-based ED method that optimizes the max-player policy, our CED method no longer relies on a generalized gradient. Experiments in matrix games, a tree-form game, and an infinite-horizon soccer game verify that CED can find an equilibrium policy for the min-player as long as the offline dataset guarantees uniform coverage. Besides, CED achieves a significantly lower NashConv compared to an existing pessimism-based method and can gradually improve the behavior policy even under non-uniform data coverages. When combined with neural networks, CED also outperforms behavior cloning and offline self-play in a large-scale two-team robotic combat game.","We usually solve adversarial games through iterative policy updates under the game model or online sampling. However, an exact game model or a large amount of online data can be expensive to obtain in the real world, especially when it comes to serious games. Therefore, we propose a novel algorithm named Constrained Exploitability Descent (CED) to solve adversarial games offline. By using techniques from offline reinforcement learning and game theory, CED can find the best strategy in adversarial games using only a finite number of game examples. We provide both theoretical evidence and simulation results to demonstrate that CED works well and has excellent properties compared to other descent-based methods. According to our theory, the final results of CED get close to the mixed-strategy Nash equilibrium as long as there is enough game data. We further verify this assertion in matrix games, a tree-form game, and a soccer game. Even when the data coverage is theoretically insufficient, CED still gradually improves over the underlying policy of these data. When equipped with neural networks, CED is also applicable to large-scale games and clearly outperforms the existing offline methods in a two-team robotic combat game."
Poster,Constrained Online Convex Optimization with Polyak Feasibility Steps,https://ICML.cc//virtual/2025/poster/45968,"Spencer Hutchinson, Mahnoosh Alizadeh","In this work, we study online convex optimization with a fixed constraint function $g : \mathbb{R}^d \rightarrow \mathbb{R}$. Prior work on this problem has shown $O(\sqrt{T})$ regret and cumulative constraint satisfaction $\sum_{t=1}^{T} g(x_t) \leq 0$, while only accessing the constraint value and subgradient at the played actions $g(x_t), \partial g(x_t)$. Using the same constraint information, we show a stronger guarantee of anytime constraint satisfaction $g(x_t) \leq 0 \ \forall t \in [T]$, and matching $O(\sqrt{T})$ regret guarantees. These contributions are thanks to our approach of using Polyak feasibility steps to ensure constraint satisfaction, without sacrificing regret. Specifically, after each step of online gradient descent, our algorithm applies a subgradient descent step on the constraint function where the step-size is chosen according to the celebrated Polyak step-size. We further validate this approach with numerical experiments.","When machine-learning algorithms are deployed in the real world, they must respect *system constraints* that specify what they may and may not do. For instance, a control algorithm in an autonomous vehicle must never steer the car into an obstacle. We study learning under such constraints through the highly general framework of *online convex optimization* (OCO), which covers a broad range of adaptive learning tasks.Our key contribution is an OCO algorithm that always stays within these safety limits while relying only on local information and limited computation. It does so by incorporating *Polyak feasibility steps* into every update—small, principled adjustments that pull each decision back toward the safe region whenever it drifts too close to the boundary. These adjustments let the algorithm learn almost as efficiently as in an unconstrained setting, making it relevant to a wide range of safety-critical applications."
Poster,Constrained Pareto Set Identification with Bandit Feedback,https://ICML.cc//virtual/2025/poster/46545,"Cyrille Kone, Emilie Kaufmann, Laura Richert","In this paper, we address the problem of identifying the Pareto Set under feasibility constraints in a multivariate bandit setting. Specifically, given a $K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the goal is to identify the set of arms whose mean is not uniformly worse than that of another arm (i.e., not smaller for all objectives), while satisfying some known set of linear constraints, expressing, for example, some minimal performance on each objective. Our focus lies in fixed-confidence identification, for which we introduce an algorithm that significantly outperforms racing-like algorithms and the intuitive two-stage approach that first identifies feasible arms and then their Pareto Set. We further prove an information-theoretic lower bound on the sample complexity of any algorithm for constrained Pareto Set identification, showing that the sample complexity of our approach is near-optimal. Our theoretical results are supported by an extensive empirical evaluation on a series of benchmarks.","We develop a method to identify the best trade-offs among multiple competing objectives (the Pareto Set) while ensuring certain performance constraints are met. Our algorithm is both theoretically efficient and practically effective, outperforming existing approaches in simulations. This has important applications in areas like clinical trials or recommendation systems, where it's crucial to balance multiple goals while respecting safety or fairness constraints."
Poster,ConText: Driving In-context Learning for Text Removal and Segmentation,https://ICML.cc//virtual/2025/poster/45093,"Fei Zhang, Pei Zhang, Baosong Yang, Fei Huang, Yanfeng Wang, Ya Zhang","This paper presents the first study on adapting the visual in-context learning (V-ICL) paradigm to optical character recognition tasks, specifically focusing on text removal and segmentation. Most existing V-ICL generalists employ a reasoning-as-reconstruction approach: they turn to using a straightforward image-label compositor as the prompt and query input, and then masking the query label to generate the desired output. This direct prompt confines the model to a challenging single-step reasoning process. To address this, we propose a task-chaining compositor in the form of image-removal-segmentation, providing an enhanced prompt that elicits reasoning with enriched intermediates. Additionally, we introduce context-aware aggregation, integrating the chained prompt pattern into the latent query representation, thereby strengthening the model's in-context reasoning. We also consider the issue of visual heterogeneity, which complicates the selection of homogeneous demonstrations in text recognition. Accordingly, this is effectively addressed through a simple self-prompting strategy, preventing the model's in-context learnability from devolving into specialist-like, context-free inference. Collectively, these insights culminate in our ConText model, which achieves new state-of-the-art across both in- and out-of-domain benchmarks. The code is available at https://github.com/Ferenas/ConText.","This paper introduces the first application of a new learning paradigm for addressing optical character recognition problems, focusing on text removal and segmentation. The authors propose a step-by-step prompting method (image-removal-segmentation) that helps models reason more effectively through task-in-chain examples. Their model, ConText, achieves state-of-the-art results across multiple benchmarks, and showcases surprising user-instruction interaction."
Poster,Context-Informed Neural ODEs Unexpectedly Identify Broken Symmetries: Insights from the Poincaré–Hopf Theorem,https://ICML.cc//virtual/2025/poster/45590,"In Huh, Changwook Jeong, Muhammad Alam","Out-Of-Domain (OOD) generalization is a significant challenge in learning dynamical systems, especially when they exhibit bifurcation, a sudden topological transition triggered by a model parameter crossing a critical threshold. A prevailing belief is that machine learning models, unless equipped with strong priors, struggle to generalize across bifurcations due to the abrupt changes in data characteristics. Contrary to this belief, we demonstrate that context-dependent Neural Ordinary Differential Equations (NODEs), trained solely on localized, pre-bifurcation, symmetric data and without physics-based priors, can still identify post-bifurcation, symmetry-breaking behaviors, even in a zero-shot manner. We interpret this capability to the model's implicit utilization of topological invariants, particularly the Poincaré index, and offer a formal explanation based on the Poincaré–Hopf theorem. We derive the conditions under which NODEs can recover—or erroneously hallucinate—broken symmetries without explicit training. Building on this insight, we showcase a topological regularizer inspired by the Poincaré–Hopf theorem and validate it empirically on phase transitions of systems described by the Landau–Khalatnikov equation.","Many events like water freezing, heart arrhythmias, or market crashes undergo sudden shifts when a small change pushes them past a critical point; such phenomena are called bifurcations. Machine learning (ML) is powerful for modeling complex systems, but it usually struggles to predict such sharp changes if it has never seen them before. Surprisingly, we discovered that certain ML models, trained only on pre-transition data, can predict what happens after a bifurcation. This is possible because the models implicitly leverage deeper structures—called topological invariants—that stay constant despite dramatic changes in the system. Just as a donut and a mug are topologically identical because each has one hole, certain topological properties remain preserved internally after a bifurcation. These act as internal guides, helping models infer what comes next. One such invariant is the Poincaré index, a hidden fingerprint of the system’s dynamics. Building on this idea, we explore how ML can leverage such clues to predict beyond observations. Furthermore, we design a new method to learn complicated phase transitions, achieving promising results. With further validation, this could help forecast events like heart issues or other critical transitions hidden in seemingly regular data—reshaping how we prepare for the unexpected."
Poster,Context is Key: A Benchmark for Forecasting with Essential Textual Information,https://ICML.cc//virtual/2025/poster/44343,"Andrew Williams, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima, Alexandre Lacoste, Irina Rish, Nicolas Chapados, Alexandre Drouin","Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce ""Context is Key"" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise.The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0.","Accurate time series forecasts in the real world rely on more than just historical numerical records. Time series are often driven by external events, or their nature gives them unique behaviours; forecasts can benefit from this additional context. However, most evaluation benchmarks for time series forecasting do not provide this kind of useful information. We built a benchmark of time series forecasting tasks that contain historical numerical data with accompanying side-information (in the form of text) that contains crucial context for making accurate forecasts. Examples include information of an upcoming event that will influence future values (e.g., a planned outage for an ATM that will force the number of withdrawals to 0), or knowledge of a constraint that applies to the forecast (e.g. if the time series to forecast is road occupancy, then it cannot be less than 0). The benchmark, named CiK (Context is Key), contains tasks that span several domains, with varied types of context. We evaluate a range of approaches on CiK, demonstrating surprising performance when using LLMs, also revealing some of their critical shortcomings.Having forecasting methods that can process such additional context unlocks many use cases and directions for research. For example, such forecasting methods can provide an intuitive interface for those without significant modelling expertise to improve their forecasts with additional information. This benchmark will facilitate research into improved context-aided forecasting methods, enabling researchers to test how well their methods fare."
Poster,Context Matters: Query-aware Dynamic Long Sequence Modeling of Gigapixel Images,https://ICML.cc//virtual/2025/poster/46090,"Zhengrui Guo, Qichen Sun, Jiabo MA, Lishuang Feng, Jinzhuo Wang, Hao Chen","Whole slide image (WSI) analysis presents significant computational challenges due to the massive number of patches in gigapixel images. While transformer architectures excel at modeling long-range correlations through self-attention, their quadratic computational complexity makes them impractical for computational pathology applications. Existing solutions like local-global or linear self-attention reduce computational costs but compromise the strong modeling capabilities of full self-attention. In this work, we propose **Querent**, *i.e.*, the **quer**y-awar**e** long co**nt**extual dynamic modeling framework, which achieves a theoretically bounded approximation of full self-attention while delivering practical efficiency. Our method adaptively predicts which surrounding regions are most relevant for each patch, enabling focused yet unrestricted attention computation only with potentially important contexts. By using efficient region-wise metadata computation and importance estimation, our approach dramatically reduces computational overhead while preserving global perception to model fine-grained patch correlations. Through comprehensive experiments on biomarker prediction, gene mutation prediction, cancer subtyping, and survival analysis across over 10 WSI datasets, our method demonstrates superior performance compared to the state-of-the-art approaches. Codes are available at https://github.com/dddavid4real/Querent.","Medical diagnosis increasingly relies on analyzing enormous microscopic images of tissue samples that contain millions of tiny patches, but current AI systems struggle with these ""whole slide images"" because they try to compare every patch with every other patch, requiring massive computational power. We developed a new AI approach called ""Querent"" that works more like human pathologists by intelligently focusing on the most relevant parts of each image rather than analyzing everything at once. Our method divides large images into regions, predicts which regions contain the most important information for each area being examined, and performs detailed analysis only between relevant sections. When tested on over 10 medical datasets for tasks like cancer detection, gene mutation prediction, and patient survival estimation, our approach consistently outperformed existing methods while delivering computational efficiency, which could make advanced AI-powered pathology analysis more accessible to hospitals worldwide, potentially improving diagnostic accuracy and patient care while reducing costs."
Poster,Contextual Bandits for Unbounded Context Distributions,https://ICML.cc//virtual/2025/poster/44484,"Puning Zhao, Rongfei Fan, Shaowei Wang, Li Shen, Qixin Zhang, Zong Ke, Tianhang Zheng","Nonparametric contextual bandit is an important model of sequential decision making problems. Under $\alpha$-Tsybakov margin condition, existing research has established a regret bound of $\tilde{O}\left(T^{1-\frac{\alpha+1}{d+2}}\right)$ for bounded supports. However, the optimal regret with unbounded contexts has not been analyzed. The challenge of solving contextual bandit problems with unbounded support is to achieve both exploration-exploitation tradeoff and bias-variance tradeoff simultaneously. In this paper, we solve the nonparametric contextual bandit problem with unbounded contexts. We propose two nearest neighbor methods combined with UCB exploration. The first method uses a fixed $k$. Our analysis shows that this method achieves minimax optimal regret under a weak margin condition and relatively light-tailed context distributions. The second method uses adaptive $k$. By a proper data-driven selection of $k$, this method achieves an expected regret of $\tilde{O}\left(T^{1-\frac{(\alpha+1)\beta}{\alpha+(d+2)\beta}}+T^{1-\beta}\right)$, in which $\beta$ is a parameter describing the tail strength. This bound matches the minimax lower bound up to logarithm factors, indicating that the second method is approximately optimal.","Contextual bandit is an important model of sequential decision. The design of methods and corresponding theoretical analysis is challenging when the support of contexts is unbounded. In this paper, we propose new methods for contextual bandits with unbounded context supports. Compared with naive extension of existing methods, our method has significantly improved performance. Our theoretical analysis suggests that the proposed method is minimax optimal. Numerical experiments also validate our proposed approach."
Poster,Contextual Linear Bandits with Delay as Payoff,https://ICML.cc//virtual/2025/poster/43482,"Mengxiao Zhang, Yingfei Wang, Haipeng Luo","A recent work by Schlisselberg et al. (2025) studies a delay-as-payoff model for stochastic multi-armed bandits, where the payoff (either loss or reward) is delayed for a period that is proportional to the payoff. While this captures many real-world applications, the simple multi-armed bandit setting limits the practicality of their results. In this paper, we address this limitation by studying the delay-as-payoff model for contextual linear bandits. Specifically, we start from the case with a fixed action set and propose an efficient algorithm whose regret overhead compared to the standard no-delay case is only of order $D\Delta_{\max}\log T$, where $T$ is the total horizon, $D$ is the maximum delay, and $\Delta_{\max}$ is the maximum suboptimality gap. When payoff is loss, we also show further improvement of the bound, demonstrating a separation between reward and loss similar to Schlisselberg et al. (2025). Contrary to standard linear bandit algorithms that construct least squares estimator and confidence ellipsoid, the main novelty of our algorithm is to apply a phased arm elimination procedure by only picking the **volumetric spanners** of the action set, which addresses challenges arising from both payoff-dependent delays and large action sets. We further extend our results to the case with varying action sets by adopting the reduction from Hanna et al. (2023). Finally, we implement our algorithm and showcase its effectiveness and superior performance in experiments.","Sequential decision-making applications often need to deal with delayed action outcomes: think of medical treatments and online advertising, where the payoff is not immediate and its delay is proportional to the payoff. A recent study [Schlisselberg et al. 2025] explored this problem but only for very basic multi-armed bandits scenarios. In our work, we tackle its contextual linear variant, where each action's payoff depends on its time-varying feature embedding, making the problem more realistic.To address this, we introduce a novel and efficient algorithm with strong theoretical and empirical guarantees. Contrary to standard linear bandit algorithms that construct least squares estimators and confidence ellipsoids, the main novelty of our algorithm is its phased arm elimination procedure by only selecting the volumetric spanners of the action set. This approach effectively addresses challenges arising from both payoff-dependent delays and large action sets.Our research initiates the study of contextual linear bandits with payoff-dependent delays, opening doors to more complicated real-world scenarios, including evolving and composite delayed feedback."
