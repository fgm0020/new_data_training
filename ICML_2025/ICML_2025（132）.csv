type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Hessian Geometry of Latent Space in Generative Models,https://ICML.cc//virtual/2025/poster/45794,"Alexander Lobashev, Dmitry Guskov, Maria Larchenko, Mikhail Tamm","This paper presents a novel method for analyzing the latent space geometry of generative models, including statistical physics models and diffusion models, by reconstructing the Fisher information metric. The method approximates the posterior distribution of latent variables given generated samples and uses this to learn the log-partition function, which defines the Fisher metric for exponential families. Theoretical convergence guarantees are provided, and the method is validated on the Ising and TASEP models, outperforming existing baselines in reconstructing thermodynamic quantities. Applied to diffusion models, the method reveals a fractal structure of phase transitions in the latent space, characterized by abrupt changes in the Fisher metric. We demonstrate that while geodesic interpolations are approximately linear within individual phases, this linearity breaks down at phase boundaries, where the diffusion model exhibits a divergent Lipschitz constant with respect to the latent space. These findings provide new insights into the complex structure of diffusion model latent spaces and their connection to phenomena like phase transitions.Our source code is available at \url{https://github.com/alobashev/hessian-geometry-of-diffusion-models}.","Diffusion and other generative models learn to create complex data—like images—by moving through an internal “latent” space. However, this latent space is not intuitive and hides surprising geometric features, making it difficult to understand why models sometimes jump between very different outputs.  We built a mathematical “map” of that landscape. Borrowing ideas from physics and information geometry, we learn the Fisher metric—a kind of built-in ruler—directly from the images a model produces. Our method works for classic physics simulations (the Ising and TASEP models) and for cutting-edge diffusion generators like Stable Diffusion. When we apply our technique to modern diffusion models, we uncover a striking, fractal-like pattern of sharp “phase transitions” in their latent space. Within each phase, straightforward paths between points work well, but at the boundaries, tiny changes can trigger huge jumps—explaining why these models sometimes behave unpredictably. Our findings pave the way for more reliable and interpretable generative AI."
Poster,Heterogeneous Data Game: Characterizing the Model Competition Across Multiple Data Sources,https://ICML.cc//virtual/2025/poster/44085,"Renzhe Xu, Kang Wang, Bo Li","Data heterogeneity across multiple sources is common in real-world machine learning (ML) settings. Although many methods focus on enabling a single model to handle diverse data, real-world markets often comprise multiple competing ML providers. In this paper, we propose a game-theoretic framework—the Heterogeneous Data Game—to analyze how such providers compete across heterogeneous data sources. We investigate the resulting pure Nash equilibria (PNE), showing that they can be non-existent, homogeneous (all providers converge on the same model), or heterogeneous (providers specialize in distinct data sources). Our analysis spans monopolistic, duopolistic, and more general markets, illustrating how factors such as the ``temperature'' of data-source choice models and the dominance of certain data sources shape equilibrium outcomes. We offer theoretical insights into both homogeneous and heterogeneous PNEs, guiding regulatory policies and practical strategies for competitive ML marketplaces.","In the real world, data used to train machine learning (ML) models often comes from different sources, like hospitals, cities, or user groups, each with its own unique characteristics. Yet, most research assumes that a single model serves all users equally. This overlooks how, in practice, multiple companies or institutions compete to offer ML models tailored to different users.We study how such competition plays out when data is heterogeneous. Using tools from game theory, we model how providers choose what kind of model to offer, and how users decide which model to adopt based on performance. We identify conditions under which competing providers end up offering the same solution and when they specialize to serve different data sources.Our results show that market diversity depends not only on the data itself, but also on how users choose models and how competitive the market is. In particular, dominant data sources often attract most providers, leaving others underserved. These insights can help platform designers and policymakers build ML ecosystems that are both efficient and equitable."
Poster,Heterogeneous Label Shift: Theory and Algorithm,https://ICML.cc//virtual/2025/poster/46108,"Chao Xu, Xijia Tang, Chenping Hou","In open-environment applications, data are often collected from heterogeneous modalities with distinct encodings, resulting in feature space heterogeneity. This heterogeneity inherently induces label shift, making cross-modal knowledge transfer particularly challenging when the source and target data exhibit simultaneous heterogeneous feature spaces and shifted label distributions. Existing studies address only partial aspects of this issue, leaving the broader problem unresolved. To bridge this gap, we introduce a new concept of Heterogeneous Label Shift (HLS), targeting this critical but underexplored challenge. We first analyze the impact of heterogeneous feature spaces and label distribution shifts on model generalization and introduce a novel error decomposition theorem. Based on these insights, we propose a bound minimization HLS framework that decouples and tackles feature heterogeneity and label shift accordingly. Extensive experiments on various benchmarks for cross-modal classification validate the effectiveness and practical relevance of the proposed approach.","We addresses the critical yet underexplored challenge of Heterogeneous Label Shift (HLS), characterized by simultaneous feature space heterogeneity and label distribution shifts in cross-modal knowledge transfer.We analyze the impact of heterogeneous feature spaces and label distribution shifts on model generalization and introduce a novel error decomposition theorem. Based on these insights, we propose a bound minimization HLS framework that decouples and tackles feature heterogeneity and label shift accordingly.Our findings highlight the importance of tackling complex, real-world distribution shifts and lay a strong foundation for future research in cross-modal knowledge transfer."
Poster,Heterogeneous Sufficient Dimension Reduction and Subspace Clustering,https://ICML.cc//virtual/2025/poster/43791,"Lei Yan, Xin Zhang, Qing Mai","Scientific and engineering applications are often heterogeneous, making it beneficial to account for latent clusters or sub-populations when learning low-dimensional subspaces in supervised learning, and vice versa. In this paper, we combine the concept of subspace clustering with model-based sufficient dimension reduction and thus generalize the sufficient dimension reduction framework from homogeneous regression setting to heterogeneous data applications. In particular, we propose the mixture of principal fitted components (mixPFC) model, a novel framework that simultaneously achieves clustering, subspace estimation, and variable selection, providing a unified solution for high-dimensional heterogeneous data analysis. We develop a group Lasso penalized expectation-maximization (EM) algorithm and obtain its non-asymptotic convergence rate. Through extensive simulation studies, mixPFC demonstrates superior performance compared to existing methods across various settings. Applications to real world datasets further highlight its effectiveness and practical advantages.","Many scientific and engineering problems involve complex, high-dimensional data that are heterogeneous and do not follow a single pattern. This paper introduces a new method called mixture of principal fitted components (mixPFC) that can uncover hidden groups in the data while also reducing the dimensionality and complexity of the data, and identifying the most important variables. The approach offers a unified solution for analyzing large, heterogeneous datasets more effectively."
Poster,Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing Censored Data with Recursively Imputed Trees,https://ICML.cc//virtual/2025/poster/45413,"Tomer Meir, Uri Shalit, Malka Gorfine","Tailoring treatments to individual needs is a central goal in fields such as medicine. A key step toward this goal is estimating Heterogeneous Treatment Effects (HTE)—the way treatments impact different subgroups. While crucial, HTE estimation is challenging with survival data, where time until an event (e.g., death) is key. Existing methods often assume complete observation, an assumption violated in survival data due to right-censoring, leading to bias and inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE estimation in survival data under no hidden confounders, combining a causal survival forest with an augmented inverse-censoring weighting estimator. However, we find it struggles under heavy censoring, which is common in rare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover, most current methods cannot handle instrumental variables, which are a crucial tool in the causal inference arsenal. We introduce Multiple Imputation for Survival Treatment Response (MISTR), a novel, general, and non-parametric method for estimating HTE in survival data. MISTR uses recursively imputed survival trees to handle censoring without directly modeling the censoring mechanism. Through extensive simulations and analysis of two real-world datasets—the AIDS Clinical Trials Group Protocol 175 and the Illinois unemployment dataset we show that MISTR outperforms prior methods under heavy censoring in the no-hidden-confounders setting, and extends to the instrumental variable setting. To our knowledge, MISTR is the first non-parametric approach for HTE estimation with unobserved confounders via instrumental variables.","Which cancer treatment would lead to longer survival for each patient? Knowing this would allow us to select the most effective treatment per individual, optimizing outcomes. However, estimating treatment effects is challenging because observed data typically show outcomes for only one treatment per person. Moreover, when the outcome is time until an event like patient death, data are often incomplete due to “right-censoring” – for instance, when subjects drop out or monitoring time is limited.To address this, we developed MISTR, a new method that constructs multiple datasets with plausible imputations for missing event times. It produces estimates using each imputed dataset and combines them for accurate, personalized treatment effect estimates. Furthermore, most existing approaches require detailed patient characteristics, which are sometimes unavailable. MISTR can bypass this by leveraging “instrumental variables,” a powerful approach for estimating treatment effects without full patient data. We tested MISTR using simulations and real-world medical and economic datasets, demonstrating its superior performance over existing methods, especially under challenging conditions like heavy censoring and unobserved crucial individual characteristics."
Poster,HetSSNet: Spatial-Spectral Heterogeneous Graph Learning Network for Panchromatic and Multispectral Images Fusion,https://ICML.cc//virtual/2025/poster/46285,"Mengting Ma, Yizhen Jiang, Mengjiao Zhao, Jiaxin Li, Wei Zhang","Remote sensing pansharpening aims to reconstruct spatial-spectral properties during the fusion of panchromatic (PAN) images and low-resolution multi-spectral (LR-MS) images, finally generating the high-resolution multi-spectral (HR-MS) images. In the mainstream modeling strategies, i.e., CNN and Transformer, the input images are treated as the equal-sized grid of pixels in the Euclidean space. They have limitations in facing remote sensing images with irregular ground objects. Graph is the more flexible structure, however, there are two major challenges when modeling spatial-spectral properties with graph: 1) constructing the customized graph structure for spatial-spectral relationship priors; 2) learning the unified spatial-spectral representation through the graph. To address these challenges, we propose the spatial-spectral heterogeneous graph learning network, named HetSSNet.Specifically, HetSSNet initially constructs the heterogeneous graph structure for pansharpening, which explicitly describes pansharpening-specific relationships. Subsequently, the basic relationship pattern generation module is designed to extract the multiple relationship patterns from the heterogeneous graph. Finally, relationship pattern aggregation module is exploited to collaborativelylearn unified spatial-spectral representation across different relationships among nodes with adaptive importance learning from local and global perspectives. Extensive experiments demonstrate the significant superiority and generalization of HetSSNet.","Remote sensing pansharpening aims to fuse high-resolution panchromatic (PAN) images with low-resolution multispectral (LR-MS) images to reconstruct spatial-spectral information and generate high-resolution multispectral (HR-MS) images. While mainstream approaches like CNNs and Transformers process images as regular pixel grids in Euclidean space, they struggle with irregular ground objects in real-world remote sensing scenes. Graph-based methods offer greater flexibility, but two key challenges remain: (1) designing a graph structure that captures spatial-spectral relationships, and (2) learning a unified representation from such a graph.To address these challenges, we propose HetSSNet, a spatial-spectral heterogeneous graph learning network. HetSSNet first constructs a task-specific heterogeneous graph to model pansharpening relationships explicitly. Next, a relationship pattern generation module extracts multiple interaction patterns from the graph. Finally, a relationship pattern aggregation module adaptively fuses these patterns, learning a unified spatial-spectral representation by combining local and global perspectives.Extensive experiments demonstrate HetSSNet’s superior performance and strong generalization capability compared to existing methods."
Poster,Hgformer: Hyperbolic Graph Transformer for Collaborative Filtering,https://ICML.cc//virtual/2025/poster/44203,"Yang Xin, Xingrun Li, Heng Chang, Yang jinze, xihong yang, Shengyu Tao, Maiko Shigeno, Ningkang Chang, Junfeng Wang, Dawei Yin, Erxue Min","Recommender systems are increasingly spreading to different areas like e-commerce or video streaming to alleviate information overload.  One of the most fundamental methods for recommendation is Collaborative Filtering (CF), which leverages historical user-item interactions to infer user preferences. In recent years, Graph Neural Networks (GNNs) have been extensively studied to capture graph structures in CF tasks. Despite this remarkable progress, local structure modeling and embedding distortion still remain two notable limitations in the majority of GNN-based CF methods.  Therefore, in this paper, we propose a novel Hyperbolic Graph Transformer architecture, to tackle the long-tail problems in CF tasks. Specifically, the proposed framework is comprised of two essential modules: 1) Local Hyperbolic Graph Convolutional Network (LHGCN), which performs graph convolution entirely in the hyperbolic manifold and captures the local structure of each node; 2) Hyperbolic Transformer, which is comprised of hyperbolic cross-attention mechanisms to capture global information. Furthermore, to enable its feasibility on large-scale data, we introduce an unbiased approximation of the cross-attention for linear computational complexity, with a theoretical guarantee in approximation errors. Empirical experiments demonstrate that our proposed model outperforms the leading collaborative filtering methods and significantly mitigates the long-tail issue in CF tasks. Our implementations are available in \url{https://github.com/EnkiXin/Hgformer}.","In recent years, recommender systems have been adopted in many areas of daily life, aiming to filter the most useful information for users. Yet the field now faces two key challenges: (1) while the recommender systems can easily capture features of popular items, they struggle to grasp those of less popular ones; and (2) they have difficulty modeling global information. Uptill now, few existing studies tackle both issues at once. In this work, we propose a unified solution that does: we incorporate spatial characteristics by combining hyperbolic geometry with Graph Transformer techniques for recommendation. Experiments across multiple datasets show our method surpasses current baselines, and detailed analyses explain why it works."
Poster,HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport,https://ICML.cc//virtual/2025/poster/44292,"Yanbei Liu, Chongxu Wang, Zhitao Xiao, Lei Geng, Yanwei Pang, Xiao Wang","Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent capabilities in processing heterogeneous information networks. Self-supervised learning on heterogeneous graphs, especially contrastive self-supervised strategy, shows great potential when there are no labels. However, this approach requires the use of carefully designed graph augmentation strategies and the selection of positive and negative samples. Determining the exact level of similarity between sample pairs is non-trivial.To solve this problem, we propose a novel self-supervised Heterogeneous graph neural network with Optimal Transport (HGOT) method which is designed to facilitate self-supervised learning for heterogeneous graphs without graph augmentation strategies. Different from traditional contrastive self-supervised learning, HGOT employs the optimal transport mechanism to relieve the laborious sampling process of positive and negative samples. Specifically, we design an aggregating view (central view) to integrate the semantic information contained in the views represented by different meta-paths (branch views). Then, we introduce an optimal transport plan to identify the transport relationship between the semantics contained in the branch view and the central view. This allows the optimal transport plan between graphs to align with the representations, forcing the encoder to learn node representations that are more similar to the graph space and of higher quality.Extensive experiments on four real-world datasets demonstrate that our proposed HGOT model can achieve state-of-the-art performance on various downstream tasks. In particular, in the node classification task, HGOT achieves an average of more than 6\% improvement in accuracy compared with state-of-the-art methods.","In this paper, we propose a novel self-supervised heterogeneous graph neural networks with optimal transport, named HGOT. The attention mechanism is employed to obtain an aggregated view which can integrate the semantic information from different meta-paths. Then, HGOT exploits the optimal transport theory to discover the optimal transport plan between the meta-path view and the aggregated view. By aligning the transport plans between graph space and representation sapce, HGOT enforces the backbone model to learn node representations that precisely preserve the matching relationships. Extensive experiments conducted on multiple datasets demonstrate the state-of-the-art performance of the proposed HGOT."
Poster,Hidden No More: Attacking and Defending Private Third-Party LLM Inference,https://ICML.cc//virtual/2025/poster/45330,"Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal","Recent advances in Large Language Models (LLMs) have led to widespread adoption of third-party inference services, raising critical privacy concerns. In this work, we introduce a novel reconstruction technique that can recover original prompts from hidden states with nearly perfect accuracy across multiple state-of-the-art LLMs in the increasingly important open-weights setting. Although the attack is conceptually simple, it has not  -- to the best of our knowledge -- previously been described nor shown to work practically. Furthermore, our attack remains effective against various permutation and noise-based defenses, challenging assumptions about the security of previously proposed schemes. To address these vulnerabilities, we propose Cascade, a multi-party inference scheme that leverages sharding in the sequence dimension to retain privacy of the user input. Through theoretical analysis and empirical evaluation, we demonstrate that Cascade is secure against both our attack as well as previous methods, while maintaining computational and communication efficiency. Our findings highlight the importance of rigorous security analysis in privacy-preserving LLM inference and offer practical solutions for secure deployment.","Large language models (LLMs) are often run by third-party services, raising serious concerns about user data privacy. This risk motivates the need for protocols which run LLMs on encrypted prompts instead of raw user data. While many such protocols are provably secure, they are too slow to be practical, so researchers have devised faster protocols that are probabilistically secure. In these protocols, the third party does not see user inputs, but receives permutations of internal LLM data instead.It was previously believed that such permutations are infeasible to reverse. Our paper introduces a new attack that can nearly perfectly reconstruct the original prompt from such data, highlighting serious security risks from the use of permutation-based schemes.Further, as an alternative mitigation, we develop an efficient protocol called Cascade, which splits the prompt among nodes and then performs sharded LLM inference. Through analysis of partial internal model data, we show that Cascade is secure against our attack and other attacks in literature.Our research emphasizes the importance of thoroughly testing AI systems for privacy vulnerabilities, and offers practical solutions to securely run LLMs via a third party, enabling safer use by the general public."
Poster,Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It,https://ICML.cc//virtual/2025/poster/46431,"Marvin F, da Silva, Felix Dangel, Sageev Oore","The concept of sharpness has been successfully applied to traditional architectures like MLPs and CNNs to predict their generalization. For transformers, however, recent work reported weak correlation between flatness and generalization. We argue that existing sharpness measures fail for transformers, because they have much richer symmetries in their attention mechanism that induce directions in parameter space along which the network or its loss remain identical. We posit that sharpness must account fully for these symmetries, and thus we redefine it on a quotient manifold that results from quotienting out the transformer symmetries, thereby removing their ambiguities. Leveraging tools from Riemannian geometry, we propose a fully general notion of sharpness, in terms of a geodesic ball on the symmetry-corrected quotient manifold. In practice, we need to resort to approximating the geodesics. Doing so up to first order yields existing adaptive sharpness measures, and we demonstrate that including higher-order terms is crucial to recover correlation with generalization. We present results on diagonal networks with synthetic data, and show that our geodesic sharpness reveals strong correlation for real-world transformers on both text and image classification tasks.","In deep learning, understanding why some neural networks make better predictions than others is an important problem. One popular idea to explain this is called sharpness. Sharpness looks at the shape of the network’s loss landscape, a kind of landscape showing how good or bad the network is doing depending on small changes in its internal parameters. Generally, if this landscape is “flat,” it means small changes don’t hurt performance much, and the model is more likely to generalize well to data it has not seen before.This idea works well for older types of neural networks like MLPs (multilayer perceptrons) and CNNs (convolutional neural networks). But for transformers this relationship breaks down. Researchers have found that sharpness, as it's usually measured, doesn't reliably predict whether a transformer will generalize well.We argue that the problem isn't with the idea of sharpness itself, but with how it's measured in transformers. Transformers have a lot of ways you can change their internal parameters without actually changing how the model behaves (symmetries). These symmetries confuse traditional sharpness measurements. Using tools from differential geometry, we introduce a more accurate definition of sharpness that takes these symmetries into account, finding that once we correct for these symmetries, sharpness is still a useful concept."
