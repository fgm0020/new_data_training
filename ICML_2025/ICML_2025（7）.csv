type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Adaptive Elicitation of Latent Information Using Natural Language,https://ICML.cc//virtual/2025/poster/45749,"Jimmy Wang, Tom Zollo, Richard Zemel, Hongseok Namkoong","Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.",We propose a framework for using LLMs to ask informative questions about variables and entities that cannot be directly observed.  Potentially impactful applications include constructing a dynamic diagnostic questionnaire that maximizes the information gained about a patient’s health or generating a personalized set of test questions that yield the most insight into a student’s learning needs.
Poster,Adaptive Estimation and Learning under Temporal Distribution Shift,https://ICML.cc//virtual/2025/poster/45680,"Dheeraj Baby, Yifei Tang, Hieu Nguyen, Yu-Xiang Wang, Rohit Pyati","In this paper, we study the problem of estimation and learning under temporal distribution shift. Consider an observation sequence of length $n$, which is a noisy realization of a time-varying ground-truth sequence. Our focus is to develop methods to estimate the groundtruth at the final time-step while providing sharp point-wise estimation error rates. We show that, *without prior knowledge* on the level of temporal shift, a wavelet soft-thresholding estimator provides an *optimal* estimation error bound for the groundtruth. Our proposed estimation method generalizes existing researches (Mazetto and Upfal, 2023) by establishing a connection between the sequence's non-stationarity level and the sparsity in the wavelet-transformed domain. Our theoretical findings are validated by numerical experiments. Additionally, we applied the estimator to derive sparsity-aware excess risk bounds for binary classification under distribution shift and to develop computationally efficient training objectives. As a final contribution, we draw parallels between our results and the classical signal processing problem of total-variation denoising (Mammen and van de Geer 1997; Tibshirani 2014 ), uncovering *novel optimal* algorithms for such task.","The paper proposes a versatile way of estimating quantities under temporal distribution shift. A simple example that illustrates the problem setup is the task of estimating trends from a univariate and noisy time series observations. We show that wavelet based denoising leads to optimal pointwise estimation error guarantees. The estimation guarantee is not just worst case optimal, but also adapts naturally to the hardness of the problem. i.e more stationary the trend is, sharper the error rates are. Further, these properties are attained with no hand tuning and no prior knowledge or assumptions on how fast the trend evolves. The methods are also applicable to training and evaluating models especially in production pipelines,  where the collected training dataset is known to have shifted data distribution over time."
Poster,Adaptive Exploration for Multi-Reward Multi-Policy Evaluation,https://ICML.cc//virtual/2025/poster/46555,"Alessio Russo, Aldo Pacchiano","We study the policy evaluation problem in an online multi-reward multi-policy discounted setting, where multiple reward functions  must be evaluated simultaneously for different policies. We adopt an $(\epsilon,\delta)$-PAC perspective to achieve $\epsilon$-accurate estimates with high confidence across finite or convex sets of rewards, a setting that has not been investigated in the literature. Building on prior work on Multi-Reward Best Policy Identification, we adapt the MR-NaS exploration scheme to jointly minimize sample complexity for evaluating different policies across different reward sets. Our approach leverages an instance-specific lower bound revealing how the sample complexity scales with a measure of value deviation, guiding the design of an efficient exploration policy. Although computing this bound entails a hard non-convex optimization, we propose an efficient convex approximation that holds for both finite and convex reward sets. Experiments in tabular domains demonstrate the effectiveness of  this adaptive exploration scheme.","Many real-world decision-making systems—such as recommendation algorithms, robotics, or personalized AI assistants—need to evaluate how well multiple strategies perform across diverse goals simultaneously. Traditionally, this process can become extremely resource-intensive, requiring significant time and data to obtain accurate results. We developed an approach that simultaneously evaluates multiple strategies across multiple objectives in a reliable and efficient way. Our method strategically chooses how to gather information at each step, ensuring accurate results with minimal data. This enables quicker, more reliable insights, improving how we develop systems that need to balance multiple goals—such as improving user satisfaction while minimizing costs or environmental impacts."
Poster,Adaptive Flow Matching for Resolving Small-Scale Physics,https://ICML.cc//virtual/2025/poster/44917,"Stathi Fotiadis, Noah Brenowitz, Tomas Geffner, Yair Cohen, Michael Pritchard, Arash Vahdat, Morteza Mardani","Conditional diffusion and flow models are effective for super-resolving small-scale details in natural images. However, in physical sciences such as weather, three major challenges arise: (i) spatially misaligned input-output distributions (PDEs at different resolutions lead to divergent trajectories), (ii) misaligned and distinct input-output channels (channel synthesis), (iii) several channels with diverse stochasticity scales (multiscale). To address these, we propose to first encode inputs into a latent base distribution that is closer to the target, then apply Flow Matching to generate small-scale physics. The encoder captures deterministic components, while Flow Matching adds stochastic details. To handle uncertainty in the deterministic part, we inject noise via an adaptive noise scaling mechanism, dynamically adjusted by maximum-likelihood estimates of the encoder’s predictions. Experiments on real-world weather data (including super-resolution from 25 km to 2 km scales in Taiwan) and in synthetic Kolmogorov flow datasets show that our proposed Adaptive Flow Matching (AFM) framework outperforms existing methods and produces better-calibrated ensembles.","Weather forecasts often miss important small-scale details, such as the exact path of a thunderstorm, because they rely on low-resolution simulations that smooth out finer structures. This limits our ability to prepare for extreme weather events like floods or heatwaves. Our research presents a machine learning method that enhances these coarse forecasts by recovering realistic small-scale features, even when the input and output data are not perfectly aligned or come from different sources. The model first captures large-scale, predictable patterns and then adds the missing fine details using controlled randomness. This combination improves both the accuracy and reliability of the predictions. Our approach could support earlier and more precise detection of severe weather, helping communities respond more effectively. Beyond weather, the technique may be useful in other scientific fields such as climate modeling or medical imaging where data resolution and alignment pose similar challenges."
Poster,Adaptive kernel predictors from feature-learning infinite limits of neural networks,https://ICML.cc//virtual/2025/poster/45506,"Clarissa Lauditi, Blake Bordelon, Cengiz Pehlevan","Previous influential work showed that infinite width limits of neural networks in the lazy training regime are described by kernel machines. Here, we show that neural networks trained in the rich infinite-width regime in two different settings are also described by kernel machines, but with data-dependent kernels.  For both cases, we provide explicit expressions for the kernel predictors and prescriptions to numerically calculate them. To derive the first predictor, we study the large-width limit of feature-learning Bayesian networks, showing how feature learning leads to task-relevant adaptation of layer kernels and preactivation densities. The saddle point equations governing this limit result in a min-max optimization problem that defines the kernel predictor. To derive the second predictor, we study gradient flow training of randomly initialized networks trained with weight decay in the infinite-width limit using dynamical mean field theory (DMFT). The fixed point equations of the arising DMFT defines the task-adapted internal representations and the kernel predictor. We compare our kernel predictors to kernels derived from lazy regime and demonstrate that our adaptive kernels achieve lower test loss on benchmark datasets.","Despite neural networks' widespread adoption in modern machine learning, the solutions that deep networks converge to after training are not well understood. One promising theoretical approach is to analyze infinite width neural networks or approximations near an infinite width limit. While many prior works analyze lazy learning limits of deep networks (where internal features in hidden layers are static over training), we instead consider Bayesian inference in networks whose large width limits maintain feature learning. The predictors in this limit are given by a kernel regression solution with a task-dependent deterministic kernel. This adapted feature kernel is the solution to a min-max optimization problem that depends on properties of both the inputs to the network, the target outputs, and the architecture of the network. We show that this limit outperforms lazy learning limits and other limits obtained from networks in NTK scaling."
Poster,Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection,https://ICML.cc//virtual/2025/poster/45663,"Matteo Zecchin, Sangwoo Park, Osvaldo Simeone","We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and prompt engineering, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds.","Selecting a reliable hyperparameter configuration for large-scale machine learning models is a costly operation, as it requires multiple rounds of testing across a set of candidate hyperparameter values. In this work, we propose adaptive learn-then-test (aLTT), a hyperparameter selection algorithm that performs this evaluation efficiently using e-value–based testing instead of p-values. The main advantage of aLTT is that it leverages data-dependent acquisition and termination, which greatly improve efficiency without sacrificing statistical validity. Our approach is shown to outperform prior methods in settings such as online policy selection and automated prompt engineering for large language models."
Poster,Adaptive Localization of Knowledge Negation for Continual LLM Unlearning,https://ICML.cc//virtual/2025/poster/43773,"Abudukelimu Wuerkaixi, Qizhou Wang, Sen Cui, Wutong Xu, Bo Han, Gang Niu, Masashi Sugiyama, Changshui Zhang","With the growing deployment of large language models (LLMs) across diverse domains, concerns regarding their safety have grown substantially.LLM unlearning has emerged as a pivotal approach to removing harmful or unlawful contents while maintaining utility.Despite increasing interest, the challenges of continual unlearning, which is common in real-world scenarios, remain underexplored.Successive unlearning tasks often lead to intensified utility degradation.To effectively unlearn targeted knowledge while preserving LLM utility, it is essential to minimize changes in model parameters by selectively updating those linked to the target knowledge, thereby ensuring other knowledge remains unaffected.Building on the task vector framework, we propose a new method named ALKN (Adaptive Localization of Knowledge Negation), which uses dynamic masking to sparsify training gradients and adaptively adjusts unlearning intensity based on inter-task relationships.Comprehensive experiments across three well-established LLM unlearning datasets demonstrate that our approach consistently outperforms baseline methods in both unlearning effectiveness and utility retention under continual unlearning settings.","Large language models (LLMs), like those powering chatbots, can sometimes store sensitive or unwanted information, raising safety and privacy concerns. This research introduces a new method called Adaptive Localization of Knowledge Negation (ALKN) to help these models ""forget"" specific information, such as personal data or harmful content, while keeping their overall usefulness intact. Unlike previous approaches that can harm a model’s performance when repeatedly asked to forget things, ALKN carefully targets only the relevant parts of the model to update, avoiding unnecessary changes. It also adjusts how strongly it forgets based on whether the information is already partially forgotten, preventing excessive loss of the model’s abilities. Tested on real-world scenarios, ALKN successfully removes unwanted information while preserving over 95% of the model’s performance, outperforming other methods. This makes it a promising tool for keeping LLMs safe and effective as they handle ongoing requests to forget specific data."
Poster,Adaptive Median Smoothing: Adversarial Defense for Unlearned Text-to-Image Diffusion Models at Inference Time,https://ICML.cc//virtual/2025/poster/45379,"XIAOXUAN HAN, Songlin Yang, Wei Wang, Yang Li, JING DONG","Text-to-image (T2I) diffusion models have raised concerns about generating inappropriate content, such as ""*nudity*"". Despite efforts to erase undesirable concepts through unlearning techniques, these unlearned models remain vulnerable to adversarial inputs that can potentially regenerate such content. To safeguard unlearned models, we propose a novel inference-time defense strategy that mitigates the impact of adversarial inputs. Specifically, we first reformulate the challenge of ensuring robustness in unlearned diffusion models as a robust regression problem. Building upon the naive median smoothing for regression robustness, which employs isotropic Gaussian noise, we develop a generalized median smoothing framework that incorporates anisotropic noise. Based on this framework, we introduce a token-wise ***Adaptive Median Smoothing*** method that dynamically adjusts noise intensity according to each token's relevance to target concepts. Furthermore, to improve inference efficiency, we explore implementations of this adaptive method at the text-encoding stage. Extensive experiments demonstrate that our approach enhances adversarial robustness while preserving model utility and inference efficiency, outperforming baseline defense techniques.","AI systems that convert text descriptions into images (text-to-image models) sometimes produce harmful or inappropriate content—even after safety measures have been implemented. This happens because malicious users craft specially designed text prompts that circumvent these safeguards.To address this vulnerability, we introduce a new defense technique called ""Adaptive Median Smoothing"" that works during the image generation process. Our approach intelligently applies varying levels of protection based on detecting which words in a prompt are likely to trigger unwanted content.This targeted approach effectively blocks inappropriate content while ensuring benign requests continue to produce high-quality results. Experiments demonstrate that our approach provides robust protection against malicious prompts, while preserving both the quality and efficiency of AI image generation systems."
Poster,"Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching",https://ICML.cc//virtual/2025/poster/44906,"Federico Errica, Henrik Christiansen, Viktor Zaverkin, Takashi Maruyama, Mathias Niepert, Francesco Alesiani","Long-range interactions are essential for the correct description of complex systems in many scientific fields. The price to pay for including them in the calculations, however, is a dramatic increase in the overall computational costs. Recently, deep graph networks have been employed as efficient, data-driven models for predicting properties of complex systems represented as graphs. These models rely on a message passing strategy that should, in principle, capture long-range information without explicitly modeling the corresponding interactions. In practice, most deep graph networks cannot really model long-range dependencies due to the intrinsic limitations of (synchronous) message passing, namely oversmoothing, oversquashing, and underreaching. This work proposes a general framework that \textit{learns to mitigate} these limitations: within a variational inference framework, we endow message passing architectures with the ability to adapt their depth and filter messages along the way. With theoretical and empirical arguments, we show that this strategy better captures long-range interactions, by competing with the state of the art on five node and graph prediction datasets.","We propose a neural network for graph-structured data that automatically learns the number of hidden layers to use. This is important because neural network for graphs work by repeatedly exchanging information across the entities of a graph, and the more message exchange iterations the more information is spread. Some tasks require information to be spread between distant entities, therefore it is important to learn the number of layers rather than manually trying different values every time. In addition, we learn to filter some of the irrelevant messages in order to avoid information overload during the spreading process."
Poster,Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection,https://ICML.cc//virtual/2025/poster/45809,"Xiang Fang, Arvind Easwaran, Blaise Genest","Out-of-distribution (OOD) detection attempts to distinguish outlier samples to prevent models trained on the in-distribution (ID) dataset from producing unavailable outputs. Most OOD detection methods require many ID samples for training, which seriously limits their real-world applications. To this end, we target a challenging setting: few-shot OOD detection, where only a few labeled ID samples are available. Therefore, few-shot OOD detection is much more challenging than the traditional OOD detection setting. Previous few-shot OOD detection works ignore the distinct diversity between different classes. In this paper, we propose a novel network: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. To compensate for the absence of OOD and scarcity of ID image samples, we leverage CLIP, connecting text with images, engineering learnable ID and OOD textual prompts. Specifically, we first generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts, and label-adaptive OOD prompts). Then, we generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, we propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts. Experimental results show that AMCN outperforms other state-of-the-art works.","Out-of-distribution (OOD) detection attempts to distinguish outlier samples to prevent models trained on the in-distribution (ID) dataset from producing unavailable outputs. we propose a novel network: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution for OOD detection. Experimental results show that our proposed method outperforms other state-of-the-art works."
