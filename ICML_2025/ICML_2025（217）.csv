type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Parameter-Efficient Fine-Tuning of State Space Models,https://ICML.cc//virtual/2025/poster/46398,"Kevin Galim, Wonjun Kang, Yuchen Zeng, HYUNG IL KOO, Kangwook Lee","Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have become powerful tools for language modeling, offering high performance and linear scalability with sequence length. However, the application of parameter-efficient fine-tuning (PEFT) methods to SSM-based models remains largely underexplored. We start by investigating two fundamental questions on existing PEFT methods: (i) How do they perform on SSM-based models? (ii) Which parameters should they target for optimal results? Our analysis shows that LoRA and its variants consistently outperform all other PEFT methods. While LoRA is effective for linear projection matrices, it fails on SSM modules—yet still outperforms other methods applicable to SSMs, indicating their limitations. This underscores the need for a specialized SSM tuning approach. To address this, we propose Sparse Dimension Tuning (SDT), a PEFT method tailored for SSM modules. Combining SDT for SSMs with LoRA for linear projection matrices, we achieve state-of-the-art performance across extensive experiments.","AI chatbots like ChatGPT rely on large models that are expensive to adapt to new tasks or update with new knowledge. To reduce this cost, researchers have developed tuning methods that adjust only a small subset of the model—a strategy that works well for traditional model architectures. However, these traditional models can be slow when processing long texts. A newer type of model architecture handles long inputs much more efficiently, but it is unclear whether existing tuning methods are still effective. We evaluated these methods on the new models and identified their strengths and weaknesses. Based on our findings, we developed a new tuning method tailored to this faster architecture. Our approach makes these models easier and more cost-effective to adapt."
Poster,Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models,https://ICML.cc//virtual/2025/poster/44214,"Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Ali, Joshua M Susskind, Vimal Thilak","Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Expert models (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the ratio of non-active to total parameters, affects model performance in terms of both pretraining and downstream performance. We find that under different constraints (e.g. parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance.These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.","(1) The capacity of language models to fit the training data depends on both the size of language models as well as the amount of compute spent per example. While it is known that increasing the size of the language leads to improved performance, less is known about how to tradeoff the two factors listed above in an optimal manner. We study this trade-off  using the popular Mixture-of-Experts (MoE) models which can have many parameters but only use some of them for each input token. The ratio of active to total parameter is called sparsity. (2) We conduct a large scae empirical study to systematically study how the sparsity level can be set in an optimal manner under a variety of settings including fixed training budget and fixed model size.(3) We find that total parameter count plays a significant role during training under a fixed compute budget while the compute per token may have a bigger influence on downstream task performance."
Poster,Parametric Scaling Law of Tuning Bias in Conformal Prediction,https://ICML.cc//virtual/2025/poster/44287,"Hao Zeng, Kangdao Liu, Bingyi Jing, Hongxin Wei","Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees. To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional hold-out set for parameter tuning. Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound. In the end, we discuss how to reduce the tuning bias, guided by the theories we developed.","Machine learning methods like conformal prediction promise reliable uncertainty estimates, but many require an additional dataset for tuning parameters, which is costly when data is limited. Reusing the same data for tuning and calibration can create a ""tuning bias,"" potentially compromising these reliability guarantees. Our research investigated this tuning bias, revealing empirically that it is often negligible for simple parameter tuning. We discovered a ""parametric scaling law"": this bias increases as the complexity of the tuning process (e.g., more parameters) grows, but decreases as the size of the calibration dataset increases. We then established a theoretical framework to quantify this tuning bias and rigorously prove its scaling behavior. This work is crucial for data-scarce applications, offering insights into when reusing data is permissible and guiding the design of more data-efficient and trustworthy AI systems without needing an extra hold-out set."
Poster,Pareto-frontier Entropy Search with Variational Lower Bound Maximization,https://ICML.cc//virtual/2025/poster/46222,"Masanori Ishikura, Masayuki Karasuyama","This study considers multi-objective Bayesian optimization (MOBO) through the information gain of the Pareto-frontier. To calculate the information gain, a predictive distribution conditioned on the Pareto-frontier plays a key role, which is defined as a distribution truncated by the Pareto-frontier. However, it is usually impossible to obtain the entire Pareto-frontier in a continuous domain, and therefore, the complete truncation cannot be known. We consider an approximation of the truncated distribution by using a mixture distribution consisting of two possible approximate truncations obtainable from a subset of the Pareto-frontier, which we call over- and under-truncation. Since the optimal balance of the mixture is unknown beforehand, we propose optimizing the balancing coefficient through the variational lower bound maximization framework, by which the approximation error of the information gain can be minimized. Our empirical evaluation demonstrates the effectiveness of the proposed method particularly when the number of objective functions is large.","In many real-world scenarios, we need to find the best balance between multiple goals such as maximizing performance while minimizing cost. This is known as the multi-objective optimization, and it is especially challenging when we do not know how these goals trade-off with each other. Our research addresses this challenge by using an approach called information-theoretic Bayesian optimization, which helps efficiently find a set of the best trade-off solutions. A key idea of this method involves evaluating how much information we can gain when new data is observed, but the calculation of this information gain is computationally difficult particularly when the candidate space is continuous. To overcome this difficulty, we developed a novel approach to approximating the information gain, by combining two approximation schemes of the so-called ``Pareto-frontier''. Further, we improved approximation accuracy by automatically adjusting this combination using a framework called variational inference. Our technique improved performance of information-theoretic Bayesian optimization especially when optimizing many objectives. This research can help improve decision-making in fields like engineering design, materials science, and hyper-parameter optimization of machine leaning."
Poster,Pareto Merging: Multi-Objective Optimization for Preference-Aware Model Merging,https://ICML.cc//virtual/2025/poster/46024,"Weiyu CHEN, James Kwok","Model merging, which combines multiple models into a single model, has gained popularity in recent years. By efficiently integrating the capabilities of various models, this significantly reduces the parameter count and memory usage. However, current methods can only produce one single merged model. This necessitates a performance trade-off due to conflicts among the various models, and the resultant one-size-fits-all model may not align with the preferences of different users who may prioritize certain models over others. To address this issue, we propose preference-aware model merging, and formulate this as a multi-objective optimization problem in which the performance of the merged model on each base model's task is treated as an objective. In a single merging process, the proposed parameter-efficient structure generates a Pareto set of merged models, with each representing a Pareto-optimal solution for a preference. Users can then select merged models tailored to their preferences from this learned Pareto set. Experimental results demonstrate that the proposed Pareto Merging produces diverse trade-off models and achieves higher test accuracy compared to state-of-the-art merging baselines.","Combining multiple deep learning models into one is a popular way to save computing resources and memory. The challenge is that current methods typically produce a single, ""one-size-fits-all"" merged model. This often means sacrificing performance on some tasks, and the final model may not align with what different users actually need, especially if they prioritize certain original models' abilities.To address this, we introduce ""Preference-Aware Model Merging."" Our approach doesn't just create one merged model; instead, it efficiently generates a collection of diverse merged models in a single process. Each model in this collection offers a unique, optimal trade-off, reflecting different possible user preferences for the strengths of the original AIs.This allows users to select a merged model from this collection that is best suited to their specific requirements. Our experiments demonstrate that this method provides a wider variety of effective models and achieves better overall performance compared to existing model merging techniques."
Poster,Pareto-Optimal Fronts for Benchmarking Symbolic Regression Algorithms,https://ICML.cc//virtual/2025/poster/44132,"Kei Sen Fong, Mehul Motani","Symbolic Regression (SR) algorithms select expressions based on prediction performance while also keeping the expression lengths short to produce explainable white box models. In this context, SR algorithms can be evaluated by measuring the extent to which the expressions discovered are Pareto-optimal, in the sense of having the best R-squared score for a given expression length. This evaluation is most commonly done based on relative performance, in the sense that an SR algorithm is judged on whether it Pareto-dominates other SR algorithms selected in the analysis, without any indication on efficiency or attainable limits. In this paper, we explore absolute Pareto-optimal (APO) solutions instead, which have the optimal tradeoff between the multiple SR objectives, for 34 datasets in the widely-used SR benchmark, SRBench, by performing exhaustive search. Additionally, we include comparisons between eight numerical optimization methods. We extract, for every dataset, an APO front of expressions that can serve as a universal baseline for SR algorithms that informs researchers of the best attainable performance for selected sizes. The APO fronts provided serves as an important benchmark and performance limit for SR algorithms and is made publicly available at: https://github.com/kentridgeai/SRParetoFronts","Symbolic regression is a field of machine learning that learns concise mathematical expressions for prediction. Most symbolic regression studies compare algorithms against each other, but it can be hard to know how close any of them get to the best possible formulas. In this work, we exhaustively search over simple mathematical expressions (up to a fixed size) for over 30 commonly used symbolic regression datasets to identify the absolute ‘best-accuracy-for-a-given-length’ trade-off, which we call the Pareto optimal front. This means that, for each allowed formula size, we aim to find an expression that maximizes predictive accuracy, rather than just outperforming other existing methods. We also examine how different numerical optimizers (e.g., BFGS, Powell, etc.) affect these results and show that, in practice, they produce very similar outcomes. By sharing these fronts, we hope to offer a helpful reference: researchers can see how close their own methods come to these best attainable targets (with some caveats) for short, interpretable equations. In this way, our work provides a clear baseline for benchmarking and suggests where there is still room to explore and improve symbolic regression approaches."
Poster,"Pareto-Optimality, Smoothness, and Stochasticity in Learning-Augmented One-Max-Search",https://ICML.cc//virtual/2025/poster/44853,"Ziyad Benomar, Lorenzo Croissant, Vianney Perchet, Spyros Angelopoulos","One-max search is a classic problem in online decision-making, in which a trader acts on a sequence of revealed prices and accepts one of them irrevocably to maximise its profit. The problem has been studied both in probabilistic and in worst-case settings, notably through competitive analysis, and more recently in learning-augmented settings in which the trader has access to a prediction on the sequence. However, existing approaches either lack smoothness, or do not achieve optimal worst-case guarantees: they do not attain the best possible trade-off between the consistency and the robustness of the algorithm. We close this gap by presenting the first algorithm that simultaneously achieves both of these important objectives. Furthermore, we show how to leverage the obtained smoothness to provide an analysis of one-max search in stochastic learning-augmented settings which capture randomness in both the observed prices and the prediction.","Consider a trader who owns a single item and observes its price evolving over time. Each day, the trader sees the current price and must decide whether to sell the item or wait, aiming to maximize the final profit. This decision problem, known as One-Max Search, is challenging because future prices are unknown.With recent progress in machine learning and forecasting, it is now possible to obtain predictions about future prices. While these predictions can offer valuable guidance, they are often imperfect, and the trader has no reliable way to assess their accuracy in advance. Prior research has developed algorithms that leverage such predictions, performing well when the forecast is accurate and remaining robust when it is not. However, these algorithms are highly sensitive to small prediction errors, which can lead to significant performance degradation.In this work, we introduce a new algorithm that retains the strengths of earlier approaches while improving the dependence on prediction accuracy. Additionally, our method demonstrates enhanced performance in settings where both prices and predictions exhibit stochastic behavior."
Poster,PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model,https://ICML.cc//virtual/2025/poster/43455,"Baijiong Lin, Weisen Jiang, Yuancheng Xu, Hao Chen, YINGCONG CHEN","Multi-objective test-time alignment aims to adapt large language models (LLMs) to diverse multi-dimensional user preferences during inference while keeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently trains Autoregressive Reward Models (ARMs) for each preference dimension without awareness of each other, then combines their outputs based on user-specific preference vectors during inference to achieve multi-objective test-time alignment, leading to two key limitations: the need for *multiple* ARMs increases the inference cost, and the *separate* training of ARMs causes the misalignment between the guided generation and the user preferences.To address these issues, we propose Preference-aware ARM (PARM), a *single* unified ARM trained across *all* preference dimensions. PARM uses our proposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs a bilinear form to condition the ARM on preference vectors, enabling it to achieve precise control over preference trade-offs during inference.Experiments demonstrate that PARM reduces inference costs and achieves better alignment with preference vectors compared with existing methods. Additionally, PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger frozen LLM without expensive training, making multi-objective alignment accessible with limited computing resources.The code is available at https://github.com/Baijiong-Lin/PARM.","Large language models (LLMs) are incredibly powerful but often need to be fine-tuned to align with specific user preferences, such as being helpful, harmless, or humorous. Current methods require fine-tuning the entire LLM or training multiple reward models for different preferences, which is computationally expensive and leads to misalignment between the LLM's generation and the user preferences.We developed PARM, a Preference-Aware Autoregressive Reward Model, which is a single unified model used to dynamically adjust LLM's responses based on user preferences during inference while keeping LLMs frozen. PARM employs a technique called Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA) to condition the model on a preference vector, allowing it to balance different preferences during inference without retraining.PARM significantly reduces the computational cost and improves the alignment between LLM's generations and user preferences. It enables weak-to-strong guidance, allowing a smaller PARM to guide a larger frozen LLM without expensive training. This makes multi-objective alignment accessible with limited computing resources, making AI assistants more versatile and efficient."
Poster,PARQ: Piecewise-Affine Regularized Quantization,https://ICML.cc//virtual/2025/poster/46282,"Lisa Jin, Jianhao Ma, Zechun Liu, Andrey Gromov, Aaron Defazio, Lin Xiao","We develop a novel optimization method for quantization-aware training (QAT). Specifically, we show that convex, piecewise-affine regularization (PAR) can effectively induce neural network weights to cluster towards discrete values. We minimize PAR-regularized loss functions using an aggregate proximal stochastic gradient method (AProx) and prove that it enjoys last-iterate convergence. Our approach provides an interpretation of the straight-through estimator (STE), a widely used heuristic for QAT, as the asymptotic form of PARQ. We conduct experiments to demonstrate that PARQ obtains competitive performance on convolution- and transformer-based vision tasks.","As modern AI models exhibit exceptional vision and language processing capabilities, but often come with excessive sizes and demands on memory and computing. Quantization is an effective approach for model compression, which can significantly reduce their memory footprint, computing cost, as well as the latency for inference. Existing methods for quantization-aware training heavily rely on heuristics and have weak convergence guarantees. We propose a new algorithms based on a rigorous optimization framework, which gives a principled interpretation of a widely used heuristic. In addition, it obtains competitive empirical performance and also enjoys strong convergence guarantees."
Poster,Parrot: Multilingual Visual Instruction Tuning,https://ICML.cc//virtual/2025/poster/44886,"Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye","The rapid development of Multimodal Large Language Models (MLLMs), such as GPT-4, marks a significant step toward artificial general intelligence. Existing methods typically align vision encoders with LLMs via supervised fine-tuning (SFT), but this often deteriorates their ability to handle multiple languages as training progresses. We empirically observe that imbalanced SFT datasets, largely English-centric, degrade performance on non-English languages due to the failure in multilingual token alignment. To address this, we propose Parrot, a novel approach that leverages textual guidance for visual token alignment at the language level. Parrot conditions visual tokens on diverse language inputs and uses Mixture-of-Experts (MoE) to align multilingual tokens. By computing cross-attention between initial visual features and textual embeddings, we select the most relevant experts, converting visual tokens into language-specific representations. Additionally, we introduce the Massive Multilingual Multimodal Benchmark (MMMB), a new benchmark comprising 6 languages, 15 categories, and 12,000 questions, to assess multilingual capabilities. Parrot achieves state-of-the-art performance on both the multilingual benchmarks and a wide range of multimodal tasks. Code and dataset are available at: \url{https://github.com/AIDC-AI/Parrot}.","Smart AI systems that can understand both images and text (like the technology behind GPT-4) are becoming very powerful. However, when we teach these systems to connect images with words, they often get worse at understanding languages other than English. This is usually because most of the learning materials are in English, making the AI biased.We've created a new method called Parrot. Imagine Parrot looks at a picture; our system helps it understand that picture not just with English descriptions but also with guidance from many other languages. It’s like having different language specialists inside the AI, ensuring it correctly links the image's content to words in various languages.Parrot significantly improves how well these AI models understand images and text in multiple languages. This is vital for creating AI tools that are fair and useful for people all over the world, no matter what language they speak. To prove Parrot skills, we also built a new challenging test with questions in six different languages."
