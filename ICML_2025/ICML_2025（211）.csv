type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Open Your Eyes: Vision Enhances Message Passing Neural Networks in Link Prediction,https://ICML.cc//virtual/2025/poster/46454,"Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, James Kwok, Yu Zhang","Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called Graph Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.","A new approach called Graph Vision Network (GVN) is changing how we predict connections in networks, like social networks or web links. Typically, these predictions rely on analyzing the structure of the network, but GVN introduces a fresh perspective by using visual insights to improve accuracy. This means it looks at the network in a way similar to how we interpret visual information, making predictions more reliable. Tests across different types of networks show that GVN consistently outperforms older methods and sets new standards for accuracy. This development opens up exciting new possibilities for using visual understanding in network analysis, making it a promising area for future research and practical applications."
Poster,Optimal Algorithm for Max-Min Fair Bandit,https://ICML.cc//virtual/2025/poster/44421,"Zilong Wang, Zhiyao Zhang, Shuai Li","Multi-player multi-armed bandit (MP-MAB) has been widely studied owing to its diverse applications across numerous domains. We consider an MP-MAB problem where $N$ players compete for $K$ arms in $T$ rounds. The reward distributions are heterogeneous where each player has a different expected reward for the same arm. When multiple players select the same arm, they collide and obtain zero rewards. In this paper, our target is to find the max-min fairness matching that maximizes the reward of the player who receives the lowest reward. This paper improves the existing max-min regret upper bound of $O(\exp(1/\Delta) + K^3 \log T\log \log T)$. More specifically, our decentralized fair elimination algorithm (DFE) deals with heterogeneity and collision carefully and attains a regret upper bound of $O((N^2+K)\log T / \Delta)$, where $\Delta$ is the minimum reward gap between max-min value and sub-optimal arms.In addition, this paper also provides an $\Omega(\max\\{ N^2, K \\} \log T / \Delta)$ regret lower bound for this problem, which indicates that our algorithm is optimal with respect to key parameters $T, N, K$, and $\Delta$. Additional numerical experiments also show the efficiency and improvement of our algorithms.","Multi-player multi-armed bandit (MP-MAB) has been widely studied owing to its diverse applications across numerous domains. We consider an MP-MAB problem where $N$ players compete for $K$ arms in $T$ rounds. The reward distributions are heterogeneous where each player has a different expected reward for the same arm. When multiple players select the same arm, they collide and obtain zero rewards. In this paper, our target is to find the max-min fairness matching that maximizes the reward of the player who receives the lowest reward. This paper improves the existing max-min regret upper bound of $O(\exp(1/\Delta) + K^3 \log T\log \log T)$. More specifically, our decentralized fair elimination algorithm (DFE) deals with heterogeneity and collision carefully and attains a regret upper bound of $O((N^2+K)\log T / \Delta)$, where $\Delta$ is the minimum reward gap between max-min value and sub-optimal arms.In addition, this paper also provides an $\Omega(\max\\{N^2, K\\} \log T / \Delta)$ regret lower bound for this problem, which indicates that our algorithm is optimal with respect to key parameters $T, N, K$, and $\Delta$. Additional numerical experiments also show the efficiency and improvement of our algorithms."
Poster,Optimal and Practical Batched Linear Bandit Algorithm,https://ICML.cc//virtual/2025/poster/45005,"Sanghoon Yu, Min-hwan Oh","We study the linear bandit problem under limited adaptivity, known as the batched linear bandit. While existing approaches can achieve near-optimal regret in theory, they are often computationally prohibitive or underperform in practice. We propose BLAE, a novel batched algorithm that integrates arm elimination with regularized G-optimal design, achieving the minimax optimal regret (up to logarithmic factors in $T$) in both large-$K$ and small-$K$ regimes for the first time, while using only $O(\log\log T)$ batches. Our analysis introduces new techniques for batch-wise optimal design and refined concentration bounds. Crucially, BLAE demonstrates low computational overhead and strong empirical performance, outperforming state-of-the-art methods in extensive numerical evaluations. Thus, BLAE is the first algorithm to combine provable minimax-optimality in all regimes and practical superiority in batched linear bandits.","Many real-world tasks involve repeatedly making decisions to achieve the best possible outcomes, such as recommending products online, selecting medical treatments in clinical trials, or dynamically setting prices for online shoppers. These problems often involve balancing the exploration of new options with the exploitation of already-known successful strategies. However, updating these decision-making systems too frequently can be impractical due to costs, ethical issues, or computational limitations.In this research, we tackle the challenge of making good decisions when updates can only happen occasionally—in batches rather than continuously. Existing methods that work well theoretically either fail in practice or require too much computation. We propose a new method, called BLAE, that carefully eliminates poor-performing options while selecting promising ones efficiently. Our method achieves optimal performance guarantees, meaning it performs nearly as well as theoretically possible under any conditions. Crucially, BLAE also performs strongly in real-world applications, requiring fewer updates and less computation.In short, our approach bridges the gap between theory and practice, providing an effective and practical solution for decision-making problems where frequent updates are not feasible."
Poster,Optimal Auction Design in the Joint Advertising,https://ICML.cc//virtual/2025/poster/46634,"Yang Li, Yuchao Ma, Qi Qi","Online advertising is a vital revenue source for major internet platforms. Recently, joint advertising, which assigns a bundle of two advertisers in an ad slot instead of allocating a single advertiser, has emerged as an effective method for enhancing allocation efficiency and revenue. However, existing mechanisms for joint advertising fail to realize the optimality, as they tend to focus on individual advertisers and overlook bundle structures. This paper identifies an optimal mechanism for joint advertising in a single-slot setting. For multi-slot joint advertising, we propose **BundleNet**, a novel bundle-based neural network approach specifically designed for joint advertising. Our extensive experiments demonstrate that the mechanisms generated by **BundleNet** approximate the theoretical analysis results in the single-slot setting and achieve state-of-the-art performance in the multi-slot setting. This significantly increases platform revenue while ensuring approximate dominant strategy incentive compatibility and individual rationality.","Online advertising is a vital revenue source for major internet platforms. Recently, joint advertising—placing a bundle of two advertisers in an ad slot instead of allocating a single advertiser —has emerged as an effective way to boost delivery efficiency and revenue. However, existing joint advertising mechanisms focus on individual advertisers while ignoring bundle structure , failing to achieve optimal revenue. Our research first identifies the optimal joint advertising mechanism for single-slot scenarios. For multi-slot scenarios, we propose BundleNet, a novel bundle-based neural network approach specifically designed for joint advertising. Experiments show that BundleNet approximates theoretical optimal solutions in single-slot settings and outperforms state-of-the-art methods in multi-slot settings. It significantly increases platform revenue while ensuring approximate dominant strategy incentive compatibility (to encourage truthful bidding) and individual rationality (to maintain advertiser participation)."
Poster,Optimal Decision Tree Pruning Revisited: Algorithms and Complexity,https://ICML.cc//virtual/2025/poster/44353,"Juha Harviainen, Frank Sommer, Manuel Sorge, Stefan Szeider","We present a comprehensive classical and parameterized complexity analysis of decision tree pruning operations, extending recent research on the complexity of learning small decision trees. Thereby, we offer new insights into the computational challenges of decision tree simplification, a crucial aspect of developing interpretable and efficient machine learning models. We focus on fundamental pruning operations of subtree replacement and raising, which are used in heuristics. Surprisingly, while optimal pruning can be performed in polynomial time for subtree replacement, the problem is NP-complete for subtree raising. Therefore, we identify parameters and combinations thereof that lead to fixed-parameter tractability or hardness, establishing a precise borderline between these complexity classes. For example, while subtree raising is hard for small domain size $D$ or number $d$ of features, it can be solved in $D^{2d} \cdot |I|^{O(1)}$ time, where $|I|$ is the input size. We complement our theoretical findings with preliminary experimental results, demonstrating the practical implications of our analysis.","Decision trees classify objects by asking a series of simple questions. The sequence of questions is determined by the answers to previous questions, creating a model with a hierarchical structure that is used to represent and organize data in the form of parent–child relationship. Traditional methods for building decision trees can result in an overly complicated structure, necessitating simplification by not asking some of the less relevant questions.However, identifying which questions to omit without compromising performance is challenging. Therefore, one typically performs educated guesses about which of them could be left out, lacking guarantees on whether the excluded questions were the best ones possible. We study which properties of decision trees can be exploited to achieve such guarantees. That is, we develop algorithms that determine the least relevant questions and are efficient if the initial tree has certain properties, or show that no efficient algorithms exist.Based on our discoveries, we create a proof-of-concept implementation that helps us to assess how well traditional methods perform on this problem when excluding a fixed number of questions. Surprisingly, we discover that they are nearly optimal despite the inherent hardness of the problem, providing new insight on the robustness of conventional methods."
Poster,Optimal Fair Learning Robust to Adversarial Distribution Shift,https://ICML.cc//virtual/2025/poster/45184,"Sushant Agarwal, Amit Jayant Deshpande, Rajmohan Rajaraman, Ravi Sundaram","Previous work in fair machine learning has characterised the Fair Bayes Optimal Classifier (BOC) on a given distribution for both deterministic and randomized classifiers. We study the robustness of the Fair BOC to adversarial noise in the data distribution. Kearns & Li (1988) implies that the accuracy of the deterministic BOC without any fairness constraints is robust (Lipschitz) to malicious noise in the data distribution. We demonstrate that their robustness guarantee breaks down when we add fairness constraints. Hence, we consider the randomized Fair BOC, and our central result is that its accuracy is robust to malicious noise in the data distribution. Our robustness result applies to various fairness constraints---Demographic Parity, Equal Opportunity, Predictive Equality. Beyond robustness, we demonstrate that randomization leads to better accuracy and efficiency. We show that the randomized Fair BOC is nearly-deterministic, and gives randomized predictions on at most one data point, hence availing numerous benefits of randomness, while using very little of it.","If an ML model has good fairness/accuracy metrics when trained in Hospital-A, it may not when tested in Hospital-B. Similarly, a model trained (subject to fairness constraints) in Hospital-A might have very different accuracy if trained in Hospital-B. The phenomenon of distribution shift is common in practice, where the training distribution differs from the testing distribution. We study how fair ML models behave under adversarial/arbitrary distribution shifts.We focus on the Bayes Optimal Classifier (BOC), i.e., the most accurate classifier among all possible classifiers on a given distribution. We consider the Fair-BOC, for both deterministic and randomised classifiers. For various group-fairness notions, we show that the randomised Fair-BOC is robust, i.e., given two similar distributions, the Fair-BOC on each distribution has similar accuracy. In contrast, this robustness guarantee breaks down for deterministic classifiers. Furthermore, we show the randomised Fair-BOC has better accuracy and efficiency than its deterministic counterpart. Our results illustrate various advantages of using randomised classifiers over deterministic ones. However, one must be cautious while introducing randomness in critical decisions. Fortunately, we show that the randomised Fair-BOC is nearly deterministic, being randomised on at most one point in the domain, and deterministic on the rest."
Poster,Optimal Information Retention for Time-Series Explanations,https://ICML.cc//virtual/2025/poster/43746,"Jinghang Yue, Jing Wang, Lu Zhang, Shuo Zhang, Da Li, Zhaoyang Ma, Youfang Lin","Explaining deep models for time-series data is crucial for identifying key patterns in sensitive domains, such as healthcare and finance. However, due to the lack of unified optimization criterion, existing explanation methods often suffer from redundancy and incompleteness, where irrelevant patterns are included or key patterns are missed in explanations. To address this challenge, we propose the Optimal Information Retention Principle, where conditional mutual information defines minimizing redundancy and maximizing completeness as optimization objectives. We then derive the corresponding objective function theoretically. As a practical framework, we introduce an explanation framework ORTE, learning a binary mask to eliminate redundant information while mining temporal patterns of explanations. We decouple the discrete mapping process to ensure the stability of gradient propagation, while employing contrastive learning to achieve precise filtering of explanatory patterns through the mask, thereby realizing a trade-off between low redundancy and high completeness. Extensive quantitative and qualitative experiments on synthetic and real-world datasets demonstrate that the proposed principle significantly improves the accuracy and completeness of explanations compared to baseline methods. The code is available at https://github.com/moon2yue/ORTE_public.","Explaining deep models for time series data is crucial in domains such as healthcare and finance. However, time-series explanation is often mixed with redundant information or incomplete key information. To solve this problem, we propose the principle of optimal information retention from the perspective of information theory, which is to minimize redundant information and maximize effective information. We further theoretically derive the objective functions based on this principle. As a practical framework, we propose an explanation framework ORTE, that captures temporal patterns by learning a mask matrix. We decouple the discrete mapping process to ensure the stability of gradient propagation. We use contrastive learning to achieve accurate filtering of interpretation patterns through masks, thus achieving a trade-off between low redundancy and high completeness. Extensive quantitative and qualitative experiments on synthetic and real-world datasets show that the proposed principles significantly improve the accuracy and completeness of interpretation compared to baseline methods."
Poster,Optimal Sensor Scheduling and Selection for Continuous-Discrete Kalman Filtering with Auxiliary Dynamics,https://ICML.cc//virtual/2025/poster/46077,"Mohamad Al Ahdab, john leth, Zheng-Hua Tan","We study the Continuous-Discrete Kalman Filter (CD-KF) for State-Space Models (SSMs) where continuous-time dynamics are observed via multiple sensors with discrete, irregularly timed measurements. Our focus extends to scenarios in which the measurement process is coupled with the states of an auxiliary SSM. For instance, higher measurement rates may increase energy consumption or heat generation, while a sensor’s accuracy can depend on its own spatial trajectory or that of the measured target. Each sensor thus carries distinct costs and constraints associated with its measurement rate and additional constraints and costs on the auxiliary state. We model measurement occurrences as independent Poisson processes with sensor-specific rates and derive an upper bound on the mean posterior covariance matrix of the CD-KF along the mean auxiliary state. The bound is continuously differentiable with respect to the measurement rates, which enables efficient gradient-based optimization. Exploiting this bound, we propose a finite-horizon optimal control framework to optimize measurement rates and auxiliary-state dynamics jointly. We further introduce a deterministic method for scheduling measurement times from the optimized rates. Empirical results in state-space filtering and dynamic temporal Gaussian process regression demonstrate that our approach achieves improved trade-offs between resource usage and estimation accuracy.","Many physical quantities change continuously over time (e.g., blood pressure, ocean temperature, radiation). To track these quantities, we often rely on multiple devices with different accuracies and operating conditions. For instance, imagine a low‐Earth‐orbit satellite observing terrestrial phenomena. Its sensors’ accuracy depends on the satellite’s orbital position. This satellite might carry high‐resolution sensors that perform best in sunlight and radar sensors that can operate anytime with consistent accuracy, but consume more energy and are less accurate. In our paper, we introduce an efficient method to decide both when to take measurements and which sensor to use, while also accounting for other changing factors (e.g., daylight, orbital position, and stored energy). We demonstrate our approach on several examples and show how it effectively balances measurement needs with resource constraints in this type of setting."
Poster,Optimal Survey Design for Private Mean Estimation,https://ICML.cc//virtual/2025/poster/46542,"Yu-Wei Chen, Raghu Pasupathy, Jordan A Awan","This work identifies the first privacy-aware stratified sampling scheme that minimizes the variance for general private mean estimation under the Laplace, Discrete Laplace (DLap) and Truncated-Uniform-Laplace (TuLap) mechanisms within the framework of differential privacy (DP). We view stratified sampling as a subsampling operation, which amplifies the privacy guarantee; however, to have the same final privacy guarantee for each group, different nominal privacy budgets need to be used depending on the subsampling rate. Ignoring the effect of DP, traditional stratified sampling strategies risk significant variance inflation. We phrase our optimal survey design as an optimization problem, where we determine the optimal subsampling sizes for each group with the goal of minimizing the variance of the resulting estimator. We establish strong convexity of the variance objective, propose an efficient algorithm to identify the integer-optimal design, and offer insights on the structure of the optimal design.","This work develops a new survey method for collecting social data that protects individual privacy while ensuring the most accurate results. By carefully choosing how many responses to gather from each group of people, the method provides an optimal estimate of the private mean."
Poster,Optimal Task Order for Continual Learning of Multiple Tasks,https://ICML.cc//virtual/2025/poster/46654,"Ziyan Li, Naoki Hiratani","Continual learning of multiple tasks remains a major challenge for neural networks. Here, we investigate how task order influences continual learning and propose a strategy for optimizing it. Leveraging a linear teacher-student model with latent factors, we derive an analytical expression relating task similarity and ordering to learning performance. Our analysis reveals two principles that hold under a wide parameter range: (1) tasks should be arranged from the least representative to the most typical, and (2) adjacent tasks should be dissimilar. We validate these rules on both synthetic data and real-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating consistent performance improvements in both multilayer perceptrons and convolutional neural networks. Our work thus presents a generalizable framework for task-order optimization in task-incremental continual learning.","Neural networks often forget old skills when they learn new ones, a problem called “catastrophic forgetting.” This study shows that the order in which a network learns its tasks can make a big difference. By analyzing a simple mathematical model and then testing real image-recognition problems, we discovered two practical rules: start with the tasks that look least like the average of the whole set, and avoid placing two very similar tasks back-to-back. Following these guidelines consistently boosted accuracy on datasets such as Fashion-MNIST and CIFAR-10/100, even when the rules were estimated from just a tiny fraction of data. The work offers a recipe for arranging learning tasks so that AI systems retain old knowledge while smoothly acquiring new skills."
