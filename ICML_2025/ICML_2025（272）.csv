type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Self-Consuming Generative Models with Adversarially Curated Data,https://ICML.cc//virtual/2025/poster/45113,"Xiukun Wei, Xueru Zhang","Recent advances in generative models have made it increasingly difficult to distinguish real data from model-generated synthetic data. Using synthetic data for successive training of future model generations creates “self-consuming loops,” which may lead to model collapse or training instability. Furthermore, synthetic data is often subject to human feedback and curated by users based on their preferences. Ferbach et al. (2024) recently showed that when data is curated according to user preferences, the self-consuming retraining loop drives the model to converge toward a distribution that optimizes those preferences. However, in practice, data curation is often noisy or adversarially manipulated. For example, competing platforms may recruit malicious users to adversarially curate data and disrupt rival models. In this paper, we study how generative models evolve under self-consuming retraining loops with noisy and adversarially curated data. We theoretically analyze the impact of such noisy data curation on generative models and identify conditions for the robustness and stability of the retraining process. Building on this analysis, we design attack algorithms for competitive adversarial scenarios, where a platform with a limited budget employs malicious users to misalign a rival’s model from actual user preferences. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithms.","Generative AI models can now create convincing images, text, and videos. They improve over time by retraining on new content, which often scraped from the internet or collected through human feedback. This creates a “self-consuming feedback loop”, where each new model learns from synthetic data generated by earlier versions of itself, often curated based on user preferences. We began this research out of concern that this loop could be disrupted if people intentionally provide misleading feedback.To explore this risk, we imagined a scenario in which two AI companies compete, and one tries to sabotage the other by manipulating its feedback. For example, it might hire people to repeatedly “like” low-quality content from the rival system. Over time, this misleads the model into learning incorrect preferences and drifting away from what real users actually want.We studied how such sabotage could happen and under what conditions models can resist it. We also designed attack strategies and tested them through experiments using both synthetic and real-world data. Our results show that even small, targeted manipulations can gradually misdirect a model. As generative AI becomes more common and self-reinforcing, our work underscores the importance of understanding these risks and developing safeguards against malicious feedback."
Poster,Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning,https://ICML.cc//virtual/2025/poster/43953,"Qi Xu, Junyang Zhu, Dongdong Zhou, Hao Chen, Yang Liu, Jiangrong Shen, Qiang Zhang","Deep neural networks (DNNs) excel in computer vision tasks, especially,  few-shot learning (FSL), which is increasingly important for generalizing from limited examples. However, DNNs are computationally expensive with scalability issues in real world. Spiking Neural Networks (SNNs), with their event-driven nature and low energy consumption, are particularly efficient in processing sparse and dynamic data, though they still encounter difficulties in capturing complex spatiotemporal features and performing accurate cross-class comparisons. To further enhance the performance and efficiency of SNNs in few-shot learning, we propose a few-shot learning framework based on SNNs, which combines a self-feature extractor module and a cross-feature contrastive module to refine feature representation and reduce power consumption. We apply the combination of temporal efficient training loss and InfoNCE loss to optimize the temporal dynamics of spike trains and enhance the discriminative power. Experimental results show that the proposed FSL-SNN significantly improves the classification performance on the neuromorphic dataset N-Omniglot, and also achieves competitive performance to ANNs on static datasets such as CUB and miniImageNet with low power consumption.","Teaching AI to learn from just a few examples—like recognizing a rare bird species from one or two photos—is extremely useful but also very hard. Most powerful AI models today need thousands of images and lots of energy to perform well. Our work uses a different kind of brain-inspired AI called Spiking Neural Networks (SNNs), which are more energy-efficient and mimic how real neurons work.We created a new system that helps SNNs learn better from small amounts of data by using two tricks: one that helps the model understand the details within each image, and another that helps it compare across different classes. We also improved the way the model learns over time, making it more accurate and robust, even when the data is noisy.Our method sets a new performance record for SNNs on a challenging dataset and performs nearly as well as traditional methods on popular benchmarks—while using much less energy. This brings us a step closer to smarter, low-power AI that can work in real-world settings like wearable devices, robots, or environmental monitors."
Poster,Self-Discriminative Modeling for Anomalous Graph Detection,https://ICML.cc//virtual/2025/poster/44215,"Jinyu Cai, Yunhe Zhang, Jicong Fan","Identifying anomalous graphs is essential in real-world scenarios such as molecular and social network analysis, yet anomalous samples are generally scarce and unavailable. This paper proposes a Self-Discriminative Modeling (SDM) framework that trains a deep neural network only on normal graphs to detect anomalous graphs. The neural network simultaneously learns to construct pseudo-anomalous graphs from normal graphs and learns an anomaly detector to recognize these pseudo-anomalous graphs. As a result, these pseudo-anomalous graphs interpolate between normal graphs and real anomalous graphs, which leads to a reliable decision boundary of anomaly detection. In this framework, we develop three algorithms with different computational efficiencies and stabilities for anomalous graph detection. Extensive experiments on 12 different graph benchmarks demonstrated that the three variants of SDM consistently outperform the state-of-the-art GLAD baselines. The success of our methods stems from the integration of the discriminative classifier and the well-posed pseudo-anomalous graphs, which provided new insights for graph-level anomaly detection.","Graphs are widely used to represent complex relationships, such as connections in social networks or molecular interactions in chemistry. Sometimes, unusual or anomalous graphs appear, which indicate problems like fraudulent activity or unique chemical properties. Detecting these anomalous graphs is crucial, yet it is challenging because they are rare and often unknown in advance. Our research introduces a graph-level anomaly detection method, which aims to detect anomalous graphs by only studying normal graphs. We achieve this via generating auxiliary examples of slight disturbances on the normal graphs, creating learning opportunities to distinguish normal graphs from these ""pseudo"" anomalies. These self-generated examples help the model refine a more reliable decision boundary between normal and anomalous data. Our experiments show that our method significantly outperforms current state-of-the-art baselines, offering a new insight to detect these rare but critical anomalies in various real-world scenarios."
Poster,Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation,https://ICML.cc//virtual/2025/poster/45574,"Jintao Tong, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li","Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a large-scale source-domain dataset to unseen target-domain datasets with limited annotated samples. Current methods typically compare the distance between training and testing samples for mask prediction. However, a problem of feature entanglement exists in this well-adopted method, which binds multiple patterns together and harms the transferability. However, we find an entanglement problem exists in this widely adopted method, which tends to bind source-domain patterns together and make each of them hard to transfer. In this paper, we aim to address this problem for the CD-FSS task. We first find a natural decomposition of the ViT structure, based on which we delve into the entanglement problem for an interpretation. We find the decomposed ViT components are crossly compared between images in distance calculation, where the rational comparisons are entangled with those meaningless ones by their equal importance, leading to the entanglement problem. Based on this interpretation, we further propose to address the entanglement problem by learning to weigh for all comparisons of ViT components, which learn disentangled features and re-compose them for the CD-FSS task, benefiting both the generalization and finetuning. Experiments show that our model outperforms the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under 1-shot and 5-shot settings, respectively.","In many image tasks, like medical imaging or satellite analysis, we often face a challenge: we want to train computers to understand images in one domain (like pictures of dogs) and then apply that knowledge to a very different domain (like X-rays or crop fields), but with only a few examples. This is known as Cross-Domain Few-Shot Segmentation (CD-FSS). Most current approaches try to compare features from training and testing images directly, but we found this often mixes up different patterns from the original training set, making it hard for the model to adapt to new tasks — a problem called “feature entanglement.” To solve this, we take a closer look at how modern vision models (specifically, Vision Transformers) break down image information internally. We discovered a way to measure which parts of the image comparison are meaningful and which are just noise. By teaching the model to weigh these comparisons differently, we help it learn cleaner, more transferable features. Our method improves performance on challenging CD-FSS tasks and outperforms leading models by nearly 2% in accuracy."
Poster,Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI,https://ICML.cc//virtual/2025/poster/43499,"Julien Pourcel, Cédric Colas, Pierre-Yves Oudeyer","Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model. We propose SOAR, a method that learns program synthesis by integrating language models into a self-improving evolutionary loop. SOAR alternates between (1) an evolutionary search that uses an LLM to sample and refine candidate solutions, and (2) a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilities—enabling increasingly effective search in subsequent iterations.On the challenging ARC-AGI benchmark, SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52\% of the public test set.","Creating computer programs automatically, a task known as program synthesis, is often too difficult for even the most advanced language models to accomplish in one go. While search-based methods that try out many possibilities offer an alternative, they are held back by the unchanging abilities of the AI model they rely on.We introduce SOAR, a new method that helps language models learn to write programs more effectively. SOAR works by having the language model go through a two-step process repeatedly. First, it searches for and improves potential program solutions. Second, it learns from these attempts, using them as examples to fine-tune its own ability to find and refine solutions in the future. This creates a cycle where the AI continuously gets better at the task.When tested on ARC-AGI, a challenging benchmark, SOAR showed significant improvements. The AI's ability to both generate initial program ideas and to refine them got better with each cycle. This allowed SOAR to successfully solve 52% of the publicly available test tasks, demonstrating its effectiveness."
Poster,Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges,https://ICML.cc//virtual/2025/poster/44828,"Nayoung Lee, Jack Cai, Avi Schwarzschild, Kangwook Lee, Dimitris Papailiopoulos","Large language models often struggle with length generalization and solving complex problem instances beyond their training distribution. We present a self-improvement approach where models iteratively generate and learn from their own solutions, progressively tackling harder problems while maintaining a standard transformer architecture. Across diverse tasks including arithmetic, string manipulation, and maze solving, our method enables models to solve problems far beyond their initial training distribution—for instance, generalizing from 10-digit to 100-digit addition without apparent saturation. We observe that filtering for correct self-generated examples leads to exponential improvements in out-of-distribution performance across training rounds. Additionally, starting from pretrained models significantly accelerates this self-improvement process for several tasks. Our results demonstrate how controlled weak-to-strong curricula can systematically expand model capabilities while preserving architectural simplicity.","Large language models excel at tasks they were trained on, but often struggle with harder or longer ones. We explore whether a model can improve by learning from its own outputs—and find that it can. Starting with easy problems, the model gradually solves harder and longer tasks by training on its own generated data, without any changes to its model architecture.We demonstrate this on structured tasks like arithmetic, string copying, and mazes. For example, the model progresses from 10-digit to 100-digit addition through self-improvement. Simple filtering methods, like length thresholds and majority voting, ensure data quality. We further show that pre-trained models can learn faster with this method."
Poster,Self-Organizing Visual Prototypes for Non-Parametric Representation Learning,https://ICML.cc//virtual/2025/poster/45509,"Thalles Silva, Helio Pedrini, Adín Ramírez Rivera","We present Self-Organizing Visual Prototypes (SOP), a new training technique for unsupervised visual feature learning. Unlike existing prototypical self-supervised learning (SSL) methods that rely on a single prototype to encode all relevant features of a hidden cluster in the data, we propose the SOP strategy. In this strategy, a prototype is represented by many semantically similar representations, or support embeddings (SEs), each containing a complementary set of features that together better characterize their region in space and maximize training performance. We reaffirm the feasibility of non-parametric SSL by introducing novel non-parametric adaptations of two loss functions that implement the SOP strategy. Notably, we introduce the SOP Masked Image Modeling (SOP-MIM) task, where masked representations are reconstructed from the perspective of multiple non-parametric local SEs. We comprehensively evaluate the representations learned using the SOP strategy on a range of benchmarks, including retrieval, linear evaluation, fine-tuning, and object detection. Our pre-trained encoders achieve state-of-the-art performance on many retrieval benchmarks and demonstrate increasing performance gains with more complex encoders.","Current computer vision systems often use a large set of prototypes to help computers learn from unlabeled pictures, but this approach can miss important details. We developed a new method that lets computers learn from many similar images at once, capturing richer and more accurate information. This makes learning from unlabeled data more reliable and could improve technologies like medical imaging, search engines, and self-driving cars."
Poster,Self-Play $Q$-Learners Can Provably Collude in the Iterated Prisoner's Dilemma,https://ICML.cc//virtual/2025/poster/43962,"Quentin Bertrand, Juan Duque, Emilio Calvano, Gauthier Gidel","A growing body of computational studies shows that simple machine learning agents converge to cooperative behaviors in social dilemmas, such as collusive price-setting in oligopoly markets, raising questions about what drives this outcome. In this work, we provide theoretical foundations for this phenomenon in the context of self-play multi-agent Q-learners in the iterated prisoner’s dilemma. We characterize broad conditions under which such agents provably learn the cooperative Pavlov (win-stay, lose-shift) policy rather than the Pareto-dominated “always defect” policy. We validate our theoretical results through additional experiments, demonstrating their robustness across a broader class of deep learning algorithms.","As artificial intelligence (AI) becomes more common in business and online marketplaces, some experts worry that smart computer programs might start cooperating in ways that hurt consumers—like raising prices together without directly communicating. This paper looks into that concern by studying how AI agents can learn to work together, even when they’re supposed to be competing.We focused on a classic decision-making scenario called the ""prisoner's dilemma,"" which is often used to study cooperation. We found that when two AI agents repeatedly play this game and use a specific type of learning called ""Q-learning,"" they can end up cooperating with each other. In fact, they often learn a cooperative strategy called ""Pavlov"" (or ""win-stay, lose-shift""), where they stick with what works and change when it doesn’t.The key finding is that under certain learning conditions, these AI agents can consistently learn to cooperate—even without being explicitly programmed to do so. This shows that AI systems can develop cooperative behavior on their own, which could have important consequences for markets and regulations."
Poster,Self-supervised Adversarial Purification for Graph Neural Networks,https://ICML.cc//virtual/2025/poster/43540,"Woohyun Lee, Hogun Park","Defending Graph Neural Networks (GNNs) against adversarial attacks requires balancing accuracy and robustness, a trade-off often mishandled by traditional methods like adversarial training that intertwine these conflicting objectives within a single classifier. To overcome this limitation, we propose a self-supervised adversarial purification framework. We separate robustness from the classifier by introducing a dedicated purifier, which cleanses the input data before classification. In contrast to prior adversarial purification methods, we propose GPR-GAE, a novel graph auto-encoder (GAE), as a specialized purifier trained with a self-supervised strategy, adapting to diverse graph structures in a data-driven manner. Utilizing multiple Generalized PageRank (GPR) filters, GPR-GAE captures diverse structural representations for robust and effective purification. Our multi-step purification process further facilitates GPR-GAE to achieve precise graph recovery and robust defense against structural perturbations. Experiments across diverse datasets and attack scenarios demonstrate the state-of-the-art robustness of GPR-GAE, showcasing it as an independent plug-and-play purifier for GNN classifiers. Our code can be found in https://github.com/woodavid31/GPR-GAE.","Graphs represent structures such as social networks or transportation systems. Graph Neural Networks (GNNs) are computational models that analyze these types of data. However, GNNs can easily be fooled by minor changes in a graph, even those unnoticeable to humans. Our work introduces a method called GPR-GAE, which corrects these changes before the model makes decisions. It employs a separate ""cleaning"" step that learns to restore the graph by examining data from multiple perspectives, capturing both local details and broader patterns without needing additional labels or human supervision. The graph is iteratively improved, providing the model with clearer, more reliable data. Our method performs well across various graph types and integrates easily with existing systems, contributing to the development of more trustworthy artificial intelligence."
Poster,Self-Supervised Learning of Intertwined Content and Positional Features for Object Detection,https://ICML.cc//virtual/2025/poster/45621,"Kang-Jun Liu, Masanori Suganuma, Takayuki Okatani","We present a novel self-supervised feature learning method using Vision Transformers (ViT) as the backbone, specifically designed for object detection and instance segmentation. Our approach addresses the challenge of extracting features that capture both class and positional information, which are crucial for these tasks. The method introduces two key components: (1) a positional encoding tied to the cropping process in contrastive learning, which utilizes a novel vector field representation for positional embeddings; and (2) masking and prediction, similar to conventional Masked Image Modeling (MIM), applied in parallel to both content and positional embeddings of image patches. These components enable the effective learning of intertwined content and positional features. We evaluate our method against state-of-the-art approaches, pre-training on ImageNet-1K and fine-tuning on downstream tasks. Our method outperforms the state-of-the-art SSL methods on the COCO object detection benchmark, achieving significant improvements with fewer pre-training epochs. These results suggest that better integration of positional information into self-supervised learning can improve performance on the dense prediction tasks.","Before tackling real‑world tasks like self‑driving or medical scans, vision systems take a label‑free warm‑up on millions of unlabeled pictures. Typical label‑free warm‑ups teach them only what objects look like and postpone where they appear to later modules. Our idea is to blend both what and where during this warm‑up, linking them early without any human annotations.During this warm‑up, we let the visual system play a hide-and-seek game. We cut random windows from each photo, with shuffling and scaling. Next, we hide some pieces or their positions and challenge the system to figure out what’s missing and where it belongs. This challenge weaves content and position tightly inside the vision system.After a short tuning step with labels, the system draws tighter boxes and cleaner masks than previous label‑free warm‑up methods. Because our recipe alters only the learning process, not the system design, it works as a plug‑and‑play upgrade that delivers accurate results with short training."
