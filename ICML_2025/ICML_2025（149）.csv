type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Integer Programming for Generalized Causal Bootstrap Designs,https://ICML.cc//virtual/2025/poster/45393,"Jennifer Brennan, Sébastien Lahaie, Adel Javanmard, Nick Doudchenko, Jean Pouget-Abadie","In experimental causal inference, we distinguish between two sources of uncertainty: design uncertainty, due to the treatment assignment mechanism, and sampling uncertainty, when the sample is drawn from a super-population. This distinction matters in settings with small fixed samples and heterogeneous treatment effects, as in geographical experiments. The standard bootstrap procedure most often used by practitioners primarily estimates sampling uncertainty, and the causal bootstrap procedure, which accounts for design uncertainty, was developed for the completely randomized design and the difference-in-means estimator, whereas non-standard designs and estimators are often used in these low-power regimes. We address this gap by proposing an integer program which computes numerically the worst-case copula used as an input to the causal bootstrap method, in a wide range of settings. Specifically, we prove the asymptotic validity of our approach for unconfounded, conditionally unconfounded, and and individualistic with bounded confoundedness assignments, as well as generalizing to any linear-in-treatment and quadratic-in-treatment estimators. We demonstrate the refined confidence intervals achieved through simulations of small geographical experiments.","When experimentally testing new ideas on small samples, like a new policy on a few different geographical regions, it can be hard to tell if observed effects are due to how the policy was assigned to each region, or just due to chance in how the population was formed. Standard statistical methods often don't fully capture the former, especially in complex experiments with varied individual responses. We introduce a method using mathematical optimization (integer programming) to figure out the most challenging, yet plausible, way potential outcomes (both observed and unobserved) could be linked. This ""worst-case"" understanding is then used to more accurately bound the uncertainty arising from the experimental design itself. Our approach is flexible, working across a wider range of experimental setups and effect measurements than previous techniques. This research offers a way to get more trustworthy and often more precise estimates of uncertainty (confidence intervals), particularly in situations with limited data, leading to better-informed choices based on experimental evidence."
Poster,Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models,https://ICML.cc//virtual/2025/poster/45930,"Yang Zheng, Wen Li, Zhaoqiang Liu","Inverse problems (IPs) involve reconstructing signals from noisy observations. Recently, diffusion models (DMs) have emerged as a powerful framework for solving IPs, achieving remarkable reconstruction performance. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approaches under appropriate conditions and validate their superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers.","Inverse problems involve recovering a clear signal (like a sharp image) from blurry or noisy data. Recently, diffusion models, a powerful class of generative models, have achieved impressive results in solving these problems. However, existing methods often face challenges such as suboptimal solutions and high computational demands. In this paper, we propose two new methods, DMILO and DMILO-PGD, to address these issues. Our first method, DMILO, reduces memory usage by decoupling the entire optimization process into a series of smaller steps, making it more efficient. We also introduce a technique that enables the model to explore a wider range of possible solutions, including those it wasn’t originally trained to handle. The second method, DMILO-PGD, combines this approach with an optimization strategy called projected gradient descent to mitigate suboptimal problems. Our experiments demonstrate the effectiveness of both methods, making it easier and more reliable to recover high-quality images from blurry or noisy data."
Poster,Integration-free Kernels for Equivariant Gaussian Process Modelling,https://ICML.cc//virtual/2025/poster/44408,"Tim Steinert, David Ginsbourger, August Lykke-Møller, Ove Christiansen, Henry Moss","We study the incorporation of equivariances into vector-valued GPs and more general classes of random field models. While kernels guaranteeing equivariances have been investigated previously, their evaluation is often computationally prohibitive due to required integrations over the involved groups. In this work, we provide a kernel characterization of stochastic equivariance for centred second-order vector-valued random fields and we construct integration-free equivariant kernels based on the notion of fundamental regions of group actions. We establish data-efficient and computationally lightweight GP models for velocity fields and molecular electric dipole moments and demonstrate that proposed integration-free kernels may also be leveraged to extract equivariant components from data.","Many natural and artificial systems change in predictable ways under certain transformations of their inputs. When using machine learning methods for such systems, incorporating such knowledge within the algorithms is essential to allow for efficiently leveraging available data. This is especially true when data are scarce, which calls in turn for methods providing prediction uncertainty. We focus here on Gaussian process models, which allow probabilistically modelling vector-valued outputs and have been found especially convenient in low data regimes. At the core of a Gaussian process is the kernel, which measures how similar responses are expected to be for different inputs. This allows the model to learn patterns in the data, including how the different parts of the output are related to each other. To ensure that GP models respect structural knowledge such as equivariances, the kernel itself must be suitably designed to reflect them. However, most classes of equivariant kernels rely on integration and the resulting computational burden can hinder their usability. In this work, we introduce a class of integration-free equivariant kernels which enable computationally efficient equivariant Gaussian process modelling. As shown in the paper, the proposed approach allows for efficient probabilistic predictions in challenging problems touching upon quantum chemistry and fluid dynamics, illustrating benefits of accounting for domain knowledge within machine learning."
Poster,Interaction-Aware Gaussian Weighting for Clustered Federated Learning,https://ICML.cc//virtual/2025/poster/44632,"Alessandro Licciardi, Davide Leo, Eros Fanì, Barbara Caputo, Marco Ciccone","Federated Learning (FL) emerged as a decentralized paradigm to train models while preserving privacy. However, conventional FL struggles with data heterogeneity and class imbalance, which degrade model performance.Clustered FL balances personalization and decentralized training by grouping clients with analogous data distributions, enabling improved accuracy while adhering to privacy constraints. This approach effectively mitigates the adverse impact of heterogeneity in FL.In this work, we propose a novel clustering method for FL, **FedGWC** (Federated Gaussian Weighting Clustering), which groups clients based on their data distribution, allowing training of a more robust and personalized model on the identified clusters. **FedGWC** identifies homogeneous clusters by transforming individual empirical losses to model client interactions with a Gaussian reward mechanism. Additionally, we introduce the *Wasserstein Adjusted Score*, a new clustering metric for FL to evaluate cluster cohesion with respect to the individual class distribution. Our experiments on benchmark datasets show that **FedGWC** outperforms existing FL algorithms in cluster quality and classification accuracy, validating the efficacy of our approach.","Training AI models usually requires centralizing vast amounts of data, which raises privacy concerns. Federated Learning (FL) offers a solution by allowing edge devices or institutions - such as smartphones and hospitals - to train a shared model collaboratively without sending their private data to a central server. However, real-world data is often messy: different devices might have very diverse types of data, or some data categories might be rare on some devices whilst common on others. This *data heterogeneity* makes it hard for FL models to perform well across all devices.Our work introduces **FedGWC**, a new method to make FL training more effective. Instead of forcing all devices to train one model, FedGWC groups devices with similar data characteristics into clusters, allowing each cluster to train its specialized model, which is much better suited to the data within that group. Think of it like organizing a study group: instead of everyone studying the same broad topic, smaller groups form to focus on specific subjects they all need help with. FedGWC does this by analyzing how well each device’s model learns from its own data without actually looking at the data itself. We also developed a new way to measure how good these clusters are, especially when some data categories are much rarer than others.Our experiments show that FedGWC significantly improves the accuracy of models in FL setups, especially when data is diverse and unevenly distributed. This means we can build more powerful and personalized AI applications while preserving sensitive private information."
Poster,Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence,https://ICML.cc//virtual/2025/poster/46588,"İlker Işık, Ramazan Gokberk Cinbis, Ebru Gol","Language models lack the notion of interchangeable tokens: symbols that are semantically equivalent yet distinct, such as bound variables in formal logic. This limitation prevents generalization to larger vocabularies and hinders the model's ability to recognize alpha-equivalence, where renaming bound variables preserves meaning. We formalize this machine learning problem and introduce alpha-covariance, a metric for evaluating robustness to such transformations. To tackle this task, we propose a dual-part token embedding strategy: a shared component ensures semantic consistency, while a randomized component maintains token distinguishability. Compared to a baseline that relies on alpha-renaming for data augmentation, our approach demonstrates improved generalization to unseen tokens in linear temporal logic solving, propositional logic assignment prediction, and copying with an extendable vocabulary, while introducing a favorable inductive bias for alpha-equivalence. Our findings establish a foundation for designing language models that can learn interchangeable token representations, a crucial step toward more flexible and systematic reasoning in formal domains. Our code and project page are available at https://necrashter.github.io/interchangeable-token-embeddings","When people read a math or logic expression like ""if x and y are odd numbers, x+y is even,"" they know that changing the names of x and y doesn't change the meaning. Because the variables (x and y) are just placeholders. But today's language models don't naturally understand this idea. If we rename those variables, the model may think the statement means something different.Our research tackles this problem by helping language models learn that some symbols are interchangeable: they look different but mean the same thing in context. We created a way to measure how well a model understands this, and we designed a new method for representing this concept. The key idea is to represent interchangeable symbols in two parts: one that captures their shared meaning and another that keeps each symbol distinct.Our approach helps models better understand variable renaming, and allows them to generalize to unfamiliar vocabulary they have not seen before. It's a step toward making AI systems better at formal reasoning, which is essential for understanding logic, code, and mathematical language."
Poster,Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors,https://ICML.cc//virtual/2025/poster/45427,"Jing Huang, Junyi Tao, Thomas Icard, Diyi Yang, Christopher Potts","Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks—including symbol manipulation, knowledge retrieval, and instruction following—we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model’s behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.","Large language models are powerful but often unpredictable, especially when they face unfamiliar inputs. A big question in interpretability research is: can we peek inside these models to understand why they behave the way they do—and use that understanding to predict how they will act on unseen inputs?Our research suggests the answer is yes. We looked at a variety of language-related tasks, such as following instructions or retrieving facts, and found that certain internal mechanisms that have causal effects on model behaviors are particularly useful. We tested two ways of using these internal mechanisms to predict whether the model’s answers would be correct. One checks if key components in the causal mechanisms were triggered (counterfactual simulation), and the other looks at the values of these components to make predictions (value probing). Both approaches worked well, especially when the model is processing unfamiliar data.This shows that understanding a model’s internal workings can help us predict its behavior more reliably—an important step toward safer, more trustworthy AI."
Poster,Interpolating Neural Network-Tensor Decomposition (INN-TD): a scalable and interpretable approach for large-scale physics-based problems,https://ICML.cc//virtual/2025/poster/43571,"Jiachen Guo, Xiaoyu Xie, Chanwook Park, Hantao Zhang, Matthew Politis, Gino Domel, Jiachen Guo","Deep learning has been extensively employed as a powerful function approximator for modeling physics-based problems described by partial differential equations (PDEs). Despite their popularity, standard deep learning models often demand prohibitively large computational resources and yield limited accuracy when scaling to large-scale, high-dimensional physical problems. Their black-box nature further hinders their application in industrial problems where interpretability and high precision are critical. To overcome these challenges, this paper introduces Interpolating Neural Network-Tensor Decomposition (INN-TD), a scalable and interpretable framework that has the merits of both machine learning and finite element methods for modeling large-scale physical systems.  By integrating locally supported interpolation functions from finite element into the network architecture, INN-TD achieves a sparse learning structure with enhanced accuracy, faster training/solving speed, and reduced memory footprint. This makes it particularly effective for tackling large-scale high-dimensional parametric PDEs in training, solving, and inverse optimization tasks in physical problems where high precision is required.","Standard deep learning models struggle with large-scale physical simulations described by partial differential equations (PDEs). They often demand prohibitive computational power, yield limited accuracy for complex, high-dimensional scenarios, and their ""black-box"" nature makes them difficult to trust for critical industrial problems requiring interpretability and high precision. We developed the Interpolating Neural Network-Tensor Decomposition (INN-TD) framework, inspired by traditional finite element methods. This approach achieves enhanced accuracy, significantly faster training and solving speeds, and a reduced memory footprint. Our framework empowers more effective and reliable modeling of large-scale, high-dimensional parametric PDEs, crucial for advancing training, solving, and inverse optimization tasks in physical systems where high precision and interpretability are paramount."
Poster,Interpreting CLIP with Hierarchical Sparse Autoencoders,https://ICML.cc//virtual/2025/poster/46435,"Vladimir Zaigrajew, Hubert Baniecki, Przemysław Biecek","Sparse autoencoders (SAEs) are useful for detecting and steering interpretable features in neural networks, with particular potential for understanding complex multimodal representations. Given their ability to uncover interpretable features, SAEs are particularly valuable for analyzing vision-language models (e.g., CLIP and SigLIP), which are fundamental building blocks in modern large-scale systems yet remain challenging to interpret and control. However, current SAE methods are limited by optimizing both reconstruction quality and sparsity simultaneously, as they rely on either activation suppression or rigid sparsity constraints. To this end, we introduce Matryoshka SAE (MSAE), a new architecture that learns hierarchical representations at multiple granularities simultaneously, enabling a direct optimization of both metrics without compromise. MSAE establishes a state-of-the-art Pareto frontier between reconstruction quality and sparsity for CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of variance unexplained while maintaining 80\% sparsity. Finally, we demonstrate the utility of MSAE as a tool for interpreting and controlling CLIP by extracting over 120 semantic concepts from its representation to perform concept-based similarity search and bias analysis in downstream tasks like CelebA. We make the codebase available at https://github.com/WolodjaZ/MSAE.","Modern multimodal AI models like CLIP exhibit remarkable performance in linking visual and textual information. Yet, they remain largely opaque-operating as ""black boxes"" with limited transparency into their internal representations. This lack of interpretability presents a critical barrier to responsible deployment and scientific understanding.To address this, we introduce Matryoshka Sparse Autoencoder (MSAE), a novel method aimed to understand and control internal representation. MSAE is designed to recover internal features at multiple levels of abstraction simultaneously, preserving both the semantic clarity and compactness of the feature representation. Applied to CLIP, our method achieves over 99% cosine similarity to the original activations while maintaining high sparsity (~80%), thereby reconciling the typical trade-off between expressiveness and simplicity.MSAE supports fine-grained inspection of model behavior. In our evaluation, we identified over 120 distinct and interpretable concepts encoded by CLIP. These features support downstream applications including concept-based retrieval, systematic bias analysis, and targeted probing."
Poster,Interpreting the Repeated Token Phenomenon in Large Language Models,https://ICML.cc//virtual/2025/poster/45013,"Itay Yona, Ilia Shumailov, Jamie Hayes, Yossi Gandelsman","Large Language Models (LLMs), despite their impressive capabilities, often fail to accurately repeat a single word when prompted to, and instead output unrelated text. This unexplained failure mode represents a *vulnerability*, allowing even end users to diverge models away from their intended behavior. We aim to explain the causes for this phenomenon and link it to the concept of ""attention sinks"", an emergent LLM behavior crucial for fluency, in which the initial token receives disproportionately high attention scores. Our investigation identifies the neural circuit responsible for attention sinks and shows how long repetitions disrupt this circuit. We extend this finding to other nonrepeating sequences that exhibit similar circuit disruptions.  To address this, we propose a targeted patch that effectively resolves the issue without negatively impacting the overall performance of the model.  This study provides a mechanistic explanation for an LLM vulnerability, demonstrating how interpretability can diagnose and address issues, and offering insights that pave the way for more secure and reliable models.","Large Language Models (LLMs) often struggle to repeat a single word, and generate unrelated text instead. This ""repeated token divergence"" is a significant vulnerability, allowing models to deviate from intended behavior. Our research explains this by linking it to ""attention sinks"", an LLM behavior crucial for fluency.We used mechanistic interpretability, analyzing the neural circuits underlying attention sinks. We found a two-stage mechanism: an initial attention layer marks the first token, and a later neuron amplifies its hidden state, creating the attention sink. When repeated tokens are present, the first attention layer mistakenly marks both initial and subsequent identical tokens, leading to abnormally high attention and model divergence.This study offers a mechanistic explanation for an LLM vulnerability, demonstrating how interpretability can diagnose and address issues, paving the way for more secure and reliable models."
Poster,Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces,https://ICML.cc//virtual/2025/poster/44714,"ERIC EATON, Marcel Hussing, Michael Kearns, Aaron Roth, Sikata Sengupta, Jessica Sorrell","In traditional reinforcement learning (RL), the learner aims to solve a single objective optimization problem: find the policy that maximizes expected reward. However, in many real-world settings, it is important to optimize over multiple objectives simultaneously. For example, when we are interested in fairness, states might have feature annotations corresponding to multiple (intersecting) demographic groups to whom reward accrues, and our goal might be to maximize the reward of the group receiving the minimal reward.   In this work, we consider a multi-objective optimization problem in which each objective is defined by a state-based reweighting of a single scalar reward function. This generalizes the problem of maximizing the reward of the minimum reward group. We provide oracle-efficient algorithms to solve these multi-objective RL problems even when the number of objectives is very large --- for tabular MDPs, as well as for large MDPs when the group functions have additional structure. The contribution of this paper is that we are able to solve this class of multi-objective RL problems with a possibly *exponentially* large class of constraints over intersecting groups in both tabular and large state space MDPs in an oracle-efficient manner. Finally, we experimentally validate our theoretical results and demonstrate applications on a preferential attachment graph MDP.","There are many decision-making problems in the real-world where multiple different demographic groups are affected by the outcome (i.e. disaster relief). Therefore, it is vital to ensure that when decision-making models are deployed to the real-world they are fair to the people they are meant to serve. In particular, one such challenge faced by models that aim to be fair is that of fairness gerrymandering, where models can be marginally fair over some sets of attributes, but systematically unfair to certain subgroups of the population. Therefore it is important to have models that are fair with respect to a sufficiently rich class of subgroups (intersectional fairness). In this paper, we address this problem by introducing a set of Reinforcement Learning algorithms that are able to provide certain types of fairness guarantees to a very large number of groups. We do this by formulating a zero-sum game between a learner that is trying to learn various policies or decision-making rules and a regulator who is choosing subgroups which loosely speaking are treated the least fairly. This approach enables us to extend Reinforcement Learning algorithms to settings where we want to ensure fairness to a large number of groups and we hope that some of this toolkit will be used beyond for other classes of problems where there are multiple objectives to consider."
