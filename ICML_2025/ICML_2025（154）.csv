type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Kernel Quantile Embeddings and Associated Probability Metrics,https://ICML.cc//virtual/2025/poster/46234,"Masha Naslidnyk, Siu Lun Chau, Francois-Xavier Briol, Krikamol Muandet","Embedding probability distributions into reproducing kernel Hilbert spaces (RKHS) has enabled powerful nonparametric methods such as the maximum mean discrepancy (MMD), a statistical distance with strong theoretical and computational properties. At its core, the MMD relies on kernel mean embeddings to represent distributions as mean functions in RKHS. However, it remains unclear if the mean function is the only meaningful RKHS representation.Inspired by generalised quantiles, we introduce the notion of *kernel quantile embeddings (KQEs)*. We then use KQEs to construct a family of distances that:(i) are probability metrics under weaker kernel conditions than MMD;(ii) recover a kernelised form of the sliced Wasserstein distance; and(iii) can be efficiently estimated with near-linear cost.Through hypothesis testing, we show that these distances offer a competitive alternative to MMD and its fast approximations.","Many modern techniques compare complex datasets by representing probability distributions in a flexible space of functions, and measuring their difference using the maximum mean discrepancy (MMD). However, this method reduces each distribution to its average representation, which may miss important details or require strict conditions to work reliably. To address this, we draw inspiration from quantiles—values that divide data evenly—and introduce kernel quantile embeddings (KQEs) as a richer way to capture the shape of a distribution. We develop a consistent estimator for these embeddings and use them to define a new family of distance measures between distributions. These distances can tell distributions apart under milder assumptions than those for the MMD, and they also recover a kernel-based version of the sliced Wasserstein distance, linking two influential statistical frameworks. Importantly, our measures can be computed with scalable, near-linear cost, making them practical for large datasets. Through hypothesis testing, we show that these new distances perform competitively against MMD and its fast approximations. By moving beyond simple averages, our work offers a powerful and flexible alternative for comparing probability distributions and opens the door to new research in data analysis and machine learning."
Poster,KGMark: A Diffusion Watermark for Knowledge Graphs,https://ICML.cc//virtual/2025/poster/45835,"Hongrui Peng, Haolang Lu, Yuanlong Yu, WeiYe Fu, Kun Wang, Guoshun Nan","Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMark, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMark properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMark.","Knowledge graphs are widely used to represent structured information, but verifying their ownership is hard once shared online. We developed a watermarking method that hides invisible signatures inside the graph's learned embeddings using a diffusion model. Our method resists both random and targeted tampering, helping protect graph-based AI assets from unauthorized use and plagiarism."
Poster,KinDEL: DNA-Encoded Library Dataset for Kinase Inhibitors,https://ICML.cc//virtual/2025/poster/45034,"Benson Chen, Tomasz Danel, Gabriel Dreiman, Patrick McEnaney, Nikhil Jain, Kirill Novikov, Spurti Akki, Joshua L. Turnbull, Virja Pandya, Boris Belotserkovskii, Jared Weaver, Ankita Biswas, Dat Nguyen, Kent Gorday, Mohammad M Sultan, Nathaniel Stanley, Daniel Whalen, Divya Kanichar, Christoph Klein, Emily Fox, R. Watts","DNA-Encoded Libraries (DELs) represent a transformative technology in drug discovery, facilitating the high-throughput exploration of vast chemical spaces. Despite their potential, the scarcity of publicly available DEL datasets presents a bottleneck for the advancement of machine learning methodologies in this domain. To address this gap, we introduce KinDEL, one of the largest publicly accessible DEL datasets and the first one that includes binding poses from molecular docking experiments. Focused on two kinases, Mitogen-Activated Protein Kinase 14 (MAPK14) and Discoidin Domain Receptor Tyrosine Kinase 1 (DDR1), KinDEL includes 81 million compounds, offering a rich resource for computational exploration. Additionally,  we provide comprehensive biophysical assay validation data, encompassing both on-DNA and off-DNA measurements, which we use to evaluate a suite of machine learning techniques, including novel structure-based probabilistic models. We hope that our benchmark, encompassing both 2D and 3D structures, will help advance the development of machine learning models for data-driven hit identification using DELs.","DNA-Encoded Libraries (DELs) are extensive collections of chemical compounds, each tagged with a unique DNA barcode. These libraries allow scientists to quickly test millions of compounds to see if they bind to specific targets involved in diseases. Currently, a significant challenge in the field is the scarcity of available DEL datasets. Without these vital resources, researchers face challenges in developing and comparing machine learning techniques effectively, which slows down progress in identifying potential new treatments.To tackle this issue, we introduce KinDEL, a robust dataset containing 81 million compounds, specifically designed to propel the development of machine learning models for DEL research. KinDEL is a vast library that includes compounds tested against two kinase targets and offers a new benchmark with biophysical data for selected compounds, both with and without DNA tags.The release of the KinDEL dataset equips the scientific community with the necessary tools to develop advanced machine learning models for DEL analysis, ultimately accelerating the discovery of new drug candidates. This initiative represents an important step forward in making DEL datasets more accessible for drug discovery research."
Poster,KIND: Knowledge Integration and Diversion for Training Decomposable Models,https://ICML.cc//virtual/2025/poster/45229,"Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng","Pre-trained models have become the preferred backbone due to the increasing complexity of model parameters. However, traditional pre-trained models often face deployment challenges due to their fixed sizes, and are prone to negative transfer when discrepancies arise between training tasks and target tasks.To address this, we propose **KIND**, a novel pre-training method designed to construct decomposable models.KIND integrates knowledge by incorporating Singular Value Decomposition (SVD) as a structural constraint, with each basic component represented as a combination of a column vector, singular value, and row vector from $U$, $\Sigma$, and $V^\top$ matrices.These components are categorized into **learngenes** for encapsulating class-agnostic knowledge and \textbf{tailors} for capturing class-specific knowledge, with knowledge diversion facilitated by a class gate mechanism during training.Extensive experiments demonstrate that models pre-trained with KIND can be decomposed into learngenes and tailors, which can be adaptively recombined for diverse resource-constrained deployments. Moreover, for tasks with large domain shifts, transferring only learngenes with task-agnostic knowledge, when combined with randomly initialized tailors, effectively mitigates domain shifts.Code will be made available at https://github.com/Te4P0t/KIND.","Modern AI models often rely on pre-trained backbones, but these models are typically fixed in size and don’t adapt well when applied to different tasks—especially if there’s a big difference between training and deployment scenarios. To overcome this, we introduce KIND, a new pre-training approach that builds models out of smaller, reusable pieces. KIND uses a mathematical technique called Singular Value Decomposition (SVD) to break knowledge into basic components. These components are grouped into learngenes, which carry general-purpose knowledge, and tailors, which adapt to specific tasks. During training, a mechanism called a “class gate” separates these two types of knowledge. Experiments show that models trained with KIND can be flexibly adapted to different devices or domains by recombining the components. In particular, transferring just the general-purpose learngenes proves effective in new tasks with limited resources or large domain differences. Code is available at: https://github.com/Te4P0t/KIND."
Poster,Kinetic Langevin Diffusion for Crystalline Materials Generation,https://ICML.cc//virtual/2025/poster/46331,"François Cornet, Federico Bergamin, Arghya Bhowmik, Juan Garcia-Lastra, Jes Frellsen, Mikkel Schmidt","Generative modeling of crystalline materials using diffusion models presents a series of challenges: the data distribution is characterized by inherent symmetries and involves multiple modalities, with some defined on specific manifolds. Notably, the treatment of fractional coordinates representing atomic positions in the unit cell requires careful consideration, as they lie on a hypertorus. In this work, we introduce Kinetic Langevin Diffusion for Materials (KLDM), a novel diffusion model for crystalline materials generation, where the key innovation resides in the modeling of the coordinates. Instead of resorting to Riemannian diffusion on the hypertorus directly, we generalize Trivialized Diffusion Model (TDM) to account for the symmetries inherent to crystals. By coupling coordinates with auxiliary Euclidean variables representing velocities, the diffusion process is now offset to a flat space. This allows us to effectively perform diffusion on the hypertorus while providing a training objective that accounts for the periodic translation symmetry of the true data distribution. We evaluate KLDM on both Crystal Structure Prediction (CSP) and De-novo Generation (DNG) tasks, demonstrating its competitive performance with current state-of-the-art models.",(1) Discovering new materials has significant implications across multiple scientific domains.  (2) We propose a new model to generate crystalline materials. (3) This is an additional tool that can be used to search the space of materials in a more effective way.
Poster,Knowledge-Guided Wasserstein Distributionally Robust Optimization,https://ICML.cc//virtual/2025/poster/43697,"Zitao Wang, Ziyuan Wang, Molei Liu, Nian Si","Wasserstein Distributionally Robust Optimization (WDRO) is a principled framework for robust estimation under distributional uncertainty. However, its standard formulation can be overly conservative, particularly in small-sample regimes. We propose a novel knowledge-guided WDRO (KG-WDRO) framework for transfer learning, which adaptively incorporates multiple sources of external knowledge to improve generalization accuracy. Our method constructs smaller Wasserstein ambiguity sets by controlling the transportation along directions informed by the source knowledge. This strategy can alleviate perturbations on the predictive projection of the covariates and protect against information loss. Theoretically, we establish the equivalence between our WDRO formulation and the knowledge-guided shrinkage estimation based on collinear similarity, ensuring tractability and geometrizing the feasible set. This also reveals a novel and general interpretation for recent shrinkage-based transfer learning approaches from the perspective of distributional robustness. In addition, our framework can adjust for scaling differences in the regression models between the source and target and accommodates general types of regularization such as lasso and ridge. Extensive simulations demonstrate the superior performance and adaptivity of KG-WDRO in enhancing small-sample transfer learning.","Machine learning models often perform poorly when applied to small datasets, especially if the new data differs from what the model was originally trained on — a common scenario in healthcare or social science. A promising technique called distributionally robust optimization (DRO) helps guard against this issue by preparing models for worst-case scenarios. However, standard DRO can be too cautious, leading to less accurate predictions.We introduce a new method called Knowledge-Guided Wasserstein DRO (KG-WDRO). It allows models to intelligently incorporate insights from previous related datasets (called ""prior knowledge"") without blindly copying them. This makes the model less conservative and better suited for small, real-world datasets.Our approach guides model uncertainty by trusting past knowledge more in areas it is confident about, and less in unfamiliar territory. We show that KG-WDRO improves predictions compared to existing methods — especially when labeled data is scarce. This technique can help machine learning systems adapt better across domains like medicine, finance, and the social sciences, where transferring knowledge wisely is key."
Poster,Knowledge Retention in Continual Model-Based Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45991,"Haotian Fu, Yixiang Sun, Michael L. Littman, George Konidaris","We propose DRAGO, a novel approach for continual model-based reinforcement learning aimed at improving the incremental development of world models across a sequence of tasks that differ in their reward functions but not the state space or dynamics. DRAGO comprises two key components: *Synthetic Experience Rehearsal*, which leverages generative models to create synthetic experiences from past tasks, allowing the agent to reinforce previously learned dynamics without storing data, and *Regaining Memories Through Exploration*, which introduces an intrinsic reward mechanism to guide the agent toward revisiting relevant states from prior tasks. Together, these components enable the agent to maintain a comprehensive and continually developing world model, facilitating more effective learning and adaptation across diverse environments. Empirical evaluations demonstrate that DRAGO is able to preserve knowledge across tasks, achieving superior performance in various continual learning scenarios.","Robots and other AI agents often need to master a series of tasks, one after another, without being allowed to keep every previous experience—think of a helper robot that moves from one apartment to the next, or a phone‑based assistant that must forget sensitive user data. Today’s learning systems quickly “forget” what they once knew and have to start almost from scratch each time.Our paper introduces DRAGO, a two‑step “dream and explore” method that helps an agent hold on to what it has learned while still making room for new skills. First, the agent dreams: it trains a small simulator that can invent realistic memories of earlier tasks and rehearse them internally, so no raw data need be stored. Then it explores: it rewards itself for revisiting parts of the world it used to understand well, stitching old and new knowledge together.In tests on grid‑world games and simulated robots, DRAGO kept its skills far better than existing methods, letting agents adapt to new goals faster and with less data—an important step toward lifelong, privacy‑aware AI."
Poster,Knowledge Swapping via Learning and Unlearning,https://ICML.cc//virtual/2025/poster/46135,"Mingyu Xing, Lechao Cheng, Shengeng Tang, Yaxiong Wang, Zhun Zhong, Meng Wang","We introduce Knowledge Swapping, a novel task designed to selectively regulate knowledge of a pretrained model by enabling the forgetting of user-specified information, retaining essential knowledge, and acquiring new knowledge simultaneously.  By delving into the analysis of knock-on feature hierarchy, we find that incremental learning typically progresses from low-level representations to higher-level semantics, whereas forgetting tends to occur in the opposite direction—starting from high-level semantics and moving down to low-level features. Building upon this, we propose to benchmark the knowledge swapping task with the strategy of Learning Before Forgetting. Comprehensive experiments on various tasks like image classification, object detection, and semantic segmentation validate the effectiveness of the proposed strategy. The source code is available at https://github.com/xingmingyu123456/KnowledgeSwapping.","As artificial intelligence (AI) systems learn more and more, one big challenge is helping them forget certain outdated or unwanted knowledge — without messing up what they already know or need to learn next. Imagine trying to forget your old home address while still remembering your phone number and learning a new one — all at once! In our research, we introduce a new task called Knowledge Swapping, which aims to give AI models the ability to do just that: forget specific information, keep what’s essential, and learn new things at the same time. We studied how AI “thinks,” or more precisely, how it builds up knowledge layer by layer — from simple visual features like edges to more complex ideas like object categories. Interestingly, we found that forgetting starts at the top (complex ideas) and then trickles down to simpler ones. Based on this insight, we propose a learning strategy called Learning Before Forgetting to guide AI systems through this process in a more stable and effective way. We tested our approach across a range of tasks, including recognizing objects in images and identifying different parts of scenes, and found that our method helps AI adapt better — learning new things while responsibly letting go of the old."
Poster,Kona: An Efficient Privacy-Preservation Framework for KNN Classification by Communication Optimization,https://ICML.cc//virtual/2025/poster/46636,"Guopeng Lin, Ruisheng Zhou, Shuyu Chen, Weili Han, Jin Tan, Wenjing Fang, Lei Wang, Tao Wei","K-nearest neighbors (KNN) classification plays a significant role in various applications due to its interpretability. The accuracy of KNN classification relies heavily on large amounts of high-quality data, which are often distributed among different parties and contain sensitive information. Dozens of privacy-preserving frameworks have been proposed for performing KNN classification with data from different parties while preserving data privacy. However, existing privacy-preserving frameworks for KNN classification demonstrate communication inefficiency in the online phase due to two main issues: (1) They suffer from huge communication size for secure Euclidean square distance computations. (2) They require numerous communication rounds to select the $k$ nearest neighbors.  In this paper, we present $\texttt{Kona}$, an efficient privacy-preserving framework for KNN classification. We resolve the above communication issues by (1) designing novel Euclidean triples, which eliminate the online communication for secure Euclidean square distance computations,  (2) proposing a divide-and-conquer bubble protocol, which significantly reduces communication rounds for selecting the $k$ nearest neighbors. Experimental results on eight real-world datasets demonstrate that $\texttt{Kona}$ significantly outperforms the state-of-the-art framework by  $1.1\times \sim 3121.2\times$ in communication size, $19.1\times \sim 5783.2\times$ in communication rounds, and $1.1\times \sim 232.6\times$ in runtime.","Imagine several hospitals each have valuable but sensitive patient data—like medical histories or test results. To improve medical decision-making, they may want to compare a new patient’s case with similar past cases from other hospitals. A common way to do this is using a machine learning method called K-nearest neighbors (KNN), which finds the most similar past cases to help predict outcomes like likely diagnoses. The more relevant data the model can access, the more accurate its predictions become.But here's the problem: due to privacy laws, hospitals can't just share raw patient data with each other.Our work introduces Kona, an efficient system that allows different organizations to use their data together to perform KNN while keeping their data private. What makes Kona special is that it solves a big problem in existing privacy-protecting systems: they often require too much online data transfer, which makes them slow and hard to use.Kona speeds things up in two smart ways. First, it changes how distances between data points are calculated, so no extra communication is needed during that step. Second, it streamlines the process of picking the nearest neighbors, cutting down the number of times systems have to ""talk"" to each other. When we tested Kona with real-world data, it was up to 3,000 times more efficient in communication and over 200 times faster than the best existing methods—all while keeping data private and getting the same accurate results."
Poster,KoNODE: Koopman-Driven Neural Ordinary Differential Equations with Evolving Parameters for Time Series Analysis,https://ICML.cc//virtual/2025/poster/45804,"Hanru Bai, Weiyang Ding","Neural ordinary differential equations (NODEs) have demonstrated strong capabilities in modeling time series. However, existing NODE- based methods often focus solely on the surface-level dynamics derived from observed states, which limits their ability to capture more complex underlying behaviors. To overcome this challenge, we propose KoNODE, a Koopman-driven NODE framework that explicitly models the evolution of ODE parameters over time to encode deep-level information. KoNODE captures the essential yet simple intrinsic linear dynamics that govern the surface dynamics by employing Koopman operators. Our framework operates at three hierarchical levels: the observed state dynamics, the parameter dynamics, and the Koopman linear dynamics, representing the fundamental driving rules of the state dynamics. The proposed approach offers significant improvements in two critical time series tasks: long-term prediction (enabled by the simple linear dynamics) and generalization to new data (driven by the evolving ODE parameters). We validate KoNODE through experiments on synthetic data from complex dynamic systems and real-world datasets, demonstrating its effectiveness in practical scenarios.","Understanding how things change over time — like weather patterns, stock markets, or heart rhythms — is a big challenge in science and technology. A popular approach is to use equations that model how these changes happen. But most current methods only look at what we can directly observe, often missing the hidden rules that actually drive the changes.Our work introduces a new method, called KoNODE, that digs deeper. Instead of just tracking what’s happening on the surface, KoNODE also models how the rules themselves evolve over time. It uses a mathematical tool called the Koopman operator, which helps uncover simple patterns beneath complex behaviors.This layered approach lets KoNODE make more accurate long-term predictions and adapt better when it sees new types of data — two big challenges in analyzing time-based information. We’ve tested KoNODE on both simulated systems and real-world data, and it consistently outperforms existing methods. We’ve also made our code freely available for others to use and build upon."
