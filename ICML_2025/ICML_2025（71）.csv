type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Detecting Strategic Deception with Linear Probes,https://ICML.cc//virtual/2025/poster/46082,"Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim, Marius Hobbhahn","AI models might use deceptive strategies as part of scheming or misaligned behaviour. Monitoring outputs alone is insufficient, since the AI might produce seemingly benign outputs while its internal reasoning is misaligned. We thus evaluate if linear probes can robustly detect deception by monitoring model activations. We test two probe-training datasets, one with contrasting instructions to be honest or deceptive (following Zou et al. (2023)) and one of responses to simple roleplaying scenarios. We test whether these probes generalize to realistic settings where Llama-3.3-70B-Instruct behaves deceptively, such as concealing insider trading Scheurer et al. (2023) and purposely underperforming on safety evaluations Benton et al. (2024). We find that our probe distinguishes honest and deceptive responses with AUROCs between 0.96 and 0.999 on our evaluation datasets. If we set the decision threshold to have a 1\% false positive rate on chat data not related to deception, our probe catches 95-99\% of the deceptive responses. Overall we think white-box probes are promising for future monitoring systems, but current performance is insufficient as a robust defence against deception. Our probes' outputs can be viewed at https://data.apolloresearch.ai/dd/ and our code at https://github.com/ApolloResearch/deception-detection.","AI assistants are trained to be helpful and truthful, yet a powerful system could one day choose to hide its real intentions. How can we spot that kind of strategic deception before it causes harm?We explore a simple detector system: a _linear probe_ that monitors the model's internal thoughts (its 'activations', or intermediate computations)—while the model responds. We first teach the probe what honest and dishonest behaviour look like on toy examples, then test it on examples where AI systems are often deceptive in practice: a chatbot that role-plays a dishonest car-salesman, a trading bot that secretly uses insider tips, and an exam-cheating agent that pretends to ""forget"" dangerous knowledge.Across these diverse settings, the probe flags 95–99\% of deceptive answers while wrongly alarming on only 1\% of ordinary chat. That's encouraging, but not enough for total safety: the probe sometimes fires for the _topic_ of deception rather than genuine intent. Our results show that peeking inside the model is promising, yet still needs sharpening before it can be relied on in the wild."
Poster,Determinant Estimation under Memory Constraints and Neural Scaling Laws,https://ICML.cc//virtual/2025/poster/44098,"Siavash Ameli, Chris van der Heide, Liam Hodgkinson, Fred Roosta, Michael Mahoney","Calculating or accurately estimating log-determinants of large positive semi-definite matrices is of fundamental importance in many machine learning tasks. While its cubic computational complexity can already be prohibitive, in modern applications even storing the matrices themselves can pose a memory bottleneck. To address this, we derive a novel hierarchical algorithm based on block-wise computation of the LDL decomposition for large-scale log-determinant calculation in memory-constrained settings. In extreme cases where matrices are highly ill-conditioned, accurately computing the full matrix itself may be infeasible. This is particularly relevant when considering kernel matrices at scale, including the empirical Neural Tangent Kernel (NTK) of neural networks trained on large datasets. Under the assumption of neural scaling laws in the test error, we show that the ratio of pseudo-determinants satisfies a power-law relationship, enabling the derivation of corresponding scaling laws. This allows for accurate estimation of NTK log-determinants from a tiny fraction of the full dataset; in our experiments, this results in a $\sim$100,000$\times$ speedup with improved accuracy to other state-of-the-art approaches. Using these techniques, we successfully estimate log-determinants for dense matrices of extreme sizes, which were previously deemed intractable and inaccessible due to their enormous scale and computational requirements.","Many machine-learning methods need to compute a summary (the “log-determinant”) from very large tables of numbers, but doing so can be extremely slow and even run out of computer memory. We created a way to compute this summary by breaking these huge data tables into smaller blocks, so the entire thing never needs to be loaded into the computer memory all at once. We also discovered a simple rule based on how neural networks behave as you add more data, that allows us to predict the needed number from just a tiny fraction of the full table. This can result in speedups of up to 100,000x in our experiments."
Poster,Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective,https://ICML.cc//virtual/2025/poster/44037,"Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Fei Chao, Rongrong Ji","In this paper, we address the challenge of determining the layer-wise sparsity rates of large language models (LLMs) through a theoretical perspective. Specifically, we identify a critical issue of **""reconstruction error explosion""** in existing LLMs sparsification methods. This refers to the cumulative effect of reconstruction errors throughout the sparsification process, where errors from earlier layers propagate and amplify in subsequent layers. As a result, the overall reconstruction error increases significantly, leading to a substantial degradation in model performance. Through theoretical analysis, we derive a simple yet effective approach to layer-wise sparsity allocation that mitigates this issue. Our method uses a monotonically increasing arithmetic progression, reducing the process of determining sparsity rates for multiple layers to the determination of a single common difference hyperparameter. Remarkably, this allows for the optimal layer-wise sparsity rates to be identified with just a few trials. Both our theoretical analysis and experimental results demonstrate that this sparsity allocation scheme is near optimal. Extensive experiments show that our method significantly improves the performance of sparse LLMs across various architectures, outperforming existing layer-wise sparsity methods. Furthermore, it enhances the performance of various compression techniques and is applicable to vision and multimodal models. Notably, our method achieves a reduction of 52.10 in perplexity for the 70% sparse LLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by 10.50%, and delivers speedups of 2.63$\times$ and 2.23$\times$ on CPU and GPU, respectively. Code is available at https://github.com/wzhuang-xmu/ATP.","Large language models like ChatGPT need heavy computing power. Researchers often simplify these models by removing less important components (""sparsification""). But current methods face a hidden problem: errors caused by this simplification accumulate across different layers of the model, like a snowball rolling downhill. These growing errors eventually crash the model’s performance – we call this ""error explosion.""We discovered a smarter way to simplify these AI systems. Imagine organizing model layers like a musical crescendo – starting with minimal simplification in early layers and gradually increasing it in later ones. This approach prevents error accumulation and only requires adjusting one key parameter. Remarkably, finding the best pattern takes just a few attempts rather than exhaustive testing.Our method makes simplified AI models significantly more accurate and efficient. When applied to a 7B model, it boosted task-solving accuracy by over 10% while making the model 70% leaner and doubled processing speed on both CPUs and GPUs. It also works for image and multimodal AI, enabling compact yet powerful models. For example, it could help run advanced AI assistants on everyday devices instead of energy-hungry servers. This breakthrough balances AI capability with real-world usability, making advanced models faster, greener, and more accessible."
Poster,Deterministic Sparse Fourier Transform for Continuous Signals with Frequency Gap,https://ICML.cc//virtual/2025/poster/44564,"Xiaoyu Li, Zhao Song, Shenghao Xie","The Fourier transform is a fundamental tool in computer science and signal processing. In particular, when the signal is sparse in the frequency domain---having only $k$ distinct frequencies---sparse Fourier transform (SFT) algorithms can recover the signal in a sublinear time (proportional to the sparsity $k$). Most prior research focused on SFT for discrete signals, designing both randomized and deterministic algorithms for one-dimensional and high-dimensional discrete signals. However, SFT for continuous signals (i.e., $x^*(t)=\sum_{j=1}^k v_j e^{2\pi \mathbf{i} f_j t}$ for $t\in [0,T]$) is a more challenging task. The discrete SFT algorithms are not directly applicable to continuous signals due to the sparsity blow-up from the discretization. Prior to this work, there is a randomized algorithm that achieves an $\ell_2$ recovery guarantee in $\widetilde{O}(k\cdot \mathrm{polylog}(F/\eta))$ time, where $F$ is the band-limit of the frequencies and $\eta$ is the frequency gap.Nevertheless, whether we can solve this problem without using randomness remains open.  In this work, we address this gap and introducethe first sublinear-time deterministic sparse Fourier transform algorithm in the continuous setting. Specifically, our algorithm uses $\widetilde{O}(k^2 \cdot \mathrm{polylog}(F/\eta))$ samples and $\widetilde{O}(k^2 \cdot \mathrm{polylog}(F/\eta))$ time to reconstruct the on-grid signal with arbitrary noise that satisfies a mild condition. This is the optimal recovery guarantee that can be achieved by any deterministic approach.","Imagine trying to identify the few distinct notes being played in a piece of music while the orchestra is still performing. Traditional tools like the Fast Fourier Transform check every possible note, which is quick but still slower than it needs to be if only a handful of notes are present, and they usually work only after the music has been neatly digitized. We introduced a new shortcut that never rolls the dice and never needs that rigid digital grid. Our method zooms straight in on the real notes of any continuous-time signal, using only a tiny fraction of the usual measurements and without ever missing a note. Because it’s both quick and fail-safe, this technique can make radar imaging, wireless communications, heart-monitor readings, and other real-time systems faster and more reliable, where every microsecond and every lost signal matter."
Poster,Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models,https://ICML.cc//virtual/2025/poster/46079,"Rafał Karczewski, Markus Heinonen, Vikas Garg","Diffusion models have emerged as a powerful class of generative models, capable of producing high-quality images by mapping noise to a data distribution. However, recent findings suggest that image likelihood does not align with perceptual quality: high-likelihood samples tend to be smooth, while lower-likelihood ones are more detailed. Controlling sample density is thus crucial for balancing realism and detail. In this paper, we analyze an existing technique, Prior Guidance, which scales the latent code to influence image detail. We introduce score alignment, a condition that explains why this method works and show that it can be tractably checked for any continuous normalizing flow model. We then propose Density Guidance, a principled modification of the generative ODE that enables exact log-density control during sampling. Finally, we extend Density Guidance to stochastic sampling, ensuring precise log-density control while allowing controlled variation in structure or fine details. Our experiments demonstrate that these techniques provide fine-grained control over image detail without compromising sample quality. Code is available at https://github.com/Aalto-QuML/density-guidance.","Modern AI systems can generate incredibly realistic images, but controlling the level of detail in these images is surprisingly tricky. Sometimes the images look too smooth or blurry, while other times they're overly complex or even nonsensical. Why does this happen?Our research explores how image detail is tied to something called ""likelihood"" — a measure of how probable an image is, according to the AI model. We found that images the model considers very likely are often too smooth, while the more detailed and realistic ones tend to be less likely.To fix this, we developed a method called Density Guidance, which gives users precise control over how ""likely"" or ""detailed"" the images should be. It works by adjusting the model's internal sampling process in a mathematically principled way. We also extended this method to support randomness, enabling variation in the image’s shape or fine texture — without losing control over quality.This makes it easier for researchers and creators to generate images with just the right amount of detail, opening the door to more reliable and tunable AI-generated content."
Poster,DexScale: Automating Data Scaling for Sim2Real Generalizable Robot Control,https://ICML.cc//virtual/2025/poster/46168,"Guiliang Liu, Yueci Deng, Runyi Zhao, Huayi Zhou, Jian Chen, Jietao Chen, Ruiyan Xu, Yunxin Tai, Kui Jia","A critical prerequisite for achieving generalizable robot control is the availability of a large-scale robot training dataset. Due to the expense of collecting realistic robotic data, recent studies explored simulating and recording robot skills in virtual environments. While simulated data can be generated at higher speeds, lower costs, and larger scales, the applicability of such simulated data remains questionable due to the gap between simulated and realistic environments. To advance the Sim2Real generalization, in this study, we present DexScale, a data engine designed to perform automatic skills simulation and scaling for learning deployable robot manipulation policies. Specifically, DexScale ensures the usability of simulated skills by integrating diverse forms of realistic data into the simulated environment, preserving semantic alignment with the target tasks. For each simulated skill in the environment, DexScale facilitates effective Sim2Real data scaling by automating the process of domain randomization and adaptation. Tuned by the scaled dataset, the control policy achieves zero-shot Sim2Real generalization across diverse tasks, multiple robot embodiments, and widely studied policy model architectures, highlighting its importance in advancing Sim2Real embodied intelligence.","Training robots to perform real-world tasks usually requires huge amounts of data collected from actual robots. This process is slow, expensive, and difficult to scale. A faster alternative is to train robots in virtual simulations. But here’s the problem: what works in simulation often fails in reality because the virtual world doesn’t fully reflect the complexity of the real one.To solve this, we developed DexScale, a tool that generates more useful and realistic robot training data in simulation. DexScale mixes in elements from real-world experiences and automatically adjusts the virtual training environment to semantically align with real-life conditions. Besides, DexScale designs automatic domain randomization and adaptation methods, which make simulations more robust and diverse.What makes DexScale especially powerful is its ability to achieve zero-shot Sim2Real transfer. This means a robot trained only in simulation, without any additional real-world fine-tuning, can successfully perform complex tasks in the real world right away. DexScale works across a variety of tasks, robot types, and learning models, making it a significant step toward building general-purpose, real-world-ready robots."
Poster,D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples,https://ICML.cc//virtual/2025/poster/45014,"Zijing Hu, Fengda Zhang, Kun Kuang","The practical applications of diffusion models have been limited by the misalignment between generated images and corresponding text prompts. Recent studies have introduced direct preference optimization (DPO) to enhance the alignment of these models. However, the effectiveness of DPO is constrained by the issue of visual inconsistency, where the significant visual disparity between well-aligned and poorly-aligned images prevents diffusion models from identifying which factors contribute positively to alignment during fine-tuning. To address this issue, this paper introduces D-Fusion, a method to construct DPO-trainable visually consistent samples. On one hand, by performing mask-guided self-attention fusion, the resulting images are not only well-aligned, but also visually consistent with given poorly-aligned images. On the other hand, D-Fusion can retain the denoising trajectories of the resulting images, which are essential for DPO training. Extensive experiments demonstrate the effectiveness of D-Fusion in improving prompt-image alignment when applied to different reinforcement learning algorithms.","Diffusion models are powerful tools for generating images from text, but they often produce images that don't quite match what users ask for. A recent technique called Direct Preference Optimization (DPO) tries to fix this by teaching the model what users prefer. However, it struggles to learn effectively when the training images look too different from each other, since it is hard to figure out which changes lead to better matches between images and text. Our research proposes a new method called D-Fusion to solve this problem. D-Fusion creates pairs of training images that are more visually similar but differ in how well they match the text prompt. It also keeps track of important intermediate states when creating images, which are needed by DPO. By applying DPO with these images, the model can effectively learn how to generate images that match the given text. This work brings us closer to building image generation tools that follow instructions more faithfully."
Poster,Diagonal Symmetrization of Neural Network Solvers for the Many-Electron Schrödinger Equation,https://ICML.cc//virtual/2025/poster/45806,"Kevin Han Huang, Ni Zhan, Elif Ertekin, Peter Orbanz, Ryan P. Adams","Incorporating group symmetries into neural networks has been a cornerstone of success in many AI-for-science applications. Diagonal groups of isometries, which describe the invariance under a simultaneous movement of multiple objects, arise naturally in many-body quantum problems. Despite their importance, diagonal groups have received relatively little attention, as they lack a natural choice of invariant maps except in special cases. We study different ways of incorporating diagonal invariance in neural network ansatze trained via variational Monte Carlo methods, and consider specifically data augmentation, group averaging and canonicalization. We show that, contrary to standard ML setups, in-training symmetrization destabilizes training and can lead to worse performance. Our theoretical and numerical results indicate that this unexpected behavior may arise from a unique computational-statistical tradeoff not found in standard ML analyses of symmetrization. Meanwhile, we demonstrate that post hoc averaging is less sensitive to such tradeoffs and emerges as a simple, flexible and effective method for improving neural network solvers.","We use neural networks to learn how electrons behave in crystals. Due to the large number of electrons, this can be very costly and difficult. Fortunately, crystals have many nice geometric symmetries, which constrains the behaviors of these electrons. There are many success stories in machine learning (ML) about designing neural networks with the right symmetries in mind, which helps to reduce the problem search space. We apply existing approaches from ML and investigate whether they can help with our task for crystals.Surprisingly, the results are mixed. We find that employing symmetries during neural network training can actually hurt. On the other hand, adding symmetries to a trained network can improve the performance substantially. We also provide theoretical explanations for why these effects occur. These results are particularly interesting, in light of recent works in AI for science that show that symmetries may in fact be unnecessary for obtaining the best performance."
Poster,Dialogue Without Limits: Constant-Sized KV Caches for Extended Response in LLMs,https://ICML.cc//virtual/2025/poster/45197,"Ravi Ghadia, Avinash Kumar, Gaurav Jain, Prashant J. Nair, Poulami Das","Autoregressive Transformers rely on Key-Value (KV) caching to accelerate inference. However, the linear growth of the KV cache with context length leads to excessive memory consumption and bandwidth constraints. Existing methods drop distant tokens or compress states in a lossy manner, sacrificing accuracy by discarding vital context or introducing bias.We propose ${MorphKV}$, an inference-time technique that maintains a constant-sized KV cache while preserving accuracy. MorphKV balances long-range dependencies and local coherence during text generation. It eliminates early-token bias while retaining high-fidelity context by adaptively ranking tokens through correlation-aware selection. Unlike heuristic retention or lossy compression, MorphKV iteratively refines the KV cache via lightweight updates guided by attention patterns of recent tokens. This approach captures inter-token correlation with greater accuracy, which is crucial for tasks like content creation and code generation. Our studies on long-response tasks show 52.9\% memory savings and 18.2\% higher accuracy on average compared to state-of-the-art prior works, enabling efficient deployment.","The increasing size of KV caches presents a critical bottleneck, particularly for long-response tasks, such as content creation, code generation, etc. Compressed KV caches address this challenge by evicting unimportant tokens, but presents a trade-off between accuracy versus memory savings. Retaining too few KVs reduces accuracy, whereas accurate methods are not memory-efficient. Ideally, we want to compress KV caches without sacrificing accuracy. We introduce $MorphKV$, a dynamic KV cache pruning method that maintains a constant-size KV cache by keeping only a select subset of KVs. MorphKV improves accuracy by dynamically preserving only those KVs that exhibit strong correlation with recently generated tokens. This enables MorphKV to efficiently handle long-context and long-response tasks, even when operating with limited hardware resources."
Poster,DiffAdvMAP: Flexible Diffusion-Based Framework for Generating Natural Unrestricted Adversarial Examples,https://ICML.cc//virtual/2025/poster/43968,"Zhengzhao Pan, Hua Chen, Xiaogang Zhang","Unrestricted adversarial examples(UAEs) have posed greater threats to deep neural networks(DNNs)  than perturbation-based adversarial examples(AEs) because they can make extensive changes to images without being restricted in a fixed norm perturbation budget. Although current diffusion-based methods can generate more natural UAEs than other unrestricted attack methods, the overall effectiveness of such methods is restricted since they are designed for specific attack conditions. Additionally, the naturalness of UAEs still has room for improvement, as these methods primarily focus on leveraging diffusion models as strong priors to enhance the generation process. This paper proposes a flexible framework named Diffusion-based Adversarial Maximum a Posterior(DiffAdvMAP) to generate more natural UAEs for various scenarios. DiffAdvMAP approaches the generation of UAEs by sampling images from posterior distributions, which is achieved by approximating the posterior distribution of UAEs using the prior distribution of real data learned by the diffusion model. This process enhances the naturalness of the UAEs. By incorporating an adversarial constraint to ensure the effectiveness of the attack, DiffAdvMAP exhibits excellent attack ability and defense robustness. A reconstruction constraint is designed to enhance its flexibility, which allows DiffAdvMAP to be tailored to various attack scenarios. Experimental results on Imagenet show that we achieve a better trade-off between image quality, flexibility, and transferability than baseline unrestricted adversarial attack methods.","Traditional ""AI-tricking images"" (like adding tiny invisible tweaks to real images) can fool AI systems, but they become less dangerous because they only make small, limited changes. Unrestricted AI-tricking images (UAEs) are way riskier—they can completely alter the content of a picture (e.g., changing shapes, colors, or even the whole structure) to trick AI more effectively. While current methods (which based on AI model that can generate realistic-looking images) can create more natural UAEs, they still have two big flaws: Limited use--They are only designed for specific situations; not realistic enough--The fake images still look slightly off because they use the AI’s ""existing knowledge"" simply. To fix these problems, we built a new tool called DiffAdvMAP. Here’s how it works: Looks real--The tool fully considers the relationship between existing knowledge and goals, which creates fake images that look closer to real images; Works reliably--These images consistently fool AI systems. Plus, the tool can adjust its attack strategy for different needs. Tests show that DiffAdvMAP’s fake images are harder to spot and more successful in tricking AI across various scenarios. Whether it's image quality, adapting to different situations, or attacking other AI models, it beats existing methods. In short, it makes ""fake images"" both sneaky and powerful."
