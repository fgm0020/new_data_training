type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Machines and Mathematical Mutations: Using GNNs to Characterize Quiver Mutation Classes,https://ICML.cc//virtual/2025/poster/44529,"Jesse He, Helen Jenne, Herman Chau, Davis Brown, Mark Raugas, Sara Billey, Henry Kvinge","Machine learning is becoming an increasingly valuable tool in mathematics, enabling one to identify subtle patterns across collections of examples so vast that they would be impossible for a single researcher to feasibly review and analyze. In this work, we use graph neural networks to investigate quiver mutation---an operation that transforms one quiver (or directed multigraph) into another---which is central to the theory of cluster algebras with deep connections to geometry, topology, and physics. In the study of cluster algebras, the question of mutation equivalence is of fundamental concern: given two quivers, can one efficiently determine if one quiver can be transformed into the other through a sequence of mutations? In this paper, we use graph neural networks and AI explainability techniques to independently discover mutation equivalence criteria for quivers of type $\tilde{D}$. Along the way, we also show that even without explicit training to do so, our model captures structure within its hidden representation that allows us to reconstruct known criteria from type $D$, adding to the growing evidence that modern machine learning models are capable of learning abstract and general rules from mathematical data.","As machine learning is applied to more and more tasks, mathematics offers a unique opportunity. On one hand, ML can help mathematicians sift through mountains of examples. On the other hand, mathematics has clear rules and logic that help us understand what ML models are doing internally. In this work, we applied a GNN---a special ML model that works on networks---to a math problem with deep connections to algebra and physics. We then use AI explainability tools to interpret the results, recovering a nontrivial theorem about this problem and independently recovering another result. Our results show that ML models can learn abstract rules on mathematical problems."
Poster,MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces,https://ICML.cc//virtual/2025/poster/44419,"Loris Gaven, Thomas Carta, Clément Romac, Cédric Colas, sylvain lamprier, Olivier Sigaud, Pierre-Yves Oudeyer","Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one’s own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and learning progress online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.","Imagine an AI agent as a curious student exploring a vast library filled with countless books on different subjects. The student wants to learn as much as possible, but with limited time, they need to choose which books will teach them the most. This is exactly the challenge faced by AI agents designed for open-ended learning.Traditional AI systems struggle with this problem because they either spend too much time testing every possible choice or rely on pre-programmed categories that don't adapt well to new situations. It's like having a student who either reads random pages from every book or only sticks to a rigid reading list that never changes.Our solution, called MAGELLAN, gives AI agents a crucial ability: self-awareness about their own learning. Just as good students develop intuition about which subjects they're ready to tackle next, MAGELLAN helps AI agents predict how much they'll learn from different goals before committing significant time to them.The key insight is that learning goals aren't isolated islands, they're connected in meaningful ways. For example, learning to ride a bicycle helps with learning to ride a motorcycle. MAGELLAN captures these relationships, allowing agents to make educated guesses about their progress on new goals based on their experience with related ones.We tested this approach in a complex learning environment where goals constantly evolved and multiplied. While other methods struggled to keep up, MAGELLAN enabled our AI agent to efficiently prioritize its learning and eventually master the entire space of available goals.This research shows how giving AI agents metacognitive abilities, essentially teaching them to think about their own thinking, can dramatically improve their ability to learn in open-ended, ever-changing environments. This could lead to more adaptable AI systems that continue learning and improving throughout their deployment."
Poster,Mahalanobis++: Improving OOD Detection via Feature Normalization,https://ICML.cc//virtual/2025/poster/43649,"Maximilian Müller, Matthias Hein","Detecting out-of-distribution (OOD) examples is an important task for deploying reliable machine learning models in safety-critial applications. While post-hoc methods based on the Mahalanobis distance applied to pre-logit features are among the most effective for ImageNet-scale OOD detection, their performance varies significantly across models. We connect this inconsistency to strong variations in feature norms, indicating severe violations of the Gaussian assumption underlying the Mahalanobis distance estimation. We show that simple $\ell_2$-normalization of the features mitigates this problem effectively, aligning better with the premise of normally distributed data with shared covariance matrix. Extensive experiments on 44 models across diverse architectures and pretraining schemes show that $\ell_2$-normalization improves the conventional Mahalanobis distance-based approaches significantly and consistently, and outperforms other recently proposed OOD detection methods.","In critical applications like healthcare or autonomous driving, it is important that AI systems can tell when they’re seeing something unfamiliar — what researchers call ""out-of-distribution"" (OOD) data. If a model cannot do this, it might make confident but dangerously wrong predictions.One popular way to detect unfamiliar images uses a mathematical technique called Mahalanobis distance. In practice, this method works very well for some AI models, but surprisingly poorly for others. We investigated why — and found that the inconsistency is due to the way these models represent images internally, especially how large or small their feature values are.Our solution is surprisingly simple: by adjusting the internal image features to all have the same size — a technique called $\ell_2$-normalization — we tackle this problem. We show that $\ell_2$-normalization makes the models' behavior more predictable and greatly improves their ability to spot unfamiliar data.We tested this across 44 models and found our method consistently outperforms existing approaches. The code is freely available online."
Poster,Maintaining Proportional Committees with Dynamic Candidate Sets,https://ICML.cc//virtual/2025/poster/44929,"Chris Dong, Jannik Peters","Multiwinner voting is the study of electing a fixed-size committee given individual agents' preferences over candidates. Most research in this field has been limited to a static setting, with only one election over a fixed set of candidates. However, this approach overlooks the dynamic nature of applications, where candidate sets are subject to change.We extend the study of proportionality in multiwinner voting to dynamic settings, allowing candidates to join or leave the election and demanding that each chosen committee satisfies proportionality without differing too much from the previously selected committee. We consider approval preferences, ranked preferences, and the proportional clustering setting. In these settings, we either give algorithms making few changes or show that such algorithms cannot exist for various proportionality axioms. In particular, we show that such algorithms cannot exist for ranked preferences and provide amortized and exact algorithms for several proportionality notions in the other two settings.","We investigate how to choose committees that represent the electorate in settings with *dynamic candidate sets*. This is important because so far (i) committees are elected to proportionally represent the electorate, but(ii) when new candidates become available or elected candidates want to give up their seat, the resulting proportionality violations may not be easily fixable.We study three settings where candidates can one by one become available, give up their seat, or either of the two scenarios could happen in each time step. We provide proportional algorithms for these dynamic settings that require a small number of changes to the committee in each round, or lower bounds on the changes per round to prove that this is not possible."
Poster,Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment,https://ICML.cc//virtual/2025/poster/45221,"Chenghao Fan, zhenyi lu, Sichen Liu, Chengfeng Gu, Xiaoye Qu, Wei Wei, Yu Cheng","While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singularvalue decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. {Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture.}{However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture.} To mitigate these issues, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE’s efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT’s state-of-the-art performance, closing the gap with Full FT. Our code is available at:  https://github.com/Facico/GOAT-PEFT","Large language models like ChatGPT are powerful but require expensive and time-consuming training to perform well on specific tasks. A popular method called LoRA allows these models to be fine-tuned more efficiently by updating only a small part of the model. However, LoRA often doesn’t perform as well as fully fine-tuning the entire model. Our work introduces a new method called GOAT, which improves LoRA by combining it with a system that uses multiple expert components, each specializing in different tasks. We also discover a simple mathematical adjustment that helps these expert components work better together. This makes our method both efficient and powerful—achieving performance close to full fine-tuning without needing extra computational cost. We tested our method on 25 tasks, ranging from language understanding to image recognition, and found that it consistently outperforms previous approaches."
Poster,Making Hard Problems Easier with Custom Data Distributions and Loss Regularization: A Case Study in Modular Arithmetic,https://ICML.cc//virtual/2025/poster/44190,"Eshika Saxena, Alberto Alfarano, Emily Wenger, Kristin Lauter","Recent work showed that ML-based attacks on Learning with Errors (LWE), a hard problem used in post-quantum cryptography, outperform classical algebraic attacks in certain settings. Although promising, ML attacks struggle to scale to more complex LWE settings. Prior work connected this issue to the difficulty of training ML models to do modular arithmetic, a core feature of the LWE problem. To address this, we develop techniques that significantly boost the performance of ML models on modular arithmetic tasks—enabling the models to sum up to $N=128$ elements modulo $q \le 974269$. Our core innovation is the use of custom training data distributions and a carefully designed loss function that better represents the problem structure. We apply an initial proof of concept of our techniques to LWE specifically and find that they allow recovery of 2x harder secrets than prior work. Our techniques also help ML models learn other well-studied problems better, including copy, associative recall, and parity, motivating further study.","Recently, researchers have found that machine learning (ML) models can be trained to solve hard math problems that are used in cryptography to keep information secure. However, these models still struggle to do modular arithmetic, which is a core part of these math problems. In this work, we develop methods that improve model performance on modular arithmetic. Namely, we train the model on a curated mixture of easy and hard problems while also penalizing the model for predicting the same output for every input. We show that these methods can be extended beyond arithmetic to assessing the security of existing cryptography systems and also improving performance on other well-studied problems in ML."
Poster,MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving,https://ICML.cc//virtual/2025/poster/46138,"Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang","Solving mathematical problems using computer-verifiable languages like Lean has significantly impacted the mathematical and computer science communities. State-of-the-art methods utilize a single Large Language Model (LLM) to generate complete proof or perform tree search, but they fail to balance these tasks. We propose **MA-LoT**: *Model-CollAboration Lean-based Long Chain-of-Thought*, a comprehensive framework for Lean4 theorem proving to solve this issue. It separates the cognition tasks of general NL for whole-proof generation and error analysis for proof correction using the model-collaboration method. We achieve this by structured interaction of the LLM and Lean4 verifier in Long CoT. To implement the framework, we propose the novel *LoT-Transfer Learning* training-inference pipeline, which enables the Long CoT thinking capability to LLMs without special data annotation. Extensive experiment shows that our framework achieves a **61.07%** accuracy rate on the Lean4 version of the MiniF2F-Test dataset, largely outperforming DeepSeek-V3 (33.61%), single-model tree search (InternLM-Step-Prover, 50.70%), and whole-proof generation (Godel-Prover, 55.33%) baselines. Furthermore, our findings highlight the potential of combining Long CoT with formal verification for a more insightful generation in a broader perspective.","How can we make AI models to reason like humans? Not just produce some words based on an educated guess of answers based on training data, but truly ground the reasoning in some concrete foundation. Researchers answer this question by putting math reasoning into verifiable frameworks and using LLMs to solve the problem.While traditional methods either educated guess the entire proof in one shot, often making mistakes and hard to correct, or tediously check each step, which slows everything down. Our work introduces **MA-LoT**, a new kind of AI framework that learns to do both: plan and double-check the proof like a mathematician. One model sketches the big idea of a proof, and another model revises it based on feedback, just like a student learning from a tutor’s comments.To make this work, we created a new training-inference method called *LoT-Transfer Learning*. It helps the model to develop deep thinking skills without needing huge amounts of hand-labeled data. In the sense of the result, our framework solved some of the toughest problems from international math contests. We provide a more accurate and efficient effort. This isn’t just about math. It’s a step toward AI systems that can think carefully, learn from mistakes, and help in fields where correctness really matters, like software verification and education."
Poster,MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models,https://ICML.cc//virtual/2025/poster/44415,"Mahir Labib Dihan, Tanvir Hassan, Md Tanvir Parvez, Hasebul Hasan, Almash Alam, Muhammad Aamir Cheema, Mohammed Eunus Ali, Md Rizwan Parvez","Recent advancements in foundation models have improved autonomous tool usage and reasoning, but their capabilities in map-based reasoning remain underexplored. To address this, we introduce MapEval, a benchmark designed to assess foundation models across three distinct tasks—textual, API-based, and visual reasoning—through 700 multiple-choice questions spanning 180 cities and 54 countries, covering spatial relationships, navigation, travel planning, and real-world map interactions. Unlike prior benchmarks that focus on simple location queries, MapEval requires models to handle long-context reasoning, API interactions and visual map analysis, making it the most comprehensive evaluation framework for geospatial AI.  On evaluation of 30 foundation models, including Claude-3.5-Sonnet, GPT-4o, Gemini-1.5-Pro, none surpasses 67% accuracy, with open-source models performing significantly worse and all models lagging over 20% behind human performance. These results expose critical gaps in spatial inference, as models struggle with distances, directions, route planning, and place-specific reasoning, highlighting the need for better geospatial AI to bridge the gap between foundation models and real-world navigation.","Imagine asking Siri or ChatGPT questions like: ""What’s the best-rated restaurant on the left side of my driving path from my home to office?"" Today’s smart AIs are great with words, but when it comes to map-based questions—about places, distances, directions, or travel—they still struggle.This research presents MapEval, which tests how well AI models can understand and reason about real-world maps, places, and navigation tasks. It poses complex, practical questions—just like the ones we ask in everyday life when using apps like Google Maps or travel assistants. If we want AI to truly help us with navigation, travel planning, or even urban logistics, they must get better at understanding spatial and map-related reasoning. MapEval helps shine a light on where current models fail and how we can improve them."
Poster,MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning,https://ICML.cc//virtual/2025/poster/45547,"Zihan Chen, Song Wang, Zhen Tan, Jundong Li, Cong Shen","In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle diverse tasks by incorporating multiple input-output examples, known as demonstrations, into the input of LLMs. More recently, advancements in the expanded context windows of LLMs have led to many-shot ICL, which uses hundreds of demonstrations and outperforms few-shot ICL, which relies on fewer examples. However, this approach is often hindered by the high cost of obtaining large amounts of labeled data. To address this challenge, we propose **M**any-Shot **A**daptive **P**seudo-**L**ab**E**ling, namely **MAPLE**, a novel influence-based many-shot ICL framework that utilizes pseudo-labeled samples to compensate for the lack of label information. We first identify a subset of impactful unlabeled samples and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled samples are then adaptively selected and tailored to each test query as input to improve the performance of many-shot ICL, without significant labeling costs.Extensive experiments on real-world datasets demonstrate the effectiveness of our framework, showcasing its ability to enhance LLM adaptability and performance with limited labeled data. Our code is provided at https://github.com/Chen-1031/MAPLE_ICL.","Large language models like ChatGPT can solve tasks such as summarizing text or answering questions by looking at a few examples—this is called in-context learning (ICL). When given more examples (many-shot ICL), models usually perform better, but collecting many labeled examples can be expensive and time-consuming.Our work proposes a way to improve many-shot ICL without needing so many human-labeled examples. Instead, we identify the most useful unlabeled examples, ask the model itself to infer their labels (a technique called pseudo-labeling), and then carefully choose which of these examples to show the model when it's solving new tasks.We call this method MAPLE. It builds a network connecting labeled and unlabeled data to determine which samples are most helpful. MAPLE then adaptively selects the most relevant examples for each question.This approach significantly boosts model performance across a wide range of tasks while keeping labeling costs low, making powerful AI more accessible and practical for real-world use."
Poster,MARGE: Improving Math Reasoning with Guided Exploration,https://ICML.cc//virtual/2025/poster/44860,"Jingyue Gao, Runji Lin, Keming Lu, Bowen Yu, Junyang Lin, Jianyu Chen","Large Language Models (LLMs) exhibit strong potential in mathematical reasoning, yet their effectiveness is often limited by a shortage of high-quality queries.This limitation necessitates scaling up computational responses through self-generated data, yet current methods struggle due to spurious correlated data caused by ineffective exploration across all reasoning stages.To address such challenge, we introduce **MARGE**: Improving **Ma**th **R**easoning with **G**uided **E**xploration, a novel method that enhances mathematical reasoning through hit-guided exploration.MARGE systematically explores intermediate reasoning states derived from self-generated solutions, enabling adequate exploration and improved credit assignment throughout the reasoning process.Notably, MARGE improves both single-shot accuracy and exploration diversity, mitigating a common trade-off in alignment methods.These results demonstrate MARGE's effectiveness in enhancing mathematical reasoning capabilities and unlocking the potential of scaling self-generated training data.","How do we improve LLMs' reasoning ability, as high-quality queries and solutions are scarce?One natural way is to ask LLMs to generate more responses for post-training.However, finding effective training data on multi-step tasks like math is hard due to a long reasoning chain.The main idea behind MARGE is to use the guidance of an existing solution to boost exploration and improve credit assignment.By completing intermediate states, the models obtain a larger training set with lower spurious correlations, which enables the scaling in self-training pipelines.More surprisingly, our method improves reasoning accuracy and diversity, indicating that unexplored patterns are found during exploration.Our research demonstrates the benefits and importance of exploration for future LLM reasoning and post-training studies."
