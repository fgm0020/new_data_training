type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,How Expressive are Knowledge Graph Foundation Models?,https://ICML.cc//virtual/2025/poster/44147,"Xingyue Huang, Pablo Barcelo, Michael Bronstein, Ismail Ceylan, Mikhail Galkin, Juan Reutter, Miguel Romero Orth","Knowledge Graph Foundation Models (KGFMs) are at the frontier for deep learning on knowledge graphs (KGs), as they can generalize to completely novel knowledge graphs with different relational vocabularies. Despite their empirical success, our theoretical understanding of KGFMs remains very limited. In this paper, we conduct a rigorous study of the expressive power of KGFMs. Specifically, we show that the expressive power of KGFMs directly depends on the *motifs* that are used to learn the relation representations. We then observe that the most typical motifs used in the existing literature are *binary*, as the representations are learned based on how pairs of relations interact, which limits the model's expressiveness. As part of our study, we design more expressive KGFMs using richer motifs, which necessitate learning relation representations based on, e.g., how triples of relations interact with each other. Finally, we empirically validate our theoretical findings, showing that the use of richer motifs results in better performance on a wide range of datasets drawn from different domains.","Knowledge graphs foundation model (KGFM) can understand and conduct predictions on entirely new knowledge graphs, a structured way to represent information with entities (like people or places) and their relationships (like ""lives in"" or ""is part of""). However, we still don’t fully understand why KGFMs work so well. In this paper, we dive into the theory behind them. We find that their success depends on the building blocks they use to understand relationships, what we call “motifs.” Current models mostly look at how two relationships interact, but we show that this is often not enough. By designing models that consider more complex patterns, like interactions between three or more relations, we can build a more expressive model that can distinguish more links in the knowledge graphs that previously could not be distinguished. We test these ideas and find that these new models do better across a variety of real-world datasets."
Poster,How Far Is Video Generation from World Model: A Physical Law Perspective,https://ICML.cc//virtual/2025/poster/46015,"Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng","Scaling video generation models is believed to be promising in building world models that adhere to fundamental physical laws. However, whether these models can discover physical laws purely from vision can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios.In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization.We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws.We focus on the scaling behavior of trainingdiffusion-based video generation models to predict object movements based on initial frames.Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios.Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit ""case-based"" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color $>$ size $>$ velocity $>$ shape.Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws.","Current video generation models are powerful and able to generate high-fidelity videos, which might help build AI systems that can simulate the future of real-world. However, it's unclear if current AI models, trained solely by watching videos, truly learn these fundamental rules.In this paper, we explore whether modern AI video models can discover and generalize basic physics principles simply by watching videos. We test the AI's ability to predict object movements in three scenarios: familiar cases (similar to the training data), completely unfamiliar situations, and new combinations of known elements.Using simplified computer-generated videos of moving and colliding objects, we systematically trained and evaluated video prediction models. Our findings show these models can handle familiar situations perfectly, and they perform reasonably well when combining known factors in new ways. However, they struggle significantly when faced with completely new situations they haven't encountered before.Further investigation revealed two important insights. First, these AI models do not truly learn general physics principles but instead rely heavily on remembering specific examples they've seen before. Second, when faced with new situations, the models prioritize features in a particular order—focusing first on color, then size, then speed, and finally shape.Overall, our research highlights that simply training larger AI models with more videos isn't enough. To create AI systems that genuinely understand physics, new approaches are needed beyond just scaling up existing methods."
Poster,How Much Can Transfer? BRIDGE: Bounded Multi-Domain Graph Foundation Model with Generalization Guarantees,https://ICML.cc//virtual/2025/poster/44723,"Haonan Yuan, Qingyun Sun, Junhua Shi, Xingcheng Fu, Bryan Hooi, Jianxin Li, Philip Yu","Graph Foundation Models hold significant potential for advancing multi-domain graph learning, yet their full capabilities remain largely untapped. Existing works show promising task performance with the “pretrain-then-prompt” paradigm, which lacks theoretical foundations to understand why it works and how much knowledge can be transferred from source domains to the target. In this paper, we introduce BRIDGE, a bounded graph foundation model pre-trained on multi-domains with Generalization guarantees. To learn discriminative source knowledge, we align multi-domain graph features with domain-invariant aligners during pre-training. Then, a lightweight Mixture of Experts (MoE) network is proposed to facilitate downstream prompting through self-supervised selective knowledge assembly and transfer. Further, to determine the maximum amount of transferable knowledge, we derive an optimizable generalization error upper bound from a graph spectral perspective given the Lipschitz continuity. Extensive experiments demonstrate the superiority of BRIDGE on both node and graph classification compared with 15 state-of-the-art baselines.","Graphs are powerful tools for representing complex systems, such as social networks, biological pathways, and transportation maps. However, building models that can learn from graphs across different domains remains a major challenge. Most current approaches focus on task performance without offering theoretical insights into how well knowledge from one domain can transfer to another.Our work presents BRIDGE, a graph foundation model designed to handle multiple domains while providing formal guarantees on generalization. During pre-training, BRIDGE aligns graph features from diverse domains using domain-invariant representations, helping the model learn shared patterns. To support efficient transfer, we introduce a lightweight Mixture of Experts network that selects and assembles relevant knowledge for new tasks without needing full retraining. Additionally, we derive a theoretical upper bound on the model’s generalization error based on graph spectral theory, offering a principled way to estimate transferability.BRIDGE outperforms 15 state-of-the-art models on both node and graph classification tasks. Our approach not only improves performance but also provides deeper understanding of what makes knowledge transfer effective in graph learning."
Poster,How Much Can We Forget about Data Contamination?,https://ICML.cc//virtual/2025/poster/45377,"Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike Luxburg","The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Llama 3 405B,  have forgotten the data seen at the beginning of training.","When training large language models (LLMs), researchers worry about ""data contamination"" - what happens when a test question accidentally appears in the training data? If a model has seen an answer during training, will it be able to ""cheat""?In this paper, we conduct controlled experiments where we deliberately provide LLMs with answers to evaluation questions. We study how a model's ability to answer depends on how often questions were seen during training, the number of model parameters, and the overall size of the training data.Our main finding is that the effect of contamination depends strongly on the training setup. On the one hand, if a model is very large or has been exposed to a test question frequently, this can lead to overfitting. On the other hand, exposure to sufficient new data mitigates the overfitting up to the point where the LLM has forgotten that it ever saw the test question.Our research demonstrates that large training datasets provide natural protection against accidental contamination, which has important implications for how we evaluate and train AI systems."
Poster,How to Evaluate and Mitigate IP Infringement in Visual Generative AI?,https://ICML.cc//virtual/2025/poster/44267,"Zhenting Wang, Chen Chen, Vikash Sehwag, Minzhou Pan, Lingjuan Lyu","The popularity of visual generative AI models like DALL-E 3, Stable Diffusion XL, Stable Video Diffusion, and Sora has been increasing. Through extensive evaluation, we discovered that the state-of-the-art visual generative models can generate content that bears a striking resemblance to characters protected by intellectual property rights held by major entertainment companies (such as Sony, Marvel, and Nintendo), which raises potential legal concerns. This happens when the input prompt contains the character's name or even just descriptive details about their characteristics. To mitigate such IP infringement problems, we also propose a defense method against it. In detail, we develop a revised generation paradigm that can identify potentially infringing generated content and prevent IP infringement by utilizing guidance techniques during the diffusion process. It has the capability to recognize generated content that may be infringing on intellectual property rights, and mitigate such infringement by employing guidance methods throughout the diffusion process without retrain or fine-tune the pretrained models. Experiments on well-known character IPs like Spider-Man, Iron Man, and Superman demonstrate the effectiveness of the proposed defense method.","Generative AI can now create lifelike images from simple text. But sometimes, they accidentally produce pictures that look too much like famous characters from movies or games — even without naming them. This can lead to legal issues. Our research shows how common this problem is, and we introduce a simple way to stop it. We built a system that checks what the AI creates and gently guides it to avoid copying well-known characters, without hurting image quality. This helps make AI image generation safer and more responsible for everyone."
Poster,How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects,https://ICML.cc//virtual/2025/poster/44086,"Wonkwang Lee, Jongwon Jeong, Taehong Moon, Hyeon-Jong Kim, Jaehyeon Kim, Gunhee Kim, Byeong-Uk Lee","Motion synthesis for diverse object categories holds great potential for 3D content creation but remains underexplored due to two key challenges: (1) the lack of comprehensive motion datasets that include a wide range of high-quality motions and annotations, and (2) the absence of methods capable of handling heterogeneous skeletal templates from diverse objects.To address these challenges, we contribute the following:First, we augment the Truebones Zoo dataset—a high-quality animal motion dataset covering over 70 species—by annotating it with detailed text descriptions, making it suitable for text-based motion synthesis.Second, we introduce rig augmentation techniques that generate diverse motion data while preserving consistent dynamics, enabling models to adapt to various skeletal configurations.Finally, we redesign existing motion diffusion models to dynamically adapt to arbitrary skeletal templates, enabling motion synthesis for a diverse range of objects with varying structures.Experiments show that our method learns to generate high-fidelity motions from textual descriptions for diverse and even unseen objects, setting a strong foundation for motion synthesis across diverse object categories and skeletal templates.Qualitative results are available on this [link](https://t2m4lvo.github.io).","Creating realistic 3D animations of animals, creatures, or objects is often a time-consuming process that requires manual effort and technical expertise. And while recent AI tools can generate motion from text descriptions, they mostly work only for human-like characters, leaving out a wide range of creatures—from birds and horses to dragons.We set out to change that. Our team developed a new approach that teaches AI how to animate a large variety of objects just by reading a short text description, like ""a bird taking flight"" or ""a dragon soaring through the sky."" We started by expanding an existing animal motion dataset with rich text labels and then created techniques to help the AI understand and adapt to different skeletons—like wings, tails, or extra limbs.This opens up exciting possibilities for animators, game developers, and artists, making it easier than ever to bring diverse creatures to life. We're also sharing our tools with the community to support future work in this space."
Poster,How to set AdamW's weight decay as you scale model and dataset size,https://ICML.cc//virtual/2025/poster/45720,"Xi Wang, Laurence Aitchison","The scaling of the optimal AdamW weight decay hyperparameter with model and dataset size is critical as we seek to build larger models, but is poorly understood. We show that weights learned by AdamW can be understood as an exponential moving average (EMA) of recent updates.  This gives critical insights for how to set the weight decay in AdamW, and how the weight decay should scale with model and dataset size.  In particular, the key hyperparameter for an exponential moving average is the EMA timescale. Intuitively, the EMA timescale can be understood as the number of recent iterations the EMA averages over. We find that the optimal timescale, measured in epochs, is roughly constant as we change model and dataset size. Moreover, given a learning rate, there is a one-to-one mapping from the EMA timescale to the weight decay hyperparameter. Thus, if the optimal EMA timescale is constant, that implies that as the dataset size increases, the optimal weight decay should fall and as the model size increases, the optimal weight decay should increase (if we follow the muP recommendation for scaling the learning rate). We validate these scaling rules on ResNet-18 and Vision Transformers trained on CIFAR-10 and ImageNet, and on NanoGPT pre-training on OpenWebText. Finally, we found that as training progresses, muP's learning rate scaling breaks down for AdamW unless weight decay is scaled appropriately.",We all know about mu-P for learning rates.  What about the weight decay?  Turns out that the weight decay is actually **more** important for pre-training over long timescales.  We show how the weight decay scales with model and dataset size
Poster,How to Synthesize Text Data without Model Collapse?,https://ICML.cc//virtual/2025/poster/44341,"Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou","Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.","This work tackles the problem of “model collapse,” where training AI models on synthetic data leads to worse performance over time. We show that synthetic data lacks the diversity of human-written content, which causes models to degrade. To solve this, we propose token-level editing, a simple technique that tweaks parts of the data while keeping its structure. This method prevents model collapse and improves AI performance across various tasks."
Poster,How to Train Your Multi-Exit Model? Analyzing the Impact of Training Strategies,https://ICML.cc//virtual/2025/poster/43657,"Piotr Kubaty, Bartosz Wójcik, Bartłomiej Krzepkowski, Monika Michaluk, Tomasz Trzcinski, Jary Pomponi, Kamil Adamczewski","Early exits enable the network's forward pass to terminate early by attaching trainable internal classifiers to the backbone network. Existing early-exit methods typically adopt either a joint training approach, where the backbone and exit heads are trained simultaneously, or a disjoint approach, where the heads are trained separately. However, the implications of this choice are often overlooked, with studies typically adopting one approach without adequate justification. This choice influences training dynamics and its impact remains largely unexplored. In this paper, we introduce a set of metrics to analyze early-exit training dynamics and guide the choice of training strategy. We demonstrate that conventionally used joint and disjoint regimes yield suboptimal performance. To address these limitations, we propose a mixed training strategy: the backbone is trained first, followed by the training of the entire multi-exit network. Through comprehensive evaluations of training strategies across various architectures, datasets, and early-exit methods we present strengths and weaknesses of the early exit training strategies. In particular, we show consistent improvements in performance and efficiency using the proposed mixed strategy.","Modern deep learning models process every input through all layers, even when some examples are easy to classify, wasting computation and energy. A popular fix is to insert small “checkpoints” (early exits) at intermediate layers so the model can stop inference early. Past work either trains the whole network and checkpoints together or trains them separately, with little guidance on which works best. We introduce simple metrics to track how different training strategies affect learning, and find that the common “joint” and “disjoint” approaches each leave accuracy or efficiency on the table. We then propose a mixed strategy: first train the core network on its own, and only afterward tune all the early-exit classifiers together. Across multiple architectures, datasets, and checkpoint designs, this two-stage method consistently improves both prediction accuracy and inference speed. Our results give clear rules for practitioners to choose and train multi-exit models, paving the way for faster, greener AI systems."
Poster,How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias,https://ICML.cc//virtual/2025/poster/43533,"Ruiquan Huang, Yingbin LIANG, Jing Yang","Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as 'even pairs' and 'parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.","Language recognition tasks are essential tools in natural language processing (NLP), both for evaluating large language models and for understanding how they work. In this study, we take a closer look at two such tasks, namely 'even pairs' and 'parity check', which test whether certain patterns appear an even number of times in a sequence.We investigate how a one-layer transformer model  can learn to solve these tasks. Through mathematical analysis, we track how the model changes as it is trained using gradient descent. We find that the training process happens in two stages. First, the attention layer quickly learns to highlight useful parts of the input. Then, the linear layer gradually learns to make the final decision, eventually drawing a clear boundary between correct and incorrect answers.Interestingly, we theoretically show that the model trained on the even pairs task can solve the parity check task through Chain-of-Thought reasoning. This reasoning can be further added during model training to enhance the power of transformers. Finally, we confirm our theoretical insights with experiments, showing that the model behaves just as our analysis predicts."
