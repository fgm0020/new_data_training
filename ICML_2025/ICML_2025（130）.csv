type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics,https://ICML.cc//virtual/2025/poster/44931,"Aleksandr Gushchin, Khaled Abud, Georgii Bychkov, Ekaterina Shumitskaya, Anna Chistyakova, Sergey Lavrushkin, Bader Rasheed, Kirill Malyshev, Dmitriy Vatolin, Anastasia Antsiferova","Modern neural-network-based Image Quality Assessment (IQA) metrics are vulnerable to adversarial attacks, which can be exploited to manipulate search engine rankings, benchmark results, and content quality assessments, raising concerns about the reliability of IQA metrics in critical applications. This paper presents the first comprehensive study of IQA defense mechanisms in response to adversarial attacks on these metrics to pave the way for safer use of IQA metrics. We systematically evaluated 30 defense strategies, including purification, training-based, and certified methods --- and applied 14 adversarial attacks in adaptive and non-adaptive settings to compare these defenses on 9 no-reference IQA metrics. Our proposed benchmark aims to guide the development of IQA defense methods and is open to submissions; the latest results and code are at https://msu-video-group.github.io/adversarial-defenses-for-iqa/.","There are a vast collection of computer programs that judge how good an image looks. But tiny, invisible tweaks to an image can fool these programs into giving a bad photo a perfect score (or vice versa). That’s dangerous if websites or benchmarks start trusting these ratings to sort search results, decide which photos to feature, or even flag inappropriate content.To make these image‑quality judges safer, we tested 30 different “defense” ideas ranging from image filters to new training tricks and even mathematical guarantees and then tried to break them with 14 different attack methods (some that know exactly how the defense works, some that don’t). We ran all of this on nine commonly used, no‑reference IQA tools (those that don’t compare to an original “perfect” image) to see which defenses really hold up under attacks.Finally, we’ve wrapped all our tests into a public benchmark so anyone can plug in their own defenses and see how they stack up. By standardizing these comparisons, we aim to accelerate the development of IQA systems that can’t be so easily deceived."
Poster,GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance,https://ICML.cc//virtual/2025/poster/44844,"Jinuk Kim, Marwa El Halabi, Wonpyo Park, Clemens Schaefer, Deokjae Lee, Yeonhong Park, Jae W. Lee, Hyun Oh Song","Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.","Modern AI Chatbots like ChatGPT are very powerful, but they rely on extremely large language models (LLMs) that require a lot of memory and computing power to run. This makes them difficult to deploy, especially on small devices like smartphones. One effective way to make them smaller and faster is through quantization, a technique that replaces the model’s parameters with lower-precision approximations. For example, a parameter with value 1.897 might be rounded to 2. But this often hurts the model’s performance.We introduce a new quantization approach called GuidedQuant, which estimates how each parameter in the model affects its overall accuracy and uses this information to decide how to approximate it. Our approach doesn’t require retraining the model and can be applied to many existing quantization methods, improving their performance. We tested GuidedQuant across a range of LLMs and quantization methods and found it consistently improved performance after compression. We also developed a new algorithm for a specific type of quantization that further boosts performance.Our work contributes to an ongoing effort to make LLMs faster and more efficient, helping bring them to more users and devices, and reducing their environmental footprint. Our code and results are available at: https://github.com/snu-mllab/GuidedQuant."
Poster,Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents,https://ICML.cc//virtual/2025/poster/45504,"Karina Zainullina, Aleksandr Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Daria Litvintseva, Simon Karasik, Filipp Fisin, Sergei Skvortsov, Maksim Nekrashevich, Anton Shevtsov, Boris Yangel","Large language models (LLMs) have recently achieved remarkable results in complex multi-step tasks, such as mathematical reasoning and agentic software engineering. However, they often struggle to maintain consistent performance across multiple solution attempts. One effective approach to narrow the gap between average-case and best-case performance is guided test-time search, which explores multiple solution paths to identify the most promising one. Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for *non-serializable* RL environments, such as Docker containers, where intermediate environment states cannot be easily saved and restored. We investigate two complementary search strategies applicable to such environments: 1-step lookahead and trajectory selection, both guided by a learned action-value function estimator. On the SWE-bench Verified benchmark, a key testbed for agentic software engineering, we find these methods to double the average success rate of a fine-tuned Qwen-72B model, achieving $40.8$\%, the new state-of-the-art for open-weights models. Additionally, we show that these techniques are transferable to more advanced closed models, yielding similar improvements with GPT-4o.","When AI agents tackle complex multi-turn tasks, their performance is often inconsistent — sometimes they produce brilliant solutions, other times fail miserably. To improve reliability, we can use search techniques to explore many possible solution paths before committing to a specific one. However, search can be difficult in environments like Docker containers, where states cannot be easily captured or ""rewound"", and such environments are important for domains like software engineering.We explore search strategies specifically designed for these challenging ""non-serializable"" environments. Our solution implements two strategies: looking $1$ step ahead to evaluate potential actions, and generating multiple complete solutions to select the best one. Both are guided by the same trained model estimating which actions will likely succeed.When applied to SWE-bench Verified, a benchmark based on real-world software engineering tasks, our approach doubles the success rate of our AI agent to $40.8$\% — the best among publicly available systems. Our techniques also improve more advanced models like GPT-4o. Our research makes AI software agents more reliable, bringing us closer to practical AI programming assistance."
Poster,Guided Structural Inference: Leveraging Priors with Soft Gating Mechanisms,https://ICML.cc//virtual/2025/poster/44486,"Aoran Wang, Xinnan Dai, Jun Pang","Existing methods for inferring latent relational structures struggle to integrate partial prior knowledge, such as known edges, node-degree constraints, and global sparsity, without destabilizing training or conflicting with probabilistic assumptions. We propose Soft-Gated Structural Inference (SGSI), a VAE framework that seamlessly incorporates domain constraints via (1) soft gating with learnable edge masks to preserve gradients, (2) cloning-clamping of deterministic edges to avoid distributional conflicts, and (3) adaptive regularization to balance data-driven learning with domain constraints. By excluding known edges from stochastic inference, SGSI reallocates capacity to uncertain interactions, optimizing the information bottleneck trade-off. Experiments on 16 datasets show SGSI improves edge recovery by up to $9$\% AUROC over baselines, scales to larger graphs ($94.2$\% AUROC), and maintains stable training. SGSI bridges domain expertise with data-driven learning, enabling interpretable and robust structural discovery in dynamical systems.","Understanding how components in complex systems interact, such as cells in biology or agents in robotics, is crucial for making accurate predictions. However, discovering these interactions from data is often challenging because existing methods struggle to integrate known relationships provided by experts. To tackle this issue, we developed a new approach called Soft-Gated Structural Inference (SGSI).SGSI enhances existing machine learning techniques by smoothly incorporating partial knowledge, such as known connections or expected sparsity, into the learning process. Instead of rigidly enforcing these constraints, SGSI uses a flexible “soft-gating” mechanism that carefully guides the learning without compromising its stability or predictive power.Our extensive experiments showed that SGSI not only improves the accuracy of identifying interactions, up to 9% better than current methods, but also works effectively on larger and more complex datasets. This capability makes SGSI especially valuable for domains like biology, physics, and robotics, where combining expert knowledge with data-driven insights can significantly enhance our understanding of complex systems."
Poster,Guided Zeroth-Order Methods for Stochastic Non-convex Problems with Decision-Dependent Distributions,https://ICML.cc//virtual/2025/poster/44686,"Yuya Hikima, Hiroshi Sawada, Akinori Fujino","In this study, we tackle an optimization problem with a known function and an unknown decision-dependent distribution, which arises in a variety of applications and is often referred to as a performative prediction problem.To solve the problem, several zeroth-order methods have been developed because the gradient of the objective function cannot be obtained explicitly due to the unknown distribution.Although these methods have theoretical convergence, they cannot utilize the information on the known function, which limits their efficiency in reducing the objective value.To overcome this issue, we propose new zeroth-order methods that generate effective update directions by utilizing information on the known function.As theoretical results, we show the convergence of our methods to stationary points and provide the worst-case sample complexity analysis, which improves the state of the arts when the maximum objective value dominates the cube root of the decision variable's dimensionality in order.Our simulation experiments on multiple applications show that our methods output solutions with lower objective values than the existing zeroth-order methods do.","In this study, we address optimization problems in which the underlying probability distribution depends on the decision variable. Such problems arise in various real-world applications. For instance, in finance, a lender may want to train a classifier with parameter $x$ to identify reliable customers. However, the distribution of customer features can change depending on the parameter $x$, since customers might adjust their features in response to the classifier’s behavior. This interaction creates the need to solve an optimization problem where the distribution is influenced by the decision variable—known as a decision-dependent distribution. To tackle this challenge, we propose new zeroth-order optimization methods that effectively construct update directions by leveraging information from the known objective function."
Poster,Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding,https://ICML.cc//virtual/2025/poster/46686,"Jinze Li, Yixing Xu, Haiduo Huang, Xuanwu Yin, Dong Li, Edith Ngai, Emad Barsoum","Speculative decoding (SPD) aims to accelerate the auto-regressive token generation process of a target Large Language Model (LLM). Some approaches employ a draft model with multiple heads to predict a sequence of future tokens, where each head handles a token in the sequence. The target LLM verifies the predicted sequence and accepts aligned tokens, enabling efficient multi-token generation. However, existing methods assume that all tokens within a sequence are equally important, employing identical head structures and relying on a single-generation paradigm, either serial or parallel. To this end, we theoretically demonstrate that initial tokens in the draft sequence are more important than later ones. Building on this insight, we propose Gumiho, a hybrid model combining serial and parallel heads. Specifically, given the critical importance of early tokens, we employ a sophisticated Transformer architecture for the early draft heads in a serial configuration to improve accuracy. For later tokens, we utilize multiple lightweight MLP heads operating in parallel to enhance efficiency. By allocating more advanced model structures and longer running times to the early heads, Gumiho achieves improved overall performance. The experimental results demonstrate that our method outperforms existing approaches, fully validating its effectiveness. Our code is available at https://github.com/AMD-AIG-AIMA/Gumiho.","Large Language Models (LLMs) generate text token by token, which can be slow. Speculative decoding (SPD) speeds this up by using a smaller ""draft"" model to predict multiple future tokens that the main LLM then verifies. This paper argues that in SPD, the initial tokens in a predicted sequence are more critical than later ones because an early error discards the entire subsequent sequence.To address this, the researchers propose Gumiho, a hybrid SPD architecture. Gumiho uses a more sophisticated Transformer-based structure in a serial manner for the crucial early tokens to improve their accuracy. For later, less critical tokens, it employs multiple lightweight MLP heads that operate in parallel for better efficiency. This targeted allocation of resources aims to boost overall performance."
Poster,Habitizing Diffusion Planning for Efficient and Effective Decision Making,https://ICML.cc//virtual/2025/poster/44966,"Haofei Lu, Yifei Shen, Dongsheng Li, Junliang Xing, Dongqi Han","Diffusion models have shown great promise in decision-making, also known as diffusion planning. However, the slow inference speeds limit their potential for broader real-world applications. Here, we introduce **Habi**, a general framework that transforms powerful but slow diffusion planning models into fast decision-making models, which mimics the cognitive process in the brain that costly goal-directed behavior gradually transitions to efficient habitual behavior with repetitive practice.   Even using a laptop CPU, the habitized model can achieve an average **800+ Hz** decision-making frequency (faster than previous diffusion planners by orders of magnitude) on standard offline reinforcement learning benchmarks D4RL, while maintaining comparable or even higher performance compared to its corresponding diffusion planner. Our work proposes a fresh perspective of leveraging powerful diffusion models for real-world decision-making tasks. We also provide robust evaluations and analysis, offering insights from both biological and engineering perspectives for efficient and effective decision-making.","Decision-making models powered by AI have become increasingly powerful — but also increasingly slow. This poses a problem when speed is critical, such as in robotics or motion planning. Inspired by how humans learn — starting with deliberate thinking and gradually forming fast habits — we developed Habi, a method that transforms slow, high-performing decision models into fast ones without sacrificing quality. Even on a regular laptop, Habi can make over 800 decisions per second, far outpacing previous methods, while still achieving equal or better results. This opens the door for using powerful AI planning models in real-time applications."
Poster,HALoS: Hierarchical Asynchronous Local SGD over Slow Networks for Geo-Distributed Large Language Model Training,https://ICML.cc//virtual/2025/poster/45594,"Geon-Woo Kim, Junbo Li, Shashidhar Gandham, Omar Baldonado, Adithya Gangidi, Pavan Balaji, Zhangyang “Atlas” Wang, Aditya Akella","Training large language models (LLMs) increasingly relies on geographically distributed accelerators, causing prohibitive communication costs across regions and uneven utilization of heterogeneous hardware. We propose HALoS, a hierarchical asynchronous optimization framework that tackles these issues by introducing local parameter servers (LPSs) within each region and a global parameter server (GPS) that merges updates across regions. This hierarchical design minimizes expensive inter-region communication, reduces straggler effects, and leverages fast intra-region links. We provide a rigorous convergence analysis for HALoS under non-convex objectives, including theoretical guarantees on the role of hierarchical momentum in asynchronous training. Empirically, HALoS attains up to 7.5× faster convergence than synchronous baselines in geo-distributed LLM training and improves upon existing asynchronous methods by up to 2.1×. Crucially, HALoS preserves the model quality of fully synchronous SGD—matching or exceeding accuracy on standard language modeling and downstream benchmarks—while substantially lowering total training time. These results demonstrate that hierarchical, server-side update accumulation and global model merging are powerful tools for scalable, efficient training of new-era LLMs in heterogeneous, geo-distributed environments.","Training today’s LLMs typically packs thousands of identical GPUs into one data center and keeps them perfectly synchronized. That approach only works if you gather that hardware in one place and pay for an ultra-fast network. Many teams instead spread GPUs across multiple cloud regions, where links are slow and hardware heterogeneous. Even hyperscalers are moving to multi-region deployments because of operational challenges.Our paper introduces HALoS, a simple hierarchical, asynchronous layer that keeps learning efficient in those settings. Each region runs local servers that collect updates from nearby GPUs and talk only occasionally to one global server. Because most messages stay on the fast regional network, accelerators rarely sit idle, and the few long-distance exchanges are efficiently merged instead of sent one by one.In experiments that mimic realistic geo-distributed settings, HALoS pretrains LLMs significantly faster than existing synchronous and asynchronous baselines while preserving model performance on downstream tasks like MMLU and Hellaswag. We expect HALoS to be a practical, powerful solution for efficient LLM pretraining in the upcoming geo-distributed training environment."
Poster,Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin,https://ICML.cc//virtual/2025/poster/45347,"Yuchen Wang, Xuefeng Bai, Xiucheng Li, Weili Guan, Liqiang Nie, Xinyang Chen","Adapting vision-language models (VLMs) to downstream tasks with pseudolabels has gained increasing attention. A major obstacle is that the pseudolabels generated by VLMs tend to be imbalanced, leading to inferior performance.While existing methods have explored various strategies to address this, the underlying causes of imbalance remain insufficiently investigated.To fill this gap, we delve into imbalanced pseudolabels and identify two primary contributing factors: concept mismatch and concept confusion. To mitigate these two issues, we propose a novel framework incorporating concept alignment and confusion-aware calibrated margin mechanisms. The core of our approach lies in enhancing underperforming classes and promoting balanced predictions across categories, thus mitigating imbalance. Extensive experiments on six benchmark datasets with three learning paradigms demonstrate that the proposed method effectively enhances the accuracy and balance of pseudolabels, achieving a relative improvement of 6.29\% over the SoTA method. Our code is avaliable at https://github.com/Noahwangyuchen/CAP","Modern AI systems that connect images with language—called vision-language models (VLMs)—are being used to label new images without human effort. However, these automatic labels (called pseudolabels) are often unbalanced. That means the model favors some categories over others, which leads to poor performance when applied to real-world tasks.We explored why this imbalance happens and discovered two key issues:  the model may fail to extract the precise meaning of certain categories (concept mismatch), or confuse similar-looking ones (concept confusion). To fix this, we designed a new framework called CAP, combining  concept alignment to help the model better match text and images, and confusion-aware calibrated margin to to help the model better tell similar categories apart.Our approach leads to more accurate and fair labels across categories. We tested it on six widely-used datasets and three learning setups, showing that it consistently improves results—by over 6% compared to the best existing method."
Poster,HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding,https://ICML.cc//virtual/2025/poster/46523,"Rui Yang, Lin Song, Yicheng Xiao, Runhui Huang, Yixiao Ge, Ying Shan, Hengshuang Zhao","Recent advancements in large language models (LLMs) have significantly propelled the development of large multi-modal models (LMMs), highlighting the potential for general and intelligent assistants. However, most LMMs model visual and textual modalities separately, leading to recent efforts to develop native LMMs using a single transformer. Despite the promise, these native models are resource-intensive and often exhibit performance gaps compared to their compositional counterparts. To alleviate this issue, we propose a simple yet efficient method to construct a baseline for the native and end-to-end large multi-modal model in a single transformer. First, we propose a new early-fusion LMM that can fuse multi-modal inputs in the early stage and respond to visual instructions in an auto-regressive manner. Second, we devise an efficient training recipe for the proposed model, which harnesses the prior knowledge of the pre-trained models, addressing both the performance limitations and the challenge of resource consumption. The proposed model demonstrates superior performance compared to other LMMs using one transformer and significantly narrows the performance gap with compositional LMMs.","Recent advances in AI have created models that can understand both images and text, but most handle them separately, which limits their abilities. Newer models try to combine both in one system, but they use a lot of resources. We developed a simpler, more efficient model that processes images and text together from the start. Our approach uses less computer power, and performs better than similar models."
