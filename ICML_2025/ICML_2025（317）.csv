type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Understanding Fixed Predictions via Confined Regions,https://ICML.cc//virtual/2025/poster/46631,"Connor Lawless, Lily Weng, Berk Ustun, Madeleine Udell","Machine learning models can assign fixed predictions that preclude individuals from changing their outcome. Existing approaches to audit fixed predictions do so on a pointwise basis, which requires access to an existing dataset of individuals and may fail to anticipate fixed predictions in out-of-sample data. This work presents a new paradigm to identify fixed predictions by finding confined regions of the feature space in which all individuals receive fixed predictions. This paradigm enables the certification of recourse for out-of-sample data, works in settings without representative datasets, and provides interpretable descriptions of individuals with fixed predictions. We develop a fast method to discover confined regions for linear classifiers using mixed-integer quadratically constrained programming. We conduct a comprehensive empirical study of confined regions across diverse applications. Our results highlight that existing pointwise verification methods fail to anticipate future individuals with fixed predictions, while our method both identifies them and provides an interpretable description.","Some machine learning models make predictions that are “fixed”—meaning no matter how a person changes their situation, the model’s decision stays the same. This can be a big problem if someone is denied a loan, job, or opportunity and has no way to improve their chances. Current tools to detect these fixed decisions work on a person-by-person basis and can miss new or unusual cases. This means that we may not know our model could deny individuals opportunities with no way to rectify them until after we’ve started using it! Our research introduces a new way to find groups of people who will get a fixed prediction. This works even when we don’t have much data and can help practitioners explain why certain people are stuck with a fixed outcome. We also show that our method finds problems that other tools miss—and it does so in a way that’s fast and easy to understand."
Poster,Understanding Generalization in Quantum Machine Learning with Margins,https://ICML.cc//virtual/2025/poster/44359,"TAK HUR, Daniel Kyungdeock Park","Understanding and improving generalization capabilities is crucial for both classical and quantum machine learning (QML). Recent studies have revealed shortcomings in current generalization theories, particularly those relying on uniform bounds, across both classical and quantum settings. In this work, we present a margin-based generalization bound for QML models, providing a more reliable framework for evaluating generalization. Our experimental studies on the quantum phase recognition dataset demonstrate that margin-based metrics are strong predictors of generalization performance, outperforming traditional metrics like parameter count. By connecting this margin-based metric to quantum information theory, we demonstrate how to enhance the generalization performance of QML through a classical-quantum hybrid approach when applied to classical data.","Quantum information processing presents exciting opportunities to push the boundaries of what machine learning can achieve. A central goal in machine learning is generalization—the ability of a model to learn from examples and make accurate predictions on new, unseen data. Generalization is a core characteristic of intelligent behavior and a critical measure of how useful a machine learning model will be in real-world applications.In this work, we contribute to a better understanding of generalization in quantum machine learning (QML). We build on a well-established idea from classical machine learning—called the margin—and develop new theoretical results for quantum models. Through both mathematical analysis and simulated experiments, we show that margin-based techniques offer a more effective way to predict generalization performance than traditional measures. Our results provide a new foundation for evaluating and improving quantum learning models, which may support the development of more reliable quantum algorithms in the future."
Poster,Understanding High-Dimensional Bayesian Optimization,https://ICML.cc//virtual/2025/poster/46626,"Leonard Papenmeier, Matthias Poloczek, Luigi Nardi","Recent work reported that simple Bayesian optimization (BO) methods perform well for high-dimensional real-world tasks, seemingly contradicting prior work and tribal knowledge. This paper investigates why. We identify underlying challenges that arise in high-dimensional BO and explain why recent methods succeed. Our empirical analysis shows that vanishing gradients caused by Gaussian process (GP) initialization schemes play a major role in the failures of high-dimensional Bayesian optimization (HDBO) and that methods that promote local search behaviors are better suited for the task. We find that maximum likelihood estimation (MLE) of GP length scales suffices for state-of-the-art performance. Based on this, we propose a simple variant of MLE called MSR that leverages these findings to achieve state-of-the-art performance on a comprehensive set of real-world applications. We present targeted experiments to illustrate and confirm our findings.","Bayesian Optimization (BO) is a technique to optimize functions that appear in engineering problems, hyperparameter optimization for machine learning, and other fields where observing the function requires considerable resources. BO learns a model of the function it aims to optimize and chooses new points to evaluate by trading off how well a new point is expected to perform and how uncertain the model is about the expected performance of that point. For functions that have many input parameters, the required number of observations to learn the model with sufficient accuracy grows so fast that it was widely believed that either only functions with a moderate number of parameters can be learned efficiently with BO or that the function has to satisfy additional assumptions that more sophisticated algorithms can take advantage of. Recent works have questioned this paradigm and show that simple BO setups scale to many more parameters than previously believed. This paper investigates why previous methods did not scale well, why current methods do, and what the limitations of high-dimensional Bayesian optimization are.  Based on our insights, we propose a simple yet effective method for high-dimensional BO and show that it is competitive with the state-of-the-art."
Poster,"Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity",https://ICML.cc//virtual/2025/poster/43867,"Ningyuan Huang, Miguel Sarabia, Abhinav Moudgil, Pau Rodriguez, Luca Zappella, Federico Danieli","State-Space Models (SSMs), and particularly Mamba, have recently emerged as a promising alternative to Transformers. Mamba introduces input selectivity to its SSM layer (S6) and incorporates convolution and gating into its block definition. While these modifications do improve Mamba's performance over its SSM predecessors, it remains largely unclear how Mamba leverages the additional functionalities provided by input selectivity, and how these interact with the other operations in the Mamba architecture. In this work, we demystify the role of input selectivity in Mamba, investigating its impact on function approximation power, long-term memorization, and associative recall capabilities.In particular: (i) we prove that the S6 layer of Mamba can represent projections onto *Haar wavelets*, providing an edge over its Diagonal SSM (S4D) predecessor in approximating discontinuous functions commonly arising in practice; (ii) we show how the S6 layer can dynamically counteract memory decay; (iii) we provide analytical solutions to the MQAR associative recall task using the Mamba architecture with different mixers --- Mamba, Mamba-2, and S4D. We demonstrate the tightness of our theoretical constructions with empirical results on concrete tasks. Our findings offer a mechanistic understanding of Mamba and reveal opportunities for improvement.","Mamba is a new architecture capable of rivaling Transformers in language modelling. The main novelty of Mamba lies in the use of a mechanism called input selectivity for processing information. But why is it so effective? And is that the sole reason behind its performance?In this work we analyze the impact of input selectivity and other components of Mamba in boosting its expressive power. We do so in three ways: by describing what types of functions Mamba can approximate; by illustrating how Mamba can effectively memorize information; and by showing how its components can act in concert to solve some associative-recall tasks.Overall, our work provides a better understanding of the inner workings of Mamba, and points to ways in which this architecture could be further improved."
Poster,Understanding Mode Connectivity via Parameter Space Symmetry,https://ICML.cc//virtual/2025/poster/45969,"Bo Zhao, Nima Dehmamy, Robin Walters, Rose Yu","Neural network minima are often connected by curves along which train and test loss remain nearly constant, a phenomenon known as mode connectivity. While this property has enabled applications such as model merging and fine-tuning, its theoretical explanation remains unclear. We propose a new approach to exploring the connectedness of minima using parameter space symmetry. By linking the topology of symmetry groups to that of the minima, we derive the number of connected components of the minima of linear networks and show that skip connections reduce this number. We then examine when mode connectivity and linear mode connectivity hold or fail, using parameter symmetries which account for a significant part of the minimum. Finally, we provide explicit expressions for connecting curves in the minima induced by symmetry. Using the curvature of these curves, we derive conditions under which linear mode connectivity approximately holds. Our findings highlight the role of continuous symmetries in understanding the neural network loss landscape.","Modern neural networks often have many solutions that perform equally well. Surprisingly, many of these solutions can be connected by paths along which performance stays consistently high. We wanted to understand why and when this happens.Our research shows that these connections often arise from symmetries---transformations of the network’s parameters that leave its performance unchanged. While the space of solutions is complex, the mathematical structure of these symmetries is well understood. By linking these two spaces, we use what we know about symmetries to uncover the structure of the solution landscape. This approach lets us count how many disconnected groups of solutions exist and shows how architectural features, like skip connections in ResNets, can make solutions more connected. We also derive exact formulas for constructing the connecting paths, instead of finding them by trial and error.Understanding how solutions are connected can improve how we merge, fine-tune, and ensemble trained models, helping to make machine learning systems more efficient and reliable."
Poster,Understanding Model Ensemble in Transferable Adversarial Attack,https://ICML.cc//virtual/2025/poster/43506,"Wei Yao, Zeliang Zhang, Huayi Tang, Yong Liu","Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples that can target even unknown models, but its theoretical foundation remains underexplored. To address this gap, we provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack. We first define transferability error to measure the error in adversarial transferability, alongside concepts of diversity and empirical model ensemble Rademacher complexity. We then decompose the transferability error into vulnerability, diversity, and a constant, which rigidly explains the origin of transferability error in model ensemble attack: the vulnerability of an adversarial example to ensemble components, and the diversity of ensemble components. Furthermore, we apply the latest mathematical tools in information theory to bound the transferability error using complexity and generalization terms, validating three practical guidelines for reducing transferability error: (1) incorporating more surrogate models, (2) increasing their diversity, and (3) reducing their complexity in cases of overfitting. Finally, extensive experiments with 54 models validate our theoretical framework, representing a significant step forward in understanding transferable model ensemble adversarial attacks.","In a transferable adversarial attack, model ensemble is like a team of students, each learning their own unique ways to cheat on different exams. If this team can collectively discover a general cheating strategy, that method can successfully fool new teachers they've never encountered before (i.e., unknown AI models).Our theoretical research reveals that the success of such attacks primarily stems from two main aspects: first, the **vulnerability** of the AI models to the attack itself – the attack needs to be effective enough. Second, the **diversity** among the AI models used to launch the attack is crucial – they shouldn't be too similar, just as a cheating team benefits from students with varied trickery. Theoretically, an attack becomes stronger if we: 1) incorporate more students (i.e., more AI models); 2) increase the differences in their cheating methods (i.e., enhance model diversity); and 3) reduce how much each student memorizes for cheating (i.e., lower model complexity in cases of overfitting). This provides theoretical support for a field that has largely relied on experimental observations."
Poster,Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts,https://ICML.cc//virtual/2025/poster/45490,"Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu","Model reprogramming adapts pretrained models to downstream tasks by modifying only the input and output spaces. *Visual reprogramming* (VR) is one instance for vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to input images to facilitate downstream classification. The existing VR approaches for CLIP train a single visual prompt using all descriptions of different downstream classes. However, the limited learning capacity may result in (1) a failure to capture diverse aspects of the descriptions (e.g., shape, color, and texture), and (2) a possible bias toward less informative attributes that do not help distinguish between classes. In this paper, we introduce a decoupling-and-reweighting framework.  Our *decoupled visual prompts* (DVP) are optimized using descriptions grouped by explicit **c**au**se**s (DVP-cse) or unsupervised **cl**u**s**ters (DVP-cls). Then, we integrate the outputs of these visual prompts with a *probabilistic reweighting matrix* (PRM) that measures their contributions to each downstream class. Theoretically, DVP lowers the empirical risk bound. Experimentally, DVP outperforms baselines on average across 11 downstream datasets. Notably, the DVP-PRM integration enables insights into how individual visual prompts influence classification decisions, providing a probabilistic framework for understanding reprogramming.","This paper presents Decoupled Visual Prompts (DVP) to improve how vision-language models like CLIP adapt to new tasks without retraining. Current methods train a single ""visual prompt"" (i.e., a small-scale, learnable pattern added to images) to align images with text descriptions, but this can miss important visual details or focus on less useful features. DVP solves this by splitting new tasks into smaller parts: it trains multiple prompts, each specialized for different aspects (like shape or color) or groups of similar descriptions. These prompts are then combined using an adaptive reweighting method that learns which features matter most for each task. Experiments show DVP outperforms existing methods across 11 datasets, and it also provides insights into how the reprogrammed vision-language model makes decisions."
Poster,Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach,https://ICML.cc//virtual/2025/poster/44373,"Changdae Oh, zhen fang, Shawn Im, Xuefeng Du, Sharon Li","Multimodal large language models (MLLMs) have shown promising capabilities but struggle under distribution shifts, where evaluation data differ from instruction tuning distributions. Although previous works have provided empirical evaluations, we argue that establishing a formal framework that can characterize and quantify the risk of MLLMs is necessary to ensure the safe and reliable application of MLLMs in the real world. By taking an information-theoretic perspective, we propose the first theoretical framework that enables the quantification of the maximum risk of MLLMs under distribution shifts. Central to our framework is the introduction of Effective Mutual Information (EMI), a principled metric that quantifies the relevance between input queries and model responses. We derive an upper bound for the EMI difference between in-distribution (ID) and out-of-distribution (OOD) data, connecting it to visual and textual distributional discrepancies.Extensive experiments on real benchmark datasets, spanning 61 shift scenarios, empirically validate our theoretical insights.","(1) Multimodal LLMs often struggle to address unfamiliar queries, whereas proficient at processing ones similar to instruction-tuning data distribution; but there is no formal framework to explain this performance gap. (2) We present the first formal framework to characterize and quantify the performance gap of multimodal LLMs under these query distribution shifts through the lens of information theory. (3) The proposed information-theoretic framework can be efficiently leveraged for reliable multimodal LLM evaluation in safety-critical real-world applications."
Poster,Understanding Nonlinear Implicit Bias via Region Counts in Input Space,https://ICML.cc//virtual/2025/poster/46690,"Jingwei Li, Jing Xu, Zifan Wang, Huishuai Zhang, Jingzhao Zhang","One explanation for the strong generalization ability of neural networks is implicit bias. Yet, the definition and mechanism of implicit bias in non-linear contexts remains little understood. In this work, we propose to characterize implicit bias by the count of connected regions in the input space with the same predicted label. Compared with parameter-dependent metrics (e.g., norm or normalized margin), region count can be better adapted to nonlinear, overparameterized models, because it is determined by the function mapping and is invariant to reparametrization. Empirically, we found that small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. We also observe that good hyper-parameter choices such as larger learning rates and smaller batch sizes can induce small region counts. We further establish the theoretical connections and explain how larger learning rate can induce small region counts in neural networks.","Neural networks often perform well even when they are much larger than the training data. One reason is called “implicit bias” — hidden preferences that shape how the network learns. In this work, we describe implicit bias by counting how many separate regions a model divides the input space into. Fewer regions often mean simpler decisions and better generalization. We show that training with large learning rates or small batch sizes naturally leads to fewer regions, helping explain why these settings often improve performance. Our method offers a simple, effective way to understand and predict how well a model will generalize."
Poster,Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods,https://ICML.cc//virtual/2025/poster/46611,"Yifan HAO, xingyuan pan, Hanning Zhang, Chenlu Ye, Rui Pan, Tong Zhang","Supervised fine-tuning (SFT) on domain-specific data is the dominant approach for adapting foundation models to specialized tasks. However, it has been observed that SFT models tend to forget knowledge acquired during pretraining. In vision models, ensembling a pretrained model with its fine-tuned counterpart has been shown to mitigate this issue. In this work, we demonstrate that the same holds for language models, and, more strikingly, we observe an overadaptation phenomenon: the ensemble model not only retains general knowledge from the foundation model but also outperforms the fine-tuned model even on the fine-tuning domain itself.Despite the empirical success of ensembling, a theoretical understanding of its benefits remains underexplored. We develop a formal theoretical analysis of the overadaptation phenomenon. Ensembling mitigates this by balancing two primary sources of error: bias, caused by insufficient fine-tuning, and variance, introduced by overfitting to fine-tuning data. While regularization techniques aim to address this trade-off, we show that ensembling provides a more effective solution. We analyze this phenomenon in over-parameterized linear settings and demonstrate that interpolating between pretrained and fine-tuned weights significantly improves performance. These findings offer theoretical justification for the observed advantages of model ensembling, supported by empirical experiments consistent with our analysis.","Foundation models, like large language models, are often adapted to new tasks using supervised fine-tuning (SFT). However, this fine-tuning can cause the model to ""forget"" useful general knowledge learned during pretraining. In vision, combining the original and fine-tuned models—called ensembling—helps retain that knowledge. We show that ensembling has similar benefits for language models and, surprisingly, can even outperform the fine-tuned model on its own task. To understand why, we study a phenomenon called overadaptation, where fine-tuning goes too far, leading to high variance in risks. We provide a theoretical explanation for the benefits of ensemble by analyzing the bias–variance trade-off: fine-tuned models suffer from high variance due to overfitting, while pretrained models have high bias due to underfitting the new domain. Ensembling strikes a balance between these extremes. Our analysis in over-parameterized linear settings shows that interpolating between pretrained and fine-tuned weights can significantly improve performance. We support our theory with experiments, showing that interpolating between pretrained and fine-tuned models leads to better performance. This work helps explain why ensembling works so well and offers guidance for more robust model adaptation."
