type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,STAIR: Improving Safety Alignment with Introspective Reasoning,https://ICML.cc//virtual/2025/poster/44809,"Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong Yan, Yinpeng Dong, Jun Zhu","Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose **STAIR**, a novel framework that integrates **S**afe**T**y **A**lignment with **I**trospective **R**easoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). Specifically, we design a theoretically grounded reward for outcome evaluation to seek balance between helpfulness and safety. We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. We have open-sourced our code, datasets and models at https://github.com/thu-ml/STAIR.","As Large Language Models (LLMs) become increasingly widespread, ensuring they remain safe and do not cause harm is crucial. This is where safety alignment comes in. One common approach is training models to refuse unsafe queries, but this strategy can be vulnerable to clever prompts, often referred to as jailbreak attacks, which can trick the AI into providing harmful responses.Our method, STAIR (SafeTy Alignment with Introspective Reasoning), guides models to think more carefully before responding. Instead of giving immediate answers, the model breaks down the question into smaller steps and assesses potential safety risks along the way. Additionally, we introduce a novel scoring system to help the model balance safety with helpfulness when exploring possible answers. In our experiments, STAIR enables modern LLMs to better avoid harmful responses while maintaining their effectiveness in general tasks. This work highlights the value of reasoning in safety alignment and represents an important step toward building more trustworthy and reliable AI systems."
Poster,STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings,https://ICML.cc//virtual/2025/poster/43951,"Saksham Rastogi, Pratyush Maini, Danish Pruthi","Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership—i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and utility of the original data. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.","Much of the publicly available data on the internet is used to train language models (such as ChatGPT), but content creators have no way of knowing if their data was used (without their permission) for training such models. In this work, we provide a tool for creators to “stamp” their writing. For a piece of original text, our approach generates several versions of that text with slightly different wordings, all conveying the same meaning. One of these versions can be publicly published, but the rest are to be kept private. Later, if a creator suspects that a language model used their published content for training, they can run our simple test to determine if the language model shows a strong preference for the public version compared to the private ones. If this is indeed the case, it suggests that the language model was trained on their public content. Our approach is simple to use, and in this paper, we show it can successfully detect whether a piece of text was used for training language models, while preserving the meaning and utility of the original content."
Poster,Star Attention: Efficient LLM Inference over Long Sequences,https://ICML.cc//virtual/2025/poster/45335,"Shantanu Acharya, Fei Jia, Boris Ginsburg","Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.","Modern AI systems built on large language models (LLMs) are increasingly being used for tasks requiring extremely long inputs, such as analyzing entire codebases or summarizing thousands of pages of documents. But there's a catch: processing such long inputs is slow and expensive because current methods force the model to look at every previous word when generating the next one.We developed a new method called Star Attention that significantly speeds up this process while maintaining high accuracy. It works by dividing the input into smaller parts and processing them independently across multiple computers. Each part is anchored by a shared section from the beginning, which helps the model stay oriented. Later, when it’s time to generate answers, the model efficiently combines relevant information from all parts without needing to process the full input at once.This approach makes LLMs up to 11 times faster on long inputs while preserving 97–100% of the original accuracy. It enables more scalable and cost-effective deployment of AI systems for real-world tasks that require understanding and reasoning over very large inputs."
Poster,STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization,https://ICML.cc//virtual/2025/poster/44123,"Hao Li, Qi Lv, Rui Shao, Xiang Deng, Yinchuan Li, Jianye Hao, Liqiang Nie","Transforming complex actions into discrete skill abstractions has demonstrated strong potential for robotic manipulation.Existing approaches mainly leverage latent variable models, e.g., VQ-VAE, to learn skill abstractions through learned vectors (codebooks), while they suffer from codebook collapse and modeling the causal relationship between learned skills. To address these limitations, we present **S**kill **T**raining with **A**ugmented **R**otation (**STAR**), a framework that advances both skill learning and composition to complete complex behaviors. Specifically, to prevent codebook collapse, we devise rotation-augmented residual skill quantization (RaRSQ).It encodes relative angles between encoder outputs into the gradient flow by rotation-based gradient mechanism. Points within the same skill code are forced to be either pushed apart or pulled closer together depending on gradient directions.Further, to capture the casual relationship between skills, we present causal skill transformer (CST) which explicitly models dependencies between skill representations through an autoregressive mechanism for coherent action generation.Extensive experiments demonstrate the superiority of STAR on both LIBERO benchmark and realworld tasks, with around 12% improvement over the baselines.","Teaching robots to perform complex, multi-step tasks like opening a drawer, placing an object inside, and closing it remains challenging because current methods struggle to break down these behaviors into reusable components. Existing approaches often fail because they either forget previously learned skills or cannot properly sequence multiple actions together.We developed STAR, a new learning framework that teaches robots by first decomposing complex actions into discrete ""skills"" - like building blocks that can be combined in different ways. Our key innovation prevents the common problem where robots overlook most of their learned skills and only remember a few, ensuring they maintain a diverse toolkit. We also developed a method for robots to understand which skills should come before others, enabling proper sequencing of multi-step tasks.We tested STAR on over 180 robotic manipulation tasks and achieved 93.6% success compared to 81.5% for previous methods. In real-world experiments, our approach successfully completed challenging sequences like drawer manipulation that require precise coordination of multiple skills."
Poster,Statistical and Computational Guarantees of Kernel Max-Sliced Wasserstein Distances,https://ICML.cc//virtual/2025/poster/46318,"Jie Wang, March Boedihardjo, Yao Xie","Optimal transport has been very successful for various machine learning tasks; however, it is known to suffer from the curse of dimensionality. Hence, dimensionality reduction is desirable when applied to high-dimensional data with low-dimensional structures. The kernel max-sliced (KMS) Wasserstein distance is developed for this purpose by finding an optimal nonlinear mapping that reduces data into $1$ dimension before computing the Wasserstein distance. However, its theoretical properties have not yet been fully developed. In this paper, we provide sharp finite-sample guarantees under milder technical assumptions compared with state-of-the-art for the KMS $p$-Wasserstein distance between two empirical distributions with $n$ samples for general $p\in[1,\infty)$. Algorithm-wise, we show that computing the KMS $2$-Wasserstein distance is NP-hard, and then we further propose a semidefinite relaxation (SDR) formulation (which can be solved efficiently in polynomial time) and provide a relaxation gap for the obtained solution. We provide numerical examples to demonstrate the good performance of our scheme for high-dimensional two-sample testing.","In ""Statistical and Computational Guarantees of Kernel Max-Sliced Wasserstein Distances"", Wang, Boedihardjo, and Xie introduce a rigorous theoretical and algorithmic framework for the Kernel Max-Sliced (KMS) Wasserstein distance, a flexible and powerful tool for comparing probability distributions in high-dimensional spaces. KMS generalizes the max-sliced Wasserstein distance by replacing linear projections with nonlinear projections in a Reproducing Kernel Hilbert Space (RKHS), capturing nonlinear differences between distributions more effectively.The authors provide dimension-free, finite-sample guarantees for the KMS p-Wasserstein distance, showing it converges at the optimal rate under mild assumptions. On the computational side, they prove that computing the KMS 2-Wasserstein distance is NP-hard and thus develop a tractable semidefinite relaxation (SDR) formulation with provable approximation bounds and efficient first-order optimization algorithms. Notably, they also establish a novel rank bound on the SDR solutions and propose a rank-reduction procedure for improved interpretability and performance.Extensive experiments demonstrate the superior performance of this framework in high-dimensional two-sample testing, human activity change detection, and generative modeling. The KMS Wasserstein distance outperforms various baselines, including MMD, Sinkhorn divergence, and sliced Wasserstein distances, especially when the data exhibits nonlinear structures."
Poster,Statistical Collusion by Collectives on Learning Platforms,https://ICML.cc//virtual/2025/poster/46504,"Etienne Gauthier, Francis Bach, Michael Jordan","As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain.","Online platforms often use automated systems to make decisions based on user data. In response, groups of people might team up to try to influence these systems so that the outcomes better reflect their interests. They can do this by intentionally changing the information they provide to the platform. But before taking such steps, these groups need to figure out whether their efforts are likely to work. They also need practical ways to organize and act based on what they can observe. In this work, we build a framework that helps groups understand how to do this."
Poster,Statistical Hypothesis Testing for Auditing Robustness in Language Models,https://ICML.cc//virtual/2025/poster/45964,"Paulius Rauba, Qiyao Wei, Mihaela van der Schaar","Consider the problem of testing whether the outputs of a large language model (LLM) system change under an arbitrary intervention, such as an input perturbation or changing the model variant. We cannot simply compare two LLM outputs since they might differ due to the stochastic nature of the system, nor can we compare the entire output distribution due to computational intractability. While existing methods for analyzing text-based outputs exist, they focus on fundamentally different problems, such as measuring bias or fairness. To this end, we introduce distribution-based perturbation analysis, a framework that reformulates LLM perturbation analysis as a frequentist hypothesis testing problem. We construct empirical null and alternative output distributions within a low-dimensional semantic similarity space via Monte Carlo sampling, enabling tractable inference without restrictive distributional assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation of arbitrary input perturbations on any black-box LLM,  (iii) yields interpretable p-values; (iv) supports multiple perturbations via controlled error rates; and (v) provides scalar effect sizes. We demonstrate the usefulness of the framework across multiple case studies, showing how we can quantify response changes, measure true/false positive rates, and evaluate alignment with reference models. Above all, we see this as a reliable frequentist hypothesis testing framework for LLM auditing.","Suppose you ask a large language model (LLM) for a treatment recommendation based on the information you provide. Then, because you are feeling adventurous, you change some information in your prompt (for example, change your gender) and ask the LLM again. To your surprise, you receive a different treatment recommendation. One question you could ask is the following: is the different response *a result of* the information you have changed? The short answer is that we cannot know by just asking the question once. This is because language models will respond differently simply by chance to the same question. Therefore, what we really care about is whether the kinds of responses the LLM provides  are different from what they would have been. Furthermore, we would even like to quantify (assign a number) how much such responses have changed and whether these changes are statistically significant. This is exactly what this paper does --- it establishes a way to quantify how much responses have changed and perform some statistics with those numbers. This has many useful applications. For example, this allows us to evaluate whether models change the way they respond to meaningful information changes (""true positives"") or whether they start responding differently when they in fact should not be responding in a different way (""false positives""). As a motivating example, we would not trust a model if it provides different treatment recommendations because we have added a typo in your name or even changed your height if that is an irrelevant characteristic to the diagnosis. On the other hand, we can now identify whether the model in facts does not change recommendations even in the presence of new, important information that should have changed the answers. The paper builds these ideas theoretically and exemplifies them in multiple case studies."
Poster,Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise,https://ICML.cc//virtual/2025/poster/45937,"Ilias Diakonikolas, Mingchen Ma, Lisheng Ren, Christos Tzamos","We study the task of Multiclass Linear Classification (MLC) in the distribution-free PAC model with Random Classification Noise (RCN). Specifically, the learner is given a set of labeled examples $(x, y)$, where $x$ is drawn from an unknown distribution on $R^d$ and the labels are generated by a multiclass linear classifier corrupted with RCN. That is, the label $y$ is flipped from $i$ to $j$ with probability $H_{ij}$ according to a known noise matrix $H$ with non-negative separation $\sigma: = \min_{i \neq j} H_{ii}-H_{ij}$. The goal is to compute a hypothesis with small 0-1 error. For the special case of two labels, prior work has given polynomial-time algorithms achieving the optimal error. Surprisingly, little is known about the complexity of this task even for three labels.As our main contribution, we show that the complexity of MLC with RCN becomes drastically different in the presence of three or more labels. Specifically, we prove super-polynomialStatistical Query (SQ) lower bounds for this problem. In more detail, even for three labels and constant separation, we give a super-polynomial lower bound on the complexity of any SQ algorithm achieving optimal error. For a larger number of labels  and smaller separation, we show a super-polynomial SQ lower bound even for the weaker goal of achieving any constant factor approximation to the optimal loss or even beating the trivial hypothesis.","We study the computational complexity of multi-class linear classification with random classification noise. We provide the first set of statistical query lower bounds for the problem, indicating that unlike binary linear classification, which can be efficiently learned, multi-class linear classification with random classification noise is hard in the distributional free setting."
Poster,Statistical Test for Feature Selection Pipelines by Selective Inference,https://ICML.cc//virtual/2025/poster/46495,"Tomohiro Shiraishi, Tatsuya Matsukawa, Shuichi Nishino, Ichiro Takeuchi","A data analysis pipeline is a structured sequence of steps that transforms raw data into meaningful insights by integrating various analysis algorithms. In this paper, we propose a novel statistical test to assess the significance of data analysis pipelines. Our approach enables the systematic development of valid statistical tests applicable to any feature selection pipeline composed of predefined components. We develop this framework based on selective inference, a statistical technique that has recently gained attention for data-driven hypotheses. As a proof of concept, we focus on feature selection pipelines for linear models, composed of three missing value imputation algorithms, three outlier detection algorithms, and three feature selection algorithms. We theoretically prove that our statistical test can control the probability of false positive feature selection at any desired level, and demonstrate its validity and effectiveness through experiments on synthetic and real data. Additionally, we present an implementation framework that facilitates testing across any configuration of these feature selection pipelines without extra implementation costs.","In practical data analysis, applying a single algorithm to raw datasets is rarely sufficient.Analysts typically construct complex pipelines that integrate multiple algorithms to extract deeper insights. However, the intricacy of these pipelines often makes it difficult to determine whether the results are genuinely meaningful or merely the result of random fluctuations. To address this challenge, we introduce a statistical testing framework designed to evaluate the reliability of such results. As a proof of concept, we focus on pipelines that perform feature selection while handling missing data and detecting outliers. For these specific scenarios, we have developed customized statistical tests to rigorously assess the significance of the selected features. Our testing methodology provides a quantitative evaluation of feature importance, thereby increasing confidence in the analytical outcomes. Through extensive numerical experiments, we demonstrate that our approach facilitates the development of more robust and reliable analytical pipelines. Furthermore, we provide user-friendly software tools to support the application of these tests across a wide range of data analysis workflows."
Poster,"Stay Hungry, Keep Learning: Sustainable Plasticity for Deep Reinforcement Learning",https://ICML.cc//virtual/2025/poster/44413,"Huaicheng Zhou, Zifeng Zhuang, Donglin Wang","The integration of Deep Neural Networks in Reinforcement Learning (RL) systems has led to remarkable progress in solving complex tasks but also introduced challenges like primacy bias and dead neurons. Primacy bias skews learning towards early experiences, while dead neurons diminish the network's capacity to acquire new knowledge. Traditional reset mechanisms aimed at addressing these issues often involve maintaining large replay buffers to train new networks or selectively resetting subsets of neurons. However, these approaches either incur prohibitive computational costs or reset network parameters without ensuring stability through recovery mechanisms, ultimately impairing learning efficiency. In this work, we introduce the novel concept of neuron regeneration, which combines reset mechanisms with knowledge recovery techniques. We also propose a new framework called Sustainable Backup Propagation(SBP) that effectively maintains plasticity in neural networks through this neuron regeneration process. The SBP framework achieves whole network neuron regeneration through two key procedures: cycle reset and inner distillation. Cycle reset involves a scheduled renewal of neurons, while inner distillation functions as a knowledge recovery mechanism at the neuron level. To validate our framework, we integrate SBP with Proximal Policy Optimization (PPO) and propose a novel distillation function for inner distillation. This integration results in Plastic PPO (P3O), a new algorithm that enables efficient cyclic regeneration of all neurons in the actor network. Extensive experiments demonstrate the approach effectively maintains policy plasticity and improves sample efficiency in reinforcement learning.","To address the loss of plasticity in Deep Neural Networks, we introduce the novel concept of neuron regeneration, which combines reset mechanisms with knowledge recovery techniques. We propose a new framework called Sustainable Backup Propagation (SBP) that effectively maintains plasticity in neural networks through this regeneration process. To validate our framework, we integrate SBP with Proximal Policy Optimization (PPO) and introduce a novel distillation function for inner distillation. This integration leads to Plastic PPO (P3O), a new algorithm that enables efficient cyclic regeneration of all neurons in the actor network. This approach aims to maintain both plasticity and stability for improved long-term learning performance."
