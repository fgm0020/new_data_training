type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Minerva: A Programmable Memory Test Benchmark for Language Models,https://ICML.cc//virtual/2025/poster/44348,"Menglin Xia, Victor Ruehle, Saravanakumar Rajmohan, Reza Shokri","How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret, and lack actionable insights--failing to pinpoint the specific capabilities a model lacks when it does not pass a test. In this paper, we present a framework for automatically generating a comprehensive set of tests to evaluate models' abilities to use their memory effectively. Our framework extends the range of capability tests beyond the commonly explored (passkey, key-value, needle in the haystack) search, a dominant focus in the literature. Specifically, we evaluate models on atomic tasks such as searching, recalling, editing, matching, comparing information in context memory, performing basic operations when inputs are structured into distinct blocks, and maintaining state while operating on memory, simulating real-world data. Additionally, we design composite tests to investigate the models' ability to perform more complex, integrated tasks. Our benchmark enables an interpretable, detailed assessment of memory capabilities of LLMs.","How well can LLM-based AI assistants use their memory (or context) to complete different tasks? Many existing benchmarks are manually created and have several drawbacks: they’re fixed, easy for models to overfit, hard to interpret, and don’t reveal exactly what a model struggles with when it fails.In this paper, we introduce a framework for testing memory use in LLMs at scale. While most prior work focuses on simple search task, like finding a key piece of information in a long context (e.g., ""needle in a haystack""), our framework goes further. We design a diverse set of task types and automatically generate many test examples for each one. These include fine-grained tasks like searching, recalling, editing, matching, and comparing information in context. We also test whether models can handle structured inputs, keep track of changing information (state). On top of that, we build composite tasks that combine multiple skills, allowing us to assess how well models handle more complex, integrated challenges. Our benchmark offers a detailed and interpretable way to understand the memory capabilities of LLMs."
Poster,Minimalist Concept Erasure in Generative Models,https://ICML.cc//virtual/2025/poster/44073,"Yang Zhang, Er Jin, Yanfei Dong, Yixuan Wu, Phil Torr, Ashkan Khakzar, Johannes Stegmaier, Kenji Kawaguchi","Recent advances in generative models have demonstrated remarkable capabilities in producing high-quality images, but their reliance on large-scale unlabeled data has raised significant safety and copyright concerns. Efforts to address these issues by erasing unwanted concepts have shown promise. However, many existing erasure methods involve excessive modifications that compromise the overall utility of the model.In this work, we address these issues by formulating a novel minimalist concept erasure objective based *only* on the distributional distance of final generation outputs. Building on our formulation, we derive a tractable loss for differentiable optimization that leverages backpropagation through all generation steps in an end-to-end manner. We also conduct extensive analysis to show theoretical connections with other models and methods. To improve the robustness of the erasure, we incorporate neuron masking as an alternative to model fine-tuning. Empirical evaluations on state-of-the-art flow-matching models demonstrate that our method robustly erases concepts without degrading overall model performance, paving the way for safer and more responsible generative models.","Generative AI models, which can create lifelike images from simple text prompts, are transforming how we design, create, and communicate. But this powerful ability comes with serious risks: these models can unknowingly generate harmful, biased, or copyrighted content. While researchers have explored ways to “erase” such unwanted concepts, most existing methods are heavy-handed, they often damage the model’s overall usefulness. Our research proposes a more precise and reliable solution. We introduce a minimalist approach to concept erasure that focuses on the model’s final outputs, avoiding unnecessary changes to its internal workings. By guiding the model through all its generation steps, we ensure that it stops producing specific content, without compromising its broader creative abilities. To strengthen this process, we use a technique called neuron masking, which allows for targeted control without the need for retraining. The result is a safer, more responsible generative AI — one that retains its strengths while respecting ethical and legal boundaries."
Poster,Minimax Optimal Regret Bound for Reinforcement Learning with Trajectory Feedback,https://ICML.cc//virtual/2025/poster/44065,"Zihan Zhang, Yuxin Chen, Jason Lee, Simon Du, Ruosong Wang","In this work, we study reinforcement learning (RL) with trajectory feedback. Compared to the standard RL setting, in RL with trajectory feedback, the agent only observes the accumulative reward along the trajectory, and therefore, this model is particularly suitable for scenarios where querying the reward in each single step incurs prohibitive cost. For a finite-horizon Markov Decision Process (MDP) with $S$ states, $A$ actions and a horizon length of $H$, we develop an algorithm that enjoys an asymptotically nearly optimal regret of $\tilde{O}\left(\sqrt{SAH^3K}\right)$ in $K$ episodes.To achieve this result, our new technical ingredients include(i) constructing a tighter confidence region for the reward function by incorporating the RL with trajectory feedback setting with techniques in linear bandits and  (ii) constructing a reference transition model to better guide the exploration process.","We study reinforcement learning (RL) with trajectory feedback, where the agent only observes the cumulative reward along the trajectory. This model is particularly suitable for scenarios where querying the reward at each step incurs prohibitive costs. We develop an algorithm that achieves asymptotically near-optimal regret for finite-horizon Markov Decision Processes."
Poster,Minimum Width for Universal Approximation using Squashable Activation Functions,https://ICML.cc//virtual/2025/poster/46122,"Jonghyun Shin, Namjun Kim, Geonho Hwang, Sejun Park","The exact minimum width that allows for universal approximation of unbounded-depth networks is known only for ReLU and its variants. In this work, we study the minimum width of networks using general activation functions. Specifically, we focus on squashable functions that can approximate the identity function and binary step function by alternatively composing with affine transformations. We show that for networks using a squashable activation function to universally approximate $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$, the minimum width is $\max\\{d_x,d_y,2\\}$ unless $d_x=d_y=1$; the same bound holds for $d_x=d_y=1$ if the activation function is monotone. We then provide sufficient conditions for squashability and show that all non-affine analytic functions and a class of piecewise functions are squashable, i.e., our minimum width result holds for those general classes of activation functions.","Neural networks are the engines behind many AI tools we use today, like voice assistants and image recognition. In this paper, we investigate how small a neural network can be while performing a target task.We show that even very narrow networks (just a few computational units wide) can still handle any task, as long as they are deep enough. Specifically, the network only needs to be as wide as the number of inputs or outputs, whichever is bigger. In the simplest case, just two units can be enough. In summary, our results show that AI systems do not always need to be wide to be powerful."
Poster,MIPT: Multilevel Informed Prompt Tuning for Robust Molecular Property Prediction,https://ICML.cc//virtual/2025/poster/44497,"Yeyun Chen, Jiangming Shi","The progress in materials science and drug discovery is impeded by the availability of labeled data and the high costs of manual annotation, driving the need for efficient strategies to capture molecular representations and enable accurate predictions. Pretrained Graph Neural Networks have shown promise in capturing universal molecular representations, but adapting them to task-specific applications remains challenging. In this paper, we propose Multilevel Informed Prompt-Tuning (MIPT), a novel framework for effectively tailoring pretrained models to molecule-related tasks. MIPT utilizes a lightweight, multi-level prompt learning module to capture node-level and graph-level task-specific knowledge, ensuring adaptable and efficient tuning. Additionally, a noise penalty mechanism is introduced to address mismatches between pretrained representations and downstream tasks, reducing irrelevant or noisy information. Experimental results show that MIPT surpasses all baselines, aligning graph space and task space while achieving significant improvements in molecule-related tasks, demonstrating its scalability and versatility for molecular tasks.","Discovering new drugs and materials is tough partly because it’s hard and expensive to collect labeled data for molecules. This makes it difficult to train models that can understand and predict molecular behavior. To solve this, we developed a method called Multilevel Informed Prompt-Tuning (MIPT). It builds on powerful pretrained models but adds lightweight “prompts” that guide the model to focus on the task at hand—both at the atom level and the whole molecule level. We also introduced a noise-reduction step to avoid misleading signals from the original training. Our approach worked better than existing methods on a range of molecular tasks. This matters because it helps AI models learn faster and more accurately in chemistry and biology, even with limited data—making drug discovery and materials design more efficient and accessible."
Poster,MiraGe: Editable 2D Images using Gaussian Splatting,https://ICML.cc//virtual/2025/poster/45385,"Joanna Waczyńska, Tomasz Szczepanik, Piotr Borycki, Slawomir Tadeja, Thomas Bohné, Przemysław Spurek","Implicit Neural Representations (INRs) approximate discrete data through continuous functions and are commonly used for encoding 2D images. Traditional image-based INRs employ neural networks to map pixel coordinates to RGB values, capturing shapes, colors, and textures within the network’s weights. Recently, GaussianImage has been proposed as an alternative, using Gaussian functions instead of neural networks to achieve comparable quality and compression. Such a solution obtains a quality and compression ratio similar to classical INR models but does not allow image modification. In contrast, our work introduces a novel method, MiraGe, which uses mirror reflections to perceive 2D images in 3D space and employs flat-controlled Gaussians for precise 2D image editing. Our approach improves the rendering quality and allows realistic image modifications, including human-inspired perception of photos in the 3D world. Thanks to modeling images in 3D space, we obtain the illusion of 3D-based modification in 2D images. We also show that our Gaussian representation can be easily combined with a physics engine to produce physics-based modification of 2D images. Consequently, MiraGe allows for better quality than the standard approach and natural modification of 2D images.","How do people perceive flat 2D objects such as a photograph or a sheet of paper in the 3D world? Inspired by this question, our work explores 2D image representation through the lens of human perception by introducing the MiraGe model. Just as people intuitively understand and manipulate physical photographs by rotating, bending or reorienting them in 3D space, our model allows image representation to simulate this perceptual process. In order to represent an object in 3D, we use parameterized 3D Gaussian primitives, which we control. This allows us not only to represent the image well but also to edit it. While the scene is not fully reconstructed in three dimensions, the use of 3D Gaussians to represent a 2D image allows the model to create new views through camera movement. This technique produces a perceptual 2.5D effect, a strategy frequently employed in computer graphics and video games in otherwise flat, distant backgrounds."
Poster,MIRROR: Make Your Object-Level Multi-View Generation More Consistent with Training-Free Rectification,https://ICML.cc//virtual/2025/poster/43478,"TianChi Xing, Bonan Li, Congying Han, XINMIN QIU, Zicheng Zhang, Tiande Guo","Multi-view Diffusion has greatly advanced the development of 3D content creation by generating multiple images from distinct views, achieving remarkable photorealistic results. However, existing works are still vulnerable to inconsistent 3D geometric structures (commonly known as Janus Problem) and severe artifacts. In this paper, we introduce MIRROR, a versatile plug-and-play method that rectifies such inconsistencies in a training-free manner, enabling the acquisition of high-fidelity, realistic structures without compromising diversity. Our key idea focuses on tracing the motion trajectory of physical points across adjacent viewpoints, enabling rectifications based on neighboring observations of the same region. Technically, MIRROR comprises two core modules: Trajectory Tracking Module (TTM) for pixel-wise trajectory tracking that labels identical points across views, and Feature Rectification Module (FRM) for explicitly adjustment of each pixel embedding on noisy synthesized images by minimizing the distance to corresponding block features in neighboring views, thereby achieving consistent outputs. Extensive evaluations demonstrate that MIRROR can seamlessly integrate with a diverse range of off-the-shelf object-level multi-view diffusion models, significantly enhancing both the consistency and the fidelity in an efficient way.","Creating realistic 3D images of objects from multiple angles is a major goal in computer graphics and AI. However, current methods often struggle to keep the shape of an object consistent when viewed from different directions. This can lead to strange visual errors, like seeing a different face from each angle, or even multiple faces appearing at the same time.To solve this, we developed MIRROR, a simple and flexible tool that can improve the consistency of 3D-generated images without requiring any extra training. MIRROR identifies how each part of an object is represented from different angles and uses this information to fix errors and ensure consistent 3D appearance. This process helps produce 3D images that look more realistic and stable.Our approach can be added to a variety of existing 3D image generators and makes them work better — not just in terms of visual quality, but also in how reliable and consistent the outputs are. This has promising applications in fields like design, virtual reality, and digital content creation."
Poster,"Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?",https://ICML.cc//virtual/2025/poster/45550,"Tom Jacobs, Chao Zhou, Rebekka Burkholz","Implicit bias plays an important role in explaining how overparameterized models generalize well. Explicit regularization like weight decay is often employed in addition to prevent overfitting. While both concepts have been studied separately, in practice, they often act in tandem. Understanding their interplay is key to controlling the shape and strength of implicit bias, as it can be modified by explicit regularization. To this end, we incorporate explicit regularization into the mirror flow framework and analyze its lasting effects on the geometry of the training dynamics, covering three distinct effects: positional bias, type of bias, and range shrinking. Our analytical approach encompasses a broad class of problems, including sparse coding, matrix sensing, single-layer attention, and LoRA, for which we demonstrate the utility of our insights. To exploit the lasting effect of regularization and highlight the potential benefit of dynamic weight decay schedules, we propose to switch off weight decay during training, which can improve generalization, as we demonstrate in experiments.","This paper explores how explicit regularization (like weight decay) influences the ""implicit bias"" of machine learning models, which refers to the natural tendency of models to favor certain solutions even without direct constraints. We extend a mathematical framework called mirror flow to include regularization and we show that it can reshape the optimization landscape in three key ways: changing the kind of solutions a model prefers (type of bias), shifting where it tends to focus in the parameter space (positional bias), and narrowing the range of solutions it can settle on (range shrinking). We find that turning off regularization partway through training can help models generalize better, this is validated with experiments in areas like matrix sensing, vision, and fine-tuning language models. The paper offers new insights into how we might control a model's learning behavior more precisely using regularization."
Poster,MissScore: High-Order Score Estimation in the Presence of Missing Data,https://ICML.cc//virtual/2025/poster/45508,"Wenqin Liu, Haoze Hou, Erdun Gao, Biwei Huang, Qiuhong Ke, Howard Bondell, Mingming Gong","Score-based generative models are essential in various machine learning applications, with strong capabilities in generation quality. In particular, high-order derivatives (scores) of data density offer deep insights into data distributions, building on the proven effectiveness of first-order scores for modeling and generating synthetic data, unlocking new possibilities for applications. However, learning them typically requires complete data, which is often unavailable in domains such as healthcare and finance due to data corruption, acquisition constraints, or incomplete records. To tackle this challenge, we introduce MissScore, a novel framework for estimating high-order scores in the presence of missing data. We derive objective functions for estimating high-order scores under different missing data mechanisms and propose a new algorithm specifically designed to handle missing data effectively. Our empirical results demonstrate that MissScore accurately and efficiently learns the high-order scores from incomplete data and generates high-quality samples, resulting in strong performance across a range of downstream tasks.","Machine learning models often need a deep understanding of data patterns to generate realistic samples or make accurate predictions. One powerful approach uses *score functions*, which describe how the likelihood of data changes as input values vary. These scores are especially useful in advanced models such as diffusion-based generators. However, calculating them typically assumes that the data is fully observed, which is often not the case in real-world domains like healthcare or finance where missing values are common.We developed **MissScore**, a new method that estimates not only first-order but also higher-order score functions directly from incomplete datasets. Unlike methods that rely on imputation or auxiliary models, MissScore works directly with observed data and is designed to handle a range of realistic missing data scenarios.Our results show that MissScore generates high-quality synthetic data, improves sampling efficiency, and helps uncover causal relationships, all without requiring complete datasets. This opens the door to more reliable, efficient, and trustworthy machine learning in messy, real-world scenarios."
Poster,Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing,https://ICML.cc//virtual/2025/poster/43678,"Tianci Liu, Ruirui Li, Zihan Dong, Hui Liu, Xianfeng Tang, Qingyu Yin, Linjun Zhang, Haoyu Wang, Jing Gao","Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates.  Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: \textit{heterogeneous token overfitting} (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates.To tackle this, we propose {OVERTONE}, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method.","Powerful AI language tools (LLMs) are like highly knowledgeable students, but their information can quickly become outdated in our fast-changing world. Our research focuses on ""knowledge editing"" – the challenge of efficiently updating these AIs with new facts, without confusing them or causing them to forget what they already know.Previous attempts to edit an AI's knowledge often resulted in the AI memorizing new information too rigidly. This is a problem we call ""heterogeneous token overfitting"" (HTO), where the AI overfits specific words in the new fact, hindering its ability to truly understand and reason with that information. We develop a new technique called ""OVERTONE."" It acts as a smarter teaching method, helping the AI learn new facts more flexibly and smoothly. OVERTONE ensures the AI doesn't just memorize words but can connect new knowledge with its existing understanding. Importantly, our method is efficient and significantly improves the AI's ability to use new information effectively, making these powerful tools more reliable and up-to-date."
