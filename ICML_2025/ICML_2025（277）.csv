type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,SKIM: Any-bit Quantization Pushing The Limits of Post-Training Quantization,https://ICML.cc//virtual/2025/poster/45671,"Runsheng Bai, Bo Liu, qiang liu","Large Language Models (LLMs) exhibit impressive performance across various tasks, but deploying them for inference poses challenges. Their high resource demands often necessitate complex, costly multi-GPU pipelines, or the use of smaller, less capable models. While quantization offers a promising solution utilizing lower precision for model storage, existing methods frequently experience significant performance drops at lower precision levels. Additionally, they typically provide only a limited set of solutions at specific bit levels, many of which are extensively manually tuned. To address these challenges, we propose a new method called \textbf{SKIM}: Scaled K-means clustering wIth Mixed precision. Our approach introduces two novel techniques: 1. A \textit{greedy algorithm} to solve approximately optimal bit allocation across weight channels, and 2. A \textit{trainable scaling vector} for non-differentiable K-means clustering. These techniques substantially improve the model performance and can be adapted to any given bit. Notably, in terms of perplexity, our method narrows the gap between quantized LLaMA models and their full precision counterparts by around \textbf{14\%} on average.","Large language models (LLMs) like ChatGPT are powerful but require enormous computing resources, making them costly and impractical for everyday devices. Current methods to shrink these models either sacrifice accuracy at certain level or only work at specific precisions, limiting flexibility.To solve this, we developed SKIM—a new quantization technique for model compression. SKIM smartly allocates different precision levels for different parts of the model using a greedy algorithm, similar to managing a tight budget. Additionally, a trainable scaling vector, another innovative component, functions as a value thermostat, smoothing the data and adjusting the compression computation to preserve accuracy. Together, these innovations allow SKIM to compress LLMs to any bit size (even fractions like 3.2 bits) while minimizing performance loss.This advance narrows the accuracy gap with full-size models by 14% on average, enabling efficient deployment under resources-constraint scenario. By making powerful AI more accessible, SKIM helps democratize language technology without high costs."
Poster,SkipGPT: Each Token is One of a Kind,https://ICML.cc//virtual/2025/poster/44647,"Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Jing Xiong, Zhiwei Fei, Hui Su, Anhao Zhao","Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multi-layered architectures. Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) *horizontal dynamics*, where token-level heterogeneity demands context-aware pruning decisions, and (2) *vertical dynamics*, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies. We introduce **SkipGPT**, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations: (1) global token-aware routing to prioritize critical tokens and (2) decoupled pruning policies for MLP and self-attention components. To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal. Extensive experiments demonstrate that SkipGPT reduces over 40% model parameters while matching or exceeding the performance of the original dense model across benchmarks. By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs. Our code is publicly available at: https://github.com/EIT-NLP/SkipGPT.","Large language models like ChatGPT are very powerful but also very expensive to run because they require a lot of computer processing. This paper introduces a new method called SkipGPT that helps these models run faster and more efficiently. Instead of treating every word and every part of the model the same, SkipGPT figures out which words are most important and which parts of the model are actually needed. By skipping unnecessary steps, it can cut the size of the model by about 40%—while still performing just as well, or even better, than before. This means we can build smarter AI systems that cost less and use less energy, making them easier to use in real-world applications like phones, robots, or smart assistants."
Poster,Skip the Equations: Learning Behavior of Personalized Dynamical Systems Directly From Data,https://ICML.cc//virtual/2025/poster/46573,"Krzysztof Kacprzyk, Julianna Piskorz, Mihaela van der Schaar","While black-box approaches are commonly used for data-driven modeling of dynamical systems, they often obscure a system's underlying behavior and properties, limiting adoption in areas such as medicine and pharmacology. A two-step process of discovering ordinary differential equations (ODEs) and their subsequent mathematical analysis can yield insights into the system's dynamics. However, this analysis may be infeasible for complex equations, and refining the ODE to meet certain behavioral requirements can be challenging. Direct semantic modeling has recently been proposed to address these issues by predicting the system's behavior, such as the trajectory's shape, directly from data, bypassing post-hoc mathematical analysis. In this work, we extend the original instantiation, limited to one-dimensional trajectories and inputs, to accommodate multi-dimensional trajectories with additional personalization, allowing evolution to depend on auxiliary static features (e.g., patient covariates). In a series of experiments, we show how our approach enables practitioners to integrate prior knowledge, understand the dynamics, ensure desired behaviors, and revise the model when necessary.","When we need to understand how something changes over time—be it tumor size, battery charge, or drug concentration—seeing the reason behind the forecast matters. Yet, most modern models remain sealed inside black boxes. We present a transparent learning framework that forecasts how any quantity evolves over time, clearly showing how each feature (e.g., age, weight, biomarkers) influences that curve. A compact decision tree first predicts the curve's overall shape (e.g., steadily rising or peaking, then falling). A simple additive model then fills in the details, such as the starting point, peak height, and long-term limit. By writing each of these values as a sum of simple one-feature curves, this means you can plot how, for instance, age impacts peak height, or temperature impacts the decay rate and read off every factor's individual effect. Because both parts are readable and editable, domain experts—from doctors to engineers—can inject prior knowledge, verify that the model adheres to known dynamics, and quickly correct any unwanted behavior. In contrast to previous work that only allowed for dependence on one variable (the starting point), we extend it to handle multiple variables simultaneously, enabling truly personalized trajectories."
Poster,SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting,https://ICML.cc//virtual/2025/poster/44949,"Yitian Zhang, Liheng Ma, Antonios Valkanas, Boris Oreshkin, Mark Coates","Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance. Our code is available at: https://github.com/networkslab/SKOLR.","From tracking weather patterns to forecasting stock prices, predicting how things change over time — known as time-series forecasting — is a tough challenge, especially when those changes follow complex, nonlinear rules.In this work, we introduce a new method that bridges an existing mathematical theory with practical machine learning techniques to tackle this problem efficiently. The proposed approach is inspired by Koopman operator theory, a powerful mathematical framework providing the foundation of modeling complex non-linear systems with linear transformations.We establish a theoretical connection between the approximation of Koopman operators and a lightweight type of neural networks called linear Recurrent Neural Networks (linear RNNs).Leveraging this theoretical connection, we develop a novel linear RNN approach called Structured Koopman Operator Linear RNNs (SKOLR).SKOLR stands out by blending mathematical foundations with efficient computation. Tests across various datasets — from physics simulations to real-world time series benchmarks — show that it performs impressively with low computational burden.This work opens new possibilities for developing efficient and theoretically grounded models in time-series forecasting and dynamical system analysis, potentially influencing future research directions in these fields."
Poster,Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation,https://ICML.cc//virtual/2025/poster/43775,"Hoigi Seo, Wongi Jeong, Jae-sun Seo, Se Young Chun","Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.","Modern tools can turn text into detailed images, helping artists, educators, and everyday users bring their ideas to life. These tools rely on powerful artificial intelligence (AI) models that understand language and generate visuals. But there’s a problem: the part of the model that reads the text — called the text encoder — uses a lot of memory, even though it only runs once per image.We created a method called Skrr to make this process more memory-efficient. Skrr works by skipping or reusing parts of the text encoder that don’t add much value. This helps reduce the size of the model without lowering the quality of the images it creates. It’s a bit like trimming unnecessary scenes from a movie without changing the story.Our experiments showed that even when we removed nearly half of the text encoder, the image quality remained high. In fact, Skrr outperformed other popular compression techniques on multiple benchmarks. This makes it easier to run text-to-image systems on devices with less memory — like phones or laptops — helping make powerful AI more accessible to more people."
Poster,SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs,https://ICML.cc//virtual/2025/poster/45942,"Xin Su, Man Luo, Kris Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard","Multimodal retrieval-augmented generation (RAG) plays a crucial role in domains such as knowledge-based visual question answering (KB-VQA), where models should effectively integrate additional knowledge to generate a response. However, existing vision and language models (VLMs) are not inherently designed for context-augmented generation, limiting their effectiveness in such tasks. While synthetic data generation has recently gained attention for training large VLMs, its application for context-augmented generation remains underexplored. To address this gap, we introduce SKVQA, a large-scale synthetic multimodal dataset containing over 2 million visual question-answer pairs, each associated with external knowledge sources to determine the final answer. Compared to previous datasets, SKVQA exhibits 11× more unique questions, greater domain diversity, and a broader spectrum of image sources. Through human evaluations, we confirm the high quality of the generated question-answer pairs and their contextual relevance. Extensive experiments show that SKVQA serves both as a challenging benchmark for knowledge-based VQA and as an effective training resource for adapting generative multimodal models to context-augmented generation. Our results further indicate that models trained on SKVQA demonstrate enhanced generalization in both context-aware VQA and multimodal RAG settings.","This paper introduces SK-VQA, a large-scale dataset containing over 2 million question-answer pairs created to help AI models better understand and answer questions about images, especially when answering those questions requires additional background knowledge."
Poster,Sleeping Reinforcement Learning,https://ICML.cc//virtual/2025/poster/45138,"Simone Drago, Marco Mussi, Alberto Maria Metelli","In the standard Reinforcement Learning (RL) paradigm, the action space is assumed to be fixed and immutable throughout the learning process. However, in many real-world scenarios, not all actions are available at every decision stage. The available action set may depend on the current environment state, domain-specific constraints, or other (potentially stochastic) factors outside the agent's control. To address these realistic scenarios, we introduce a novel paradigm called *Sleeping Reinforcement Learning*, where the available action set varies during the interaction with the environment. We start with the simpler scenario in which the available action sets are revealed at the beginning of each episode. We show that a modification of UCBVI achieves regret of order $\widetilde{\mathcal{O}}(H\sqrt{SAT})$, where $H$ is the horizon, $S$ and $A$ are the cardinalities of the state and action spaces, respectively, and $T$ is the learning horizon. Next, we address the more challenging and realistic scenario in which the available actions are disclosed only at each decision stage. By leveraging a novel construction, we establish a minimax lower bound of order $\Omega(\sqrt{T 2^{A/2}})$ when the availability of actions is governed by a Markovian process, establishing a statistical barrier of the problem. Focusing on the statistically tractable case where action availability depends only on the current state and stage, we propose a new optimistic algorithm that achieves regret guarantees of order $\widetilde{\mathcal{O}}(H\sqrt{SAT})$, showing that the problem shares the same complexity of standard RL.","In Reinforcement Learning , an agent learns which actions to perform, i.e., a behavior, in order to solve a sequential decision making problem. The standard assumption is that, at each decision step, the agent selects an action from a fixed and immutable action space. However, in real-world applications, not all actions may be available at every decision stage, with their availability depending on the environment state, on domain-specific constraints, or on other (potentially stochastic) exogenous factors. To address this scenarios, we propose the Sleeping Reinforcement Learning paradigm, extending the standard episodic tabular Reinforcement Learning setting with an action availability model. We study two scenarios, namely action availability revealed for the entire episode and availability revealed for a single stage at a time, and two action availability models, namely independent and Markovian. Using the *regret* (i.e., how much is lost w.r.t. always making optimal decisions) as a performance index, we study the *lower bound*, i.e., the theoretical limit, of the regret and propose algorithms based on the state-of-the-art for standard RL that match such lower bounds up to logarithmic terms."
Poster,Sliding Puzzles Gym: A Scalable Benchmark for State Representation in Visual Reinforcement Learning,https://ICML.cc//virtual/2025/poster/43653,"Bryan L. M. de Oliveira, Luana G. B. Martins, Bruno Brandão, Murilo L. da Luz, Telma Woerle de Lima Soares, Luckeciano C. Melo","Effective visual representation learning is crucial for reinforcement learning (RL) agents to extract task-relevant information from raw sensory inputs and generalize across diverse environments. However, existing RL benchmarks lack the ability to systematically evaluate representation learning capabilities in isolation from other learning challenges. To address this gap, we introduce the Sliding Puzzles Gym (SPGym), a novel benchmark that transforms the classic 8-tile puzzle into a visual RL task with images drawn from arbitrarily large datasets. SPGym's key innovation lies in its ability to precisely control representation learning complexity through adjustable grid sizes and image pools, while maintaining fixed environment dynamics, observation, and action spaces. This design enables researchers to isolate and scale the visual representation challenge independently of other learning components. Through extensive experiments with model-free and model-based RL algorithms, we uncover fundamental limitations in current methods' ability to handle visual diversity. As we increase the pool of possible images, all algorithms exhibit in- and out-of-distribution performance degradation, with sophisticated representation learning techniques often underperforming simpler approaches like data augmentation. These findings highlight critical gaps in visual representation learning for RL and establish SPGym as a valuable tool for driving progress in robust, generalizable decision-making systems.","Teaching AI systems to interpret visual information is crucial for applications ranging from robotic navigation to game-playing agents. However, existing tests for these AI systems mix together visual understanding with other skills, making it difficult to tell whether poor performance comes from not seeing properly or from bad decision-making.To address this challenge, we created a new testing framework called Sliding Puzzles Gym, based on the classic sliding tile puzzle game. Instead of numbered tiles, our puzzles use pieces of real photographs. We can control how challenging the visual task is by changing how many different images the AI sees during training, while keeping everything else about the puzzle exactly the same. This lets researchers isolate and study just the visual learning component of these decision-making agents.Our findings reveal a critical limitation in current AI systems: as we increased the variety of images, all tested methods struggled more and performed worse, even when evaluated on previously seen images. Surprisingly, simple techniques like slightly modifying training images often outperformed sophisticated methods. These results expose fundamental gaps in how current AI systems process visual information and provide researchers with a powerful tool to develop more robust and reliable visual AI systems."
Poster,SlimLLM: Accurate Structured Pruning for Large Language Models,https://ICML.cc//virtual/2025/poster/46559,"Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang","Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.","Large Language Models (LLMs) represent a highly promising computing paradigm, but deployment is constrained by substantial computational requirements. We focus on pruning and compensating model parameters based on their output relevance. This approach helps minimize performance degradation while enhancing inference efficiency."
Poster,SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models,https://ICML.cc//virtual/2025/poster/45388,"Wei Huang, Haotong Qin, Yangdong Liu, Yawei Li, Qinshuo Liu, Xianglong Liu, Luca Benini, Michele Magno, Shiming Zhang, XIAOJUAN QI","Post-training quantization (PTQ) is an effective technique for compressing large language models (LLMs). However, while uniform-precision quantization is computationally efficient, it often compromises model performance. To address this, we propose SliM-LLM, a salience-driven mixed-precision quantization framework that allocates bit-widths at the group-wise with high accuracy. Our approach leverages the observation that important weights follow a structured distribution and introduces two key components: 1) Salience-Determined Bit Allocation adaptively assigns bit-widths to groups within each layer based on their salience; and 2) Salience-Weighted Quantizer Calibration optimizes quantizer parameters by incorporating element-level salience, retain essential information. With its structured group-wise partitioning, SliM-LLM provides a hardware-friendly solution that matches the efficiency of uniform quantization methods while significantly improving accuracy. Experiments show that SliM-LLM achieves superior performance across various LLMs at low bit-widths. For example, a 2-bit quantized LLaMA-7B model reduces memory usage by nearly 6x compared to the floating-point baseline, decreases perplexity by 48% compared to state-of-the-art gradient-free PTQ methods, and maintains GPU inference speed. Additionally, the extended version, SliM-LLM+, which incorporates gradient-based quantization, further reduces perplexity by 35.1%. Our code isavailable at https://github.com/Aaronhuang-778/SliM-LLM.","When large language models (LLMs) are used, they can require a lot of memory and computing power. To make these models smaller and more efficient, we use a process called quantization, which reduces the amount of information the model needs to store. However, standard methods of quantization can hurt the model’s performance and previous mixed-precision quantization is not hardware-friendly.To solve this, we developed SliM-LLM, a smarter way of quantizing models. Instead of treating all parts of the model the same, SliM-LLM uses a ""mixed-precision"" approach and makes it structural inner matrix. It assigns more storage (bit-width) to the important groups of in each matrix and less to the less important parts, based on their importance, or ""salience."" This method keeps the model accurate while staying efficient.Our tests show SliM-LLM dramatically improves performance compared to other methods, especially for small memory sizes. For example, it reduces memory use by 6x for the LLaMA-7B model while maintaining speed and outperforming other approaches in accuracy."
