type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Securing Equal Share: A Principled Approach for Learning Multiplayer Symmetric Games,https://ICML.cc//virtual/2025/poster/43902,"Jiawei Ge, Yuanhao Wang, Wenzhe Li, Chi Jin","This paper examines multiplayer symmetric constant-sum games with more than two players in a competitive setting, such as Mahjong, Poker, and various board and video games. In contrast to two-player zero-sum games, equilibria in multiplayer games are neither unique nor non-exploitable, failing to provide meaningful guarantees when competing against opponents who play different equilibria or non-equilibrium strategies. This gives rise to a series of long-lasting fundamental questions in multiplayer games regarding suitable objectives, solution concepts, and principled algorithms. This paper takes an initial step towards addressing these challenges by focusing on the natural objective of *equal share*—securing an expected payoff of $C/n$ in an $n$-player symmetric game with a total payoff of $C$. We rigorously identify the theoretical conditions under which achieving an equal share is tractable and design a series of efficient algorithms, inspired by no-regret learning, that *provably* attain approximate equal share across various settings. Furthermore, we provide complementary lower bounds that justify the sharpness of our theoretical results. Our experimental results highlight worst-case scenarios where meta-algorithms from prior state-of-the-art systems for multiplayer games fail to secure an equal share, while our algorithm succeeds, demonstrating the effectiveness of our approach.","Many popular games—like Mahjong, Poker, and online multiplayer games—involve more than two players competing for a fixed reward. In these settings, traditional tools from game theory that work well for two-player games, such as the concept of ""Nash equilibrium,"" often fail to provide meaningful guarantees. As a result, players can end up with highly unequal rewards, even if they play reasonably well.A long-standing question in multiplayer games is: What should the goal be? We focus on a simple but powerful idea called the equal share—securing a fair portion of the total prize. We identify the mathematical conditions under which this goal is achievable and design new algorithms that can reliably secure this outcome. These algorithms are not only theoretically sound but also effective in practice: in our experiments, they consistently achieve fair results, even in challenging scenarios where prior state-of-the-art methods fall short."
Poster,SeedLoRA: A Fusion Approach to Efficient LLM Fine-Tuning,https://ICML.cc//virtual/2025/poster/46328,"Yong Liu, Di Fu, Shenggan Cheng, Zirui Zhu, Yang Luo, Minhao Cheng, Cho-Jui Hsieh, Yang You","Despite Low-Rank Adaptation (LoRA)'s popularity for fine-tuning large models, it often exhibits a noticeable performance gap compared to full fine-tuning, particularly in complex tasks such as mathematical reasoning and code generation. Motivated by this discrepancy, we propose a novel fusion approach for LoRA fine-tuned models. Our key insight is that LoRA models trained with different random seeds on the same task often exhibit complementary strengths. In contrast to existing research that typically focuses on fusing models trained on diverse tasks, we explore the potential of combining multiple LoRA models fine-tuned on the same task with different random seeds. This intra-task fusion method aims to leverage the strengths of various fine-tuned models to create a more robust and effective adaptation. To validate our approach, we conducted comprehensive experiments across three key areas: mathematical reasoning, code generation, and general instruction-tuning tasks. The results demonstrate that our fusion method significantly enhances LoRA's performance, outperforming both standalone LoRA models and current fusion methods. Notably, this advancement substantially narrows the gap between LoRA and full fine-tuning, thus offering a more effective approach to model adaptation without the GPU memory burden of full parameter fine-tuning.","Despite its popularity for making large language models (LLMs) more efficient, a technique called Low-Rank Adaptation (LoRA) often falls short of full fine-tuning, especially for challenging tasks like solving math problems or generating code.Our research, ""SeedLoRA,"" introduces a new method to close this performance gap. We observed that LoRA models, even when trained on the same task, develop unique strengths if they start with different random initial settings (seeds). Unlike existing methods that merge models trained on different tasks, SeedLoRA specifically focuses on combining these ""same-task, different-seed"" LoRA models.SeedLoRA uses a two-stage process: first, it identifies and preserves the strong, consistent knowledge shared across these models; then, it cleverly fuses the unique, complementary insights from each model in a shared digital space.Our experiments demonstrated that SeedLoRA significantly improves performance across various challenging tasks, effectively matching or even exceeding the capabilities of full fine-tuning while still benefiting from LoRA's efficiency. This breakthrough highlights that by cleverly combining the diverse strengths learned by models starting from different random points, we can create more robust and effective large language models, suggesting a new path for optimizing their training."
Poster,SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning,https://ICML.cc//virtual/2025/poster/43771,"Jinpeng Chen, Runmin Cong, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, Horace Ip, Sam Kwong","Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting, thus adapting to evolving requirements. In this paper, we explore the forgetting caused by such incremental training, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model’s knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks’ answer styles, making the results unusable. On the other hand, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can conceal the model’s knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for data style transformations across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization to LoRA’s weight update matrices, enabling the model to retain existing competencies while remaining adaptable to new tasks. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.","Multimodal large language models often suffer from forgetting previously acquired capabilities when adapted to new tasks. We categorize this forgetting into two distinct types: superficial forgetting, which reflects a loss of correct response style, and essential forgetting, which corresponds to the actual degradation of learned knowledge. To mitigate both types, we propose SEFE, a simple yet effective method that promotes consistent response styles across tasks to reduce superficial forgetting, while constraining updates to critical parameters to mitigate essential forgetting. Experimental results demonstrate that SEFE effectively alleviates forgetting and improves overall performance."
Poster,Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation,https://ICML.cc//virtual/2025/poster/46216,"Zhihua Liu, Amrutha Saseendran, Lei Tong, Xilin He, Fariba Yousefi, Nikolay Burlutskiy, Dino Oglic, Tom Diethe, Philip Teare, Huiyu Zhou, Chen Jin","Open-set image segmentation poses a significant challenge because existing methods often demand extensive training or fine-tuning and generally struggle to segment unified objects consistently across diverse text reference expressions. Motivated by this, we propose Segment Anyword, a novel training-free visual concept prompt learning approach for open-set language grounded segmentation that relies on token-level cross-attention maps from a frozen diffusion model to produce segmentation surrogates or *mask prompts*, which are then refined into targeted object masks. Initial prompts typically lack coherence and consistency as the complexity of the image-text increases, resulting in suboptimal mask fragments. To tackle this issue, we further introduce a novel linguistic-guided visual prompt regularization that binds and clusters visual prompts based on sentence dependency and syntactic structural information, enabling the extraction of robust, noise-tolerant mask prompts, and significant improvements in segmentation accuracy. The proposed approach is effective, generalizes across different open-set segmentation tasks, and achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative to fine-tuned methods) mIoU on GranDf, which is the most complex open-set grounded segmentation task in the field.","Segmenting objects in images based on textual descriptions is challenging for computers, especially when those descriptions vary. Different people might describe the same object in different ways. For example, a doctor and a patient, or a child and an adult, might use different terms to refer to the same thing. This variation in expression makes it difficult for current systems to consistently identify the correct object in images.We propose a new approach to make these systems more flexible and robust. We adapt a pre-trained model to learn how to recognize specific concepts described in varied language. We found that the way this model connects words and images internally can serve as a powerful guide for identifying objects. We further incorporate basic linguistic rules, such as how adjectives relate to nouns, to help the system handle noisy or ambiguous guidance more effectively.Our method is training-free, as it works entirely at test time without requiring access to a training or tuning dataset. We demonstrate that our approach achieves stable and accurate performance across a range of segmentation tasks, with promising results in segmenting not only nouns (objects), but also predicates in object-object relationships and human-object interactions."
Poster,Selective Preference Aggregation,https://ICML.cc//virtual/2025/poster/45248,"Shreyas Kadekodi, Hayden McTavish, Berk Ustun","Many applications in machine learning and decision-making rely on procedures to aggregate human preferences.In such tasks, individual express ordinal preferences over a set of items through votes, ratings, or pairwise comparisons. We then summarize their collective preferences as a ranking. Standard methods for preference aggregation are designed to return rankings that arbitrate individual disagreements in ways that are faithful and fair. In this work, we introduce a paradigm for *selective aggregation*, where we can avoid the need to arbitrate dissent by abstaining from comparison. We summarize collective preferences as a *selective ranking* -- i.e., a partial order where we can only compare items where at least $100\cdot(1 - \tau)\%$ of individuals agree. We develop algorithms to build selective rankings that achieve all possible trade-offs between comparability and disagreement, and derive formal guarantees on their safety and stability. We conduct an extensive set of experiments on real-world datasets to benchmark our approach and demonstrate its functionality. Our results show selective aggregation can promote transparency and robustness by revealing disagreement and abstaining from arbitration.","Many decision‑making pipelines, such as grant panels, content moderation, or choosing the best model among many, combine people’s ranked opinions into one result. Standard aggregation algorithms force a total order where nearly every item is ranked independently; when voters disagree, they silently break ties.**Selective Preference Aggregation (SPA)** produces a *tiered* list. Items are only ordered when at least  1- \$\tau\$ voters agree, while disputed pairs stay in the same tier. A lightweight graph algorithm finds the unique ranking that maximizes  comparisons while respecting the chosen dissent threshold $\tau$.Our experiments demonstrate that SPA preserves consensus information while eliminating preference inversions under adversarial noise. Our results show SPA is safe (never contradicts the majority), stable (robust to missing votes), and efficient. By signalling where consensus is strong or weak, SPA  yields rankings that are transparent, robust, and harder to manipulate."
Poster,Selective Prompt Anchoring for Code Generation,https://ICML.cc//virtual/2025/poster/44812,"Yuan Tian, Tianyi Zhang","Recent advances in large language models (LLMs) have transformed software development by automatically generating code from natural language. Yet challenges remain in generating fully correct code that aligns with user intent. Our study reveals that LLMs tend to pay less attention to user prompts as more code tokens are generated. We hypothesize that this attention dilution issue is an important reason for code generation errors. To mitigate this issue, we propose ***S**elective **P**rompt **A**nchoring* (SPA) to guide code LLMs to pay more attention to user intent when generating code. We evaluate SPA using six base LLMs across six benchmarks. Our results demonstrate that SPA enhances Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods in all settings. Our code is available at https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.","Large language models (LLMs) are changing the way people write code, making it possible to generate programs simply via natural language. However, LLMs sometimes fail to capture the user's intent and make mistakes in the generated code. We identified a surprising reason for code generation mistakes: as the model writes more code, it gradually “forgets” to focus on the user’s original instructions (i.e., user prompt). To fix this, we introduce a model-agnostic and training-free mechanism to control LLMs’ attention. We use this attention-controlling mechanism to ensure that LLMs attend to users’ instructions throughout the code generation process. We show that this idea significantly and consistently improves code generation performance. To help other researchers explore more ideas, we have released a general and easy-to-use API for controlling the attention of any LLMs."
Poster,Selective Response Strategies for GenAI,https://ICML.cc//virtual/2025/poster/45233,"Boaz Taitler, Omer Ben-Porat","The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare.","When people use LLM, they don't generate data in public Q&A forums. Yet, such human-generated data is vital for training future LLMs. We propose a way, which we call selective response, where LLMs strategically provide partial or conservative responses, encouraging users to seek answers on human-driven forums occasionally. We build a theoretical model to test this strategy, identifying scenarios where it benefits both users and the LLM itself, and propose practical algorithms for real-world implementation."
Poster,Self-Bootstrapping for Versatile Test-Time Adaptation,https://ICML.cc//virtual/2025/poster/45578,"Shuaicheng Niu, Guohao Chen, Peilin Zhao, Tianyi Wang, Pengcheng Wu, Zhiqi Shen","In this paper, we seek to develop a versatile test-time adaptation (TTA) objective for a variety of tasks — classification and regression across image-, object-, and pixel-level predictions. We achieve this through a self-bootstrapping scheme that optimizes prediction consistency between the test image (as target) and its deteriorated view. The key challenge lies in devising effective augmentations/deteriorations that: i) preserve the image’s geometric information, e.g., object sizes and locations, which is crucial for TTA on object/pixel-level tasks, and ii) provide sufficient learning signals for TTA. To this end, we analyze how common distribution shifts affect the image's information power across spatial frequencies in the Fourier domain, and reveal that low-frequency components carry high power and masking these components supplies more learning signals, while masking high-frequency components can not. In light of this, we randomly mask the low-frequency amplitude of an image in its Fourier domain for augmentation. Meanwhile, we also augment the image with noise injection to compensate for missing learning signals at high frequencies, by enhancing the information power there. Experiments show that, either independently or as a plug-and-play module, our method achieves superior results across classification, segmentation, and 3D monocular detection tasks with both transformer and CNN models.","We seek to develop a versatile test-time adaptation (TTA) objective for a variety of tasks — classification and regression across image-, object-, and pixel-level predictions. We achieve this through a self-bootstrapping scheme that optimizes prediction consistency between the test image (as target) and its deteriorated view. The deteriorated view is constructed through a low-frequency amplitude mask in the frequency domain and high-frequency noise injection. These deteriorations keep the geometry information of the original image, and meanwhile supply sufficient learning signals for self-bootstrapping TTA. Experiments show that, either independently or as a plug-and-play module, our method achieves superior results across classification, segmentation, and 3D monocular detection tasks with both transformer and CNN models."
Poster,SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models,https://ICML.cc//virtual/2025/poster/43899,"Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Scott Yih","We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through *context ablation*: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. The source code is available at https://github.com/facebookresearch/SelfCite.","LLMs can support their answers with citations, but these citations are usually not accurate or fine-grained enough.Current methods rely on human-labeled data to train LLMs to generate citations, which is costly and time-consuming.We built SelfCite, a self-rewarding system that enables LLMs to self-teach and judge their own citations without additional human labeling.The idea is simple: if removing cited sentences from the source documents changes the LLM’s answer, those sentences were necessary; if keeping only those sentences still yields the same answer, they were sufficient.By rewarding the LLM based on these two checks, SelfCite helps it choose better citations—ones that are both necessary and sufficient—as it generates responses.The LLM drafts several possible citations and selects the best candidate with the highest self-score. We can also train the model to directly produce better candidates.On a challenging benchmark of long, open-ended questions, this approach improved citation quality by up to five points.Clearer citations make it easier for journalists, educators, and everyday readers to trust or critically assess AI-generated content, bringing us closer to transparent and verifiable AI systems."
Poster,Self-Consistency Preference Optimization,https://ICML.cc//virtual/2025/poster/46253,"Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, JASON WESTON, Jane Dwivedi-Yu","Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.","To teach AI models to reason better, such as solving math problems, we usually need correct answers to those problems. In this paper, we explore the possibility of training without access to any answers, enabling the model to improve its reasoning capabilities without any external help. Our key insight is that while the model sometimes solves a problem correctly, it also makes mistakes. To distinguish correct answers from incorrect ones, we can make use of the consistency of the model’s outputs. Correct solutions tend to converge on the same answer and are thus more consistent, while the mistakes often lead to different answers that lack any consistency. Building on this insight, we propose Self-consistency Preference Optimization (ScPO), which trains the model to favor consistent answers and reduce the chance of generating inconsistent, and likely incorrect, outputs. Our experiments show that this works well and the model reasons better after our ScPO training, narrowing the gap with methods that rely on human-provided correct answers. Notably, for logical puzzles, ScPO even enables a smaller Llama-3 8B model to outperform much larger models like Llama-3 70B and Claude-3 Haiku."
