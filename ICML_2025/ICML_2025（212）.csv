type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Optimal Transfer Learning for Missing Not-at-Random Matrix Completion,https://ICML.cc//virtual/2025/poster/45622,"Akhil Jalan, Yassir Jedra, Arya Mazumdar, Soumendu Sundar Mukherjee, Purnamrita Sarkar","We study transfer learning for matrix completion in a Missing Not-at-Random (MNAR) setting that is motivated by biological problems. The target matrix $Q$ has entire rows and columns missing, making estimation impossible without side information. To address this, we usea noisy and incomplete source matrix $P$, which relates to $Q$ via a feature shift in latent space. We consider both the *active* and *passive* sampling of rows and columns.  We establish minimax lower bounds for entrywise estimation error in each setting. Our computationally efficient estimation framework achieves this lower bound for the active setting, which leverages the source data to query the most informative rows and columns of $Q$. This avoids the need for *incoherence* assumptions required for rate optimality in the passive sampling setting. We demonstrate the effectiveness of our approach through comparisons with existing algorithms on real-world biological datasets.","The problem that prompted our research is matrix completion, which takes a noisy and incomplete matrix as input and requires a full matrix as output. Matrix completion arises in many application areas; our motivation comes from missing data in biological settings such as gene sequencing, metabolic network construction, and companion diagnostics. In these settings, entire rows and columns of the data matrix can be missing, making traditional matrix completion algorithms ineffective. We formulate this specific kind of matrix completion problem as a transfer learning problem, in which we have access to a source matrix P as well as target matrix Q. The matrix Q typically has more observational noise and missing entries, such as when Q corresponds to the metabolic network of a rarely studied species while P is that of a more common species. We then present optimal estimation algorithms for Q in two settings: active sampling (where we have to decide what entries of Q to observe) and passive sampling (where we simply have some pre-observed Q). The algorithms are optimal in the sense of achieving the best possible estimation error, given a certain amount of data and underlying data distribution. This research matters because it can directly contribute to applied studies in biostatistics and bioinformatics, as we demonstrate in our own experiments. Additionally, we make progress on transfer learning, which is an important area of machine learning in its own right."
Poster,Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization,https://ICML.cc//virtual/2025/poster/45464,"Kaheon Kim, Rentian Yao, Changbo Zhu, Xiaohui Chen","The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a fundamental notion of averaging that extends from the Euclidean space to the Wasserstein space of probability distributions. Computation of the *unregularized* barycenter for discretized probability distributions on point clouds is a challenging task when the domain dimension $d > 1$. Most practical algorithms for approximating the barycenter problem are based on entropic regularization. In this paper, we introduce a nearly linear time $O(m \log{m})$ and linear space complexity $O(m)$ primal-dual algorithm, the *Wasserstein-Descent $\dot{\mathbb{H}}^1$-Ascent* (WDHA) algorithm, for computing the *exact* barycenter when the input probability density functions are discretized on an $m$-point grid. The key success of the WDHA algorithm hinges on alternating between two different yet closely related Wasserstein and Sobolev optimization geometries for the primal barycenter and dual Kantorovich potential subproblems. Under reasonable assumptions, we establish the convergence rate and iteration complexity of WDHA to its stationary point when the step size is appropriately chosen. Superior computational efficacy, scalability, and accuracy over the existing Sinkhorn-type algorithms are demonstrated on high-resolution (e.g., $1024 \times 1024$ images) 2D synthetic and real data.","The Wasserstein barycenter represents the ""average"" of a given set of probability distributions. Computing this average exactly becomes challenging when the dimension of the domain is greater than one. Most existing methods simplify the problem by adding a smoothing term (called entropic regularization), but this introduces some inaccuracy. In this work, we present a new algorithm called *Wasserstein-Descent $\dot{\mathbb{H}}^1$-Ascent* (WDHA) that can compute the exact average efficiently, using nearly linear time and memory when the densities of the distributions are known on a common grid. We demonstrate that WDHA outperforms popular existing methods in both speed and accuracy on high-resolution (e.g., $1024 \times 1024$ images) 2D synthetic and real data."
Poster,Optimal transport-based conformal prediction,https://ICML.cc//virtual/2025/poster/44266,"Gauthier Thurin, Kimia Nadjahi, Claire Boyer","Conformal Prediction (CP) is a principled framework for quantifying uncertainty in black-box learning models, by constructing prediction sets with finite-sample coverage guarantees. Traditional approaches rely on scalar nonconformity scores, which fail to fully exploit the geometric structure of multivariate outputs, such as in multi-output regression or multiclass classification. Recent methods addressing this limitation impose predefined convex shapes for the prediction sets, potentially misaligning with the intrinsic data geometry. We introduce a novel CP procedure handling multivariate score functions through the lens of optimal transport. Specifically, we leverage Monge-Kantorovich vector ranks and quantiles to construct prediction region with flexible, potentially non-convex shapes, better suited to the complex uncertainty patterns encountered in multivariate learning tasks. We prove that our approach ensures finite-sample, distribution-free coverage properties, similar to typical CP methods. We then adapt our method for multi-output regression and multiclass classification, and also propose simple adjustments to generate adaptive prediction regions with asymptotic conditional coverage guarantees. Finally, we evaluate our method on practical regression and classification problems, illustrating its advantages in terms of (conditional) coverage and efficiency.","Most of machine learning models used on a daily basis are black boxes, meaning that it is difficult to control their uncertainty.  Conformal prediction gathers techniques used for quantifying this uncertainty. However, the traditional paradigm is mostly used to target a single variable. We use recent statistical tools (optimal-transport based quantiles) to extend the usual methodology towards multiple variables."
Poster,Optimistic Algorithms for Adaptive Estimation of the Average Treatment Effect,https://ICML.cc//virtual/2025/poster/46638,"Ojash Neopane, Aaditya Ramdas, Aarti Singh","Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation trade-offs relevant in finite-sample regimes or rely on simpler but suboptimal estimators.In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multi-armed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to previous methods. Our findings mark a step forward in the advancement of adaptive causal inference methods in theory and practice.","Randomized controlled trials determine whether new treatments—such as drugs or educational programs—work better than existing approaches. These trials are expensive and time-consuming, requiring many participants to reach reliable conclusions. We developed a method that uses information from early trial participants to intelligently select future participants, potentially cutting the required sample size significantly. While previous research addressed this challenge assuming unlimited participants, real clinical trials often work with small groups where every participant counts. Our approach specifically targets these small-sample scenarios. The key insight is that the algorithm must be ""optimistic""—actively testing groups where it suspects larger treatment differences might exist, rather than playing it safe. This strategic optimism allows the trial to focus resources where they matter most. In practical terms, this could mean reaching the same conclusions with half the participants, reducing both costs and the time needed to bring effective treatments to patients."
Poster,Optimization for Neural Operators can Benefit from Width,https://ICML.cc//virtual/2025/poster/44158,"Pedro Cisneros-Velarde, Bhavesh Shrimali, Arindam Banerjee","Neural Operators that directly learn mappings between function spaces, such as Deep Operator Networks (DONs) and Fourier Neural Operators (FNOs), have received considerable attention. Despite the universal approximation guarantees for DONs and FNOs, there is currently no optimization convergence guarantee for learning such networks using gradient descent (GD). In this paper, we address this open problem by presenting a unified framework for optimization based on GD and applying it to establish convergence guarantees for both DONs and FNOs. In particular, we show that the losses associated with both of these neural operators satisfy two conditions—restricted strong convexity (RSC) and smoothness—that guarantee a decrease on their loss values due to GD. Remarkably, these two conditions are satisfied for each neural operator due to different reasons associated with the architectural differences of the respective models. One takeaway that emerges from the theory is that wider networks benefit optimization convergence guarantees for both DONs and FNOs. We present empirical results on canonical operator learning problems to support our theoretical results and find that larger widths benefit training.","Neural operators are widely used for numerical applications in scientific computing—two of the most widely used are Deep Operator Networks (DONs) and Fourier Neural Operators (FNOs). Although these models have been extensively trained, there has been no formal understanding of why their training works. To fill this gap, we first propose a general mathematical framework that provides conditions under which we can explain the training of any model. Then, we instantiate these conditions to the case of DONs and FNOs—a non-trivial task in itself—to guarantee the training success of these technologies. Importantly, we find that the wider the neural operators, the larger the benefit on our training conditions. Experimentally, we find that larger widths lead to faster convergence and lower training error. These results are important not only for explaining neural operator training, but also to potentially understand the training of other deep learning technologies."
Poster,Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees,https://ICML.cc//virtual/2025/poster/46136,"William de Vazelhes, Xiaotong Yuan, Bin Gu","In sparse optimization,  enforcing hard constraints using the $\ell_0$ pseudo-norm offers advantages like controlled sparsity compared to convex relaxations. However, many real-world applications demand not only sparsity constraints but also some extra constraints. While prior algorithms have been developed to address this complex scenario with mixed combinatorial and convex constraints, they typically require the closed form projection onto the mixed constraints which might not exist, and/or only provide local guarantees of convergence which is different from the global guarantees commonly sought in sparse optimization. To fill this gap, in this paper, we study the problem of sparse optimization with extra *support-preserving* constraints commonly encountered in the literature. We present a new variant of iterative hard-thresholding algorithm equipped with a two-step consecutive projection operator customized for these mixed constraints,  serving as a simple alternative to the Euclidean projection onto the mixed constraint. By introducing a novel trade-off between sparsity relaxation and sub-optimality, we provide global guarantees in objective value for the output of our algorithm, in the deterministic, stochastic, and zeroth-order settings, under the conventional restricted strong-convexity/smoothness assumptions.  As a fundamental contribution in  proof techniques, we develop a novel extension of the classic three-point lemma to the considered two-step non-convex projection operator, which allows us to analyze the convergence in objective value in an elegant way that has not been possible with existing techniques. In the zeroth-order case, such technique also improves upon the state-of-the-art result from de Vazelhes et. al. (2022), even in the case without additional constraints, by allowing us to remove a non-vanishing system error present in their work.","Many real-world problems, from sensor selection to feature selection in machine learning, involve picking just a few important variables while meeting additional conditions. This is called “sparse optimization with constraints.” Solving such problems can be tricky, especially when the usual mathematical tools don’t work well due to complex requirements.Our work introduces a new algorithm that can handle these situations more effectively. It builds on a popular method called iterative hard thresholding but adapts it to deal with both sparsity and additional constraints by breaking the problem into two simple steps.We also developed simpler and more extensible mathematical tool to help analyze how well the algorithm performs, improving on previous results.This work helps make powerful optimization techniques more widely usable in practical, real-world situations."
Poster,Optimization Proxies using Limited Labeled Data and Training Time -- A Semi-Supervised Bayesian Neural Network Approach,https://ICML.cc//virtual/2025/poster/45674,"Parikshit Pareek, Abhijith Jayakumar, Kaarthik Sundar, Sidhant Misra, Deepjyoti Deka","Constrained optimization problems arise in various engineering systems such as inventory management and power grids. Standard deep neural network (DNN) based machine learning proxies are ineffective in practical settings where labeled data is scarce and training times are limited. We propose a semi-supervised Bayesian Neural Networks (BNNs) based optimization proxy for this complex regime, wherein training commences in a sandwiched fashion, alternating between a supervised learning step for minimizing cost, and an unsupervised learning step for enforcing constraint feasibility. We show that the proposed semi-supervised BNN outperforms DNN architectures on important non-convex constrained optimization problems from energy network operations, achieving up to a tenfold reduction in expected maximum equality gap and halving the inequality gaps. Further, the BNN's ability to provide posterior samples is leveraged to construct practically meaningful probabilistic confidence bounds on performance using a limited validation data, unlike prior methods.","Constrained optimization problems arise frequently when we try to optimize certain features of a system (like cost of operation or power produced) while trying to stay feasible i.e., within valid operating parameters. Optimization algorithms can find the optimum of such problems, but they get prohibitively expensive as the system size increases. Hence it is necessary to look to ML-based approaches that reuse past solutions to predict solutions to new instances of the problem almost instantly. In this work, we propose a semi-supervised approach to train Bayesian Neural Networks (BNN) as ML surrogates for constrained optimization problems. BNNs, compared to vanilla neural nets, are known for their superior performance in the low-data regime. We train BNN in an alternating fashion, first training it purely on supervised data (instances of solved optimization problems) and then on unsupervised data (data with un-optimized, but feasible instances). This alternating pattern is repeated till convergence. We test our method on publicly available datasets on electrical power flow optimization and find that this method works exceedingly well compared to other methods especially in the low-data and time-constrained setting. BNNs, being inherently probabilistic, produce a distribution of predictions for every problem instance. We leverage this property to give confidence intervals for the optimal operating point predicted by BNN. This allows practitioners to see not just solutions, but also how confident the model is about a solution. This property makes our approach ideal for safety-critical applications like power flow optimization."
Poster,Optimizing Adaptive Attacks against Watermarks for Language Models,https://ICML.cc//virtual/2025/poster/46148,"Abdulrahman Diaa, Toluwani Aremu, Nils Lukas","Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret *watermarking key*. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against *non-adaptive* attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune *adaptive* attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against *any* watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively tuned paraphrasers at <https://github.com/nilslukas/ada-wm-evasion>.","Large Language Models (LLMs) like ChatGPT can produce realistic text that might be misused for spreading misinformation or spam. To address this issue, researchers use a technique called watermarking, which secretly embeds patterns into generated text, making it possible to detect and verify its origin. These watermarks are meant to be difficult to remove without significantly degrading the text quality. Our research demonstrates that current watermarking methods have a critical vulnerability: they are only tested against attackers who don't consider how the watermarking method works. Utilizing the knowledge of the watermarking algorithms, we developed a method to create optimized text rewriters using small publicly available models. Surprisingly, we found that all current watermarking systems can be evaded with over 96% success rate, even with limited computing resources (costing less than $10). Even more concerning, our attack methods worked effectively against watermarking systems they weren't specifically designed for. These findings demonstrate the urgent need for more robust watermarking techniques, ensuring that LLM-generated content remains reliably traceable."
Poster,Optimizing Language Models for Inference Time Objectives using Reinforcement Learning,https://ICML.cc//virtual/2025/poster/44851,"Yunhao Tang, Kunhao Zheng, Gabriel Synnaeve, REMI MUNOS","In this work, we investigate the merits of explicitly optimizing for inference time algorithmic performance during model training. We show how optimizing for inference time performance can improve overall model efficacy. We consider generic inference time objectives with $k$ samples, with focus on pass@$k$ and majority voting as two main applications. With language model training on reasoning datasets, we showcase the performance trade-off enabled by training with such objectives. When training on code generation tasks, we show that the approach significantly improves pass@$k$ objectives compared to the baseline method.",Learning to do better inference time computation with Reinforcement Learning
Poster,Optimizing Large Language Model Training Using FP4 Quantization,https://ICML.cc//virtual/2025/poster/43733,"Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zheng-Jun Zha, Peng CHENG","The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training.","Training today’s powerful AI language models takes enormous computing power, money, and time. As models get bigger, this challenge only grows. One way to make training more efficient is to use simpler numbers—fewer bits—to do the math. But using very small numbers, like 4-bit formats, often makes the models less accurate.To solve this, we created a new training method that lets models use 4-bit numbers without losing performance. We designed smart tricks to help the model handle tiny numbers better—by improving how it learns from data and by managing unusual spikes in values during training.Our tests show that models trained with this method perform almost as well as those trained with much more complex numbers. This means we can train powerful models faster, cheaper, and with less energy. As future hardware gets better at handling 4-bit math, this approach could help make advanced AI more accessible and sustainable."
