type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,"Position: Build Agent Advocates, Not Platform Agents",https://ICML.cc//virtual/2025/poster/40111,"Sayash Kapoor, Noam Kolt, Seth Lazar","Language model agents are poised to mediate how people navigate and act online. If the companies that already dominate internet search, communication, and commerce—or the firms trying to unseat them—control these agents, the resulting *platform agents* will likely deepen surveillance, tighten lock-in, and further entrench incumbents. To resist that trajectory, this position paper argues that we should promote *agent advocates*: user-controlled agents that safeguard individual autonomy and choice. Doing so demands three coordinated moves: broad public access to both compute and capable AI models that are not platform-owned, open interoperability and safety standards, and market regulation that prevents platforms from foreclosing competition.","Many companies are developing AI agents to browse the web and act on users' behalf. If existing platform companies control these assistants, they'll likely use them to reduce privacy and make it harder to switch to competitors. This paper proposes an alternative: ""agent advocates""—AI agents that users control and that work for users' benefit. To make this possible, we need to ensure public access to the computing power required to develop and run agents, standards that allow interoperability as well as safety, and market regulation that increases competition."
Poster,Position: Causal Machine Learning Requires Rigorous Synthetic Experiments for Broader Adoption,https://ICML.cc//virtual/2025/poster/40145,"Audrey Poinsot, Panayiotis Panayiotou, Alessandro Leite, Nicolas CHESNEAU, Özgür Şimşek, Marc Schoenauer","Causal machine learning has the potential to revolutionize decision-making by combining the predictive power of machine learning algorithms with the theory of causal inference. However, these methods remain underutilized by the broader machine learning community, in part because current empirical evaluations do not permit assessment of their reliability and robustness, undermining their practical utility. Specifically, one of the principal criticisms made by the community is the extensive use of synthetic experiments. We argue, on the contrary, that **synthetic experiments are essential and necessary to precisely assess and understand the capabilities of causal machine learning methods**. To substantiate our position, we critically review the current evaluation practices, spotlight their shortcomings, and propose a set of principles for conducting rigorous empirical analyses with synthetic data. Adopting the proposed principles will enable comprehensive evaluations that build trust in causal machine learning methods, driving their broader adoption and impactful real-world use.","Causal Machine Learning algorithms have the potential to revolutionize decision-making. However, they remain underutilized because the existing evaluation methodologies do not adequately assess their reliability and robustness.In this work, we argue that synthetic experiments are essential and necessary to precisely assess and understand the capabilities of causal machine learning methods. To explain our position, we critically review the current evaluation practices, highlight their weaknesses through concrete experiments, and propose a set of principles for conducting rigorous evaluations with synthetic data.Adopting our principles will enable rigorous evaluation, build trust, and drive broader real-world adoption of Causal ML methods."
Poster,Position: Certified Robustness Does Not (Yet) Imply Model Security,https://ICML.cc//virtual/2025/poster/40159,"Andrew C. Cullen, Paul MONTAGUE, Sarah Erfani, Benjamin Rubinstein","While certified robustness is widely promoted as a solution to adversarial examples in Artificial Intelligence systems, significant challenges remain before these techniques can be meaningfully deployed in real-world applications. We identify critical gaps in current research, including the paradox of detection without distinction, the lack of clear criteria for practitioners to evaluate certification schemes, and the potential security risks arising from users' expectations surrounding ``guaranteed"" robustness claims. This position paper is a call to arms for the certification research community, proposing concrete steps to address these fundamental challenges and advance the field toward practical applicability.","Certified Robustness is a form of guaranteed defense of AI and Machine Learning Models against manipulation. However, these models are a long way away from being able to be deployed to help protect real world systems. Complicating this is the way these systems are presented and named - as it is completely reasonable to think that being certifiably robust would mean that no additional security provisions would be required. This work presents a case for how AI security can be moved forward, by drawing more direct inspiration from the needs of real world users, and by creating more honest and task appropriate measures of success. In doing so, we hope to lay the foundations for the next era of developments in secure AI."
Poster,Position: Challenges and Future Directions of Data-Centric AI Alignment,https://ICML.cc//virtual/2025/poster/40126,"Min-Hsuan Yeh, Jeffrey Wang, Xuefeng Du, Seongheon Park, Leitian Tao, Shawn Im, Sharon Li","As AI systems become increasingly capable and influential, ensuring their alignment with human values, preferences, and goals has become a critical research focus. Current alignment methods primarily focus on designing algorithms and loss functions but often underestimate the crucial role of data. This paper advocates for a shift towards data-centric AI alignment, emphasizing the need to enhance the quality and representativeness of data used in aligning AI systems. In this position paper, we highlight key challenges associated with both human-based and AI-based feedback within the data-centric alignment framework. Through qualitative analysis, we identify multiple sources of unreliability in human feedback, as well as problems related to temporal drift, context dependence, and AI-based feedback failing to capture human values due to inherent model limitations. We propose future research directions, including improved feedback collection practices, robust data-cleaning methodologies, and rigorous feedback verification processes. We call for future research into these critical directions to ensure, addressing gaps that persist in understanding and improvingdata-centric alignment practices.","AI systems are playing an increasingly important role in our daily lives, from recommending what we watch to helping with medical decisions. But how do we make sure these systems truly reflect what people care about—our values, goals, and preferences?Most efforts to align AI with human values focus on how the algorithms are built. Our work argues that we’re missing a big part of the picture: the data used to train and guide these systems. If the feedback data—whether it comes from people or from other AI—is flawed, the AI may learn the wrong lessons.We explore how feedback can be inconsistent, biased, or unclear, and how these feedback can be outdated over time or miss important human perspectives. We call for better ways to collect, clean, and verify this feedback to make sure it truly represents what people want."
Poster,Position: Constants are Critical in Regret Bounds for Reinforcement Learning,https://ICML.cc//virtual/2025/poster/40179,"Simone Drago, Marco Mussi, Alberto Maria Metelli","Mainstream research in theoretical RL is currently focused on designing online learning algorithms with regret bounds that match the corresponding regret lower bound up to multiplicative constants (and, sometimes, logarithmic terms). In this position paper, we constructively question this trend, arguing that algorithms should be designed to at least minimize the amount of unnecessary exploration, and we highlight the significant role constants play in algorithms' actual performances. This trend also exacerbates the misalignment between theoretical researchers and practitioners. As an emblematic example, we consider the case of regret minimization in finite-horizon tabular MDPs. Starting from the well-known UCBVI algorithm, we improve the bonus terms and the corresponding regret analysis. Additionally, we compare our version of UCBVI with both its original version and the state-of-the-art MVP algorithm. Our empirical validation successfully demonstrates how improving the multiplicative constants has significant positive effects on the actual empirical performances of the algorithm under analysis. This raises the question of whether ignoring constants when assessing whether algorithms match is the proper approach.","One goal of Reinforcement Learning (RL) research is to devise algorithms that learn how to make optimal decisions using the concept of *regret* (i.e., how much is lost w.r.t. always making optimal decisions) as a performance metric.From a theoretical perspective, the focus is on matching the order of the theoretical limit of the regret, known as *lower bound*, up to constant (and sometimes logarithmic) multiplicative terms. This, however, can sometimes lead to ignoring the effects that such lower-order terms have on the practical performance of an algorithm.In this paper, we constructively question this approach, considering the tabular RL setting as a main case study. We compare the first algorithm to match the lower bound, UCBVI, its improvement in terms of lower-order terms, and the state-of-the-art algorithm, MVP, showing via an empirical validation the significant impact of lower-order terms on the performance of an algorithm.The goal of this position paper is to highlight theimportance of considering lower-order terms when transitioning algorithms from theoretical frameworksto experimental settings, aiming to reduce the gap between theoretical guarantees and real-world performance, and leading to a more integrated view within the RL community."
Poster,Position: Contextual Integrity is Inadequately Applied to Language Models,https://ICML.cc//virtual/2025/poster/40131,"Yan Shvartzshnaider, Vasisht Duddu","Machine learning community is discovering Contextual Integrity (CI) as a useful framework to assess the privacy implications of large language models (LLMs). This is an encouraging development.The CI theory emphasizes sharing information in accordance with *privacy norms* and can bridge the social, legal, political, and technical aspects essential for evaluating privacy in LLMs. However, this is also a good point to reflect on use of CI for LLMs. *This position paper argues that existing literature inadequately applies CI for LLMs without embracing the theory’s fundamental tenets.*Inadequate applications of CI could lead to incorrect conclusions and flawed privacy-preserving designs. We clarify the four fundamental tenets of CI theory, systematize prior work on whether they deviate from these tenets, and highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt sensitivity, positional bias).","The growing use of large language models (LLMs) to automate tasks across and within social contexts (e.g., workplaces, households, health, and education) raises questions about the privacy implications of these models and how best to evaluate them. The machine learning community is exploring the use of Contextual Integrity (CI) as a useful framework for assessing the privacy implications of LLMs. CI theory emphasizes sharing information in accordance with privacy norms and can help bridge the social, legal, political, and technical aspects essential to evaluating privacy in LLMs. However, this is also an opportunity to reflect on the use of CI for LLMs.Despite the apparent simplicity of the CI framework, its application is far from trivial.  A rote use of the framework—while potentially insightful—does not deepen our understanding of LLMs' privacy implications. To meaningfully operationalize CI theory, we must support its four essential tenets.This position paper argues that existing literature inadequately applies CI to LLMs by failing to fully embrace these core tenets. We clarify the four fundamental tenets of CI theory, systematize prior work to examine whether it deviates from these tenets, and highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt sensitivity and positional bias)."
Poster,Position: Current Model Licensing Practices are Dragging Us into a Quagmire of Legal Noncompliance,https://ICML.cc//virtual/2025/poster/40180,"Moming Duan, Mingzhe Du, Rui Zhao, Mengying Wang, Yinghui Wu, Nigel Shadbolt, Bingsheng He","The Machine Learning (ML) community has wit- nessed explosive growth, with millions of ML models being published on the Web. Reusing ML model components has been prevalent nowadays. Developers are often required to choose a license to publish and govern the use of their models. Popular options include Apache-2.0, OpenRAIL (Responsible AI Licenses), Creative Commons Licenses (CCs), Llama2, and GPL-3.0. Currently, no standard or widely accepted best practices ex- ist for model licensing. But does this lack of standardization lead to undesired consequences? Our answer is Yes. After reviewing the clauses of the most widely adopted licenses, we take the position that current model licensing practices are dragging us into a quagmire of legal noncom- pliance. To support this view, we explore the cur- rent practices in model licensing and highlight the differences between various model licenses. We then identify potential legal risks associated with these licenses and demonstrate these risks using examples from real-world repositories on Hug- ging Face. To foster a more standardized future for model licensing, we also propose a new draft of model licenses, ModelGo Licenses (MGLs), to address these challenges and promote better compliance. https://www.modelgo.li/","Machine learning (ML) is rapidly expanding, with millions of models published online and widely reused by developers. When sharing their models, developers must choose a license to define how others can use their work. Common licenses include Apache-2.0, Creative Commons, OpenRAIL, and GPL-3.0. However, there is no widely accepted standard for licensing ML models, which can cause confusion and legal risks.This paper examines the current state of ML model licensing, comparing popular licenses and identifying the legal uncertainties they create. We analyze real-world examples from Hugging Face to demonstrate how these risks appear in practice. Our findings suggest that current model licensing practices have already led to a quagmire of legal problems for developers and users alike, creating an urgent need for input and attention from the ML community.We also introduce ModelGo Licenses (MGLs), a new draft license framework designed to promote clearer, more consistent, and legally sound licensing for ML models, fostering a standard and more collaborative ML ecosystem."
Poster,Position: Deep Learning is Not So Mysterious or Different,https://ICML.cc//virtual/2025/poster/40178,Andrew Wilson,"Deep neural networks are often seen as different from other model classes by defying conventional notions of generalization. Popular examples of anomalous generalization behaviour include benign overfitting, double descent, and the success of overparametrization.This position paper argues that these phenomena are not distinct to neural networks, or particularly mysterious. Moreover, this generalization behaviour can be intuitively understood, and rigorously characterized, using long-standing generalization frameworks such as PAC-Bayes and countable hypothesis bounds. We present soft inductive biases as a key unifying principle in explaining these phenomena: rather than restricting the hypothesis space to avoid overfitting, embrace a flexible hypothesis space, with a soft preference for simpler solutions that are consistent with the data. This principle can be encoded in many model classes, and thus deep learning is not as mysterious or different from other model classes as it might seem. However, we also highlight how deep learning is relatively distinct in other ways, such as its ability for representation learning, phenomena such as mode connectivity, and its relative universality.","Machine learning means algorithmic learning by example. In order to learn by example, we need to make assumptions. These assumptions can be represented in terms of restricting the possible explanations for our observations. Alternatively, they can be represented as having soft preferences for certain types of explanations over others. In this paper, we show that the latter way of representing assumptions can be used to intuitively explain many phenomena that are considered mysterious in deep learning. These phenomena include the success of (1) having more parameters than datapoints, (2) predictive accuracy that increases, decreases, and then increases again with increases in model flexibility, and (3) the ability to fit both noise and signal, but still make accurate predictions on withheld data. We show that classical statistical models also exhibit these phenomena, which can be explained in the same way. Moreover, in contrast to many claims, this behaviour can be understood with rigorous theoretical frameworks that have existed for many decades, even when considering the solutions that neural networks reach, rather than analogies given by simple linear models that are easier to analyze. So deep learning is not so mysterious or different. However, we also consider other ways in which deep learning is genuinely different, including its relatively singular ability to work well in many different settings at once."
Poster,Position: Democratic AI is Possible. The Democracy Levels Framework Shows How It Might Work.,https://ICML.cc//virtual/2025/poster/40100,"Aviv Ovadya, Kyle Redman, Luke Thorburn, Quan Ze Chen, Oliver Smith, Flynn Devine, Andrew Konya, Smitha Milli, Manon Revel, Kevin Feng, Amy Zhang, Bilva Chandra, Michiel Bakker, Atoosa Kasirzadeh","This position paper argues that effectively ""democratizing AI"" requires democratic governance and alignment of AI, and that this is particularly valuable for decisions with systemic societal impacts. Initial steps—such as Meta's *Community Forums* and Anthropic's *Collective Constitutional AI*—have illustrated a promising direction, where democratic processes could be used to meaningfully improve public involvement and trust in critical decisions. To more concretely explore what increasingly democratic AI might look like, we provide a ""Democracy Levels"" framework and associated tools that: (i) define milestones toward meaningfully democratic AI—which is also crucial for substantively pluralistic, human-centered, participatory, and public-interest AI, (ii) can help guide organizations seeking to increase the legitimacy of their decisions on difficult AI governance and alignment questions, and (iii) support the evaluation of such efforts.","The power to make decisions about AI is becoming increasingly important as AI systems become more ubiquitous and capable. This has led to talk about ""democratizing"" AI, but that often just means making AI accessible. We argue that it is also important to have democratic decision-making about how AI is built and used.To make this more concrete, we introduce the Democracy Levels Framework, which defines milestones (L0-L5) for transferring decision-making power to democratic systems. Building on this framework, we provide tools for helping identify when it makes sense to make decisions more democratic, and to evaluate the quality of a democratic system. We also address concerns about why this matters, and how to make it feasible. Ultimately, truly democratic AI could prevent unprecedented power concentration as AI capabilities increase."
Poster,Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints,https://ICML.cc//virtual/2025/poster/40132,"Sam Bowyer, Laurence Aitchison, Desi Ivanova","Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios.","AI researchers often evaluate large language models (LLMs) on test benchmarks -- collections of questions from a given subject area, such as maths, coding or general knowledge. The proportion of correct answers an LLM achieves on a benchmark gives us a rough idea of how capable that LLM is at a given subject. However, LLMs don't always give the same answer every time you give them the same question, so researchers calculate 'confidence intervals' which tell us that if we were to run the benchmark many times, then 95% of the time, for instance, we'd expect the LLM's score to fall between, say, 0.82 and 0.88.The problem is that there are many ways to construct these intervals, and one of the most common methods (based on an important result from statistics called the 'central limit theorem' (CLT)) relies on assumptions that aren't always satisfied by these benchmarks. Specifically, we show that CLT-based intervals underperform (give inaccurate confidence intervals) when the benchmark contains few questions (which is an increasingly common scenario). We suggest simple, alternative ways to construct these intervals in a range of settings where CLT-based methods fail."
