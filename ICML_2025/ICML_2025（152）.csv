type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Iterative Vectors: In-Context Gradient Steering without Backpropagation,https://ICML.cc//virtual/2025/poster/46614,"Yiting Liu, Zhi-Hong Deng","In-context learning has become a standard approach for utilizing language models.However, selecting and processing suitable demonstration examples can be challenging and time-consuming, especially when dealing with large numbers of them.We propose Iterative Vectors (IVs), a technique that explores activation space to enhance in-context performance by simulating gradient updates during inference.IVs extract and iteratively refine activation-based meta-gradients, applying them during inference without requiring backpropagation at any stage.We evaluate IVs across various tasks using four popular models and observe significant improvements.Our findings suggest that in-context activation steering is a promising direction, opening new avenues for future research.","Large language models (LLMs) are powerful, often adapted for specific tasks using ""In-Context Learning"" (ICL) by showing examples in the prompt. However, ICL struggles with choosing examples, prompt length, and consistency.Our research presents Iterative Vectors (IVs) as a new approach. IVs capture hidden, task-specific internal adjustments that LLMs learn from examples. The key is an iterative process: we extract and refine these internal adjustments by processing examples in batches, making the IVs more stable and effective.Once refined, these IVs can ""steer"" the model's internal state to perform a task. This happens without the examples used to produce the IVs or requiring expensive retraining.Tests show IVs significantly improve performance compared to standard ICL, while also being more efficient and reliable. This work offers a promising new way to adapt LLMs by directly leveraging and manipulating their internal task representations."
Poster,ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset,https://ICML.cc//virtual/2025/poster/45847,"Yilin Wang, Peixuan Lei, Jie Song, Haoyuzhe, chen tao, Yuxuan Zhang, LEI JIA, Yuanxiang Li, Zhongyu Wei","Time-series data are critical in diverse applications, such as industrial monitoring, medical diagnostics, and climate research. However, effectively integrating these high-dimensional temporal signals with natural language for dynamic, interactive tasks remains a significant challenge. To address this, we introduce the Time-Series Question Answering (Time-Series QA) task and release EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset designed to capture complex interactions between time-series signals and natural language. Building on this resource, we propose the Instruct Time Transformer (ITFormer), a novel framework that bridges time-series encoders with frozen large language models (LLMs). ITFormer effectively extracts, aligns, and fuses temporal and textual features, achieving a strong improvement in QA accuracy over strong baselines with fewer than 1\% additional trainable parameters. By combining computational efficiency with robust cross-modal modeling, our work establishes a adaptable paradigm for integrating temporal data with natural language, paving the way for new research and applications in multi-modal AI. More details about the project, including datasets and code, are available at: https://pandalin98.github.io/itformer_site/.","Many systems we rely on—such as aircraft engines or medical monitors—generate large amounts of sensor data over time. Interpreting this data quickly and accurately is crucial for safety, maintenance, and decision-making. However, it's difficult for humans to make sense of these complex signals directly, and even today’s most advanced AI models struggle to answer questions about them.Our work introduces a new approach that helps AI models understand and respond to natural language questions about time-series data. For example, a mechanic might ask, ""Does this engine signal show signs of failure?"" or ""What actions should I take based on recent data?"" To make this possible, we created a new dataset based on real aircraft engine data, containing over 110,000 question-answer examples. This dataset captures different types of questions, such as understanding patterns, diagnosing faults, predicting risks, and making operational decisions.We also developed a new AI system called ITFormer, which connects time-series data with large language models like ChatGPT. ITFormer learns how to explain sensor data using natural language while using very few extra parameters. It outperforms existing AI models in both accuracy and speed and works well even with limited computing resources.In short, this research makes time-series data more understandable and actionable by humans and machines, with applications in aerospace, healthcare, energy, and beyond."
Poster,"I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",https://ICML.cc//virtual/2025/poster/46563,"Zhenxing Mi, Kuan-Chieh Wang, Guocheng Qian, Hanrong Ye, Runtao Liu, Sergey Tulyakov, Kfir Aberman, Dan Xu","This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the **LLM decoder** shares the same input feature space with **diffusion decoders** that use the corresponding **LLM encoder** for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff.","Can we train a unified model that excels at both multimodal understanding and in-context image generation? Common diffusion models are good at generating images strictly following text or image prompts. However, they often fall short when it comes to understanding and reasoning about complex input contexts, especially when combining multiple images and texts. Our work, ThinkDiff, introduces a new approach that empowers image generation models with multimodal in-context understanding and reasoning. Instead of relying on scarce and complex reasoning datasets to train diffusion models, we align vision-language models (VLMs) with the decoder of a large language model (LLM), which shares a common feature space with diffusion decoders. This proxy task allows us to transfer reasoning ability from the VLMs to the diffusion models without direct diffusion training.With only 5 hours of training on 4 A100 GPUs,  ThinkDiff effectively unifies the understanding, reasoning, and generation capabilities in one model."
Poster,It's My Data Too: Private ML for Datasets with Multi-User Training Examples,https://ICML.cc//virtual/2025/poster/46273,"Arun Ganesh, Ryan McKenna, Hugh B McMahan, Adam Smith, Fan Wu","We initiate a study of algorithms for model training with user-level differential privacy (DP), where each example may be attributed to multiple users, which we call the multi-attribution model. We first provide a carefully chosen definition of user-level DP under the multi-attribution model. Training in the multi-attribution model is facilitated by solving the contribution bounding problem, i.e. the problem of selecting a subset of the dataset for which each user is associated with a limited number of examples. We propose a greedy baseline algorithm for the contribution bounding problem. We then empirically study this algorithm for a synthetic logistic regression task and a transformer training task, including studying variants of this baseline algorithm that optimize the subset chosen using different techniques and criteria. We find that the baseline algorithm remains competitive with its variants in most settings, and build a better understanding of the practical importance of a bias-variance tradeoff inherent in solutions to the contribution bounding problem.","When we train models with privacy guarantees, we usually assume each piece of data we train on only has privacy implications for a single person. However, in many settings a piece of data could have privacy implications for multiple people. For example, a text or email message could contain privacy-sensitive information about both the sender and recipients, or a photo may have multiple people's faces. We introduce a privacy guarantee that accommodates having multiple people's privacy associated with each piece of data, and introduce a framework for training models with this privacy guarantee and demonstrate our framework's effectiveness."
Poster,"Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations",https://ICML.cc//virtual/2025/poster/45175,"Lucy Farnik, Tim Lawson, Conor Houghton, Laurence Aitchison","Sparse autoencoders (SAEs) have been successfully used to discover sparse and human-interpretable representations of the latent activations of language models (LLMs). However, we would ultimately like to understand the computations performed by LLMs and not just their representations. The extent to which SAEs can help us understand computations is unclear because they are not designed to “sparsify” computations in any sense, only latent activations. To solve this, we propose Jacobian sparse autoencoders (JSAEs), which yield not only sparsity in the input and output activations of a given model component but also sparsity in the computation (formally, the Jacobian) connecting them. With a naïve implementation, the Jacobians in LLMs would be computationally intractable due to their size. Our key technical contribution is thus finding an efficient way of computing Jacobians in this setup. We find that JSAEs extract a relatively large degree of computational sparsity while preserving downstream LLM performance approximately as well as traditional SAEs. We also show that JSAEs achieve a greater degree of computational sparsity on pre-trained LLMs than on the equivalent randomized LLM. This shows that the sparsity of the computational graph appears to be a property that LLMs learn through training, and suggests that JSAEs might be more suitable for understanding learned transformer computations than standard SAEs.","Modern language models are powerful but their internal “wiring” is so densely connected that humans struggle to see how one idea influences another. Existing sparse auto-encoders—tools that compress the model’s signals into simpler pieces—tidy up those signals, yet the hidden wiring itself remains tangled. We introduce Jacobian Sparse Autoencoders (JSAEs), a new method that encourages most of the hidden connections to fade out, spotlighting only the truly important ones. The trick is to make the Jacobian—the mathematical map linking each input feature to each output feature—mostly zeros, and we show how to compute this map efficiently even inside very large models. On several GPT-2-style networks, JSAEs significantly cut the number of active connections while keeping language performance virtually unchanged. When we applied the same procedure to a randomly re-initialised copy of each model, the wiring stayed dense, suggesting that sparsity is something real models learn, not a fluke of our method. Because the surviving connections form a small, readable circuit, researchers can more easily trace cause and effect inside the model. This clarity is a step toward safer, more transparent AI systems and provides a new foundation for fine-grained audits of model behaviour."
Poster,Janus: Dual-Server Multi-Round Secure Aggregation with Verifiability for Federated Learning,https://ICML.cc//virtual/2025/poster/45774,"Lang Pu, Jingjing Gu, Chao Lin, Xinyi Huang","Secure Aggregation (SA) is a cornerstone of Federated Learning (FL), ensuring that user updates remain hidden from servers. The advanced Flamingo (S\&P'23) has realized multi-round aggregation and improved efficiency. However, it still faces several key challenges: scalability issues with dynamic user participation, a lack of verifiability for server-side aggregation results, and vulnerability to Model Inconsistency Attacks (MIA) caused by a malicious server distributing inconsistent models. To address these issues, we propose $\textit{Janus}$, a generic SA scheme based on dual-server architecture. Janus ensures security against up to $n-2$ colluding clients (where $n$ is the total client count), which prevents privacy breaches for non-colluders. Additionally, Janus is model-independent, ensuring applicability across any FL model without specific adaptations. Furthermore, Janus introduces a new cryptographic primitive, Separable Homomorphic Commitment, which enables clients to efficiently verify the correctness of aggregation. Finally, extensive experiments show that Janus not only significantly enhances security but also reduces per-client communication and computation overhead from logarithmic to constant scale, with a tolerable impact on model performance.","Federated Learning (FL) enables collaborative model training without sharing raw data. To keep individual updates private, Secure Aggregation (SA) combines updates in a way that hides each user's contribution. However, existing SA schemes struggle with user dropout, are prone to attacks causing model inconsistency, and lack verifiability.We propose Janus, a new SA method that overcomes these challenges through several key innovations. First, it introduces a dual-server architecture that splits responsibilities, improving both security and flexibility. Second, it uses a novel cryptographic tool called Separable Homomorphic Commitment, enabling users to efficiently verify aggregation correctness. Third, Janus supports model-independent use and scales efficiently, even with changing user participation.Our theoretical analysis and experimental results demonstrate that Janus advances secure federated learning with strong privacy, low overhead, and robust performance across diverse settings."
Poster,Joint Learning of Energy-based Models and their Partition Function,https://ICML.cc//virtual/2025/poster/43730,"Michael Sander, Vincent Roulet, Tianlin Liu, Mathieu Blondel","Energy-based models (EBMs) offer a flexible framework for parameterizing probability distributions using neural networks.However, learning EBMs by exact maximum likelihood estimation (MLE) is generally intractable, due to the need to compute the partition function.In this paper, we propose a novel min-min formulation for approximately learning probabilistic EBMs in combinatorially-large discrete spaces, such as sets or permutations. Our key idea is to jointly learn both an energy model and its log-partition, parameterized as a neural network. Our approach not only provides a novel tractable objective criterion to learn EBMs by stochastic gradient descent (without relying on MCMC), but also a novel means to estimate the log-partition function on unseen data points.On the theoretical side, we show that our approach recovers the optimal MLE solution when optimizing in the space of continuous functions.Furthermore, we show that our approach naturally extends to the broader family of Fenchel-Young losses, allowing us to obtainthe first tractable method for optimizing the sparsemax loss in combinatorially-large spaces.We demonstrate our approach on multilabel classification and label ranking.",We propose a new method to learn probabilistic energy-based models.
Poster,Joint Localization and Activation Editing for Low-Resource Fine-Tuning,https://ICML.cc//virtual/2025/poster/45572,"Wen Lai, Alexander Fraser, Ivan Titov","Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing (or steering) techniques, which modify the activations of specific model components. Due to their extremely small parameter counts, these methods show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods.","(1) We introduce JoLA, a novel activation editing approach that jointly optimizes the selection of intervention components and the intervention strategy, specifically tailored for low-resource scenarios. (2) We demonstrate that JoLA achieves stable and consistent performance across diverse tasks, addressing key limitations of existing methods. We further validate its effectiveness across different data scales and model sizes. (3) We provide new insights into the role of attention heads in activation editing, showing that they are the most impactful components for fine-tuning."
Poster,Joint Metric Space Embedding by Unbalanced Optimal Transport with Gromov–Wasserstein Marginal Penalization,https://ICML.cc//virtual/2025/poster/46678,"Florian Beier, Moritz Piening, Robert Beinert, Gabriele Steidl","We propose a new approach for unsupervised alignment of heterogeneous datasets, which maps data from two different domains without any known correspondences to a common metric space. Our method is based on an unbalanced optimal transport problem with Gromov-Wasserstein marginal penalization. It can be seen as a counterpart to the recently introduced joint multidimensional scaling method. We prove that there exists a minimizer of our functional and that for penalization parameters going to infinity, the corresponding sequence of minimizers converges to a minimizer of the so-called embedded Wasserstein distance. Our model can be reformulated as a quadratic, multi-marginal, unbalanced optimal transport problem, for which a bi-convex relaxation admits a numerical solver via block-coordinate descent. We provide numerical examples for joint embeddings in Euclidean as well as non-Euclidean spaces.","We propose a method for aligning two different datasets without clear connections in a shared representation space. For this, we used an optimal transport approach that finds the best way to match the data. In particular, we rely on the Gromov-Wasserstein distance for data geometry preservation and the classical Wasserstein distance for alignment.Our research provides a way to visualize and compare heterogeneous data, enhancing data analysis."
Poster,Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient,https://ICML.cc//virtual/2025/poster/46349,"Jan Ludziejewski, Maciej Pióro, Jakub Krajewski, Maciej Stefaniak, Michał Krutul, Jan Małaśnicki, Marek Cygan, Piotr Sankowski, Kamil Adamczewski, Piotr Milos, Sebastian Jaszczur","Mixture of Experts (MoE) architectures have significantly increased computational efficiency in both research and real-world applications of large-scale machine learning models. However, their scalability and efficiency under memory constraints remain relatively underexplored. In this work, we present joint scaling laws for dense and MoE models, incorporating key factors such as the number of active parameters, dataset size, and the number of experts. Our findings provide a principled framework for selecting the optimal MoE configuration under fixed memory and compute budgets. Surprisingly, we show that MoE models can be more memory-efficient than dense models, contradicting conventional wisdom. Extensive empirical validation confirms the theoretical predictions of our scaling laws. These results offer actionable insights for designing and deploying MoE models in practical large-scale training scenarios.","Mixture of Experts (MoE) models are LLMs that consist of multiple smaller models, but use only some of them each time they are used. It makes these models computationally efficient. However, their memory usage is less understood. This paper introduces an equation, showing how MoE models can, surprisingly, be more memory-efficient than traditional models. Extensive experiments confirm these findings, offering clear guidelines for choosing the best MoE configurations within practical memory and compute limits."
