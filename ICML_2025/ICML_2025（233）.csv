type,name,virtualsite_url,speakers/authors,abstract,lay_summary
Poster,Positive-unlabeled AUC Maximization under Covariate Shift,https://ICML.cc//virtual/2025/poster/45782,"Atsutoshi Kumagai, Tomoharu Iwata, Hiroshi Takahashi, Taishi Nishiyama, Kazuki Adachi, Yasuhiro Fujiwara","Maximizing the area under the receiver operating characteristic curve (AUC) is a standard approach to imbalanced binary classification tasks. Existing AUC maximization methods typically assume that training and test distributions are identical. However, this assumption is often violated due to {\it a covariate shift}, where the input distribution can vary but the conditional distribution of the class label given the input remains unchanged. The importance weighting is a common approach to the covariate shift, which minimizes the test risk with importance-weighted training data. However, it cannot maximize the AUC. In this paper, to achieve this, we theoretically derive two estimators of the test AUC risk under the covariate shift by using positive and unlabeled (PU) data in the training distribution and unlabeled data in the test distribution. Our first estimator is calculated from importance-weighted PU data in the training distribution, and the second one is calculated from importance-weighted positive data in the training distribution and unlabeled data in the test distribution. We train classifiers by minimizing a weighted sum of the two AUC risk estimators that approximates the test AUC risk. Unlike the existing importance weighting, our method does not require negative labels and class-priors. We show the effectiveness of our method with six real-world datasets.","Various practical applications, such as malware detection, medical diagnosis, and fault detection, are often formulated as binary classification problems with class-imbalance data in machine learning. AUC maximization is a standard approach for learning accurate classifiers from such imbalanced data. However, in real-world scenarios, data tendencies (i.e., data distributions) often differ between training and testing phases, leading to performance degradation when using classifiers trained with standard methods. In this study, we focus on the situation where the input distribution changes—known as covariate shift—and show that it is possible to maximize the test AUC using only positive and unlabeled data in the training phase and unlabeled data in the test phase. This implies that one can learn appropriate classifiers for imbalanced data while reducing the annotation cost for expensive samples. Our findings are expected to contribute to the development of more practical detection and diagnostic systems."
Poster,Posterior Inference with Diffusion Models for High-dimensional Black-box Optimization,https://ICML.cc//virtual/2025/poster/45940,"Taeyoung Yun, Kiyoung Om, Jaewoo Lee, Sujin Yun, Jinkyoo Park","Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications.While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations. Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework.However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces.To overcome these issues, we introduce \textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems.Our method iterates two stages. First, we train a diffusion model to capture the data distribution and deep ensembles to predict function values with uncertainty quantification.Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces. Concretely, we fine-tune diffusion models to amortize posterior inference.Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across synthetic and real-world tasks. Our code is publicly available \href{https://github.com/umkiyoung/DiBO}{here}.","Many real-world scientific problems—like designing new materials or tuning AI systems—require finding the best solution from a vast space of possibilities. But evaluating each possibility is expensive, and traditional methods like Bayesian optimization struggle when the number of variables gets too high.To tackle this, researchers have recently turned to generative models, which can learn patterns from past attempts and propose smarter guesses. However, these models often fail in complex scenarios due to poor uncertainty estimates and limited flexibility.Our research introduces DiBO, a new method that combines the strengths of generative models and uncertainty-aware prediction to solve these hard problems. DiBO trains a diffusion model to understand the search space, and a deep ensemble to estimate how good each solution might be.We then refine the generative model to focus on the most promising areas, effectively balancing between exploring new ideas and improving known ones.Through extensive experiments, we show that DiBO consistently finds better solutions than existing methods, even in very high-dimensional settings. This makes it a powerful tool for scientific discovery, engineering, and beyond."
Poster,Potemkin Understanding in Large Language Models,https://ICML.cc//virtual/2025/poster/44050,"Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan","Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs---such as AP exams---are also those used to test people. However, this raises an implication: such benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates **potemkin understanding:** the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.","Large language models (LLMs), like ChatGPT, are typically assessed using benchmarks—standardized tests similar to those used for humans. Our research reveals an implicit assumption in this approach: that LLMs who score well possess the same capabilities as humans who do. If this isn't true, good benchmark performance reflects what we term “potemkin understanding,” where models correctly answer benchmark questions but fail simpler tasks any human with true conceptual understanding would handle easily.We developed two methods to detect potemkin understanding. The first method involves creating a dataset of potemkins in three areas: literary techniques, game theory, and psychological biases. The second method automatically identifies potemkins without needing human-labeled data. Applying these methods to various models, we found potemkin understanding to be widespread.Identifying potemkin understanding challenges the validity of current AI evaluations, helping distinguish superficial correct answers from true conceptual understanding and guiding the development of more reliable, intelligent models."
Poster,Power Mean Estimation in Stochastic Continuous Monte-Carlo Tree Search,https://ICML.cc//virtual/2025/poster/45596,Tuan Dam,"Monte-Carlo Tree Search (MCTS) has demonstrated success in online planning for deterministic environments, yet significant challenges remain in adapting it to stochastic Markov Decision Processes (MDPs), particularly in continuous state-action spaces. Existing methods, such as HOOT, which combines MCTS with the Hierarchical Optimistic Optimization (HOO) bandit strategy, address continuous spaces but rely on a logarithmic exploration bonus that lacks theoretical guarantees in non-stationary, stochastic settings. Recent advancements, such as Poly-HOOT, introduced a polynomial bonus term to achieve convergence in deterministic MDPs, though a similar theory for stochastic MDPs remains undeveloped. In this paper, we propose a novel MCTS algorithm, Stochastic-Power-HOOT, designed for continuous, stochastic MDPs. Stochastic-Power-HOOT integrates a power mean as a value backup operator, alongside a polynomial exploration bonus to address the non-stationarity inherent in continuous action spaces. Our theoretical analysis establishes that Stochastic-Power-HOOT converges at a polynomial rate of $\mathcal{O}(n^{-1/2})$, where \( n \) is the number of visited trajectories, thereby extending the non-asymptotic convergence guarantees of Poly-HOOT to stochastic environments. Experimental results on synthetic and stochastic tasks validate our theoretical findings, demonstrating the effectiveness of Stochastic-Power-HOOT in continuous, stochastic domains.","Imagine you're playing a complex video game where you need to make decisions continuously (like steering a car smoothly rather than just turning left or right) and the game world is unpredictable (sometimes the same action leads to different outcomes). Traditional artificial intelligence planning methods struggle in such scenarios because they were designed for simpler, more predictable environments.Our research tackles this challenge by developing a new AI planning algorithm called Stochastic-Power-HOOT. Think of it as a smarter way for computers to ""think ahead"" when making decisions in complex, uncertain environments. The key innovation is using a mathematical technique called ""power mean"" - imagine it as a sophisticated averaging method that can be tuned to be more optimistic or conservative depending on the situation, combined with a systematic way to explore the vast space of possible actions.The traditional approach is like trying to navigate a city by only considering a few predetermined routes. Our method is more like having a GPS that can dynamically explore the entire road network while learning which paths are most promising. We prove mathematically that our algorithm will eventually find near-optimal solutions, and importantly, we show it works even when the environment is unpredictable.We tested our approach on robotic control tasks, from simple balance problems to complex humanoid robots with hundreds of moving parts. The results show that our method consistently outperforms existing approaches, especially in noisy, uncertain environments. This advance could lead to better autonomous vehicles, more capable robots, and AI systems that can handle real-world complexity more effectively.The broader impact is significant: as AI systems increasingly operate in unpredictable real-world environments - from self-driving cars navigating busy streets to robots working alongside humans - having reliable planning algorithms that can handle uncertainty is crucial for both performance and safety."
Poster,PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design,https://ICML.cc//virtual/2025/poster/44475,"Zhenqiao Song, Tianxiao Li, Lei Li, Martin Min","Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiff builds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, $k$-nearest neighbor ($k$NN) equivariant graph convolutional layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBench and finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiff consistently surpasses baseline methods, achieving success rates of 50.00\%, 23.16\%, and 16.89\% for the pretraining task and the two downstream applications, respectively.","How can we automatically design high-affinity protein binders for arbitrary protein targets? We present PPDiff, a novel generative framework based on diffusion models, for the design of protein-binding proteins with high affinity.PPDiff operates in a hybrid sequence–structure space, enabling the simultaneous generation of both binder sequences and their corresponding backbone structures for a given protein target. This joint modeling approach allows PPDiff to effectively capture the complex interplay between sequence, structure, and binding specificity in protein–protein interactions.To support research in this area, we create PPBench, a curated dataset of protein–protein complexes designed for benchmarking binder design tasks. PPDiff achieves high success rates on PPBench, as well as two additional challenging tasks: target protein–mini binder complex design and antigen–antibody complex design. Furthermore, our model demonstrates strong generalization ability, producing diverse and novel binders with high affinities across a broad range of protein targets."
Poster,Preconditioned Riemannian Gradient Descent Algorithm for Low-Multilinear-Rank Tensor Completion,https://ICML.cc//virtual/2025/poster/44012,"Yuanwei Zhang, Fengmiao Bian, Xiaoqun Zhang, Jian-Feng Cai","Tensors play a crucial role in numerous scientific and engineering fields. This paper addresses the low-multilinear-rank tensor completion problem, a fundamental task in tensor-related applications. By exploiting the manifold structure inherent to the fixed-multilinear-rank tensor set, we introduce a simple yet highly effective preconditioned Riemannian metric and propose the Preconditioned Riemannian Gradient Descent (PRGD) algorithm. Compared to the standard Riemannian Gradient Descent (RGD), PRGD achieves faster convergence while maintaining the same order of per-iteration computational complexity. Theoretically, we provide the recovery guarantee for PRGD under near-optimal sampling complexity. Numerical results highlight the efficiency of PRGD, outperforming state-of-the-art methods on both synthetic data and real-world video inpainting tasks.","**Problems:** * Many real-world tasks - from Netflix recommendations to restoring damaged photos and vedios - rely on reconstructing missing data from partial observations, a task called low-multilinear-rank tensor completion. * While people can treat this as optimization on curved surfaces (smooth manifold of low-multilinear-rank tensor set), existing methods use an oversimplified ""ruler"" (Riemannian metric) that ignores the data's natural geometry.**Solutions:** * We design a ""smarter ruler"", that automatically adapts to the lanscape of the optimization problem. * Using this, we propose the ""Preconditioned Riemannian Gradient Descent"" (PRGD) algorithm. Think of it like giving a mountain climber a topographic map instead of assuming all slopes are equally steep.**Impact:** * **Speed:** PRGD converges over 10× faster than standard methods (like RGD) with marginal extra computational cost per step.* **Theory:** We prove PRGD recovers missing data reliably, even with very few observed entries (near-optimal sampling complexity).* **Applications:** From video and image restorations, PRGD improves real-world performance while maintaining mathematical guarantees."
Poster,Predicting High-precision Depth on Low-Precision Devices Using 2D Hilbert Curves,https://ICML.cc//virtual/2025/poster/44496,"Mykhailo Uss, Ruslan Yermolenko, Oleksii Shashko, Olena Kolodiazhna, Ivan Safonov, Volodymyr Savin, Yoonjae Yeo, Seowon Ji, Jaeyun Jeong","Dense depth prediction deep neural networks (DNN) have achieved impressive results for both monocular and binocular data but they are limited by high computational complexity, restricting their use on low-end devices. For better on-device efficiency and hardware utilization, weights and activations of the DNN should be converted to low-bit precision. However, this precision is not sufficient for representing high dynamic range depth. In this paper, we aim to overcome this limitation and restore high-precision depth from low-bit precision predictions. To achieve this, we propose to represent high dynamic range depth as two low dynamic range components of a Hilbert curve, and to train the full precision DNN to directly predict the latter. For on-device deployment, we use standard quantization methods and add a post-processing step that reconstructs depth from the Hilbert curve components predicted in low-bit precision. Extensive experiments demonstrate that our method increases bit precision of predicted depth by up to three bits with little computational overhead. We also observe a positive side effect of quantization error reduction by up to five times. Our method enables effective and accurate depth prediction with DNN weights and activations quantized to eight bit precision.","We use depth prediction to help machines understand and interact with their surroundings in such areas as augmented reality, robotics, and autonomous vehicles. These applications often rely on devices with limited computing power. Deep neural networks (DNNs) are powerful tools for depth prediction but require significant processing resources, which low-end devices struggle to provide. To address this, DNN’s are converted to a lower precision, reducing its demands on hardware. Unfortunately, this conversion often compromises the accuracy of depth predictions.Our task was to overcome this limitation and restore high-precision depth from low-precision predictions. We took a novel approach: breaking down complex depth information into simpler parts using a transformation based on special Hilbert curves. By training the DNN to predict these simpler parts, we could later predict them on device in low precision an reconstruct the full depth with higher precision. Extensive experiments demonstrated that our method increases the quality of details in depth predicted on device to a level comparable with the original full precision model. This innovation makes it possible for DNNs to deliver accurate depth predictions even when their calculations are simplified for low-end devices, paving the way for more accessible and effective technology in everyday applications."
Poster,Predicting mutational effects on protein binding from folding energy,https://ICML.cc//virtual/2025/poster/45926,"Arthur Deng, Karsten Householder, Fang Wu, K. Garcia, Brian Trippe","Accurate estimation of mutational effects on protein-protein binding energies is an open problem with applications in structural biology and therapeutic design. Several deep learning predictors for this task have been proposed but, presumably due to the scarcity of binding data, these methods under-perform computationally expensive estimates based on empirical force-fields. In response, we propose a transfer-learning approach that leverages advances in protein sequence modeling and folding stability prediction for this task. The key idea is to parameterize the binding energy as the difference between the folding energy of the protein complex and the sum of the folding energies of its binding partners. We show that using a pre-trained inverse-folding model as a proxy for folding energy provides strong zero-shot performance, and can be fine-tuned with (1) copious folding energy measurements and (2) more limited binding energy measurements.The resulting predictor, StaB-ddG, is the first deep learning predictor to match the accuracy of the state-of-the-art empirical force-field method Flex ddG, while offering an over 10,000x speed-up.","Understanding how changes (mutations) in proteins affect the way they stick to each other is important for developing new medicines and studying biology. Today’s best computer methods for predicting these effects are slow and need a lot of computing power. We developed a new approach using machine learning that learns from many existing protein examples. Our key insight is a way to combine different sources of existing data. Our method is much faster than previous tools and just as accurate. This means scientists can now quickly predict how mutations might affect protein interactions, which could help speed up drug discovery and other research."
Poster,Predicting the Susceptibility of Examples to Catastrophic Forgetting,https://ICML.cc//virtual/2025/poster/43833,"Guy Hacohen, Tinne Tuytelaars","Catastrophic forgetting -- the tendency of neural networks to forget previously learned data when learning new information -- remains a central challenge in continual learning. In this work, we adopt a behavioral approach, observing a connection between learning speed and forgetting: examples learned more quickly are less prone to forgetting. Focusing on replay-based continual learning, we show that the composition of the replay buffer -- specifically, whether it contains quickly or slowly learned examples -- has a significant effect on forgetting. Motivated by this insight, we introduce Speed-Based Sampling (SBS), a simple yet general strategy that selects replay examples based on their learning speed. SBS integrates easily into existing buffer-based methods and improves performance across a wide range of competitive continual learning benchmarks, advancing state-of-the-art results. Our findings underscore the value of accounting for the forgetting dynamics when designing continual learning algorithms.","When teaching neural networks to perform a series of tasks, they often forget how to do the earlier ones after learning the new ones. This problem, called catastrophic forgetting, makes it hard to build AI systems that learn continuously, like humans do. In this study, we ask: Which examples are more likely to be forgotten? We find that examples the network learns quickly -- typically simpler ones  -- are remembered better, while harder, slower-to-learn examples are more likely to be forgotten. Using this insight, we develop a method that identifies which examples are at risk of being forgotten and helps the network focus more on them during training. Our findings deepen the understanding of catastrophic forgetting and offer a step toward building AI that can learn over time without losing past knowledge."
Poster,Prediction-Aware Learning in Multi-Agent Systems,https://ICML.cc//virtual/2025/poster/45828,"Aymeric Capitaine, Etienne Boursier, Eric Moulines, Michael Jordan, Alain Oliviero Durmus","The framework of uncoupled online learning in multiplayer games has made significant progress in recent years. In particular, the development of  time-varying games has considerably expanded its modeling capabilities. However, current regret bounds quickly become vacuous when the game undergoes significant variations over time, even when these variations are easy to predict. Intuitively, the ability of players to forecast future payoffs should lead to tighter guarantees, yet existing approaches fail to incorporate this aspect. This work aims to fill this gap by introducing a novel prediction-aware framework for time-varying games, where agents can forecast future payoffs and adapt their strategies accordingly. In this framework, payoffs depend on an underlying state of nature that agents predict in an online manner. To leverage these predictions, we propose the POMWU algorithm, a contextual extension of the optimistic Multiplicative Weight Update algorithm, for which we establish theoretical guarantees on social welfare and convergence to equilibrium. Our results demonstrate that, under bounded prediction errors, the proposed framework achieves performance comparable to the static setting. Finally, we empirically demonstrate the effectiveness of POMWU in a traffic routing experiment.","This research looks at how people (or computer programs) learn to make decisions in changing environments where they don’t know everything about what others are doing—like drivers choosing routes in busy traffic without knowing what roads others will take. Traditionally, these types of decision-making systems struggle when the situation changes a lot over time, even if those changes are predictable. This paper introduces a new approach that allows decision-makers to use their predictions about future changes to make better choices. We create a new method (called POWMU) that helps these decision-makers adjust their strategies based on what they expect will happen. We show that when predictions are fairly accurate, our method performs just as well as if the situation weren’t changing at all. We also test it with a traffic routing simulation and found it works well in practice."
