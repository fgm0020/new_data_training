type,name,virtualsite_url,speakers/authors,abstract
Poster,Streaming Algorithms For $\ell_p$ Flows and $\ell_p$ Regression,https://iclr.cc//virtual/2025/poster/30029,"Amit Chakrabarti, Jeffrey Jiang, David Woodruff, Taisuke Yasuda","We initiate the study of one-pass streaming algorithms for underdetermined $\ell_p$ linear regression problems of the form  $$      \min_{\mathbf A\mathbf x = \mathbf b} \lVert\mathbf x\rVert_p \,, \qquad       \text{where } \mathbf A \in \mathbb R^{n \times d} \text{ with } n \ll d \,,  $$  which generalizes basis pursuit ($p = 1$) and least squares solutions to  underdetermined linear systems ($p = 2$). We study the column-arrival  streaming model, in which the columns of $\mathbf A$ are presented one by one in a  stream. When $\mathbf A$ is the incidence matrix of a graph, this corresponds to an  edge insertion graph stream, and the regression problem captures $\ell_p$  flows which includes transshipment ($p = 1$), electrical flows ($p = 2$), and  max flow ($p = \infty$) on undirected graphs as special cases. Our goal is to  design algorithms which use space much less than the entire stream, which has  a length of $d$.  For the task of estimating the cost of the $\ell_p$ regression problem for  $p\in[2,\infty]$, we show a streaming algorithm which constructs a sparse  instance supported on $\tilde O(\varepsilon^{-2}n)$ columns of $\mathbf A$  which approximates the cost up to a $(1\pm\varepsilon)$ factor, which  corresponds to $\tilde O(\varepsilon^{-2}n^2)$ bits of space in general and  an $\tilde O(\varepsilon^{-2}n)$ space semi-streaming algorithm for  constructing $\ell_p$ flow sparsifiers on graphs. This extends to $p\in(1,  2)$ with $\tilde O(\varepsilon^{2}n^{q/2})$ columns, where $q$ is the H\""older  conjugate exponent of $p$. For $p = 2$, we show that $\Omega(n^2)$ bits of  space are required in general even for outputting a constant factor  solution. For $p = 1$, we show that the cost cannot be estimated even to an  $o(\sqrt n)$ factor in $\mathrm{poly}(n)$ space.  On the other hand, if we are interested in outputting a solution $\mathbf  x$, then we show that $(1+\varepsilon)$-approximations require $\Omega(d)$  space for $p > 1$, and in general, $\kappa$-approximations require  $\tilde\Omega(d/\kappa^{2q})$ space for $p > 1$. We complement these lower  bounds with the first sublinear space upper bounds for this problem, showing  that we can output a $\kappa$-approximation using space only  $\mathrm{poly}(n) \cdot \tilde O(d/\kappa^q)$ for $p > 1$, as well as a  $\sqrt n$-approximation using $\mathrm{poly}(n, \log d)$ space for $p = 1$."
Poster,Streaming Video Question-Answering with In-context Video KV-Cache Retrieval,https://iclr.cc//virtual/2025/poster/30750,"Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, ZhongTao, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, Hao Jiang","We propose ReKV, a novel training-free approach that enables efficient streaming video question-answering (StreamingVQA), by seamlessly integrating with existing Video Large Language Models (Video-LLMs). Traditional VideoQA systems struggle with long videos, as they must process entire videos before responding to queries, and repeat this process for each new question. In contrast, our approach analyzes long videos in a streaming manner, allowing for prompt responses as soon as user queries are received. Building on a common Video-LLM, we first incorporate a sliding-window attention mechanism, ensuring that input frames attend to a limited number of preceding frames, thereby reducing computational overhead. To prevent information loss, we store processed video key-value caches (KV-Caches) in RAM and disk, reloading them into GPU memory as needed. Additionally, we introduce a retrieval method that leverages an external retriever or the parameters within Video-LLMs to retrieve only query-relevant KV-Caches, ensuring both efficiency and accuracy in question answering. ReKV enables the separation of video analyzing and question-answering across different processes and GPUs, significantly enhancing the efficiency of StreamingVQA. Through comprehensive experimentation, we validate the efficacy and practicality of our approach, which significantly boosts efficiency and enhances applicability over existing VideoQA models."
Poster,Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge,https://iclr.cc//virtual/2025/poster/30097,"Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, Huchuan Lu","Recent advances in Large Language Models (LLMs) have enabled the development of Video-LLMs, advancing multimodal learning by bridging video data with language tasks. However, current video understanding models struggle with processing long video sequences, supporting multi-turn dialogues, and adapting to real-world dynamic scenarios. To address these issues, we propose StreamChat, a training-free framework for streaming video reasoning and conversational interaction. StreamChat leverages a novel hierarchical memory system to efficiently process and compress video features over extended sequences, enabling real-time, multi-turn dialogue. Our framework incorporates a parallel system scheduling strategy that enhances processing speed and reduces latency, ensuring robust performance in real-world applications. Furthermore, we introduce StreamBench, a versatile benchmark that evaluates streaming video understanding across diverse media types and interactive scenarios, including multi-turn interactions and complex reasoning tasks.  Extensive evaluations on StreamBench and other public benchmarks demonstrate that  StreamChat significantly outperforms existingstate-of-the-art models in terms of accuracy and response times, confirming its effectiveness for streaming video understanding. Code is available at StreamChat."
Poster,Streamlining Prediction in Bayesian Deep Learning,https://iclr.cc//virtual/2025/poster/28294,"Rui Li, Marcus Klasson, Arno Solin, Martin Trapp","The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration remaining the standard. In this work we examine streamlining prediction in BDL through a single forward pass without sampling. For this, we use local linearisation of activation functions and local Gaussian approximations at linear layers. Thus allowing us to analytically compute an approximation of the posterior predictive distribution. We showcase our approach for both MLP and transformers, such as ViT and GPT-2, and assess its performance on regression and classification tasks.Open-source library: https://github.com/AaltoML/SUQ."
Poster,Streamlining Redundant Layers to Compress Large Language Models,https://iclr.cc//virtual/2025/poster/30185,"Xiaodong Chen, Yuxuan Hu, Jing Zhang, Yanling Wang, Cuiping Li, Hong Chen","This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. LLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss. Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression. Experiments show that LLM-Streamline outperforms both previous and concurrent state-of-the-art pruning methods in terms of both performance and training efficiency. Our code is available at \href{https://github.com/RUCKBReasoning/LLM-Streamline}{this repository}."
Poster,Strength Estimation and Human-Like Strength Adjustment in Games,https://iclr.cc//virtual/2025/poster/30488,"Chun Jung Chen, Chung-Chin Shih, Ti-Rong Wu","Strength estimation and adjustment are crucial in designing human-AI interactions, particularly in games where AI surpasses human players. This paper introduces a novel strength system, including a *strength estimator* (SE) and an SE-based Monte Carlo tree search, denoted as *SE-MCTS*, which predicts strengths from games and offers different playing strengths with human styles. The strength estimator calculates strength scores and predicts ranks from games without direct human interaction. SE-MCTS utilizes the strength scores in a Monte Carlo tree search to adjust playing strength and style. We first conduct experiments in Go, a challenging board game with a wide range of ranks. Our strength estimator significantly achieves over 80% accuracy in predicting ranks by observing 15 games only, whereas the previous method reached 49% accuracy for 100 games. For strength adjustment, SE-MCTS successfully adjusts to designated ranks while achieving a 51.33% accuracy in aligning to human actions, outperforming a previous state-of-the-art, with only 42.56% accuracy. To demonstrate the generality of our strength system, we further apply SE and SE-MCTS to chess and obtain consistent results. These results show a promising approach to strength estimation and adjustment, enhancing human-AI interactions in games. Our code is available at https://rlg.iis.sinica.edu.tw/papers/strength-estimator."
Poster,StringLLM: Understanding the String Processing Capability of Large Language Models,https://iclr.cc//virtual/2025/poster/28579,"Xilong Wang, Hao Fu, Jindong Wang, Neil Gong","String processing, which mainly involves the analysis and manipulation of strings, is a fundamental component of modern computing. Despite the significant advancements of large language models (LLMs) in various natural language processing (NLP) tasks, their capability in string processing remains underexplored and underdeveloped. To bridge this gap, we present a comprehensive study of LLMs' string processing capability. In particular, we first propose StringLLM, a method to construct datasets for benchmarking string processing capability of LLMs. We use StringLLM to build a series of datasets, referred to as StringBench. It encompasses a wide range of string processing tasks, allowing us to systematically evaluate LLMs' performance in this area. Our evaluations indicate that LLMs struggle with accurately processing strings compared to humans. To uncover the underlying reasons for this limitation, we conduct an in-depth analysis and subsequently propose an effective approach that significantly enhances LLMs' string processing capability via fine-tuning. This work provides a foundation for future research to understand LLMs' string processing capability. Our code and data are available at https://github.com/wxl-lxw/StringLLM."
Poster,Strong Model Collapse,https://iclr.cc//virtual/2025/poster/32070,"Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, Julia Kempe","Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1 per 1000) can still lead to model collapse: larger and larger training sets do not enhance performance.  We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and neural networks for images."
Poster,Strong Preferences Affect the Robustness of Preference Models and Value Alignment,https://iclr.cc//virtual/2025/poster/29453,"Ziwei Xu, Mohan Kankanhalli","Value alignment, which aims to ensure that large language models (LLMs) and other AI agents behave in accordance with human values, is critical for ensuring safety and trustworthiness of these systems. A key component of value alignment is the modeling of human preferences as a representation of human values. In this paper, we investigate the robustness of value alignment by examining the sensitivity of preference models. Specifically, we ask: how do changes in the probabilities of some preferences affect the predictions of these models for other preferences? To answer this question, we theoretically analyze the robustness of widely used preference models by examining their sensitivities to minor changes in preferences they model. Our findings reveal that, in the Bradley-Terry and the Placket-Luce model, the probability of a preference can change significantly as other preferences change, especially when these preferences are dominant (i.e., with probabilities near zero or one). We identify specific conditions where this sensitivity becomes significant for these models and discuss the practical implications for the robustness and safety of value alignment in AI systems."
Poster,StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization,https://iclr.cc//virtual/2025/poster/30265,"Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Le Sun, Yongbin Li","Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation.In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications."
