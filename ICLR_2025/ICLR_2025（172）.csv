type,name,virtualsite_url,speakers/authors,abstract
Poster,Inverse decision-making using neural amortized Bayesian actors,https://iclr.cc//virtual/2025/poster/27641,"Dominik Straub, Tobias Fabian Niehues, Jan Peters, Constantin Rothkopf","Bayesian observer and actor models have provided normative explanations for many behavioral phenomena in perception, sensorimotor control, and other areas of cognitive science and neuroscience. They attribute behavioral variability and biases to interpretable entities such as perceptual and motor uncertainty, prior beliefs, and behavioral costs. However, when extending these models to more naturalistic tasks with continuous actions, solving the Bayesian decision-making problem is often analytically intractable. Inverse decision-making, i.e. performing inference over the parameters of such models given behavioral data, is computationally even more difficult. Therefore, researchers typically constrain their models to easily tractable components, such as Gaussian distributions or quadratic cost functions, or resort to numerical approximations. To overcome these limitations, we amortize the Bayesian actor using a neural network trained on a wide range of parameter settings in an unsupervised fashion. Using the pre-trained neural network enables performing efficient gradient-based Bayesian inference of the Bayesian actor model's parameters. We show on synthetic data that the inferred posterior distributions are in close alignment with those obtained using analytical solutions where they exist. Where no analytical solution is available, we recover posterior distributions close to the ground truth. We then show how our method allows for principled model comparison and how it can be used to disentangle factors that may lead to unidentifiabilities between priors and costs. Finally, we apply our method to empirical data from three sensorimotor tasks and compare model fits with different cost functions to show that it can explain individuals' behavioral patterns."
Poster,Inverse Rendering using Multi-Bounce Path Tracing and Reservoir Sampling,https://iclr.cc//virtual/2025/poster/30069,"Yuxin Dai, Qi Wang, Jingsen Zhu, xi db, Yuchi Huo, Chen Qian, Ying He","We introduce MIRReS, a novel two-stage inverse rendering framework thatjointly reconstructs and optimizes explicit geometry, materials, and lightingfrom multi-view images. Unlike previous methods that rely on implicit irradiance fields or oversimplified ray tracing, our method begins with an initialstage that extracts an explicit triangular mesh. In the second stage, we refine this representation using a physically-based inverse rendering modelwith multi-bounce path tracing and Monte Carlo integration. This enables our method to accurately estimate indirect illumination effects, including self-shadowing and internal reflections, leading to a more preciseintrinsic decomposition of shape, material, and lighting. To address thenoise issue in Monte Carlo integration, we incorporate reservoir sampling,improving convergence and enabling efficient gradient-based optimizationwith low sample counts. Through both qualitative and quantitative assessments across various scenarios, especially those with complex shadows,we demonstrate that our method achieves state-of-the-art decompositionperformance. Furthermore, our optimized explicit geometry seamlesslyintegrates with modern graphics engines supporting downstream applications such as scene editing, relighting, and material editing."
Poster,Inverse Scaling: When Bigger Isn't Better,https://iclr.cc//virtual/2025/poster/31511,"Joe Cavanagh, Andrew Gritsevskiy, Najoung Kim, Derik Kauffman, Zhengping Zhou, Daniel Wurgaft, Alicia Parrish, Max Weiss, Alexis Ross, Gabriel Recchia, Xudong Shen, Alisa Liu, Jiacheng Liu, Tom Tseng, Aaron T. Kirtland, Tomek Korbak, Aaron Mueller, Alexander Lyzhov, Sam Bowman, Sicong(Sheldon) Huang, Yuhui Zhang, Ethan Perez, Ian McKenzie, Ameya Prabhu, Michael Pieler, Euan McLean","Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling:
(i) preference to repeat memorized sequences over following in-context instructions,
(ii) imitation of undesirable patterns in the training data,
(iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and
(iv) correct but misleading few-shot demonstrations of the task.
We release the winning datasets at inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models."
Poster,InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization,https://iclr.cc//virtual/2025/poster/28405,"Yifan Niu, Ziqi Gao, Tingyang Xu, Yang Liu, Yatao Bian, Yu Rong, Junzhou Huang, Jia Li","Exploring chemical space to find novel molecules that simultaneously satisfy multiple properties is crucial in drug discovery. However, existing methods often struggle with trading off multiple properties due to the conflicting or correlated nature of chemical properties.  To tackle this issue, we introduce InversionGNN framework, an effective yet sample-efficient dual-path graph neural network (GNN) for multi-objective drug discovery.  In the direct prediction path of InversionGNN, we train the model for multi-property prediction to acquire knowledge of the optimal combination of functional groups.Then the learned chemical knowledge helps the inversion generation path to generate molecules with required properties. In order to decode the complex knowledge of multiple properties in the inversion path, we propose a gradient-based Pareto search method to balance conflicting properties and generate Pareto optimal molecules. Additionally, InversionGNN is able to search the full Pareto front approximately in discrete chemical space. Comprehensive experimental evaluations show that InversionGNN is both effective and sample-efficient in various discrete multi-objective settings including drug discovery."
Poster,InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma,https://iclr.cc//virtual/2025/poster/31135,"Xiaoxuan Hou, Jiayi Yuan, Joel Z Leibo, Natasha Jaques","**InvestESG** is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. The benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. We are releasing open-source versions of InvestESG in both PyTorch and JAX, which enable scalable and hardware-accelerated simulations for investigating competing incentives in mitigate climate change. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL's potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs."
Poster,Investigating Pattern Neurons in Urban Time Series Forecasting,https://iclr.cc//virtual/2025/poster/29185,"Chengxin Wang, Yiran Zhao, shaofeng cai, Gary Tan","Urban time series forecasting is crucial for smart city development and is key to sustainable urban management. Although urban time series models (UTSMs) are effective in general forecasting, they often overlook low-frequency events, such as holidays and extreme weather, leading to degraded performance in practical applications. In this paper, we first investigate how UTSMs handle these infrequent patterns from a neural perspective. Based on our findings, we propose $\textbf{P}$attern $\textbf{N}$euron guided $\textbf{Train}$ing ($\texttt{PN-Train}$), a novel training method that features (i) a $\textit{perturbation-based detector}$ to identify neurons responsible for low-frequency patterns in UTSMs, and (ii) a $\textit{fine-tuning mechanism}$ that enhances these neurons without compromising representation learning on high-frequency patterns. Empirical results demonstrate that $\texttt{PN-Train}$ considerably improves forecasting accuracy for low-frequency events while maintaining high performance for high-frequency events. The code is available at https://github.com/cwang-nus/PN-Train."
Poster,Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning,https://iclr.cc//virtual/2025/poster/28736,"Xiaolei Wang, Xinyu Tang, Junyi Li, Xin Zhao, Ji-Rong Wen","The emergence of in-context learning (ICL) is potentially attributed to two major abilities: task recognition (TR) for recognizing the task from demonstrations and utilizing pre-trained priors, and task learning (TL) for learning from demonstrations. However, relationships between the two abilities and how such relationships affect the emergence of ICL is unclear. In this paper, we take the first step by examining the pre-training dynamics of the emergence of ICL. With carefully designed metrics, we find that these two abilities are, in fact, competitive during pre-training. Moreover, we observe a negative correlation between the competition and the performance of ICL. Further analysis of common pre-training factors (i.e., model size, dataset size, and data curriculum) demonstrates possible ways to regulate the competition. Based on these insights, we propose a simple yet effective method to better integrate these two abilities for ICL at inference time. Through adaptive ensemble learning, the performance of ICL can be significantly boosted, enabling two small models to outperform a larger one with more than twice the parameters."
Poster,In vivo cell-type and brain region classification via multimodal contrastive learning,https://iclr.cc//virtual/2025/poster/31226,"Han Yu, Hanrui Lyu, YiXun Xu, Charlie Windolf, Eric Lee, Fan Yang, Andrew Shelton, Olivier Winter, International Brain Laboratory, Eva Dyer, Chandramouli Chandrasekaran, Nicholas Steinmetz, Liam Paninski, Cole Hurwitz","Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for two opto-tagged datasets and brain region classification for the public International Brain Laboratory Brain-wide Map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings."
Poster,IPDreamer: Appearance-Controllable 3D Object Generation with Complex Image Prompts,https://iclr.cc//virtual/2025/poster/31077,"Bohan Zeng, Shanglin Li, Yutang Feng, Ling Yang, Juan Zhang, Hong Li, Jiaming Liu, Conghui He, Wentao Zhang, Jianzhuang Liu, Baochang Zhang, Shuicheng YAN","Recent advances in 3D generation have been remarkable, with methods such as DreamFusion leveraging large-scale text-to-image diffusion-based models to guide 3D object generation. These methods enable the synthesis of detailed and photorealistic textured objects. However, the appearance of 3D objects produced by such text-to-3D models is often unpredictable, and it is hard for single-image-to-3D methods to deal with images lacking a clear subject, complicating the generation of appearance-controllable 3D objects from complex images. To address these challenges, we present IPDreamer, a novel method that captures intricate appearance features from complex **I**mage **P**rompts and aligns the synthesized 3D object with these extracted features, enabling high-fidelity, appearance-controllable 3D object generation. Our experiments demonstrate that IPDreamer consistently generates high-quality 3D objects that align with both the textual and complex image prompts, highlighting its promising capability in appearance-controlled, complex 3D object generation."
Poster,IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,https://iclr.cc//virtual/2025/poster/30698,"Ziyang Li, Saikat Dutta, Mayur Naik","Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or LLMs) have shown impressive code generation capabilities but they cannot do complex reasoning over code to detect such vulnerabilities especially since this task requires whole-repository analysis. We propose IRIS, a neuro-symbolic approach that systematically combines LLMs with static analysis to perform whole-repository reasoning for security vulnerability detection. Specifically, IRIS leverages LLMs to infer taint specifications and perform contextual analysis, alleviating needs for human specifications and inspection. For evaluation, we curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. A state-of-the-art static analysis tool CodeQL detects only 27 of these vulnerabilities whereas IRIS with GPT-4 detects 55 (+28) and improves upon CodeQL's average false discovery rate by 5% points.Furthermore, IRIS identifies 4 previously unknown vulnerabilities which cannot be found by existing tools. IRIS is available publicly at https://github.com/iris-sast/iris."
